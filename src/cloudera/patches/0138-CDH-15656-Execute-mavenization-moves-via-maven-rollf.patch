From de13cefc87cf909725632ea25008cc6fdcab7ef2 Mon Sep 17 00:00:00 2001
From: Brock Noland <brock@cloudera.com>
Date: Tue, 12 Nov 2013 11:17:13 -0600
Subject: [PATCH 138/375] CDH-15656 - Execute mavenization moves via maven-rollforward.sh

---
 .../org/apache/hive/beeline/BeeLine.properties     |  168 --
 .../apache/hive/beeline/sql-keywords.properties    |    1 -
 beeline/src/main/resources/BeeLine.properties      |  168 ++
 beeline/src/main/resources/sql-keywords.properties |    1 +
 .../hive/beeline/src/test/TestSchemaTool.java      |  383 ---
 common/src/java/conf/hive-log4j.properties         |   88 -
 common/src/main/resources/hive-log4j.properties    |   88 +
 .../apache/hadoop/hive/hbase/HBaseQTestUtil.java   |   38 -
 .../apache/hadoop/hive/hbase/HBaseTestSetup.java   |  161 --
 .../mapreduce/TestHCatHiveCompatibility.java       |  132 -
 .../mapreduce/TestHCatHiveThriftCompatibility.java |  119 -
 .../mapreduce/TestSequenceFileReadWrite.java       |  278 --
 .../mapreduce/TestHCatHiveCompatibility.java       |  129 -
 .../mapreduce/TestHCatHiveThriftCompatibility.java |  116 -
 .../mapreduce/TestSequenceFileReadWrite.java       |  275 --
 .../hcatalog/hbase/TestPigHBaseStorageHandler.java |  361 ---
 .../CustomNonSettableListObjectInspector1.java     |   67 +
 .../CustomNonSettableStructObjectInspector1.java   |  137 +
 .../CustomNonSettableUnionObjectInspector1.java    |   82 +
 .../apache/hadoop/hive/serde2/CustomSerDe1.java    |  113 +
 .../apache/hadoop/hive/serde2/CustomSerDe2.java    |  113 +
 .../apache/hadoop/hive/serde2/CustomSerDe3.java    |   77 +
 .../apache/hadoop/hive/serde2/CustomSerDe4.java    |   66 +
 .../apache/hadoop/hive/serde2/CustomSerDe5.java    |   63 +
 .../mapreduce/TestHCatHiveCompatibility.java       |  132 +
 .../mapreduce/TestHCatHiveThriftCompatibility.java |  119 +
 .../mapreduce/TestSequenceFileReadWrite.java       |  278 ++
 .../hcatalog/hbase/TestPigHBaseStorageHandler.java |  361 +++
 .../mapreduce/TestHCatHiveCompatibility.java       |  129 +
 .../mapreduce/TestHCatHiveThriftCompatibility.java |  116 +
 .../mapreduce/TestSequenceFileReadWrite.java       |  275 ++
 .../apache/hadoop/hive/jdbc/TestJdbcDriver.java    | 1143 +++++++++
 .../hive/metastore/TestEmbeddedHiveMetaStore.java  |   52 +
 .../hadoop/hive/metastore/TestHiveMetaStore.java   | 2679 ++++++++++++++++++++
 .../TestHiveMetaStoreWithEnvironmentContext.java   |  222 ++
 .../hadoop/hive/metastore/TestHiveMetaTool.java    |  247 ++
 .../hadoop/hive/metastore/TestMarkPartition.java   |  107 +
 .../hive/metastore/TestMarkPartitionRemote.java    |   55 +
 .../hive/metastore/TestMetaStoreAuthorization.java |  125 +
 .../metastore/TestMetaStoreConnectionUrlHook.java  |   62 +
 .../TestMetaStoreEndFunctionListener.java          |  142 ++
 .../hive/metastore/TestMetaStoreEventListener.java |  362 +++
 .../TestMetaStoreEventListenerOnlyOnCommit.java    |  104 +
 .../hive/metastore/TestMetaStoreInitListener.java  |   69 +
 .../metastore/TestMetaStoreListenersError.java     |   84 +
 .../hive/metastore/TestMetastoreVersion.java       |  188 ++
 .../TestPartitionNameWhitelistValidation.java      |  146 ++
 .../hadoop/hive/metastore/TestRawStoreTxn.java     |  101 +
 .../hive/metastore/TestRemoteHiveMetaStore.java    |   57 +
 .../TestRemoteHiveMetaStoreIpAddress.java          |   77 +
 .../TestRemoteUGIHiveMetaStoreIpAddress.java       |   28 +
 .../hive/metastore/TestRetryingHMSHandler.java     |  116 +
 .../metastore/TestSetUGIOnBothClientServer.java    |   31 +
 .../hive/metastore/TestSetUGIOnOnlyClient.java     |   28 +
 .../hive/metastore/TestSetUGIOnOnlyServer.java     |   28 +
 .../org/apache/hadoop/hive/ql/BaseTestQueries.java |   49 +
 .../apache/hadoop/hive/ql/TestLocationQueries.java |  116 +
 .../org/apache/hadoop/hive/ql/TestMTQueries.java   |   47 +
 .../hadoop/hive/ql/history/TestHiveHistory.java    |  232 ++
 .../metadata/TestSemanticAnalyzerHookLoading.java  |   57 +
 .../TestAuthorizationPreEventListener.java         |  303 +++
 .../TestClientSideAuthorizationProvider.java       |  214 ++
 .../TestMetastoreAuthorizationProvider.java        |  287 +++
 ...torageBasedClientSideAuthorizationProvider.java |  102 +
 ...StorageBasedMetastoreAuthorizationProvider.java |  105 +
 .../hive/serde2/TestSerdeWithFieldComments.java    |   72 +
 .../hive/serde2/dynamic_type/TestDynamicSerDe.java |  861 +++++++
 .../apache/hadoop/hive/service/TestHiveServer.java |  424 ++++
 .../hadoop/hive/thrift/TestDBTokenStore.java       |   94 +
 .../hive/thrift/TestHadoop20SAuthBridge.java       |  418 +++
 .../hive/thrift/TestZooKeeperTokenStore.java       |  181 ++
 .../org/apache/hive/beeline/TestSchemaTool.java    |  383 +++
 .../java/org/apache/hive/jdbc/TestJdbcDriver2.java | 1745 +++++++++++++
 .../service/auth/TestCustomAuthentication.java     |  122 +
 .../cli/TestEmbeddedThriftBinaryCLIService.java    |   59 +
 .../cli/thrift/TestThriftBinaryCLIService.java     |  105 +
 .../cli/thrift/TestThriftHttpCLIService.java       |  216 ++
 .../service/server/TestHS2ThreadAllocation.java    |  256 ++
 .../service/server/TestHiveServer2Concurrency.java |  103 +
 .../org/apache/hadoop/hive/serde2/TestSerDe.java   |  199 ++
 .../apache/hadoop/hive/hbase/HBaseQTestUtil.java   |   38 +
 .../apache/hadoop/hive/hbase/HBaseTestSetup.java   |  161 ++
 .../java/org/apache/hadoop/hive/ql/QTestUtil.java  | 1536 +++++++++++
 .../hive/ql/hooks/CheckColumnAccessHook.java       |   84 +
 .../hive/ql/hooks/CheckQueryPropertiesHook.java    |   53 +
 .../hadoop/hive/ql/hooks/CheckTableAccessHook.java |   85 +
 .../hadoop/hive/ql/hooks/MapJoinCounterHook.java   |   75 +
 .../hadoop/hive/ql/hooks/OptrStatGroupByHook.java  |  109 +
 .../ql/hooks/VerifyCachingPrintStreamHook.java     |   46 +
 .../ql/hooks/VerifyContentSummaryCacheHook.java    |   44 +
 .../hooks/VerifyHiveSortedInputFormatUsedHook.java |   46 +
 .../hive/ql/hooks/VerifyHooksRunInOrder.java       |  228 ++
 .../hive/ql/hooks/VerifyIsLocalModeHook.java       |   42 +
 .../hive/ql/hooks/VerifyNumReducersHook.java       |   50 +
 .../VerifyOutputTableLocationSchemeIsFileHook.java |   34 +
 .../hive/ql/hooks/VerifyOverriddenConfigsHook.java |   59 +
 ...erifyPartitionIsNotSubdirectoryOfTableHook.java |   49 +
 .../VerifyPartitionIsSubdirectoryOfTableHook.java  |   48 +
 .../hooks/VerifySessionStateLocalErrorsHook.java   |   47 +
 .../hooks/VerifySessionStateStackTracesHook.java   |   52 +
 .../ql/hooks/VerifyTableDirectoryIsEmptyHook.java  |   36 +
 .../hadoop/hive/ql/io/udf/Rot13InputFormat.java    |   71 +
 .../hadoop/hive/ql/io/udf/Rot13OutputFormat.java   |   75 +
 .../ql/metadata/DummySemanticAnalyzerHook.java     |  104 +
 .../ql/metadata/DummySemanticAnalyzerHook1.java    |   77 +
 .../hive/ql/security/DummyAuthenticator.java       |   63 +
 .../DummyHiveMetastoreAuthorizationProvider.java   |  204 ++
 .../ql/security/InjectableDummyAuthenticator.java  |  105 +
 .../hadoop/hive/ql/stats/DummyStatsAggregator.java |   61 +
 .../hadoop/hive/ql/stats/DummyStatsPublisher.java  |   69 +
 .../hive/ql/stats/KeyVerifyingStatsAggregator.java |   52 +
 .../org/apache/hadoop/hive/ql/udf/UDAFTestMax.java |  295 +++
 .../hadoop/hive/ql/udf/UDFTestErrorOnFalse.java    |   35 +
 .../apache/hadoop/hive/ql/udf/UDFTestLength.java   |   39 +
 .../apache/hadoop/hive/ql/udf/UDFTestLength2.java  |   35 +
 .../hive/ql/udf/generic/DummyContextUDF.java       |   54 +
 .../hive/ql/udf/generic/GenericUDAFSumList.java    |  160 ++
 .../hive/ql/udf/generic/GenericUDFEvaluateNPE.java |   81 +
 .../udf/generic/GenericUDFTestGetJavaBoolean.java  |   56 +
 .../udf/generic/GenericUDFTestGetJavaString.java   |   52 +
 .../ql/udf/generic/GenericUDFTestTranslate.java    |  122 +
 .../org/apache/hadoop/hive/scripts/extracturl.java |   58 +
 .../apache/hadoop/hive/jdbc/TestJdbcDriver.java    | 1143 ---------
 .../test/org/apache/hive/jdbc/TestJdbcDriver2.java | 1745 -------------
 maven-rollforward.sh                               |    2 +-
 .../hive/metastore/TestEmbeddedHiveMetaStore.java  |   52 -
 .../hadoop/hive/metastore/TestHiveMetaStore.java   | 2679 --------------------
 .../TestHiveMetaStoreWithEnvironmentContext.java   |  222 --
 .../hadoop/hive/metastore/TestHiveMetaTool.java    |  247 --
 .../hadoop/hive/metastore/TestMarkPartition.java   |  107 -
 .../hive/metastore/TestMarkPartitionRemote.java    |   55 -
 .../hive/metastore/TestMetaStoreAuthorization.java |  125 -
 .../metastore/TestMetaStoreConnectionUrlHook.java  |   62 -
 .../TestMetaStoreEndFunctionListener.java          |  142 --
 .../hive/metastore/TestMetaStoreEventListener.java |  362 ---
 .../TestMetaStoreEventListenerOnlyOnCommit.java    |  104 -
 .../hive/metastore/TestMetaStoreInitListener.java  |   69 -
 .../metastore/TestMetaStoreListenersError.java     |   84 -
 .../hive/metastore/TestMetastoreVersion.java       |  188 --
 .../TestPartitionNameWhitelistValidation.java      |  146 --
 .../hadoop/hive/metastore/TestRawStoreTxn.java     |  101 -
 .../hive/metastore/TestRemoteHiveMetaStore.java    |   57 -
 .../TestRemoteHiveMetaStoreIpAddress.java          |   77 -
 .../TestRemoteUGIHiveMetaStoreIpAddress.java       |   28 -
 .../hive/metastore/TestRetryingHMSHandler.java     |  116 -
 .../metastore/TestSetUGIOnBothClientServer.java    |   31 -
 .../hive/metastore/TestSetUGIOnOnlyClient.java     |   28 -
 .../hive/metastore/TestSetUGIOnOnlyServer.java     |   28 -
 .../hive/metastore/VerifyingObjectStore.java       |  178 ++
 ql/src/java/conf/hive-exec-log4j.properties        |   77 -
 .../hive/ql/hooks/EnforceReadOnlyTables.java       |   76 +
 .../hadoop/hive/ql/hooks/PostExecutePrinter.java   |  175 ++
 .../hadoop/hive/ql/hooks/PreExecutePrinter.java    |   76 +
 ql/src/main/resources/hive-exec-log4j.properties   |   77 +
 .../hive/metastore/VerifyingObjectStore.java       |  178 --
 .../org/apache/hadoop/hive/ql/BaseTestQueries.java |   49 -
 .../test/org/apache/hadoop/hive/ql/QTestUtil.java  | 1536 -----------
 .../apache/hadoop/hive/ql/TestLocationQueries.java |  116 -
 .../org/apache/hadoop/hive/ql/TestMTQueries.java   |   47 -
 .../hadoop/hive/ql/history/TestHiveHistory.java    |  232 --
 .../hive/ql/hooks/CheckColumnAccessHook.java       |   84 -
 .../hive/ql/hooks/CheckQueryPropertiesHook.java    |   53 -
 .../hadoop/hive/ql/hooks/CheckTableAccessHook.java |   85 -
 .../hive/ql/hooks/EnforceReadOnlyTables.java       |   76 -
 .../hadoop/hive/ql/hooks/MapJoinCounterHook.java   |   75 -
 .../hadoop/hive/ql/hooks/OptrStatGroupByHook.java  |  109 -
 .../hadoop/hive/ql/hooks/PostExecutePrinter.java   |  175 --
 .../hadoop/hive/ql/hooks/PreExecutePrinter.java    |   76 -
 .../ql/hooks/VerifyCachingPrintStreamHook.java     |   46 -
 .../ql/hooks/VerifyContentSummaryCacheHook.java    |   44 -
 .../hooks/VerifyHiveSortedInputFormatUsedHook.java |   46 -
 .../hive/ql/hooks/VerifyHooksRunInOrder.java       |  228 --
 .../hive/ql/hooks/VerifyIsLocalModeHook.java       |   42 -
 .../hive/ql/hooks/VerifyNumReducersHook.java       |   50 -
 .../VerifyOutputTableLocationSchemeIsFileHook.java |   34 -
 .../hive/ql/hooks/VerifyOverriddenConfigsHook.java |   59 -
 ...erifyPartitionIsNotSubdirectoryOfTableHook.java |   49 -
 .../VerifyPartitionIsSubdirectoryOfTableHook.java  |   48 -
 .../hooks/VerifySessionStateLocalErrorsHook.java   |   47 -
 .../hooks/VerifySessionStateStackTracesHook.java   |   52 -
 .../ql/hooks/VerifyTableDirectoryIsEmptyHook.java  |   36 -
 .../hadoop/hive/ql/io/udf/Rot13InputFormat.java    |   71 -
 .../hadoop/hive/ql/io/udf/Rot13OutputFormat.java   |   75 -
 .../ql/metadata/DummySemanticAnalyzerHook.java     |  104 -
 .../ql/metadata/DummySemanticAnalyzerHook1.java    |   77 -
 .../metadata/TestSemanticAnalyzerHookLoading.java  |   57 -
 .../hive/ql/security/DummyAuthenticator.java       |   63 -
 .../DummyHiveMetastoreAuthorizationProvider.java   |  204 --
 .../ql/security/InjectableDummyAuthenticator.java  |  105 -
 .../TestAuthorizationPreEventListener.java         |  303 ---
 .../TestClientSideAuthorizationProvider.java       |  214 --
 .../TestMetastoreAuthorizationProvider.java        |  287 ---
 ...torageBasedClientSideAuthorizationProvider.java |  102 -
 ...StorageBasedMetastoreAuthorizationProvider.java |  105 -
 .../hadoop/hive/ql/stats/DummyStatsAggregator.java |   61 -
 .../hadoop/hive/ql/stats/DummyStatsPublisher.java  |   69 -
 .../hive/ql/stats/KeyVerifyingStatsAggregator.java |   52 -
 .../org/apache/hadoop/hive/ql/udf/UDAFTestMax.java |  295 ---
 .../hadoop/hive/ql/udf/UDFTestErrorOnFalse.java    |   35 -
 .../apache/hadoop/hive/ql/udf/UDFTestLength.java   |   39 -
 .../apache/hadoop/hive/ql/udf/UDFTestLength2.java  |   35 -
 .../hive/ql/udf/generic/DummyContextUDF.java       |   54 -
 .../hive/ql/udf/generic/GenericUDAFSumList.java    |  160 --
 .../hive/ql/udf/generic/GenericUDFEvaluateNPE.java |   81 -
 .../udf/generic/GenericUDFTestGetJavaBoolean.java  |   56 -
 .../udf/generic/GenericUDFTestGetJavaString.java   |   52 -
 .../ql/udf/generic/GenericUDFTestTranslate.java    |  122 -
 .../org/apache/hadoop/hive/scripts/extracturl.java |   58 -
 .../CustomNonSettableListObjectInspector1.java     |   67 -
 .../CustomNonSettableStructObjectInspector1.java   |  137 -
 .../CustomNonSettableUnionObjectInspector1.java    |   82 -
 .../apache/hadoop/hive/serde2/CustomSerDe1.java    |  113 -
 .../apache/hadoop/hive/serde2/CustomSerDe2.java    |  113 -
 .../apache/hadoop/hive/serde2/CustomSerDe3.java    |   77 -
 .../apache/hadoop/hive/serde2/CustomSerDe4.java    |   66 -
 .../apache/hadoop/hive/serde2/CustomSerDe5.java    |   63 -
 .../org/apache/hadoop/hive/serde2/TestSerDe.java   |  199 --
 .../hive/serde2/TestSerdeWithFieldComments.java    |   72 -
 .../hive/serde2/dynamic_type/TestDynamicSerDe.java |  861 -------
 .../apache/hadoop/hive/service/TestHiveServer.java |  424 ----
 .../service/auth/TestCustomAuthentication.java     |  122 -
 .../cli/TestEmbeddedThriftBinaryCLIService.java    |   59 -
 .../cli/thrift/TestThriftBinaryCLIService.java     |  105 -
 .../cli/thrift/TestThriftHttpCLIService.java       |  216 --
 .../service/server/TestHS2ThreadAllocation.java    |  256 --
 .../service/server/TestHiveServer2Concurrency.java |  103 -
 .../apache/hadoop/hive/shims/Hadoop20Shims.java    |  762 ++++++
 .../org/apache/hadoop/hive/shims/Jetty20Shims.java |   56 +
 .../apache/hadoop/hive/shims/Hadoop20SShims.java   |  350 +++
 .../apache/hadoop/hive/shims/Jetty20SShims.java    |   53 +
 .../org/apache/hadoop/mapred/WebHCatJTShim20S.java |   99 +
 .../apache/hadoop/hive/shims/Hadoop23Shims.java    |  390 +++
 .../org/apache/hadoop/hive/shims/Jetty23Shims.java |   56 +
 .../org/apache/hadoop/mapred/WebHCatJTShim23.java  |   91 +
 .../hadoop/hive/shims/HadoopShimsSecure.java       |  628 +++++
 .../apache/hadoop/hive/thrift/DBTokenStore.java    |  153 ++
 .../hive/thrift/DelegationTokenIdentifier.java     |   52 +
 .../hive/thrift/DelegationTokenSecretManager.java  |   87 +
 .../hive/thrift/DelegationTokenSelector.java       |   33 +
 .../hadoop/hive/thrift/DelegationTokenStore.java   |  113 +
 .../hive/thrift/HadoopThriftAuthBridge20S.java     |  645 +++++
 .../hadoop/hive/thrift/MemoryTokenStore.java       |  115 +
 .../TokenStoreDelegationTokenSecretManager.java    |  334 +++
 .../hadoop/hive/thrift/ZooKeeperTokenStore.java    |  468 ++++
 .../hive/thrift/client/TUGIAssumingTransport.java  |   74 +
 .../delegation/HiveDelegationTokenSupport.java     |   68 +
 .../java/org/apache/hadoop/fs/ProxyFileSystem.java |  291 +++
 .../org/apache/hadoop/fs/ProxyLocalFileSystem.java |   72 +
 .../hadoop/hive/io/HiveIOExceptionHandler.java     |   52 +
 .../hive/io/HiveIOExceptionHandlerChain.java       |  124 +
 .../hadoop/hive/io/HiveIOExceptionHandlerUtil.java |   82 +
 .../hive/io/HiveIOExceptionNextHandleResult.java   |   55 +
 .../apache/hadoop/hive/shims/CombineHiveKey.java   |   54 +
 .../org/apache/hadoop/hive/shims/HadoopShims.java  |  555 ++++
 .../apache/hadoop/hive/shims/HiveEventCounter.java |  102 +
 .../hadoop/hive/shims/HiveHarFileSystem.java       |   66 +
 .../org/apache/hadoop/hive/shims/JettyShims.java   |   44 +
 .../org/apache/hadoop/hive/shims/ShimLoader.java   |  174 ++
 .../hadoop/hive/thrift/HadoopThriftAuthBridge.java |  101 +
 .../hadoop/hive/thrift/TFilterTransport.java       |   99 +
 .../hive/thrift/TUGIContainingTransport.java       |   96 +
 .../apache/hadoop/hive/shims/Hadoop20Shims.java    |  762 ------
 .../org/apache/hadoop/hive/shims/Jetty20Shims.java |   56 -
 .../apache/hadoop/hive/shims/Hadoop20SShims.java   |  350 ---
 .../apache/hadoop/hive/shims/Jetty20SShims.java    |   53 -
 .../org/apache/hadoop/mapred/WebHCatJTShim20S.java |   99 -
 .../apache/hadoop/hive/shims/Hadoop23Shims.java    |  390 ---
 .../org/apache/hadoop/hive/shims/Jetty23Shims.java |   56 -
 .../org/apache/hadoop/mapred/WebHCatJTShim23.java  |   91 -
 .../hadoop/hive/shims/HadoopShimsSecure.java       |  628 -----
 .../apache/hadoop/hive/thrift/DBTokenStore.java    |  153 --
 .../hive/thrift/DelegationTokenIdentifier.java     |   52 -
 .../hive/thrift/DelegationTokenSecretManager.java  |   87 -
 .../hive/thrift/DelegationTokenSelector.java       |   33 -
 .../hadoop/hive/thrift/DelegationTokenStore.java   |  113 -
 .../hive/thrift/HadoopThriftAuthBridge20S.java     |  645 -----
 .../hadoop/hive/thrift/MemoryTokenStore.java       |  115 -
 .../TokenStoreDelegationTokenSecretManager.java    |  334 ---
 .../hadoop/hive/thrift/ZooKeeperTokenStore.java    |  468 ----
 .../hive/thrift/client/TUGIAssumingTransport.java  |   74 -
 .../delegation/HiveDelegationTokenSupport.java     |   68 -
 .../hadoop/hive/thrift/TestDBTokenStore.java       |   94 -
 .../hive/thrift/TestHadoop20SAuthBridge.java       |  418 ---
 .../hive/thrift/TestZooKeeperTokenStore.java       |  181 --
 .../java/org/apache/hadoop/fs/ProxyFileSystem.java |  291 ---
 .../org/apache/hadoop/fs/ProxyLocalFileSystem.java |   72 -
 .../hadoop/hive/io/HiveIOExceptionHandler.java     |   52 -
 .../hive/io/HiveIOExceptionHandlerChain.java       |  124 -
 .../hadoop/hive/io/HiveIOExceptionHandlerUtil.java |   82 -
 .../hive/io/HiveIOExceptionNextHandleResult.java   |   55 -
 .../apache/hadoop/hive/shims/CombineHiveKey.java   |   54 -
 .../org/apache/hadoop/hive/shims/HadoopShims.java  |  555 ----
 .../apache/hadoop/hive/shims/HiveEventCounter.java |  102 -
 .../hadoop/hive/shims/HiveHarFileSystem.java       |   66 -
 .../org/apache/hadoop/hive/shims/JettyShims.java   |   44 -
 .../org/apache/hadoop/hive/shims/ShimLoader.java   |  174 --
 .../hadoop/hive/thrift/HadoopThriftAuthBridge.java |  101 -
 .../hadoop/hive/thrift/TFilterTransport.java       |   99 -
 .../hive/thrift/TUGIContainingTransport.java       |   96 -
 299 files changed, 27355 insertions(+), 27355 deletions(-)
 delete mode 100644 beeline/src/java/org/apache/hive/beeline/BeeLine.properties
 delete mode 100644 beeline/src/java/org/apache/hive/beeline/sql-keywords.properties
 create mode 100644 beeline/src/main/resources/BeeLine.properties
 create mode 100644 beeline/src/main/resources/sql-keywords.properties
 delete mode 100644 beeline/src/test/org/apache/hive/beeline/src/test/TestSchemaTool.java
 delete mode 100644 common/src/java/conf/hive-log4j.properties
 create mode 100644 common/src/main/resources/hive-log4j.properties
 delete mode 100644 hbase-handler/src/test/org/apache/hadoop/hive/hbase/HBaseQTestUtil.java
 delete mode 100644 hbase-handler/src/test/org/apache/hadoop/hive/hbase/HBaseTestSetup.java
 delete mode 100644 hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveCompatibility.java
 delete mode 100644 hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
 delete mode 100644 hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java
 delete mode 100644 hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveCompatibility.java
 delete mode 100644 hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
 delete mode 100644 hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
 delete mode 100644 hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java
 create mode 100644 itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableListObjectInspector1.java
 create mode 100644 itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableStructObjectInspector1.java
 create mode 100644 itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableUnionObjectInspector1.java
 create mode 100644 itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe1.java
 create mode 100644 itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe2.java
 create mode 100644 itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe3.java
 create mode 100644 itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe4.java
 create mode 100644 itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe5.java
 create mode 100644 itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveCompatibility.java
 create mode 100644 itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
 create mode 100644 itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java
 create mode 100644 itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java
 create mode 100644 itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveCompatibility.java
 create mode 100644 itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
 create mode 100644 itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestEmbeddedHiveMetaStore.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreWithEnvironmentContext.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaTool.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMarkPartition.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreAuthorization.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreConnectionUrlHook.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEndFunctionListener.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListenerOnlyOnCommit.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreInitListener.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreListenersError.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestPartitionNameWhitelistValidation.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRawStoreTxn.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStore.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreIpAddress.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteUGIHiveMetaStoreIpAddress.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRetryingHMSHandler.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnBothClientServer.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyClient.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyServer.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/BaseTestQueries.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestMTQueries.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestAuthorizationPreEventListener.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestClientSideAuthorizationProvider.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMetastoreAuthorizationProvider.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedClientSideAuthorizationProvider.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationProvider.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/serde2/TestSerdeWithFieldComments.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/serde2/dynamic_type/TestDynamicSerDe.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/service/TestHiveServer.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestDBTokenStore.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hive/beeline/TestSchemaTool.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hive/service/auth/TestCustomAuthentication.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hive/service/cli/thrift/TestThriftBinaryCLIService.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hive/service/cli/thrift/TestThriftHttpCLIService.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hive/service/server/TestHS2ThreadAllocation.java
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hive/service/server/TestHiveServer2Concurrency.java
 create mode 100644 itests/test-serde/src/main/java/org/apache/hadoop/hive/serde2/TestSerDe.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseQTestUtil.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseTestSetup.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckQueryPropertiesHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/MapJoinCounterHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/OptrStatGroupByHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyCachingPrintStreamHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyContentSummaryCacheHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyHiveSortedInputFormatUsedHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyHooksRunInOrder.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyIsLocalModeHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyNumReducersHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifySessionStateLocalErrorsHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifySessionStateStackTracesHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyTableDirectoryIsEmptyHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/io/udf/Rot13InputFormat.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/io/udf/Rot13OutputFormat.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook1.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/security/DummyAuthenticator.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/security/DummyHiveMetastoreAuthorizationProvider.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDAFTestMax.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestErrorOnFalse.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestLength.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestLength2.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/DummyContextUDF.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSumList.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFEvaluateNPE.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java
 create mode 100644 itests/util/src/main/java/org/apache/hadoop/hive/scripts/extracturl.java
 delete mode 100644 jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
 delete mode 100644 jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestEmbeddedHiveMetaStore.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStoreWithEnvironmentContext.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaTool.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartition.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreAuthorization.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreConnectionUrlHook.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEndFunctionListener.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEventListenerOnlyOnCommit.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreInitListener.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreListenersError.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestPartitionNameWhitelistValidation.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestRawStoreTxn.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStore.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreIpAddress.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteUGIHiveMetaStoreIpAddress.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestRetryingHMSHandler.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnBothClientServer.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyClient.java
 delete mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyServer.java
 create mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/VerifyingObjectStore.java
 delete mode 100644 ql/src/java/conf/hive-exec-log4j.properties
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/hooks/EnforceReadOnlyTables.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java
 create mode 100644 ql/src/main/resources/hive-exec-log4j.properties
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/metastore/VerifyingObjectStore.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/BaseTestQueries.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/TestLocationQueries.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/TestMTQueries.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckQueryPropertiesHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/EnforceReadOnlyTables.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/MapJoinCounterHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/OptrStatGroupByHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyCachingPrintStreamHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyContentSummaryCacheHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyHiveSortedInputFormatUsedHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyHooksRunInOrder.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyIsLocalModeHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyNumReducersHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifySessionStateLocalErrorsHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifySessionStateStackTracesHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyTableDirectoryIsEmptyHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/udf/Rot13InputFormat.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/udf/Rot13OutputFormat.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook1.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/security/DummyAuthenticator.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/security/DummyHiveMetastoreAuthorizationProvider.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/security/TestAuthorizationPreEventListener.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/security/TestClientSideAuthorizationProvider.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/security/TestMetastoreAuthorizationProvider.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/security/TestStorageBasedClientSideAuthorizationProvider.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationProvider.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/udf/UDAFTestMax.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestErrorOnFalse.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestLength.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestLength2.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/udf/generic/DummyContextUDF.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSumList.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFEvaluateNPE.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/scripts/extracturl.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableListObjectInspector1.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableStructObjectInspector1.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableUnionObjectInspector1.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe1.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe2.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe3.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe4.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe5.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/serde2/TestSerDe.java
 delete mode 100644 serde/src/test/org/apache/hadoop/hive/serde2/TestSerdeWithFieldComments.java
 delete mode 100644 serde/src/test/org/apache/hadoop/hive/serde2/dynamic_type/TestDynamicSerDe.java
 delete mode 100644 service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java
 delete mode 100644 service/src/test/org/apache/hive/service/auth/TestCustomAuthentication.java
 delete mode 100644 service/src/test/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java
 delete mode 100644 service/src/test/org/apache/hive/service/cli/thrift/TestThriftBinaryCLIService.java
 delete mode 100644 service/src/test/org/apache/hive/service/cli/thrift/TestThriftHttpCLIService.java
 delete mode 100644 service/src/test/org/apache/hive/service/server/TestHS2ThreadAllocation.java
 delete mode 100644 service/src/test/org/apache/hive/service/server/TestHiveServer2Concurrency.java
 create mode 100644 shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
 create mode 100644 shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Jetty20Shims.java
 create mode 100644 shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
 create mode 100644 shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Jetty20SShims.java
 create mode 100644 shims/0.20S/src/main/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java
 create mode 100644 shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
 create mode 100644 shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Jetty23Shims.java
 create mode 100644 shims/0.23/src/main/java/org/apache/hadoop/mapred/WebHCatJTShim23.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DBTokenStore.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenIdentifier.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenSecretManager.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenSelector.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenStore.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/MemoryTokenStore.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/client/TUGIAssumingTransport.java
 create mode 100644 shims/common-secure/src/main/java/org/apache/hadoop/security/token/delegation/HiveDelegationTokenSupport.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/fs/ProxyFileSystem.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandler.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionNextHandleResult.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/shims/CombineHiveKey.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/shims/HiveEventCounter.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/shims/HiveHarFileSystem.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/shims/JettyShims.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/thrift/TFilterTransport.java
 create mode 100644 shims/common/src/main/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java
 delete mode 100644 shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
 delete mode 100644 shims/src/0.20/java/org/apache/hadoop/hive/shims/Jetty20Shims.java
 delete mode 100644 shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
 delete mode 100644 shims/src/0.20S/java/org/apache/hadoop/hive/shims/Jetty20SShims.java
 delete mode 100644 shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java
 delete mode 100644 shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
 delete mode 100644 shims/src/0.23/java/org/apache/hadoop/hive/shims/Jetty23Shims.java
 delete mode 100644 shims/src/0.23/java/org/apache/hadoop/mapred/WebHCatJTShim23.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DBTokenStore.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenIdentifier.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenSecretManager.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenSelector.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenStore.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/hive/thrift/MemoryTokenStore.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/hive/thrift/client/TUGIAssumingTransport.java
 delete mode 100644 shims/src/common-secure/java/org/apache/hadoop/security/token/delegation/HiveDelegationTokenSupport.java
 delete mode 100644 shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestDBTokenStore.java
 delete mode 100644 shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java
 delete mode 100644 shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/fs/ProxyFileSystem.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandler.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionNextHandleResult.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/shims/CombineHiveKey.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/shims/HiveEventCounter.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/shims/HiveHarFileSystem.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/shims/JettyShims.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/shims/ShimLoader.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/thrift/TFilterTransport.java
 delete mode 100644 shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java

diff --git a/src/beeline/src/java/org/apache/hive/beeline/BeeLine.properties b/src/beeline/src/java/org/apache/hive/beeline/BeeLine.properties
deleted file mode 100644
index 9947007..0000000
--- a/src/beeline/src/java/org/apache/hive/beeline/BeeLine.properties
+++ /dev/null
@@ -1,168 +0,0 @@
-app-introduction: {0} version {1} by {2}
-
-jline-version: The version of the required {0} library is too old. Version \
-				"{1}" was found, but "{2}" is required.
-
-enter-for-more: [ Hit "enter" for more ("q" to exit) ]
-no-manual: Could not find manual resource.
-executing-command:	Executing command: {0}
-unknown-command: Unknown command: {0}
-autocommit-needs-off: Operation requires that autocommit be turned off.
-no-current-connection: No current connection
-connection-is-closed: Connection is closed
-reconnecting: Reconnecting to "{0}"...
-connecting: Connecting to "{0}"...
-no-driver: No known driver to handle "{0}"
-setting-prop: Setting property: {0}
-saving-options: Saving preferences to: {0}
-loaded-options: Loaded preferences from: {0}
-
-jdbc-level: JDBC level
-compliant: Compliant
-jdbc-version: Version
-driver-class: Driver Class
-
-help-quit: Exits the program
-help-dropall: Drop all tables in the current database
-help-connect: Open a new connection to the database.
-help-manual: Display the BeeLine manual
-help-typeinfo: Display the type map for the current connection
-help-describe: Describe a table
-help-reconnect: Reconnect to the database
-help-metadata: Obtain metadata information
-help-dbinfo: Give metadata information about the database
-help-rehash: Fetch table and column names for command completion
-help-verbose: Set verbose mode on
-help-run: Run a script from the specified file
-help-list: List the current connections
-help-all: Execute the specified SQL against all the current connections
-help-go: Select the current connection
-help-script: Start saving a script to a file
-help-brief: Set verbose mode off
-help-close: Close the current connection to the database
-help-closeall: Close all current open connections
-help-isolation: Set the transaction isolation for this connection
-help-nativesql: Show the native SQL for the specified statement
-help-call: Execute a callable statement
-help-autocommit: Set autocommit mode on or off
-help-commit: Commit the current transaction (if autocommit is off)
-help-rollback: Roll back the current transaction (if autocommit is off)
-help-batch: Start or execute a batch of statements
-help-help: Print a summary of command usage
-help-set: Set a beeline variable
-help-save: Save the current variabes and aliases
-help-native: Show the database''s native SQL for a command
-help-alias: Create a new command alias
-help-unalias: Unset a command alias
-help-scan: Scan for installed JDBC drivers
-help-sql: Execute a SQL command
-help-history: Display the command history
-help-record: Record all output to the specified file
-help-indexes: List all the indexes for the specified table
-help-primarykeys: List all the primary keys for the specified table
-help-exportedkeys: List all the exported keys for the specified table
-help-importedkeys: List all the imported keys for the specified table
-help-procedures: List all the procedures
-help-tables: List all the tables in the database
-help-columns: List all the columns for the specified table
-help-properties: Connect to the database specified in the properties file(s)
-help-outputformat: Set the output format for displaying results (table,vertical,csv,tsv,xmlattrs,xmlelements)
-
-jline-missing: The JLine jar was not found. Please ensure it is installed.
-
-batch-start: Batching SQL statements. Run "batch" again to execute the batch.
-running-batch: Running batched SQL statements...
-
-arg-usage: Usage: {0} <{1}>
-
-scanning: Scanning {0}...
-no-such-method: No such method "{0}"
-possible-methods: Possible methods:
-
-closing: Closing: {0}
-already-closed: Connection is already closed.
-error-setting: Error setting configuration: {0}: {1}
-no-method: No method matching "{0}" was found in {1}.
-
-
-connected: Connected to: {0} (version {1})
-driver: Driver: {0} (version {1})
-autocommit-status: Autocommit status: {0}
-isolation-status: Transaction isolation: {0}
-unknown-format: Unknown output format "{0}". Possible values: {1}
-
-closed: closed
-open: open
-
-executing-con: Executing SQL against: {0}
-comments: Comments, bug reports, and patches go to {0}
-building-tables: Building list of tables and columns for tab-completion \
-	(set fastconnect to true to skip)...
-done: Done
-state: state
-code: code
-
-invalid-connections: Invalid connection: {0}
-
-script-closed: Script closed. Enter "run {0}" to replay it.
-script-already-running: Script ({0}) is already running. Enter "script" with no arguments to stop it.
-script-started: Saving command script to "{0}". Enter "script" with no arguments to stop it.
-
-
-record-closed: Recording stopped.
-record-already-running: Output already being saved to ({0}). Enter "record" with no arguments to stop it.
-record-started: Saving all output to "{0}". Enter "record" with no arguments to stop it.
-
-autoloading-known-drivers: No known driver to handle "{0}". Searching for known drivers...
-
-Warning: Warning: {0} (state={1},code={2,number,#})
-Error: Error: {0} (state={1},code={2,number,#})
-
-commit-complete: Commit complete
-rollback-complete: Rollback complete
-
-abort-on-error: Aborting command set because "force" is false and \
-				 command failed: "{0}"
-
-multiple-matches: Ambiguous command: {0}
-
-really-drop-all: Really drop every table in the database? (y/n)\
-abort-drop-all: Aborting drop all tables.
-
-drivers-found-count: 0#No driver classes found|1#{0} driver class found|1<{0} driver classes found
-rows-selected: 0#No rows selected|1#{0} row selected|1<{0} rows selected
-rows-affected: 0#No rows affected|1#{0} row affected|1<{0} rows affected|0>Unknown rows affected
-active-connections: 0#No active connections|1#{0} active connection:|1<{0} active connections:
-
-time-ms: ({0,number,#.###} seconds)
-
-cmd-usage: Usage: java org.apache.hive.cli.beeline.BeeLine \n \
-\  -u <database url>               the JDBC URL to connect to\n \
-\  -n <username>                   the username to connect as\n \
-\  -p <password>                   the password to connect as\n \
-\  -d <driver class>               the driver class to use\n \
-\  -e <query>                      query that should be executed\n \
-\  -f <file>                       script file that should be executed\n \
-\  --hivevar name=value            hive variable name and value\n \
-\                                  This is Hive specific settings in which variables\n \
-\                                  can be set at session level and referenced in Hive\n \
-\                                  commands or queries.\n \
-\  --color=[true/false]            control whether color is used for display\n \
-\  --showHeader=[true/false]       show column names in query results\n \
-\  --headerInterval=ROWS;          the interval between which heades are displayed\n \
-\  --fastConnect=[true/false]      skip building table/column list for tab-completion\n \
-\  --autoCommit=[true/false]       enable/disable automatic transaction commit\n \
-\  --verbose=[true/false]          show verbose error messages and debug info\n \
-\  --showWarnings=[true/false]     display connection warnings\n \
-\  --showNestedErrs=[true/false]   display nested errors\n \
-\  --numberFormat=[pattern]        format numbers using DecimalFormat pattern\n \
-\  --force=[true/false]            continue running script even after errors\n \
-\  --maxWidth=MAXWIDTH             the maximum width of the terminal\n \
-\  --maxColumnWidth=MAXCOLWIDTH    the maximum width to use when displaying columns\n \
-\  --silent=[true/false]           be more silent\n \
-\  --autosave=[true/false]         automatically save preferences\n \
-\  --outputformat=[table/vertical/csv/tsv]   format mode for result display\n \
-\  --isolation=LEVEL               set the transaction isolation level\n \
-\  --help                          display this message
-
-
diff --git a/src/beeline/src/java/org/apache/hive/beeline/sql-keywords.properties b/src/beeline/src/java/org/apache/hive/beeline/sql-keywords.properties
deleted file mode 100644
index 0f1eb65..0000000
--- a/src/beeline/src/java/org/apache/hive/beeline/sql-keywords.properties
+++ /dev/null
@@ -1 +0,0 @@
-ABSOLUTE,ACTION,ADD,ALL,ALLOCATE,ALTER,AND,ANY,ARE,AS,ASC,ASSERTION,AT,AUTHORIZATION,AVG,BEGIN,BETWEEN,BIT,BIT_LENGTH,BOTH,BY,CASCADE,CASCADED,CASE,CAST,CATALOG,CHAR,CHARACTER,CHAR_LENGTH,CHARACTER_LENGTH,CHECK,CLOSE,CLUSTER,COALESCE,COLLATE,COLLATION,COLUMN,COMMIT,CONNECT,CONNECTION,CONSTRAINT,CONSTRAINTS,CONTINUE,CONVERT,CORRESPONDING,COUNT,CREATE,CROSS,CURRENT,CURRENT_DATE,CURRENT_TIME,CURRENT_TIMESTAMP,CURRENT_USER,CURSOR,DATE,DAY,DEALLOCATE,DEC,DECIMAL,DECLARE,DEFAULT,DEFERRABLE,DEFERRED,DELETE,DESC,DESCRIBE,DESCRIPTOR,DIAGNOSTICS,DISCONNECT,DISTINCT,DOMAIN,DOUBLE,DROP,ELSE,END,END-EXEC,ESCAPE,EXCEPT,EXCEPTION,EXEC,EXECUTE,EXISTS,EXTERNAL,EXTRACT,FALSE,FETCH,FIRST,FLOAT,FOR,FOREIGN,FOUND,FROM,FULL,GET,GLOBAL,GO,GOTO,GRANT,GROUP,HAVING,HOUR,IDENTITY,IMMEDIATE,IN,INDICATOR,INITIALLY,INNER,INPUT,INSENSITIVE,INSERT,INT,INTEGER,INTERSECT,INTERVAL,INTO,IS,ISOLATION,JOIN,KEY,LANGUAGE,LAST,LEADING,LEFT,LEVEL,LIKE,LOCAL,LOWER,MATCH,MAX,MIN,MINUTE,MODULE,MONTH,NAMES,NATIONAL,NATURAL,NCHAR,NEXT,NO,NOT,NULL,NULLIF,NUMERIC,OCTET_LENGTH,OF,ON,ONLY,OPEN,OPTION,OR,ORDER,OUTER,OUTPUT,OVERLAPS,OVERWRITE,PAD,PARTIAL,PARTITION,POSITION,PRECISION,PREPARE,PRESERVE,PRIMARY,PRIOR,PRIVILEGES,PROCEDURE,PUBLIC,READ,REAL,REFERENCES,RELATIVE,RESTRICT,REVOKE,RIGHT,ROLLBACK,ROWS,SCHEMA,SCROLL,SECOND,SECTION,SELECT,SESSION,SESSION_USER,SET,SIZE,SMALLINT,SOME,SPACE,SQL,SQLCODE,SQLERROR,SQLSTATE,SUBSTRING,SUM,SYSTEM_USER,TABLE,TEMPORARY,THEN,TIME,TIMESTAMP,TIMEZONE_HOUR,TIMEZONE_MINUTE,TO,TRAILING,TRANSACTION,TRANSLATE,TRANSLATION,TRIM,TRUE,UNION,UNIQUE,UNKNOWN,UPDATE,UPPER,USAGE,USER,USING,VALUE,VALUES,VARCHAR,VARYING,VIEW,WHEN,WHENEVER,WHERE,WITH,WORK,WRITE,YEAR,ZONE,ADA,C,CATALOG_NAME,CHARACTER_SET_CATALOG,CHARACTER_SET_NAME,CHARACTER_SET_SCHEMA,CLASS_ORIGIN,COBOL,COLLATION_CATALOG,COLLATION_NAME,COLLATION_SCHEMA,COLUMN_NAME,COMMAND_FUNCTION,COMMITTED,CONDITION_NUMBER,CONNECTION_NAME,CONSTRAINT_CATALOG,CONSTRAINT_NAME,CONSTRAINT_SCHEMA,CURSOR_NAME,DATA,DATETIME_INTERVAL_CODE,DATETIME_INTERVAL_PRECISION,DYNAMIC_FUNCTION,FORTRAN,LENGTH,MESSAGE_LENGTH,MESSAGE_OCTET_LENGTH,MESSAGE_TEXT,MORE,MUMPS,NAME,NULLABLE,NUMBER,PASCAL,PLI,REPEATABLE,RETURNED_LENGTH,RETURNED_OCTET_LENGTH,RETURNED_SQLSTATE,ROW_COUNT,SCALE,SCHEMA_NAME,SERIALIZABLE,SERVER_NAME,SUBCLASS_ORIGIN,TABLE_NAME,TYPE,UNCOMMITTED,UNNAMED
diff --git a/src/beeline/src/main/resources/BeeLine.properties b/src/beeline/src/main/resources/BeeLine.properties
new file mode 100644
index 0000000..9947007
--- /dev/null
+++ b/src/beeline/src/main/resources/BeeLine.properties
@@ -0,0 +1,168 @@
+app-introduction: {0} version {1} by {2}
+
+jline-version: The version of the required {0} library is too old. Version \
+				"{1}" was found, but "{2}" is required.
+
+enter-for-more: [ Hit "enter" for more ("q" to exit) ]
+no-manual: Could not find manual resource.
+executing-command:	Executing command: {0}
+unknown-command: Unknown command: {0}
+autocommit-needs-off: Operation requires that autocommit be turned off.
+no-current-connection: No current connection
+connection-is-closed: Connection is closed
+reconnecting: Reconnecting to "{0}"...
+connecting: Connecting to "{0}"...
+no-driver: No known driver to handle "{0}"
+setting-prop: Setting property: {0}
+saving-options: Saving preferences to: {0}
+loaded-options: Loaded preferences from: {0}
+
+jdbc-level: JDBC level
+compliant: Compliant
+jdbc-version: Version
+driver-class: Driver Class
+
+help-quit: Exits the program
+help-dropall: Drop all tables in the current database
+help-connect: Open a new connection to the database.
+help-manual: Display the BeeLine manual
+help-typeinfo: Display the type map for the current connection
+help-describe: Describe a table
+help-reconnect: Reconnect to the database
+help-metadata: Obtain metadata information
+help-dbinfo: Give metadata information about the database
+help-rehash: Fetch table and column names for command completion
+help-verbose: Set verbose mode on
+help-run: Run a script from the specified file
+help-list: List the current connections
+help-all: Execute the specified SQL against all the current connections
+help-go: Select the current connection
+help-script: Start saving a script to a file
+help-brief: Set verbose mode off
+help-close: Close the current connection to the database
+help-closeall: Close all current open connections
+help-isolation: Set the transaction isolation for this connection
+help-nativesql: Show the native SQL for the specified statement
+help-call: Execute a callable statement
+help-autocommit: Set autocommit mode on or off
+help-commit: Commit the current transaction (if autocommit is off)
+help-rollback: Roll back the current transaction (if autocommit is off)
+help-batch: Start or execute a batch of statements
+help-help: Print a summary of command usage
+help-set: Set a beeline variable
+help-save: Save the current variabes and aliases
+help-native: Show the database''s native SQL for a command
+help-alias: Create a new command alias
+help-unalias: Unset a command alias
+help-scan: Scan for installed JDBC drivers
+help-sql: Execute a SQL command
+help-history: Display the command history
+help-record: Record all output to the specified file
+help-indexes: List all the indexes for the specified table
+help-primarykeys: List all the primary keys for the specified table
+help-exportedkeys: List all the exported keys for the specified table
+help-importedkeys: List all the imported keys for the specified table
+help-procedures: List all the procedures
+help-tables: List all the tables in the database
+help-columns: List all the columns for the specified table
+help-properties: Connect to the database specified in the properties file(s)
+help-outputformat: Set the output format for displaying results (table,vertical,csv,tsv,xmlattrs,xmlelements)
+
+jline-missing: The JLine jar was not found. Please ensure it is installed.
+
+batch-start: Batching SQL statements. Run "batch" again to execute the batch.
+running-batch: Running batched SQL statements...
+
+arg-usage: Usage: {0} <{1}>
+
+scanning: Scanning {0}...
+no-such-method: No such method "{0}"
+possible-methods: Possible methods:
+
+closing: Closing: {0}
+already-closed: Connection is already closed.
+error-setting: Error setting configuration: {0}: {1}
+no-method: No method matching "{0}" was found in {1}.
+
+
+connected: Connected to: {0} (version {1})
+driver: Driver: {0} (version {1})
+autocommit-status: Autocommit status: {0}
+isolation-status: Transaction isolation: {0}
+unknown-format: Unknown output format "{0}". Possible values: {1}
+
+closed: closed
+open: open
+
+executing-con: Executing SQL against: {0}
+comments: Comments, bug reports, and patches go to {0}
+building-tables: Building list of tables and columns for tab-completion \
+	(set fastconnect to true to skip)...
+done: Done
+state: state
+code: code
+
+invalid-connections: Invalid connection: {0}
+
+script-closed: Script closed. Enter "run {0}" to replay it.
+script-already-running: Script ({0}) is already running. Enter "script" with no arguments to stop it.
+script-started: Saving command script to "{0}". Enter "script" with no arguments to stop it.
+
+
+record-closed: Recording stopped.
+record-already-running: Output already being saved to ({0}). Enter "record" with no arguments to stop it.
+record-started: Saving all output to "{0}". Enter "record" with no arguments to stop it.
+
+autoloading-known-drivers: No known driver to handle "{0}". Searching for known drivers...
+
+Warning: Warning: {0} (state={1},code={2,number,#})
+Error: Error: {0} (state={1},code={2,number,#})
+
+commit-complete: Commit complete
+rollback-complete: Rollback complete
+
+abort-on-error: Aborting command set because "force" is false and \
+				 command failed: "{0}"
+
+multiple-matches: Ambiguous command: {0}
+
+really-drop-all: Really drop every table in the database? (y/n)\
+abort-drop-all: Aborting drop all tables.
+
+drivers-found-count: 0#No driver classes found|1#{0} driver class found|1<{0} driver classes found
+rows-selected: 0#No rows selected|1#{0} row selected|1<{0} rows selected
+rows-affected: 0#No rows affected|1#{0} row affected|1<{0} rows affected|0>Unknown rows affected
+active-connections: 0#No active connections|1#{0} active connection:|1<{0} active connections:
+
+time-ms: ({0,number,#.###} seconds)
+
+cmd-usage: Usage: java org.apache.hive.cli.beeline.BeeLine \n \
+\  -u <database url>               the JDBC URL to connect to\n \
+\  -n <username>                   the username to connect as\n \
+\  -p <password>                   the password to connect as\n \
+\  -d <driver class>               the driver class to use\n \
+\  -e <query>                      query that should be executed\n \
+\  -f <file>                       script file that should be executed\n \
+\  --hivevar name=value            hive variable name and value\n \
+\                                  This is Hive specific settings in which variables\n \
+\                                  can be set at session level and referenced in Hive\n \
+\                                  commands or queries.\n \
+\  --color=[true/false]            control whether color is used for display\n \
+\  --showHeader=[true/false]       show column names in query results\n \
+\  --headerInterval=ROWS;          the interval between which heades are displayed\n \
+\  --fastConnect=[true/false]      skip building table/column list for tab-completion\n \
+\  --autoCommit=[true/false]       enable/disable automatic transaction commit\n \
+\  --verbose=[true/false]          show verbose error messages and debug info\n \
+\  --showWarnings=[true/false]     display connection warnings\n \
+\  --showNestedErrs=[true/false]   display nested errors\n \
+\  --numberFormat=[pattern]        format numbers using DecimalFormat pattern\n \
+\  --force=[true/false]            continue running script even after errors\n \
+\  --maxWidth=MAXWIDTH             the maximum width of the terminal\n \
+\  --maxColumnWidth=MAXCOLWIDTH    the maximum width to use when displaying columns\n \
+\  --silent=[true/false]           be more silent\n \
+\  --autosave=[true/false]         automatically save preferences\n \
+\  --outputformat=[table/vertical/csv/tsv]   format mode for result display\n \
+\  --isolation=LEVEL               set the transaction isolation level\n \
+\  --help                          display this message
+
+
diff --git a/src/beeline/src/main/resources/sql-keywords.properties b/src/beeline/src/main/resources/sql-keywords.properties
new file mode 100644
index 0000000..0f1eb65
--- /dev/null
+++ b/src/beeline/src/main/resources/sql-keywords.properties
@@ -0,0 +1 @@
+ABSOLUTE,ACTION,ADD,ALL,ALLOCATE,ALTER,AND,ANY,ARE,AS,ASC,ASSERTION,AT,AUTHORIZATION,AVG,BEGIN,BETWEEN,BIT,BIT_LENGTH,BOTH,BY,CASCADE,CASCADED,CASE,CAST,CATALOG,CHAR,CHARACTER,CHAR_LENGTH,CHARACTER_LENGTH,CHECK,CLOSE,CLUSTER,COALESCE,COLLATE,COLLATION,COLUMN,COMMIT,CONNECT,CONNECTION,CONSTRAINT,CONSTRAINTS,CONTINUE,CONVERT,CORRESPONDING,COUNT,CREATE,CROSS,CURRENT,CURRENT_DATE,CURRENT_TIME,CURRENT_TIMESTAMP,CURRENT_USER,CURSOR,DATE,DAY,DEALLOCATE,DEC,DECIMAL,DECLARE,DEFAULT,DEFERRABLE,DEFERRED,DELETE,DESC,DESCRIBE,DESCRIPTOR,DIAGNOSTICS,DISCONNECT,DISTINCT,DOMAIN,DOUBLE,DROP,ELSE,END,END-EXEC,ESCAPE,EXCEPT,EXCEPTION,EXEC,EXECUTE,EXISTS,EXTERNAL,EXTRACT,FALSE,FETCH,FIRST,FLOAT,FOR,FOREIGN,FOUND,FROM,FULL,GET,GLOBAL,GO,GOTO,GRANT,GROUP,HAVING,HOUR,IDENTITY,IMMEDIATE,IN,INDICATOR,INITIALLY,INNER,INPUT,INSENSITIVE,INSERT,INT,INTEGER,INTERSECT,INTERVAL,INTO,IS,ISOLATION,JOIN,KEY,LANGUAGE,LAST,LEADING,LEFT,LEVEL,LIKE,LOCAL,LOWER,MATCH,MAX,MIN,MINUTE,MODULE,MONTH,NAMES,NATIONAL,NATURAL,NCHAR,NEXT,NO,NOT,NULL,NULLIF,NUMERIC,OCTET_LENGTH,OF,ON,ONLY,OPEN,OPTION,OR,ORDER,OUTER,OUTPUT,OVERLAPS,OVERWRITE,PAD,PARTIAL,PARTITION,POSITION,PRECISION,PREPARE,PRESERVE,PRIMARY,PRIOR,PRIVILEGES,PROCEDURE,PUBLIC,READ,REAL,REFERENCES,RELATIVE,RESTRICT,REVOKE,RIGHT,ROLLBACK,ROWS,SCHEMA,SCROLL,SECOND,SECTION,SELECT,SESSION,SESSION_USER,SET,SIZE,SMALLINT,SOME,SPACE,SQL,SQLCODE,SQLERROR,SQLSTATE,SUBSTRING,SUM,SYSTEM_USER,TABLE,TEMPORARY,THEN,TIME,TIMESTAMP,TIMEZONE_HOUR,TIMEZONE_MINUTE,TO,TRAILING,TRANSACTION,TRANSLATE,TRANSLATION,TRIM,TRUE,UNION,UNIQUE,UNKNOWN,UPDATE,UPPER,USAGE,USER,USING,VALUE,VALUES,VARCHAR,VARYING,VIEW,WHEN,WHENEVER,WHERE,WITH,WORK,WRITE,YEAR,ZONE,ADA,C,CATALOG_NAME,CHARACTER_SET_CATALOG,CHARACTER_SET_NAME,CHARACTER_SET_SCHEMA,CLASS_ORIGIN,COBOL,COLLATION_CATALOG,COLLATION_NAME,COLLATION_SCHEMA,COLUMN_NAME,COMMAND_FUNCTION,COMMITTED,CONDITION_NUMBER,CONNECTION_NAME,CONSTRAINT_CATALOG,CONSTRAINT_NAME,CONSTRAINT_SCHEMA,CURSOR_NAME,DATA,DATETIME_INTERVAL_CODE,DATETIME_INTERVAL_PRECISION,DYNAMIC_FUNCTION,FORTRAN,LENGTH,MESSAGE_LENGTH,MESSAGE_OCTET_LENGTH,MESSAGE_TEXT,MORE,MUMPS,NAME,NULLABLE,NUMBER,PASCAL,PLI,REPEATABLE,RETURNED_LENGTH,RETURNED_OCTET_LENGTH,RETURNED_SQLSTATE,ROW_COUNT,SCALE,SCHEMA_NAME,SERIALIZABLE,SERVER_NAME,SUBCLASS_ORIGIN,TABLE_NAME,TYPE,UNCOMMITTED,UNNAMED
diff --git a/src/beeline/src/test/org/apache/hive/beeline/src/test/TestSchemaTool.java b/src/beeline/src/test/org/apache/hive/beeline/src/test/TestSchemaTool.java
deleted file mode 100644
index 71c7756..0000000
--- a/src/beeline/src/test/org/apache/hive/beeline/src/test/TestSchemaTool.java
+++ /dev/null
@@ -1,383 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hive.beeline;
-
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.util.Random;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.commons.lang.StringUtils;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.HiveMetaException;
-import org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo;
-import org.apache.hive.beeline.HiveSchemaHelper;
-import org.apache.hive.beeline.HiveSchemaHelper.NestedScriptParser;
-import org.apache.hive.beeline.HiveSchemaTool;
-
-public class TestSchemaTool extends TestCase {
-  private HiveSchemaTool schemaTool;
-  private HiveConf hiveConf;
-  private String testMetastoreDB;
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    testMetastoreDB = System.getProperty("java.io.tmpdir") +
-        File.separator + "test_metastore-" + new Random().nextInt();
-    System.setProperty(HiveConf.ConfVars.METASTORECONNECTURLKEY.varname,
-        "jdbc:derby:" + testMetastoreDB + ";create=true");
-    hiveConf = new HiveConf(this.getClass());
-    schemaTool = new HiveSchemaTool(System.getProperty("test.tmp.dir"), hiveConf, "derby");
-    System.setProperty("beeLine.system.exit", "true");
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    File metaStoreDir = new File(testMetastoreDB);
-    if (metaStoreDir.exists()) {
-      FileUtils.deleteDirectory(metaStoreDir);
-    }
-  }
-
-  /**
-   * Test dryrun of schema initialization
-   * @throws Exception
-   */
-  public void testSchemaInitDryRun() throws Exception {
-    schemaTool.setDryRun(true);
-    schemaTool.doInit("0.7.0");
-    schemaTool.setDryRun(false);
-    try {
-      schemaTool.verifySchemaVersion();
-    } catch (HiveMetaException e) {
-      // The connection should fail since it the dry run
-      return;
-    }
-    fail("Dry run shouldn't create actual metastore");
-  }
-
-  /**
-   * Test dryrun of schema upgrade
-   * @throws Exception
-   */
-  public void testSchemaUpgradeDryRun() throws Exception {
-    schemaTool.doInit("0.7.0");
-
-    schemaTool.setDryRun(true);
-    schemaTool.doUpgrade("0.7.0");
-    schemaTool.setDryRun(false);
-    try {
-      schemaTool.verifySchemaVersion();
-    } catch (HiveMetaException e) {
-      // The connection should fail since it the dry run
-      return;
-    }
-    fail("Dry run shouldn't upgrade metastore schema");
-  }
-
-  /**
-   * Test schema initialization
-   * @throws Exception
-   */
-  public void testSchemaInit() throws Exception {
-    schemaTool.doInit(MetaStoreSchemaInfo.getHiveSchemaVersion());
-    schemaTool.verifySchemaVersion();
-    }
-
-  /**
-   * Test schema upgrade
-   * @throws Exception
-   */
-  public void testSchemaUpgrade() throws Exception {
-    boolean foundException = false;
-    // Initialize 0.7.0 schema
-    schemaTool.doInit("0.7.0");
-    // verify that driver fails due to older version schema
-    try {
-      schemaTool.verifySchemaVersion();
-    } catch (HiveMetaException e) {
-      // Expected to fail due to old schema
-      foundException = true;
-    }
-    if (!foundException) {
-      throw new Exception("Hive operations shouldn't pass with older version schema");
-    }
-
-    // upgrade schema from 0.7.0 to latest
-    schemaTool.doUpgrade("0.7.0");
-    // verify that driver works fine with latest schema
-    schemaTool.verifySchemaVersion();
-  }
-
-  /**
-   * Test script formatting
-   * @throws Exception
-   */
-  public void testScripts() throws Exception {
-    String testScript[] = {
-        "-- this is a comment",
-      "DROP TABLE IF EXISTS fooTab;",
-      "/*!1234 this is comment code like mysql */;",
-      "CREATE TABLE fooTab(id INTEGER);",
-      "DROP TABLE footab;",
-      "-- ending comment"
-    };
-    String resultScript[] = {
-      "DROP TABLE IF EXISTS fooTab",
-      "/*!1234 this is comment code like mysql */",
-      "CREATE TABLE fooTab(id INTEGER)",
-      "DROP TABLE footab",
-    };
-    String expectedSQL = StringUtils.join(resultScript, System.getProperty("line.separator")) +
-        System.getProperty("line.separator");
-    File testScriptFile = generateTestScript(testScript);
-    String flattenedSql = HiveSchemaTool.buildCommand(
-        HiveSchemaHelper.getDbCommandParser("derby"),
-        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
-
-    assertEquals(expectedSQL, flattenedSql);
-  }
-
-  /**
-   * Test nested script formatting
-   * @throws Exception
-   */
-  public void testNestedScriptsForDerby() throws Exception {
-    String childTab1 = "childTab1";
-    String childTab2 = "childTab2";
-    String parentTab = "fooTab";
-
-    String childTestScript1[] = {
-      "-- this is a comment ",
-      "DROP TABLE IF EXISTS " + childTab1 + ";",
-      "CREATE TABLE " + childTab1 + "(id INTEGER);",
-      "DROP TABLE " + childTab1 + ";"
-    };
-    String childTestScript2[] = {
-        "-- this is a comment",
-        "DROP TABLE IF EXISTS " + childTab2 + ";",
-        "CREATE TABLE " + childTab2 + "(id INTEGER);",
-        "-- this is also a comment",
-        "DROP TABLE " + childTab2 + ";"
-    };
-
-    String parentTestScript[] = {
-        " -- this is a comment",
-        "DROP TABLE IF EXISTS " + parentTab + ";",
-        " -- this is another comment ",
-        "CREATE TABLE " + parentTab + "(id INTEGER);",
-        "RUN '" + generateTestScript(childTestScript1).getName() + "';",
-        "DROP TABLE " + parentTab + ";",
-        "RUN '" + generateTestScript(childTestScript2).getName() + "';",
-        "--ending comment ",
-      };
-
-    File testScriptFile = generateTestScript(parentTestScript);
-    String flattenedSql = HiveSchemaTool.buildCommand(
-        HiveSchemaHelper.getDbCommandParser("derby"),
-        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
-    assertFalse(flattenedSql.contains("RUN"));
-    assertFalse(flattenedSql.contains("comment"));
-    assertTrue(flattenedSql.contains(childTab1));
-    assertTrue(flattenedSql.contains(childTab2));
-    assertTrue(flattenedSql.contains(parentTab));
-  }
-
-  /**
-   * Test nested script formatting
-   * @throws Exception
-   */
-  public void testNestedScriptsForMySQL() throws Exception {
-    String childTab1 = "childTab1";
-    String childTab2 = "childTab2";
-    String parentTab = "fooTab";
-
-    String childTestScript1[] = {
-      "/* this is a comment code */",
-      "DROP TABLE IF EXISTS " + childTab1 + ";",
-      "CREATE TABLE " + childTab1 + "(id INTEGER);",
-      "DROP TABLE " + childTab1 + ";"
-    };
-    String childTestScript2[] = {
-        "/* this is a special exec code */;",
-        "DROP TABLE IF EXISTS " + childTab2 + ";",
-        "CREATE TABLE " + childTab2 + "(id INTEGER);",
-        "-- this is a comment",
-        "DROP TABLE " + childTab2 + ";"
-    };
-
-    String parentTestScript[] = {
-        " -- this is a comment",
-        "DROP TABLE IF EXISTS " + parentTab + ";",
-        " /* this is special exec code */;",
-        "CREATE TABLE " + parentTab + "(id INTEGER);",
-        "SOURCE " + generateTestScript(childTestScript1).getName() + ";",
-        "DROP TABLE " + parentTab + ";",
-        "SOURCE " + generateTestScript(childTestScript2).getName() + ";",
-        "--ending comment ",
-      };
-
-    File testScriptFile = generateTestScript(parentTestScript);
-    String flattenedSql = HiveSchemaTool.buildCommand(
-        HiveSchemaHelper.getDbCommandParser("mysql"),
-        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
-    assertFalse(flattenedSql.contains("RUN"));
-    assertFalse(flattenedSql.contains("comment"));
-    assertTrue(flattenedSql.contains(childTab1));
-    assertTrue(flattenedSql.contains(childTab2));
-    assertTrue(flattenedSql.contains(parentTab));
-  }
-
-  /**
-   * Test script formatting
-   * @throws Exception
-   */
-  public void testScriptWithDelimiter() throws Exception {
-    String testScript[] = {
-        "-- this is a comment",
-      "DROP TABLE IF EXISTS fooTab;",
-      "DELIMITER $$",
-      "/*!1234 this is comment code like mysql */$$",
-      "CREATE TABLE fooTab(id INTEGER)$$",
-      "CREATE PROCEDURE fooProc()",
-      "SELECT * FROM fooTab;",
-      "CALL barProc();",
-      "END PROCEDURE$$",
-      "DELIMITER ;",
-      "DROP TABLE footab;",
-      "-- ending comment"
-    };
-    String resultScript[] = {
-      "DROP TABLE IF EXISTS fooTab",
-      "/*!1234 this is comment code like mysql */",
-      "CREATE TABLE fooTab(id INTEGER)",
-      "CREATE PROCEDURE fooProc()" + " " +
-      "SELECT * FROM fooTab;" + " " +
-      "CALL barProc();" + " " +
-      "END PROCEDURE",
-      "DROP TABLE footab",
-    };
-    String expectedSQL = StringUtils.join(resultScript, System.getProperty("line.separator")) +
-        System.getProperty("line.separator");
-    File testScriptFile = generateTestScript(testScript);
-    NestedScriptParser testDbParser = HiveSchemaHelper.getDbCommandParser("mysql");
-    String flattenedSql = HiveSchemaTool.buildCommand(testDbParser,
-        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
-
-    assertEquals(expectedSQL, flattenedSql);
-  }
-
-  /**
-   * Test script formatting
-   * @throws Exception
-   */
-  public void testScriptMultiRowComment() throws Exception {
-    String testScript[] = {
-        "-- this is a comment",
-      "DROP TABLE IF EXISTS fooTab;",
-      "DELIMITER $$",
-      "/*!1234 this is comment code like mysql */$$",
-      "CREATE TABLE fooTab(id INTEGER)$$",
-      "DELIMITER ;",
-      "/* multiline comment started ",
-      " * multiline comment continue",
-      " * multiline comment ended */",
-      "DROP TABLE footab;",
-      "-- ending comment"
-    };
-    String parsedScript[] = {
-      "DROP TABLE IF EXISTS fooTab",
-      "/*!1234 this is comment code like mysql */",
-      "CREATE TABLE fooTab(id INTEGER)",
-      "DROP TABLE footab",
-    };
-
-    String expectedSQL = StringUtils.join(parsedScript, System.getProperty("line.separator")) +
-        System.getProperty("line.separator");
-    File testScriptFile = generateTestScript(testScript);
-    NestedScriptParser testDbParser = HiveSchemaHelper.getDbCommandParser("mysql");
-    String flattenedSql = HiveSchemaTool.buildCommand(testDbParser,
-        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
-
-    assertEquals(expectedSQL, flattenedSql);
-  }
-
-  /**
-   * Test nested script formatting
-   * @throws Exception
-   */
-  public void testNestedScriptsForOracle() throws Exception {
-    String childTab1 = "childTab1";
-    String childTab2 = "childTab2";
-    String parentTab = "fooTab";
-
-    String childTestScript1[] = {
-      "-- this is a comment ",
-      "DROP TABLE IF EXISTS " + childTab1 + ";",
-      "CREATE TABLE " + childTab1 + "(id INTEGER);",
-      "DROP TABLE " + childTab1 + ";"
-    };
-    String childTestScript2[] = {
-        "-- this is a comment",
-        "DROP TABLE IF EXISTS " + childTab2 + ";",
-        "CREATE TABLE " + childTab2 + "(id INTEGER);",
-        "-- this is also a comment",
-        "DROP TABLE " + childTab2 + ";"
-    };
-
-    String parentTestScript[] = {
-        " -- this is a comment",
-        "DROP TABLE IF EXISTS " + parentTab + ";",
-        " -- this is another comment ",
-        "CREATE TABLE " + parentTab + "(id INTEGER);",
-        "@" + generateTestScript(childTestScript1).getName() + ";",
-        "DROP TABLE " + parentTab + ";",
-        "@" + generateTestScript(childTestScript2).getName() + ";",
-        "--ending comment ",
-      };
-
-    File testScriptFile = generateTestScript(parentTestScript);
-    String flattenedSql = HiveSchemaTool.buildCommand(
-        HiveSchemaHelper.getDbCommandParser("oracle"),
-        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
-    assertFalse(flattenedSql.contains("@"));
-    assertFalse(flattenedSql.contains("comment"));
-    assertTrue(flattenedSql.contains(childTab1));
-    assertTrue(flattenedSql.contains(childTab2));
-    assertTrue(flattenedSql.contains(parentTab));
-  }
-
-  private File generateTestScript(String [] stmts) throws IOException {
-    File testScriptFile = File.createTempFile("schematest", ".sql");
-    testScriptFile.deleteOnExit();
-    FileWriter fstream = new FileWriter(testScriptFile.getPath());
-    BufferedWriter out = new BufferedWriter(fstream);
-    for (String line: stmts) {
-      out.write(line);
-      out.newLine();
-    }
-    out.close();
-    return testScriptFile;
-  }
-}
diff --git a/src/common/src/java/conf/hive-log4j.properties b/src/common/src/java/conf/hive-log4j.properties
deleted file mode 100644
index 212156e..0000000
--- a/src/common/src/java/conf/hive-log4j.properties
+++ /dev/null
@@ -1,88 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-# Define some default values that can be overridden by system properties
-hive.log.threshold=ALL
-hive.root.logger=INFO,DRFA
-hive.log.dir=${java.io.tmpdir}/${user.name}
-hive.log.file=hive.log
-
-# Define the root logger to the system property "hadoop.root.logger".
-log4j.rootLogger=${hive.root.logger}, EventCounter
-
-# Logging Threshold
-log4j.threshold=${hive.log.threshold}
-
-#
-# Daily Rolling File Appender
-#
-# Use the PidDailyerRollingFileAppend class instead if you want to use separate log files
-# for different CLI session.
-#
-# log4j.appender.DRFA=org.apache.hadoop.hive.ql.log.PidDailyRollingFileAppender
-
-log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
-
-log4j.appender.DRFA.File=${hive.log.dir}/${hive.log.file}
-
-# Rollver at midnight
-log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
-
-# 30-day backup
-#log4j.appender.DRFA.MaxBackupIndex=30
-log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
-
-# Pattern format: Date LogLevel LoggerName LogMessage
-#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
-# Debugging Pattern format
-log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
-
-
-#
-# console
-# Add "console" to rootlogger above if you want to use this
-#
-
-log4j.appender.console=org.apache.log4j.ConsoleAppender
-log4j.appender.console.target=System.err
-log4j.appender.console.layout=org.apache.log4j.PatternLayout
-log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
-log4j.appender.console.encoding=UTF-8
-
-#custom logging levels
-#log4j.logger.xxx=DEBUG
-
-#
-# Event Counter Appender
-# Sends counts of logging messages at different severity levels to Hadoop Metrics.
-#
-log4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter
-
-
-log4j.category.DataNucleus=ERROR,DRFA
-log4j.category.Datastore=ERROR,DRFA
-log4j.category.Datastore.Schema=ERROR,DRFA
-log4j.category.JPOX.Datastore=ERROR,DRFA
-log4j.category.JPOX.Plugin=ERROR,DRFA
-log4j.category.JPOX.MetaData=ERROR,DRFA
-log4j.category.JPOX.Query=ERROR,DRFA
-log4j.category.JPOX.General=ERROR,DRFA
-log4j.category.JPOX.Enhancer=ERROR,DRFA
-
-
-# Silence useless ZK logs
-log4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN,DRFA
-log4j.logger.org.apache.zookeeper.ClientCnxnSocketNIO=WARN,DRFA
diff --git a/src/common/src/main/resources/hive-log4j.properties b/src/common/src/main/resources/hive-log4j.properties
new file mode 100644
index 0000000..212156e
--- /dev/null
+++ b/src/common/src/main/resources/hive-log4j.properties
@@ -0,0 +1,88 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Define some default values that can be overridden by system properties
+hive.log.threshold=ALL
+hive.root.logger=INFO,DRFA
+hive.log.dir=${java.io.tmpdir}/${user.name}
+hive.log.file=hive.log
+
+# Define the root logger to the system property "hadoop.root.logger".
+log4j.rootLogger=${hive.root.logger}, EventCounter
+
+# Logging Threshold
+log4j.threshold=${hive.log.threshold}
+
+#
+# Daily Rolling File Appender
+#
+# Use the PidDailyerRollingFileAppend class instead if you want to use separate log files
+# for different CLI session.
+#
+# log4j.appender.DRFA=org.apache.hadoop.hive.ql.log.PidDailyRollingFileAppender
+
+log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
+
+log4j.appender.DRFA.File=${hive.log.dir}/${hive.log.file}
+
+# Rollver at midnight
+log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
+
+# 30-day backup
+#log4j.appender.DRFA.MaxBackupIndex=30
+log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
+
+# Pattern format: Date LogLevel LoggerName LogMessage
+#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+# Debugging Pattern format
+log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
+
+
+#
+# console
+# Add "console" to rootlogger above if you want to use this
+#
+
+log4j.appender.console=org.apache.log4j.ConsoleAppender
+log4j.appender.console.target=System.err
+log4j.appender.console.layout=org.apache.log4j.PatternLayout
+log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
+log4j.appender.console.encoding=UTF-8
+
+#custom logging levels
+#log4j.logger.xxx=DEBUG
+
+#
+# Event Counter Appender
+# Sends counts of logging messages at different severity levels to Hadoop Metrics.
+#
+log4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter
+
+
+log4j.category.DataNucleus=ERROR,DRFA
+log4j.category.Datastore=ERROR,DRFA
+log4j.category.Datastore.Schema=ERROR,DRFA
+log4j.category.JPOX.Datastore=ERROR,DRFA
+log4j.category.JPOX.Plugin=ERROR,DRFA
+log4j.category.JPOX.MetaData=ERROR,DRFA
+log4j.category.JPOX.Query=ERROR,DRFA
+log4j.category.JPOX.General=ERROR,DRFA
+log4j.category.JPOX.Enhancer=ERROR,DRFA
+
+
+# Silence useless ZK logs
+log4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN,DRFA
+log4j.logger.org.apache.zookeeper.ClientCnxnSocketNIO=WARN,DRFA
diff --git a/src/hbase-handler/src/test/org/apache/hadoop/hive/hbase/HBaseQTestUtil.java b/src/hbase-handler/src/test/org/apache/hadoop/hive/hbase/HBaseQTestUtil.java
deleted file mode 100644
index 0558048..0000000
--- a/src/hbase-handler/src/test/org/apache/hadoop/hive/hbase/HBaseQTestUtil.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.hbase;
-
-import org.apache.hadoop.hive.ql.QTestUtil;
-
-/**
- * HBaseQTestUtil initializes HBase-specific test fixtures.
- */
-public class HBaseQTestUtil extends QTestUtil {
-  public HBaseQTestUtil(
-    String outDir, String logDir, boolean miniMr, HBaseTestSetup setup)
-    throws Exception {
-
-    super(outDir, logDir, miniMr, null);
-    setup.preTest(conf);
-    super.init();
-  }
-
-  public void init() throws Exception {
-    // defer
-  }
-}
diff --git a/src/hbase-handler/src/test/org/apache/hadoop/hive/hbase/HBaseTestSetup.java b/src/hbase-handler/src/test/org/apache/hadoop/hive/hbase/HBaseTestSetup.java
deleted file mode 100644
index cdc0a65..0000000
--- a/src/hbase-handler/src/test/org/apache/hadoop/hive/hbase/HBaseTestSetup.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.hbase;
-
-import java.io.File;
-import java.io.IOException;
-import java.net.ServerSocket;
-import java.util.Arrays;
-
-import junit.extensions.TestSetup;
-import junit.framework.Test;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HConnectionManager;
-import org.apache.hadoop.hbase.client.HTable;
-import org.apache.hadoop.hbase.MiniHBaseCluster;
-import org.apache.hadoop.hbase.client.Put;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.zookeeper.Watcher;
-
-/**
- * HBaseTestSetup defines HBase-specific test fixtures which are
- * reused across testcases.
- */
-public class HBaseTestSetup extends TestSetup {
-
-  private MiniHBaseCluster hbaseCluster;
-  private int zooKeeperPort;
-  private String hbaseRoot;
-
-  private static final int NUM_REGIONSERVERS = 1;
-
-  public HBaseTestSetup(Test test) {
-    super(test);
-  }
-
-  void preTest(HiveConf conf) throws Exception {
-
-    setUpFixtures(conf);
-
-    conf.set("hbase.rootdir", hbaseRoot);
-    conf.set("hbase.master", hbaseCluster.getMaster().getServerName().getHostAndPort());
-    conf.set("hbase.zookeeper.property.clientPort", Integer.toString(zooKeeperPort));
-    String auxJars = conf.getAuxJars();
-    auxJars = ((auxJars == null) ? "" : (auxJars + ",")) + "file:///"
-      + new JobConf(conf, HBaseConfiguration.class).getJar();
-    auxJars += ",file:///" + new JobConf(conf, HBaseSerDe.class).getJar();
-    auxJars += ",file:///" + new JobConf(conf, Watcher.class).getJar();
-    conf.setAuxJars(auxJars);
-  }
-
-  private void setUpFixtures(HiveConf conf) throws Exception {
-    /* We are not starting zookeeper server here because
-     * QTestUtil already starts it.
-     */
-    int zkPort = conf.getInt("hive.zookeeper.client.port", -1);
-    if ((zkPort == zooKeeperPort) && (hbaseCluster != null)) {
-      return;
-    }
-    zooKeeperPort = zkPort;
-    String tmpdir =  System.getProperty("test.tmp.dir");
-    this.tearDown();
-    conf.set("hbase.master", "local");
-
-    hbaseRoot = "file:///" + tmpdir + "/hbase";
-    conf.set("hbase.rootdir", hbaseRoot);
-
-    conf.set("hbase.zookeeper.property.clientPort",
-      Integer.toString(zooKeeperPort));
-    Configuration hbaseConf = HBaseConfiguration.create(conf);
-    hbaseConf.setInt("hbase.master.port", findFreePort());
-    hbaseConf.setInt("hbase.master.info.port", -1);
-    hbaseConf.setInt("hbase.regionserver.port", findFreePort());
-    hbaseConf.setInt("hbase.regionserver.info.port", -1);
-    hbaseCluster = new MiniHBaseCluster(hbaseConf, NUM_REGIONSERVERS);
-    conf.set("hbase.master", hbaseCluster.getMaster().getServerName().getHostAndPort());
-    // opening the META table ensures that cluster is running
-    new HTable(hbaseConf, HConstants.META_TABLE_NAME);
-    createHBaseTable(hbaseConf);
-  }
-
-  private void createHBaseTable(Configuration hbaseConf) throws IOException {
-    final String HBASE_TABLE_NAME = "HiveExternalTable";
-    HTableDescriptor htableDesc = new HTableDescriptor(HBASE_TABLE_NAME.getBytes());
-    HColumnDescriptor hcolDesc = new HColumnDescriptor("cf".getBytes());
-    htableDesc.addFamily(hcolDesc);
-    HBaseAdmin hbaseAdmin = new HBaseAdmin(hbaseConf);
-    if(Arrays.asList(hbaseAdmin.listTables()).contains(htableDesc)){
-      // if table is already in there, don't recreate.
-      return;
-    }
-    hbaseAdmin.createTable(htableDesc);
-    HTable htable = new HTable(hbaseConf, HBASE_TABLE_NAME);
-
-    // data
-    Put [] puts = new Put [] {
-        new Put("key-1".getBytes()), new Put("key-2".getBytes()), new Put("key-3".getBytes()) };
-
-    boolean [] booleans = new boolean [] { true, false, true };
-    byte [] bytes = new byte [] { Byte.MIN_VALUE, -1, Byte.MAX_VALUE };
-    short [] shorts = new short [] { Short.MIN_VALUE, -1, Short.MAX_VALUE };
-    int [] ints = new int [] { Integer.MIN_VALUE, -1, Integer.MAX_VALUE };
-    long [] longs = new long [] { Long.MIN_VALUE, -1, Long.MAX_VALUE };
-    String [] strings = new String [] { "Hadoop, HBase,", "Hive", "Test Strings" };
-    float [] floats = new float [] { Float.MIN_VALUE, -1.0F, Float.MAX_VALUE };
-    double [] doubles = new double [] { Double.MIN_VALUE, -1.0, Double.MAX_VALUE };
-
-    // store data
-    for (int i = 0; i < puts.length; i++) {
-      puts[i].add("cf".getBytes(), "cq-boolean".getBytes(), Bytes.toBytes(booleans[i]));
-      puts[i].add("cf".getBytes(), "cq-byte".getBytes(), new byte [] { bytes[i] });
-      puts[i].add("cf".getBytes(), "cq-short".getBytes(), Bytes.toBytes(shorts[i]));
-      puts[i].add("cf".getBytes(), "cq-int".getBytes(), Bytes.toBytes(ints[i]));
-      puts[i].add("cf".getBytes(), "cq-long".getBytes(), Bytes.toBytes(longs[i]));
-      puts[i].add("cf".getBytes(), "cq-string".getBytes(), Bytes.toBytes(strings[i]));
-      puts[i].add("cf".getBytes(), "cq-float".getBytes(), Bytes.toBytes(floats[i]));
-      puts[i].add("cf".getBytes(), "cq-double".getBytes(), Bytes.toBytes(doubles[i]));
-
-      htable.put(puts[i]);
-    }
-  }
-
-  private static int findFreePort() throws IOException {
-    ServerSocket server = new ServerSocket(0);
-    int port = server.getLocalPort();
-    server.close();
-    return port;
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    if (hbaseCluster != null) {
-      HConnectionManager.deleteAllConnections(true);
-      hbaseCluster.shutdown();
-      hbaseCluster = null;
-    }
-  }
-}
diff --git a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveCompatibility.java b/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveCompatibility.java
deleted file mode 100644
index e1e7669..0000000
--- a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveCompatibility.java
+++ /dev/null
@@ -1,132 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.hcatalog.mapreduce;
-
-import java.io.File;
-import java.io.FileWriter;
-import java.util.Arrays;
-import java.util.Iterator;
-
-import junit.framework.Assert;
-import org.apache.hadoop.hive.metastore.api.Partition;
-import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hcatalog.common.HCatConstants;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * @deprecated Use/modify {@link org.apache.hive.hcatalog.mapreduce.TestHCatHiveCompatibility} instead
- */
-public class TestHCatHiveCompatibility extends HCatBaseTest {
-  private static final String INPUT_FILE_NAME = TEST_DATA_DIR + "/input.data";
-
-  @BeforeClass
-  public static void createInputData() throws Exception {
-    int LOOP_SIZE = 11;
-    File file = new File(INPUT_FILE_NAME);
-    file.deleteOnExit();
-    FileWriter writer = new FileWriter(file);
-    for (int i = 0; i < LOOP_SIZE; i++) {
-      writer.write(i + "\t1\n");
-    }
-    writer.close();
-  }
-
-  @Test
-  public void testUnpartedReadWrite() throws Exception {
-
-    driver.run("drop table if exists junit_unparted_noisd");
-    String createTable = "create table junit_unparted_noisd(a int) stored as RCFILE";
-    Assert.assertEquals(0, driver.run(createTable).getResponseCode());
-
-    // assert that the table created has no hcat instrumentation, and that we're still able to read it.
-    Table table = client.getTable("default", "junit_unparted_noisd");
-    Assert.assertTrue(table.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
-
-    PigServer server = new PigServer(ExecType.LOCAL);
-    logAndRegister(server, "A = load '" + INPUT_FILE_NAME + "' as (a:int);");
-    logAndRegister(server, "store A into 'default.junit_unparted_noisd' using org.apache.hcatalog.pig.HCatStorer();");
-    logAndRegister(server, "B = load 'default.junit_unparted_noisd' using org.apache.hcatalog.pig.HCatLoader();");
-    Iterator<Tuple> itr = server.openIterator("B");
-
-    int i = 0;
-
-    while (itr.hasNext()) {
-      Tuple t = itr.next();
-      Assert.assertEquals(1, t.size());
-      Assert.assertEquals(t.get(0), i);
-      i++;
-    }
-
-    Assert.assertFalse(itr.hasNext());
-    Assert.assertEquals(11, i);
-
-    // assert that the table created still has no hcat instrumentation
-    Table table2 = client.getTable("default", "junit_unparted_noisd");
-    Assert.assertTrue(table2.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
-
-    driver.run("drop table junit_unparted_noisd");
-  }
-
-  @Test
-  public void testPartedRead() throws Exception {
-
-    driver.run("drop table if exists junit_parted_noisd");
-    String createTable = "create table junit_parted_noisd(a int) partitioned by (b string) stored as RCFILE";
-    Assert.assertEquals(0, driver.run(createTable).getResponseCode());
-
-    // assert that the table created has no hcat instrumentation, and that we're still able to read it.
-    Table table = client.getTable("default", "junit_parted_noisd");
-    Assert.assertTrue(table.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
-
-    PigServer server = new PigServer(ExecType.LOCAL);
-    logAndRegister(server, "A = load '" + INPUT_FILE_NAME + "' as (a:int);");
-    logAndRegister(server, "store A into 'default.junit_parted_noisd' using org.apache.hcatalog.pig.HCatStorer('b=42');");
-    logAndRegister(server, "B = load 'default.junit_parted_noisd' using org.apache.hcatalog.pig.HCatLoader();");
-    Iterator<Tuple> itr = server.openIterator("B");
-
-    int i = 0;
-
-    while (itr.hasNext()) {
-      Tuple t = itr.next();
-      Assert.assertEquals(2, t.size()); // Contains explicit field "a" and partition "b".
-      Assert.assertEquals(t.get(0), i);
-      Assert.assertEquals(t.get(1), "42");
-      i++;
-    }
-
-    Assert.assertFalse(itr.hasNext());
-    Assert.assertEquals(11, i);
-
-    // assert that the table created still has no hcat instrumentation
-    Table table2 = client.getTable("default", "junit_parted_noisd");
-    Assert.assertTrue(table2.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
-
-    // assert that there is one partition present, and it had hcat instrumentation inserted when it was created.
-    Partition ptn = client.getPartition("default", "junit_parted_noisd", Arrays.asList("42"));
-
-    Assert.assertNotNull(ptn);
-    Assert.assertTrue(ptn.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
-    driver.run("drop table junit_unparted_noisd");
-  }
-}
diff --git a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java b/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
deleted file mode 100644
index 28c11b8..0000000
--- a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
+++ /dev/null
@@ -1,119 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.hcatalog.mapreduce;
-
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.serde2.thrift.test.IntString;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.data.DataType;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.logicalLayer.schema.Schema;
-import org.apache.thrift.protocol.TBinaryProtocol;
-import org.apache.thrift.transport.TIOStreamTransport;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.ByteArrayOutputStream;
-import java.util.Iterator;
-
-/**
- * @deprecated Use/modify {@link org.apache.hive.hcatalog.mapreduce.TestHCatHiveThriftCompatibility} instead
- */
-public class TestHCatHiveThriftCompatibility extends HCatBaseTest {
-
-  private boolean setUpComplete = false;
-  private Path intStringSeq;
-
-  @Before
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    if (setUpComplete) {
-      return;
-    }
-
-    ByteArrayOutputStream out = new ByteArrayOutputStream();
-    TIOStreamTransport transport = new TIOStreamTransport(out);
-    TBinaryProtocol protocol = new TBinaryProtocol(transport);
-
-    IntString intString = new IntString(1, "one", 1);
-    intString.write(protocol);
-    BytesWritable bytesWritable = new BytesWritable(out.toByteArray());
-
-    intStringSeq = new Path(TEST_DATA_DIR + "/data/intString.seq");
-    LOG.info("Creating data file: " + intStringSeq);
-
-    SequenceFile.Writer seqFileWriter = SequenceFile.createWriter(
-        intStringSeq.getFileSystem(hiveConf), hiveConf, intStringSeq,
-        NullWritable.class, BytesWritable.class);
-    seqFileWriter.append(NullWritable.get(), bytesWritable);
-    seqFileWriter.close();
-
-    setUpComplete = true;
-  }
-
-  /**
-   *  Create a table with no explicit schema and ensure its correctly
-   *  discovered from the thrift struct.
-   */
-  @Test
-  public void testDynamicCols() throws Exception {
-    Assert.assertEquals(0, driver.run("drop table if exists test_thrift").getResponseCode());
-    Assert.assertEquals(0, driver.run(
-        "create external table test_thrift " +
-            "partitioned by (year string) " +
-            "row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' " +
-            "with serdeproperties ( " +
-            "  'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString', " +
-            "  'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') " +
-            "stored as" +
-            "  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'" +
-            "  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'")
-        .getResponseCode());
-    Assert.assertEquals(0,
-        driver.run("alter table test_thrift add partition (year = '2012') location '" +
-            intStringSeq.getParent() + "'").getResponseCode());
-
-    PigServer pigServer = new PigServer(ExecType.LOCAL);
-    pigServer.registerQuery("A = load 'test_thrift' using org.apache.hcatalog.pig.HCatLoader();");
-
-    Schema expectedSchema = new Schema();
-    expectedSchema.add(new Schema.FieldSchema("myint", DataType.INTEGER));
-    expectedSchema.add(new Schema.FieldSchema("mystring", DataType.CHARARRAY));
-    expectedSchema.add(new Schema.FieldSchema("underscore_int", DataType.INTEGER));
-    expectedSchema.add(new Schema.FieldSchema("year", DataType.CHARARRAY));
-
-    Assert.assertEquals(expectedSchema, pigServer.dumpSchema("A"));
-
-    Iterator<Tuple> iterator = pigServer.openIterator("A");
-    Tuple t = iterator.next();
-    Assert.assertEquals(1, t.get(0));
-    Assert.assertEquals("one", t.get(1));
-    Assert.assertEquals(1, t.get(2));
-    Assert.assertEquals("2012", t.get(3));
-
-    Assert.assertFalse(iterator.hasNext());
-  }
-}
diff --git a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java b/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java
deleted file mode 100644
index 61d14f1..0000000
--- a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java
+++ /dev/null
@@ -1,278 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.hcatalog.mapreduce;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertTrue;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hcatalog.HcatTestUtils;
-import org.apache.hcatalog.common.HCatConstants;
-import org.apache.hcatalog.common.HCatException;
-import org.apache.hcatalog.common.HCatUtil;
-import org.apache.hcatalog.data.DefaultHCatRecord;
-import org.apache.hcatalog.data.schema.HCatFieldSchema;
-import org.apache.hcatalog.data.schema.HCatSchema;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.data.Tuple;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-/**
- * @deprecated Use/modify {@link org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite} instead
- */
-public class TestSequenceFileReadWrite {
-
-  private File dataDir;
-  private String warehouseDir;
-  private String inputFileName;
-  private Driver driver;
-  private PigServer server;
-  private String[] input;
-  private HiveConf hiveConf;
-
-  @Before
-  public void setup() throws Exception {
-    dataDir = new File(System.getProperty("java.io.tmpdir") + File.separator +
-        TestSequenceFileReadWrite.class.getCanonicalName() + "-" + System.currentTimeMillis());
-    hiveConf = new HiveConf(this.getClass());
-    warehouseDir = new File(dataDir, "warehouse").getAbsolutePath();
-    inputFileName = new File(dataDir, "input.data").getAbsolutePath();
-    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehouseDir);
-    driver = new Driver(hiveConf);
-    SessionState.start(new CliSessionState(hiveConf));
-
-    if(!(new File(warehouseDir).mkdirs())) {
-      throw new RuntimeException("Could not create " + warehouseDir);
-    }
-
-    int numRows = 3;
-    input = new String[numRows];
-    for (int i = 0; i < numRows; i++) {
-      String col1 = "a" + i;
-      String col2 = "b" + i;
-      input[i] = i + "," + col1 + "," + col2;
-    }
-    HcatTestUtils.createTestDataFile(inputFileName, input);
-    server = new PigServer(ExecType.LOCAL);
-  }
-
-  @After
-  public void teardown() throws IOException {
-    if(dataDir != null) {
-      FileUtils.deleteDirectory(dataDir);
-    }
-  }
-
-  @Test
-  public void testSequenceTableWriteRead() throws Exception {
-    String createTable = "CREATE TABLE demo_table(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
-    driver.run("drop table demo_table");
-    int retCode1 = driver.run(createTable).getResponseCode();
-    assertTrue(retCode1 == 0);
-
-    server.setBatchOn();
-    server.registerQuery("A = load '"
-        + inputFileName
-        + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
-    server.registerQuery("store A into 'demo_table' using org.apache.hcatalog.pig.HCatStorer();");
-    server.executeBatch();
-
-    server.registerQuery("B = load 'demo_table' using org.apache.hcatalog.pig.HCatLoader();");
-    Iterator<Tuple> XIter = server.openIterator("B");
-    int numTuplesRead = 0;
-    while (XIter.hasNext()) {
-      Tuple t = XIter.next();
-      assertEquals(3, t.size());
-      assertEquals(t.get(0).toString(), "" + numTuplesRead);
-      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
-      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
-      numTuplesRead++;
-    }
-    assertEquals(input.length, numTuplesRead);
-  }
-
-  @Test
-  public void testTextTableWriteRead() throws Exception {
-    String createTable = "CREATE TABLE demo_table_1(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
-    driver.run("drop table demo_table_1");
-    int retCode1 = driver.run(createTable).getResponseCode();
-    assertTrue(retCode1 == 0);
-
-    server.setBatchOn();
-    server.registerQuery("A = load '"
-        + inputFileName
-        + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
-    server.registerQuery("store A into 'demo_table_1' using org.apache.hcatalog.pig.HCatStorer();");
-    server.executeBatch();
-
-    server.registerQuery("B = load 'demo_table_1' using org.apache.hcatalog.pig.HCatLoader();");
-    Iterator<Tuple> XIter = server.openIterator("B");
-    int numTuplesRead = 0;
-    while (XIter.hasNext()) {
-      Tuple t = XIter.next();
-      assertEquals(3, t.size());
-      assertEquals(t.get(0).toString(), "" + numTuplesRead);
-      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
-      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
-      numTuplesRead++;
-    }
-    assertEquals(input.length, numTuplesRead);
-  }
-
-  @Test
-  public void testSequenceTableWriteReadMR() throws Exception {
-    String createTable = "CREATE TABLE demo_table_2(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
-    driver.run("drop table demo_table_2");
-    int retCode1 = driver.run(createTable).getResponseCode();
-    assertTrue(retCode1 == 0);
-
-    Configuration conf = new Configuration();
-    conf.set(HCatConstants.HCAT_KEY_HIVE_CONF,
-        HCatUtil.serialize(hiveConf.getAllProperties()));
-    Job job = new Job(conf, "Write-hcat-seq-table");
-    job.setJarByClass(TestSequenceFileReadWrite.class);
-
-    job.setMapperClass(Map.class);
-    job.setOutputKeyClass(NullWritable.class);
-    job.setOutputValueClass(DefaultHCatRecord.class);
-    job.setInputFormatClass(TextInputFormat.class);
-    TextInputFormat.setInputPaths(job, inputFileName);
-
-    HCatOutputFormat.setOutput(job, OutputJobInfo.create(
-        MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_2", null));
-    job.setOutputFormatClass(HCatOutputFormat.class);
-    HCatOutputFormat.setSchema(job, getSchema());
-    job.setNumReduceTasks(0);
-    assertTrue(job.waitForCompletion(true));
-    if (!HCatUtil.isHadoop23()) {
-      new FileOutputCommitterContainer(job, null).commitJob(job);
-    }
-    assertTrue(job.isSuccessful());
-
-    server.setBatchOn();
-    server.registerQuery("C = load 'default.demo_table_2' using org.apache.hcatalog.pig.HCatLoader();");
-    server.executeBatch();
-    Iterator<Tuple> XIter = server.openIterator("C");
-    int numTuplesRead = 0;
-    while (XIter.hasNext()) {
-      Tuple t = XIter.next();
-      assertEquals(3, t.size());
-      assertEquals(t.get(0).toString(), "" + numTuplesRead);
-      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
-      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
-      numTuplesRead++;
-    }
-    assertEquals(input.length, numTuplesRead);
-  }
-
-  @Test
-  public void testTextTableWriteReadMR() throws Exception {
-    String createTable = "CREATE TABLE demo_table_3(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
-    driver.run("drop table demo_table_3");
-    int retCode1 = driver.run(createTable).getResponseCode();
-    assertTrue(retCode1 == 0);
-
-    Configuration conf = new Configuration();
-    conf.set(HCatConstants.HCAT_KEY_HIVE_CONF,
-        HCatUtil.serialize(hiveConf.getAllProperties()));
-    Job job = new Job(conf, "Write-hcat-text-table");
-    job.setJarByClass(TestSequenceFileReadWrite.class);
-
-    job.setMapperClass(Map.class);
-    job.setOutputKeyClass(NullWritable.class);
-    job.setOutputValueClass(DefaultHCatRecord.class);
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setNumReduceTasks(0);
-    TextInputFormat.setInputPaths(job, inputFileName);
-
-    HCatOutputFormat.setOutput(job, OutputJobInfo.create(
-        MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_3", null));
-    job.setOutputFormatClass(HCatOutputFormat.class);
-    HCatOutputFormat.setSchema(job, getSchema());
-    assertTrue(job.waitForCompletion(true));
-    if (!HCatUtil.isHadoop23()) {
-      new FileOutputCommitterContainer(job, null).commitJob(job);
-    }
-    assertTrue(job.isSuccessful());
-
-    server.setBatchOn();
-    server.registerQuery("D = load 'default.demo_table_3' using org.apache.hcatalog.pig.HCatLoader();");
-    server.executeBatch();
-    Iterator<Tuple> XIter = server.openIterator("D");
-    int numTuplesRead = 0;
-    while (XIter.hasNext()) {
-      Tuple t = XIter.next();
-      assertEquals(3, t.size());
-      assertEquals(t.get(0).toString(), "" + numTuplesRead);
-      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
-      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
-      numTuplesRead++;
-    }
-    assertEquals(input.length, numTuplesRead);
-  }
-
-
-  public static class Map extends Mapper<LongWritable, Text, NullWritable, DefaultHCatRecord> {
-
-    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
-      String[] cols = value.toString().split(",");
-      DefaultHCatRecord record = new DefaultHCatRecord(3);
-      record.set(0, Integer.parseInt(cols[0]));
-      record.set(1, cols[1]);
-      record.set(2, cols[2]);
-      context.write(NullWritable.get(), record);
-    }
-  }
-
-  private HCatSchema getSchema() throws HCatException {
-    HCatSchema schema = new HCatSchema(new ArrayList<HCatFieldSchema>());
-    schema.append(new HCatFieldSchema("a0", HCatFieldSchema.Type.INT,
-        ""));
-    schema.append(new HCatFieldSchema("a1",
-        HCatFieldSchema.Type.STRING, ""));
-    schema.append(new HCatFieldSchema("a2",
-        HCatFieldSchema.Type.STRING, ""));
-    return schema;
-  }
-
-}
diff --git a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveCompatibility.java b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveCompatibility.java
deleted file mode 100644
index c5dfa43..0000000
--- a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveCompatibility.java
+++ /dev/null
@@ -1,129 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.hive.hcatalog.mapreduce;
-
-import java.io.File;
-import java.io.FileWriter;
-import java.util.Arrays;
-import java.util.Iterator;
-
-import junit.framework.Assert;
-import org.apache.hadoop.hive.metastore.api.Partition;
-import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hive.hcatalog.common.HCatConstants;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestHCatHiveCompatibility extends HCatBaseTest {
-  private static final String INPUT_FILE_NAME = TEST_DATA_DIR + "/input.data";
-
-  @BeforeClass
-  public static void createInputData() throws Exception {
-    int LOOP_SIZE = 11;
-    File file = new File(INPUT_FILE_NAME);
-    file.deleteOnExit();
-    FileWriter writer = new FileWriter(file);
-    for (int i = 0; i < LOOP_SIZE; i++) {
-      writer.write(i + "\t1\n");
-    }
-    writer.close();
-  }
-
-  @Test
-  public void testUnpartedReadWrite() throws Exception {
-
-    driver.run("drop table if exists junit_unparted_noisd");
-    String createTable = "create table junit_unparted_noisd(a int) stored as RCFILE";
-    Assert.assertEquals(0, driver.run(createTable).getResponseCode());
-
-    // assert that the table created has no hcat instrumentation, and that we're still able to read it.
-    Table table = client.getTable("default", "junit_unparted_noisd");
-    Assert.assertTrue(table.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
-
-    PigServer server = new PigServer(ExecType.LOCAL);
-    logAndRegister(server, "A = load '" + INPUT_FILE_NAME + "' as (a:int);");
-    logAndRegister(server, "store A into 'default.junit_unparted_noisd' using org.apache.hive.hcatalog.pig.HCatStorer();");
-    logAndRegister(server, "B = load 'default.junit_unparted_noisd' using org.apache.hive.hcatalog.pig.HCatLoader();");
-    Iterator<Tuple> itr = server.openIterator("B");
-
-    int i = 0;
-
-    while (itr.hasNext()) {
-      Tuple t = itr.next();
-      Assert.assertEquals(1, t.size());
-      Assert.assertEquals(t.get(0), i);
-      i++;
-    }
-
-    Assert.assertFalse(itr.hasNext());
-    Assert.assertEquals(11, i);
-
-    // assert that the table created still has no hcat instrumentation
-    Table table2 = client.getTable("default", "junit_unparted_noisd");
-    Assert.assertTrue(table2.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
-
-    driver.run("drop table junit_unparted_noisd");
-  }
-
-  @Test
-  public void testPartedRead() throws Exception {
-
-    driver.run("drop table if exists junit_parted_noisd");
-    String createTable = "create table junit_parted_noisd(a int) partitioned by (b string) stored as RCFILE";
-    Assert.assertEquals(0, driver.run(createTable).getResponseCode());
-
-    // assert that the table created has no hcat instrumentation, and that we're still able to read it.
-    Table table = client.getTable("default", "junit_parted_noisd");
-    Assert.assertTrue(table.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
-
-    PigServer server = new PigServer(ExecType.LOCAL);
-    logAndRegister(server, "A = load '" + INPUT_FILE_NAME + "' as (a:int);");
-    logAndRegister(server, "store A into 'default.junit_parted_noisd' using org.apache.hive.hcatalog.pig.HCatStorer('b=42');");
-    logAndRegister(server, "B = load 'default.junit_parted_noisd' using org.apache.hive.hcatalog.pig.HCatLoader();");
-    Iterator<Tuple> itr = server.openIterator("B");
-
-    int i = 0;
-
-    while (itr.hasNext()) {
-      Tuple t = itr.next();
-      Assert.assertEquals(2, t.size()); // Contains explicit field "a" and partition "b".
-      Assert.assertEquals(t.get(0), i);
-      Assert.assertEquals(t.get(1), "42");
-      i++;
-    }
-
-    Assert.assertFalse(itr.hasNext());
-    Assert.assertEquals(11, i);
-
-    // assert that the table created still has no hcat instrumentation
-    Table table2 = client.getTable("default", "junit_parted_noisd");
-    Assert.assertTrue(table2.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
-
-    // assert that there is one partition present, and it had hcat instrumentation inserted when it was created.
-    Partition ptn = client.getPartition("default", "junit_parted_noisd", Arrays.asList("42"));
-
-    Assert.assertNotNull(ptn);
-    Assert.assertTrue(ptn.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
-    driver.run("drop table junit_unparted_noisd");
-  }
-}
diff --git a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
deleted file mode 100644
index 470ff58..0000000
--- a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
+++ /dev/null
@@ -1,116 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.hive.hcatalog.mapreduce;
-
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.serde2.thrift.test.IntString;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.data.DataType;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.logicalLayer.schema.Schema;
-import org.apache.thrift.protocol.TBinaryProtocol;
-import org.apache.thrift.transport.TIOStreamTransport;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.ByteArrayOutputStream;
-import java.util.Iterator;
-
-public class TestHCatHiveThriftCompatibility extends HCatBaseTest {
-
-  private boolean setUpComplete = false;
-  private Path intStringSeq;
-
-  @Before
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    if (setUpComplete) {
-      return;
-    }
-
-    ByteArrayOutputStream out = new ByteArrayOutputStream();
-    TIOStreamTransport transport = new TIOStreamTransport(out);
-    TBinaryProtocol protocol = new TBinaryProtocol(transport);
-
-    IntString intString = new IntString(1, "one", 1);
-    intString.write(protocol);
-    BytesWritable bytesWritable = new BytesWritable(out.toByteArray());
-
-    intStringSeq = new Path(TEST_DATA_DIR + "/data/intString.seq");
-    LOG.info("Creating data file: " + intStringSeq);
-
-    SequenceFile.Writer seqFileWriter = SequenceFile.createWriter(
-        intStringSeq.getFileSystem(hiveConf), hiveConf, intStringSeq,
-        NullWritable.class, BytesWritable.class);
-    seqFileWriter.append(NullWritable.get(), bytesWritable);
-    seqFileWriter.close();
-
-    setUpComplete = true;
-  }
-
-  /**
-   *  Create a table with no explicit schema and ensure its correctly
-   *  discovered from the thrift struct.
-   */
-  @Test
-  public void testDynamicCols() throws Exception {
-    Assert.assertEquals(0, driver.run("drop table if exists test_thrift").getResponseCode());
-    Assert.assertEquals(0, driver.run(
-        "create external table test_thrift " +
-            "partitioned by (year string) " +
-            "row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' " +
-            "with serdeproperties ( " +
-            "  'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString', " +
-            "  'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') " +
-            "stored as" +
-            "  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'" +
-            "  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'")
-        .getResponseCode());
-    Assert.assertEquals(0,
-        driver.run("alter table test_thrift add partition (year = '2012') location '" +
-            intStringSeq.getParent() + "'").getResponseCode());
-
-    PigServer pigServer = new PigServer(ExecType.LOCAL);
-    pigServer.registerQuery("A = load 'test_thrift' using org.apache.hive.hcatalog.pig.HCatLoader();");
-
-    Schema expectedSchema = new Schema();
-    expectedSchema.add(new Schema.FieldSchema("myint", DataType.INTEGER));
-    expectedSchema.add(new Schema.FieldSchema("mystring", DataType.CHARARRAY));
-    expectedSchema.add(new Schema.FieldSchema("underscore_int", DataType.INTEGER));
-    expectedSchema.add(new Schema.FieldSchema("year", DataType.CHARARRAY));
-
-    Assert.assertEquals(expectedSchema, pigServer.dumpSchema("A"));
-
-    Iterator<Tuple> iterator = pigServer.openIterator("A");
-    Tuple t = iterator.next();
-    Assert.assertEquals(1, t.get(0));
-    Assert.assertEquals("one", t.get(1));
-    Assert.assertEquals(1, t.get(2));
-    Assert.assertEquals("2012", t.get(3));
-
-    Assert.assertFalse(iterator.hasNext());
-  }
-}
diff --git a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
deleted file mode 100644
index e567c23..0000000
--- a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
+++ /dev/null
@@ -1,275 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.hive.hcatalog.mapreduce;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertTrue;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hive.hcatalog.HcatTestUtils;
-import org.apache.hive.hcatalog.common.HCatConstants;
-import org.apache.hive.hcatalog.common.HCatException;
-import org.apache.hive.hcatalog.common.HCatUtil;
-import org.apache.hive.hcatalog.data.DefaultHCatRecord;
-import org.apache.hive.hcatalog.data.schema.HCatFieldSchema;
-import org.apache.hive.hcatalog.data.schema.HCatSchema;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.data.Tuple;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestSequenceFileReadWrite {
-
-  private File dataDir;
-  private String warehouseDir;
-  private String inputFileName;
-  private Driver driver;
-  private PigServer server;
-  private String[] input;
-  private HiveConf hiveConf;
-
-  @Before
-  public void setup() throws Exception {
-    dataDir = new File(System.getProperty("java.io.tmpdir") + File.separator +
-        TestSequenceFileReadWrite.class.getCanonicalName() + "-" + System.currentTimeMillis());
-    hiveConf = new HiveConf(this.getClass());
-    warehouseDir = new File(dataDir, "warehouse").getAbsolutePath();
-    inputFileName = new File(dataDir, "input.data").getAbsolutePath();
-    hiveConf = new HiveConf(this.getClass());
-    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehouseDir);
-    driver = new Driver(hiveConf);
-    SessionState.start(new CliSessionState(hiveConf));
-
-    if(!(new File(warehouseDir).mkdirs())) {
-      throw new RuntimeException("Could not create " + warehouseDir);
-    }
-
-    int numRows = 3;
-    input = new String[numRows];
-    for (int i = 0; i < numRows; i++) {
-      String col1 = "a" + i;
-      String col2 = "b" + i;
-      input[i] = i + "," + col1 + "," + col2;
-    }
-    HcatTestUtils.createTestDataFile(inputFileName, input);
-    server = new PigServer(ExecType.LOCAL);
-  }
-  @After
-  public void teardown() throws IOException {
-    if(dataDir != null) {
-      FileUtils.deleteDirectory(dataDir);
-    }
-  }
-
-  @Test
-  public void testSequenceTableWriteRead() throws Exception {
-    String createTable = "CREATE TABLE demo_table(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
-    driver.run("drop table demo_table");
-    int retCode1 = driver.run(createTable).getResponseCode();
-    assertTrue(retCode1 == 0);
-
-    server.setBatchOn();
-    server.registerQuery("A = load '"
-        + inputFileName
-        + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
-    server.registerQuery("store A into 'demo_table' using org.apache.hive.hcatalog.pig.HCatStorer();");
-    server.executeBatch();
-
-    server.registerQuery("B = load 'demo_table' using org.apache.hive.hcatalog.pig.HCatLoader();");
-    Iterator<Tuple> XIter = server.openIterator("B");
-    int numTuplesRead = 0;
-    while (XIter.hasNext()) {
-      Tuple t = XIter.next();
-      assertEquals(3, t.size());
-      assertEquals(t.get(0).toString(), "" + numTuplesRead);
-      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
-      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
-      numTuplesRead++;
-    }
-    assertEquals(input.length, numTuplesRead);
-  }
-
-  @Test
-  public void testTextTableWriteRead() throws Exception {
-    String createTable = "CREATE TABLE demo_table_1(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
-    driver.run("drop table demo_table_1");
-    int retCode1 = driver.run(createTable).getResponseCode();
-    assertTrue(retCode1 == 0);
-
-    server.setBatchOn();
-    server.registerQuery("A = load '"
-        + inputFileName
-        + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
-    server.registerQuery("store A into 'demo_table_1' using org.apache.hive.hcatalog.pig.HCatStorer();");
-    server.executeBatch();
-
-    server.registerQuery("B = load 'demo_table_1' using org.apache.hive.hcatalog.pig.HCatLoader();");
-    Iterator<Tuple> XIter = server.openIterator("B");
-    int numTuplesRead = 0;
-    while (XIter.hasNext()) {
-      Tuple t = XIter.next();
-      assertEquals(3, t.size());
-      assertEquals(t.get(0).toString(), "" + numTuplesRead);
-      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
-      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
-      numTuplesRead++;
-    }
-    assertEquals(input.length, numTuplesRead);
-  }
-
-  @Test
-  public void testSequenceTableWriteReadMR() throws Exception {
-    String createTable = "CREATE TABLE demo_table_2(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
-    driver.run("drop table demo_table_2");
-    int retCode1 = driver.run(createTable).getResponseCode();
-    assertTrue(retCode1 == 0);
-
-    Configuration conf = new Configuration();
-    conf.set(HCatConstants.HCAT_KEY_HIVE_CONF,
-        HCatUtil.serialize(hiveConf.getAllProperties()));
-    Job job = new Job(conf, "Write-hcat-seq-table");
-    job.setJarByClass(TestSequenceFileReadWrite.class);
-
-    job.setMapperClass(Map.class);
-    job.setOutputKeyClass(NullWritable.class);
-    job.setOutputValueClass(DefaultHCatRecord.class);
-    job.setInputFormatClass(TextInputFormat.class);
-    TextInputFormat.setInputPaths(job, inputFileName);
-
-    HCatOutputFormat.setOutput(job, OutputJobInfo.create(
-        MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_2", null));
-    job.setOutputFormatClass(HCatOutputFormat.class);
-    HCatOutputFormat.setSchema(job, getSchema());
-    job.setNumReduceTasks(0);
-    assertTrue(job.waitForCompletion(true));
-    if (!HCatUtil.isHadoop23()) {
-      new FileOutputCommitterContainer(job, null).commitJob(job);
-    }
-    assertTrue(job.isSuccessful());
-
-    server.setBatchOn();
-    server.registerQuery("C = load 'default.demo_table_2' using org.apache.hive.hcatalog.pig.HCatLoader();");
-    server.executeBatch();
-    Iterator<Tuple> XIter = server.openIterator("C");
-    int numTuplesRead = 0;
-    while (XIter.hasNext()) {
-      Tuple t = XIter.next();
-      assertEquals(3, t.size());
-      assertEquals(t.get(0).toString(), "" + numTuplesRead);
-      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
-      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
-      numTuplesRead++;
-    }
-    assertEquals(input.length, numTuplesRead);
-  }
-
-  @Test
-  public void testTextTableWriteReadMR() throws Exception {
-    String createTable = "CREATE TABLE demo_table_3(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
-    driver.run("drop table demo_table_3");
-    int retCode1 = driver.run(createTable).getResponseCode();
-    assertTrue(retCode1 == 0);
-
-    Configuration conf = new Configuration();
-    conf.set(HCatConstants.HCAT_KEY_HIVE_CONF,
-        HCatUtil.serialize(hiveConf.getAllProperties()));
-    Job job = new Job(conf, "Write-hcat-text-table");
-    job.setJarByClass(TestSequenceFileReadWrite.class);
-
-    job.setMapperClass(Map.class);
-    job.setOutputKeyClass(NullWritable.class);
-    job.setOutputValueClass(DefaultHCatRecord.class);
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setNumReduceTasks(0);
-    TextInputFormat.setInputPaths(job, inputFileName);
-
-    HCatOutputFormat.setOutput(job, OutputJobInfo.create(
-        MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_3", null));
-    job.setOutputFormatClass(HCatOutputFormat.class);
-    HCatOutputFormat.setSchema(job, getSchema());
-    assertTrue(job.waitForCompletion(true));
-    if (!HCatUtil.isHadoop23()) {
-      new FileOutputCommitterContainer(job, null).commitJob(job);
-    }
-    assertTrue(job.isSuccessful());
-
-    server.setBatchOn();
-    server.registerQuery("D = load 'default.demo_table_3' using org.apache.hive.hcatalog.pig.HCatLoader();");
-    server.executeBatch();
-    Iterator<Tuple> XIter = server.openIterator("D");
-    int numTuplesRead = 0;
-    while (XIter.hasNext()) {
-      Tuple t = XIter.next();
-      assertEquals(3, t.size());
-      assertEquals(t.get(0).toString(), "" + numTuplesRead);
-      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
-      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
-      numTuplesRead++;
-    }
-    assertEquals(input.length, numTuplesRead);
-  }
-
-
-  public static class Map extends Mapper<LongWritable, Text, NullWritable, DefaultHCatRecord> {
-
-    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
-      String[] cols = value.toString().split(",");
-      DefaultHCatRecord record = new DefaultHCatRecord(3);
-      record.set(0, Integer.parseInt(cols[0]));
-      record.set(1, cols[1]);
-      record.set(2, cols[2]);
-      context.write(NullWritable.get(), record);
-    }
-  }
-
-  private HCatSchema getSchema() throws HCatException {
-    HCatSchema schema = new HCatSchema(new ArrayList<HCatFieldSchema>());
-    schema.append(new HCatFieldSchema("a0", HCatFieldSchema.Type.INT,
-        ""));
-    schema.append(new HCatFieldSchema("a1",
-        HCatFieldSchema.Type.STRING, ""));
-    schema.append(new HCatFieldSchema("a2",
-        HCatFieldSchema.Type.STRING, ""));
-    return schema;
-  }
-
-}
diff --git a/src/hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java b/src/hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java
deleted file mode 100644
index 6bd278e..0000000
--- a/src/hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java
+++ /dev/null
@@ -1,361 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.hive.hcatalog.hbase;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertTrue;
-
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.net.URI;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
-import org.apache.hadoop.hbase.client.Put;
-import org.apache.hadoop.hbase.client.Result;
-import org.apache.hadoop.hbase.client.ResultScanner;
-import org.apache.hadoop.hbase.client.Scan;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.data.DataType;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.logicalLayer.schema.Schema;
-import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
-import org.junit.Test;
-
-public class TestPigHBaseStorageHandler extends SkeletonHBaseTest {
-
-  private static HiveConf   hcatConf;
-  private static Driver driver;
-  private static String mypath;
-
-  private final byte[] FAMILY     = Bytes.toBytes("testFamily");
-  private final byte[] QUALIFIER1 = Bytes.toBytes("testQualifier1");
-  private final byte[] QUALIFIER2 = Bytes.toBytes("testQualifier2");
-
-  public void Initialize() throws Exception {
-
-    hcatConf = new HiveConf(this.getClass());
-    //hcatConf.set(ConfVars.SEMANTIC_ANALYZER_HOOK.varname,
-    //		HCatSemanticAnalyzer.class.getName());
-    URI fsuri = getFileSystem().getUri();
-    Path whPath = new Path(fsuri.getScheme(), fsuri.getAuthority(),
-        getTestDir());
-    hcatConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    hcatConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    hcatConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-    hcatConf.set(ConfVars.METASTOREWAREHOUSE.varname, whPath.toString());
-
-    //Add hbase properties
-    for (Map.Entry<String, String> el : getHbaseConf()) {
-      if (el.getKey().startsWith("hbase.")) {
-        hcatConf.set(el.getKey(), el.getValue());
-      }
-    }
-
-    driver = new Driver(hcatConf);
-    SessionState.start(new CliSessionState(hcatConf));
-
-  }
-
-  private void populateHBaseTable(String tName) throws IOException {
-    List<Put> myPuts = generatePuts(tName);
-    HTable table = new HTable(getHbaseConf(), Bytes.toBytes(tName));
-    table.put(myPuts);
-  }
-
-  private List<Put> generatePuts(String tableName) throws IOException {
-
-    List<String> columnFamilies = Arrays.asList("testFamily");
-    List<Put> myPuts;
-    myPuts = new ArrayList<Put>();
-    for (int i = 1; i <=10; i++) {
-      Put put = new Put(Bytes.toBytes(i));
-      put.add(FAMILY, QUALIFIER1, 1, Bytes.toBytes("textA-" + i));
-      put.add(FAMILY, QUALIFIER2, 1, Bytes.toBytes("textB-" + i));
-      myPuts.add(put);
-    }
-    return myPuts;
-  }
-
-  public static void createTestDataFile(String filename) throws IOException {
-    FileWriter writer = null;
-    int LOOP_SIZE = 10;
-    float f = -100.1f;
-    try {
-      File file = new File(filename);
-      file.deleteOnExit();
-      writer = new FileWriter(file);
-
-      for (int i =1; i <= LOOP_SIZE; i++) {
-        writer.write(i+ "\t" +(f+i)+ "\t" + "textB-" + i + "\n");
-      }
-    } finally {
-      if (writer != null) {
-        writer.close();
-      }
-    }
-
-  }
-
-  @Test
-  public void testPigHBaseSchema() throws Exception {
-    Initialize();
-
-    String tableName = newTableName("MyTable");
-    String databaseName = newTableName("MyDatabase");
-    //Table name will be lower case unless specified by hbase.table.name property
-    String hbaseTableName = "testTable";
-    String db_dir = getTestDir() + "/hbasedb";
-
-    String dbQuery = "CREATE DATABASE IF NOT EXISTS " + databaseName + " LOCATION '"
-        + db_dir + "'";
-
-    String deleteQuery = "DROP TABLE "+databaseName+"."+tableName;
-
-    String tableQuery = "CREATE TABLE " + databaseName + "." + tableName
-        + "(key float, testqualifier1 string, testqualifier2 int) STORED BY " +
-        "'org.apache.hadoop.hive.hbase.HBaseStorageHandler'"
-        + " WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,testFamily:testQualifier1,testFamily:testQualifier2')"
-        +  " TBLPROPERTIES ('hbase.table.name'='"+hbaseTableName+"')";
-
-    CommandProcessorResponse responseOne = driver.run(deleteQuery);
-    assertEquals(0, responseOne.getResponseCode());
-
-
-    CommandProcessorResponse responseTwo = driver.run(dbQuery);
-    assertEquals(0, responseTwo.getResponseCode());
-
-
-    CommandProcessorResponse responseThree = driver.run(tableQuery);
-
-    HBaseAdmin hAdmin = new HBaseAdmin(getHbaseConf());
-    boolean doesTableExist = hAdmin.tableExists(hbaseTableName);
-    assertTrue(doesTableExist);
-
-    PigServer server = new PigServer(ExecType.LOCAL,hcatConf.getAllProperties());
-    server.registerQuery("A = load '"+databaseName+"."+tableName+"' using org.apache.hive.hcatalog.pig.HCatLoader();");
-
-    Schema dumpedASchema = server.dumpSchema("A");
-
-    List<FieldSchema> fields = dumpedASchema.getFields();
-    assertEquals(3, fields.size());
-
-    assertEquals(DataType.FLOAT,fields.get(0).type);
-    assertEquals("key",fields.get(0).alias.toLowerCase());
-
-    assertEquals( DataType.CHARARRAY,fields.get(1).type);
-    assertEquals("testQualifier1".toLowerCase(), fields.get(1).alias.toLowerCase());
-
-    assertEquals( DataType.INTEGER,fields.get(2).type);
-    assertEquals("testQualifier2".toLowerCase(), fields.get(2).alias.toLowerCase());
-
-  }
-
-
-  @Test
-  public void testPigFilterProjection() throws Exception {
-    Initialize();
-
-    String tableName = newTableName("MyTable");
-    String databaseName = newTableName("MyDatabase");
-    //Table name will be lower case unless specified by hbase.table.name property
-    String hbaseTableName = (databaseName + "." + tableName).toLowerCase();
-    String db_dir = getTestDir() + "/hbasedb";
-
-    String dbQuery = "CREATE DATABASE IF NOT EXISTS " + databaseName + " LOCATION '"
-        + db_dir + "'";
-
-    String deleteQuery = "DROP TABLE "+databaseName+"."+tableName;
-
-    String tableQuery = "CREATE TABLE " + databaseName + "." + tableName
-        + "(key int, testqualifier1 string, testqualifier2 string) STORED BY " +
-        "'org.apache.hadoop.hive.hbase.HBaseStorageHandler'" +
-        " WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,testFamily:testQualifier1,testFamily:testQualifier2')" +
-        " TBLPROPERTIES ('hbase.table.default.storage.type'='binary')";
-
-    CommandProcessorResponse responseOne = driver.run(deleteQuery);
-    assertEquals(0, responseOne.getResponseCode());
-
-
-    CommandProcessorResponse responseTwo = driver.run(dbQuery);
-    assertEquals(0, responseTwo.getResponseCode());
-
-
-    CommandProcessorResponse responseThree = driver.run(tableQuery);
-
-    HBaseAdmin hAdmin = new HBaseAdmin(getHbaseConf());
-    boolean doesTableExist = hAdmin.tableExists(hbaseTableName);
-    assertTrue(doesTableExist);
-
-    populateHBaseTable(hbaseTableName);
-
-    Configuration conf = new Configuration(getHbaseConf());
-    HTable table = new HTable(conf, hbaseTableName);
-    Scan scan = new Scan();
-    scan.addFamily(Bytes.toBytes("testFamily"));
-    ResultScanner scanner = table.getScanner(scan);
-    int index=1;
-
-    PigServer server = new PigServer(ExecType.LOCAL,hcatConf.getAllProperties());
-    server.registerQuery("A = load '"+databaseName+"."+tableName+"' using org.apache.hive.hcatalog.pig.HCatLoader();");
-    server.registerQuery("B = filter A by key < 5;");
-    server.registerQuery("C = foreach B generate key,testqualifier2;");
-    Iterator<Tuple> itr = server.openIterator("C");
-    //verify if the filter is correct and returns 2 rows and contains 2 columns and the contents match
-    while(itr.hasNext()){
-      Tuple t = itr.next();
-      assertTrue(t.size() == 2);
-      assertTrue(t.get(0).getClass() == Integer.class);
-      assertEquals(index,t.get(0));
-      assertTrue(t.get(1).getClass() == String.class);
-      assertEquals("textB-"+index,t.get(1));
-      index++;
-    }
-    assertEquals(index-1,4);
-  }
-
-  @Test
-  public void testPigPopulation() throws Exception {
-    Initialize();
-
-    String tableName = newTableName("MyTable");
-    String databaseName = newTableName("MyDatabase");
-    //Table name will be lower case unless specified by hbase.table.name property
-    String hbaseTableName = (databaseName + "." + tableName).toLowerCase();
-    String db_dir = getTestDir() + "/hbasedb";
-    String POPTXT_FILE_NAME = db_dir+"testfile.txt";
-    float f = -100.1f;
-
-    String dbQuery = "CREATE DATABASE IF NOT EXISTS " + databaseName + " LOCATION '"
-        + db_dir + "'";
-
-    String deleteQuery = "DROP TABLE "+databaseName+"."+tableName;
-
-    String tableQuery = "CREATE TABLE " + databaseName + "." + tableName
-        + "(key int, testqualifier1 float, testqualifier2 string) STORED BY " +
-        "'org.apache.hadoop.hive.hbase.HBaseStorageHandler'"
-        + " WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,testFamily:testQualifier1,testFamily:testQualifier2')"
-        + " TBLPROPERTIES ('hbase.table.default.storage.type'='binary')";
-
-
-    String selectQuery = "SELECT * from "+databaseName.toLowerCase()+"."+tableName.toLowerCase();
-
-
-    CommandProcessorResponse responseOne = driver.run(deleteQuery);
-    assertEquals(0, responseOne.getResponseCode());
-
-
-    CommandProcessorResponse responseTwo = driver.run(dbQuery);
-    assertEquals(0, responseTwo.getResponseCode());
-
-
-    CommandProcessorResponse responseThree = driver.run(tableQuery);
-
-    HBaseAdmin hAdmin = new HBaseAdmin(getHbaseConf());
-    boolean doesTableExist = hAdmin.tableExists(hbaseTableName);
-    assertTrue(doesTableExist);
-
-
-    createTestDataFile(POPTXT_FILE_NAME);
-
-    PigServer server = new PigServer(ExecType.LOCAL,hcatConf.getAllProperties());
-    server.registerQuery("A = load '"+POPTXT_FILE_NAME+"' using PigStorage() as (key:int, testqualifier1:float, testqualifier2:chararray);");
-    server.registerQuery("B = filter A by (key > 2) AND (key < 8) ;");
-    server.registerQuery("store B into '"+databaseName.toLowerCase()+"."+tableName.toLowerCase()+"' using  org.apache.hive.hcatalog.pig.HCatStorer();");
-    server.registerQuery("C = load '"+databaseName.toLowerCase()+"."+tableName.toLowerCase()+"' using org.apache.hive.hcatalog.pig.HCatLoader();");
-    // Schema should be same
-    Schema dumpedBSchema = server.dumpSchema("C");
-
-    List<FieldSchema> fields = dumpedBSchema.getFields();
-    assertEquals(3, fields.size());
-
-    assertEquals(DataType.INTEGER,fields.get(0).type);
-    assertEquals("key",fields.get(0).alias.toLowerCase());
-
-    assertEquals( DataType.FLOAT,fields.get(1).type);
-    assertEquals("testQualifier1".toLowerCase(), fields.get(1).alias.toLowerCase());
-
-    assertEquals( DataType.CHARARRAY,fields.get(2).type);
-    assertEquals("testQualifier2".toLowerCase(), fields.get(2).alias.toLowerCase());
-
-    //Query the hbase table and check the key is valid and only 5  are present
-    Configuration conf = new Configuration(getHbaseConf());
-    HTable table = new HTable(conf, hbaseTableName);
-    Scan scan = new Scan();
-    scan.addFamily(Bytes.toBytes("testFamily"));
-    byte[] familyNameBytes = Bytes.toBytes("testFamily");
-    ResultScanner scanner = table.getScanner(scan);
-    int index=3;
-    int count=0;
-    for(Result result: scanner) {
-      //key is correct
-      assertEquals(index,Bytes.toInt(result.getRow()));
-      //first column exists
-      assertTrue(result.containsColumn(familyNameBytes,Bytes.toBytes("testQualifier1")));
-      //value is correct
-      assertEquals((index+f),Bytes.toFloat(result.getValue(familyNameBytes,Bytes.toBytes("testQualifier1"))),0);
-
-      //second column exists
-      assertTrue(result.containsColumn(familyNameBytes,Bytes.toBytes("testQualifier2")));
-      //value is correct
-      assertEquals(("textB-"+index).toString(),Bytes.toString(result.getValue(familyNameBytes,Bytes.toBytes("testQualifier2"))));
-      index++;
-      count++;
-    }
-    // 5 rows should be returned
-    assertEquals(count,5);
-
-    //Check if hive returns results correctly
-    driver.run(selectQuery);
-    ArrayList<String> result = new ArrayList<String>();
-    driver.getResults(result);
-    //Query using the hive command line
-    assertEquals(5, result.size());
-    Iterator<String> itr = result.iterator();
-    for(int i = 3; i <= 7; i++) {
-      String tokens[] = itr.next().split("\\s+");
-      assertEquals(i,Integer.parseInt(tokens[0]));
-      assertEquals(i+f,Float.parseFloat(tokens[1]),0);
-      assertEquals(("textB-"+i).toString(),tokens[2]);
-    }
-
-    //delete the table from the database
-    CommandProcessorResponse responseFour = driver.run(deleteQuery);
-    assertEquals(0, responseFour.getResponseCode());
-
-  }
-
-}
diff --git a/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableListObjectInspector1.java b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableListObjectInspector1.java
new file mode 100644
index 0000000..245e4f5
--- /dev/null
+++ b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableListObjectInspector1.java
@@ -0,0 +1,67 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.serde2;
+
+import java.util.List;
+
+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+
+public class CustomNonSettableListObjectInspector1  implements ListObjectInspector {
+
+  private ObjectInspector listElementObjectInspector;
+
+  protected CustomNonSettableListObjectInspector1() {
+    super();
+  }
+  protected CustomNonSettableListObjectInspector1(
+      ObjectInspector listElementObjectInspector) {
+    this.listElementObjectInspector = listElementObjectInspector;
+  }
+
+  public final Category getCategory() {
+    return Category.LIST;
+  }
+
+  // without data
+  public ObjectInspector getListElementObjectInspector() {
+    return listElementObjectInspector;
+  }
+
+  // Not supported for the test case
+  public Object getListElement(Object data, int index) {
+    return null;
+  }
+
+  // Not supported for the test case
+  public int getListLength(Object data) {
+    return 0;
+  }
+
+  // Not supported for the test case
+  public List<?> getList(Object data) {
+    return null;
+  }
+
+  public String getTypeName() {
+    return org.apache.hadoop.hive.serde.serdeConstants.LIST_TYPE_NAME + "<"
+        + listElementObjectInspector.getTypeName() + ">";
+  }
+}
+
diff --git a/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableStructObjectInspector1.java b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableStructObjectInspector1.java
new file mode 100644
index 0000000..c09fd61
--- /dev/null
+++ b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableStructObjectInspector1.java
@@ -0,0 +1,137 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.serde2;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.objectinspector.StructField;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+
+public class CustomNonSettableStructObjectInspector1 extends
+StructObjectInspector {
+  public static final Log LOG = LogFactory
+      .getLog(CustomNonSettableStructObjectInspector1.class.getName());
+
+  protected static class MyField implements StructField {
+    protected int fieldID;
+    protected String fieldName;
+    protected ObjectInspector fieldObjectInspector;
+    protected String fieldComment;
+
+    protected MyField() {
+      super();
+    }
+
+    public MyField(int fieldID, String fieldName,
+        ObjectInspector fieldObjectInspector) {
+      this.fieldID = fieldID;
+      this.fieldName = fieldName.toLowerCase();
+      this.fieldObjectInspector = fieldObjectInspector;
+    }
+
+    public MyField(int fieldID, String fieldName,
+        ObjectInspector fieldObjectInspector, String fieldComment) {
+      this(fieldID, fieldName, fieldObjectInspector);
+      this.fieldComment = fieldComment;
+    }
+
+    public int getFieldID() {
+      return fieldID;
+    }
+
+    public String getFieldName() {
+      return fieldName;
+    }
+
+    public ObjectInspector getFieldObjectInspector() {
+      return fieldObjectInspector;
+    }
+
+    public String getFieldComment() {
+      return fieldComment;
+    }
+
+    @Override
+    public String toString() {
+      return "" + fieldID + ":" + fieldName;
+    }
+  }
+
+  protected List<MyField> fields;
+
+  protected CustomNonSettableStructObjectInspector1() {
+    super();
+  }
+  /**
+   * Call ObjectInspectorFactory.getNonSettableStructObjectInspector instead.
+   */
+  protected CustomNonSettableStructObjectInspector1(List<String> structFieldNames,
+      List<ObjectInspector> structFieldObjectInspectors) {
+    init(structFieldNames, structFieldObjectInspectors);
+  }
+
+  protected void init(List<String> structFieldNames,
+      List<ObjectInspector> structFieldObjectInspectors) {
+    assert (structFieldNames.size() == structFieldObjectInspectors.size());
+
+    fields = new ArrayList<MyField>(structFieldNames.size());
+    for (int i = 0; i < structFieldNames.size(); i++) {
+      fields.add(new MyField(i, structFieldNames.get(i),
+          structFieldObjectInspectors.get(i), null));
+    }
+  }
+
+  public String getTypeName() {
+    return ObjectInspectorUtils.getStandardStructTypeName(this);
+  }
+
+  public final Category getCategory() {
+    return Category.STRUCT;
+  }
+
+  // Without Data
+  @Override
+  public StructField getStructFieldRef(String fieldName) {
+    return ObjectInspectorUtils.getStandardStructFieldRef(fieldName, fields);
+  }
+
+  @Override
+  public List<? extends StructField> getAllStructFieldRefs() {
+    return fields;
+  }
+
+  // With Data - Unsupported for the test case
+  @Override
+  @SuppressWarnings("unchecked")
+  public Object getStructFieldData(Object data, StructField fieldRef) {
+    return null;
+  }
+
+  // Unsupported for the test case
+  @Override
+  @SuppressWarnings("unchecked")
+  public List<Object> getStructFieldsDataAsList(Object data) {
+    return null;
+  }
+}
diff --git a/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableUnionObjectInspector1.java b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableUnionObjectInspector1.java
new file mode 100644
index 0000000..8e9ce78
--- /dev/null
+++ b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomNonSettableUnionObjectInspector1.java
@@ -0,0 +1,82 @@
+package org.apache.hadoop.hive.serde2;
+
+import java.util.List;
+
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.UnionObject;
+import org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector;
+
+public class CustomNonSettableUnionObjectInspector1 implements UnionObjectInspector{
+
+  private final List<ObjectInspector> children;
+
+  public CustomNonSettableUnionObjectInspector1(List<ObjectInspector> children) {
+    this.children = children;
+  }
+
+  public static class StandardUnion implements UnionObject {
+    protected byte tag;
+    protected Object object;
+
+    public StandardUnion() {
+    }
+
+    public StandardUnion(byte tag, Object object) {
+      this.tag = tag;
+      this.object = object;
+    }
+
+    @Override
+    public Object getObject() {
+      return object;
+    }
+
+    @Override
+    public byte getTag() {
+      return tag;
+    }
+
+    @Override
+    public String toString() {
+      return tag + ":" + object;
+    }
+  }
+
+  /**
+   * Return the tag of the object.
+   */
+  public byte getTag(Object o) {
+    if (o == null) {
+      return -1;
+    }
+    return ((UnionObject) o).getTag();
+  }
+
+  /**
+   * Return the field based on the tag value associated with the Object.
+   */
+  public Object getField(Object o) {
+    if (o == null) {
+      return null;
+    }
+    return ((UnionObject) o).getObject();
+  }
+
+  public Category getCategory() {
+    return Category.UNION;
+  }
+
+  public String getTypeName() {
+    return null;
+  }
+
+  @Override
+  public String toString() {
+    return null;
+  }
+
+  @Override
+  public List<ObjectInspector> getObjectInspectors() {
+    return children;
+  }
+}
diff --git a/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe1.java b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe1.java
new file mode 100644
index 0000000..e1352c3
--- /dev/null
+++ b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe1.java
@@ -0,0 +1,113 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.serde2;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Properties;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+
+public class CustomSerDe1 extends AbstractSerDe {
+
+  int numColumns;
+
+  StructObjectInspector rowOI;
+  ArrayList<String> row;
+
+  @Override
+  public void initialize(Configuration conf, Properties tbl)
+      throws SerDeException {
+
+    // Read the configuration parameters
+    String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
+    String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
+
+    // The input column can either be a string or a list of integer values.
+    List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
+    List<TypeInfo> columnTypes = TypeInfoUtils
+        .getTypeInfosFromTypeString(columnTypeProperty);
+    assert columnNames.size() == columnTypes.size();
+    numColumns = columnNames.size();
+
+    // No exception for type checking for simplicity
+    // Constructing the row ObjectInspector:
+    // The row consists of some string columns, some Array<int> columns.
+    List<ObjectInspector> columnOIs = new ArrayList<ObjectInspector>(
+        columnNames.size());
+    for (int c = 0; c < numColumns; c++) {
+      if (columnTypes.get(c).equals(TypeInfoFactory.stringTypeInfo)) {
+        columnOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
+      } else {
+        // Blindly add this as a integer list, should be sufficient for the test case.
+        // Use the non-settable list object inspector.
+        columnOIs.add(new CustomNonSettableListObjectInspector1(
+            PrimitiveObjectInspectorFactory.javaIntObjectInspector));
+      }
+    }
+    // Use non-settable struct object inspector.
+    rowOI = new CustomNonSettableStructObjectInspector1(
+        columnNames, columnOIs);
+
+    // Constructing the row object, etc, which will be reused for all rows.
+    row = new ArrayList<String>(numColumns);
+    for (int c = 0; c < numColumns; c++) {
+      row.add(null);
+    }
+  }
+
+  @Override
+  public ObjectInspector getObjectInspector() throws SerDeException {
+    return rowOI;
+  }
+
+  @Override
+  public Class<? extends Writable> getSerializedClass() {
+    return Text.class;
+  }
+
+  @Override
+  public Object deserialize(Writable blob) throws SerDeException {
+    // Now all the column values should always return NULL!
+    return row;
+  }
+
+  @Override
+  public Writable serialize(Object obj, ObjectInspector objInspector)
+      throws SerDeException {
+    return null;
+  }
+
+  @Override
+  public SerDeStats getSerDeStats() {
+    // no support for statistics
+    return null;
+  }
+
+}
diff --git a/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe2.java b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe2.java
new file mode 100644
index 0000000..f85439c
--- /dev/null
+++ b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe2.java
@@ -0,0 +1,113 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.serde2;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Properties;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+
+public class CustomSerDe2 extends AbstractSerDe {
+
+  int numColumns;
+
+  StructObjectInspector rowOI;
+  ArrayList<String> row;
+
+  @Override
+  public void initialize(Configuration conf, Properties tbl)
+      throws SerDeException {
+
+    // Read the configuration parameters
+    String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
+    String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
+
+    // The input column can either be a string or a list of integer values.
+    List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
+    List<TypeInfo> columnTypes = TypeInfoUtils
+        .getTypeInfosFromTypeString(columnTypeProperty);
+    assert columnNames.size() == columnTypes.size();
+    numColumns = columnNames.size();
+
+    // No exception for type checking for simplicity
+    // Constructing the row ObjectInspector:
+    // The row consists of some string columns, some Array<int> columns.
+    List<ObjectInspector> columnOIs = new ArrayList<ObjectInspector>(
+        columnNames.size());
+    for (int c = 0; c < numColumns; c++) {
+      if (columnTypes.get(c).equals(TypeInfoFactory.stringTypeInfo)) {
+        columnOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
+      } else {
+        // Blindly add this as a integer list! Should be sufficient for the test case.
+        columnOIs.add(ObjectInspectorFactory.getStandardListObjectInspector(
+            PrimitiveObjectInspectorFactory.javaIntObjectInspector));
+      }
+    }
+    // StandardStruct uses ArrayList to store the row.
+    rowOI = ObjectInspectorFactory.getStandardStructObjectInspector(
+        columnNames, columnOIs);
+
+    // Constructing the row object, etc, which will be reused for all rows.
+    row = new ArrayList<String>(numColumns);
+    for (int c = 0; c < numColumns; c++) {
+      row.add(null);
+    }
+  }
+
+  @Override
+  public ObjectInspector getObjectInspector() throws SerDeException {
+    return rowOI;
+  }
+
+  @Override
+  public Class<? extends Writable> getSerializedClass() {
+    return Text.class;
+  }
+
+  @Override
+  public Object deserialize(Writable blob) throws SerDeException {
+    // Now all the column values should always return NULL!
+    return row;
+  }
+
+  @Override
+  public Writable serialize(Object obj, ObjectInspector objInspector)
+      throws SerDeException {
+    return null;
+  }
+
+  @Override
+  public SerDeStats getSerDeStats() {
+    // no support for statistics
+    return null;
+  }
+
+}
diff --git a/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe3.java b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe3.java
new file mode 100644
index 0000000..311718e
--- /dev/null
+++ b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe3.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.serde2;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Properties;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+
+public class CustomSerDe3 extends CustomSerDe1 {
+  @Override
+  public void initialize(Configuration conf, Properties tbl)
+      throws SerDeException {
+
+    // Read the configuration parameters
+    String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
+    String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
+
+    // The input column can either be a string or a list of list of integer values.
+    List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
+    List<TypeInfo> columnTypes = TypeInfoUtils
+        .getTypeInfosFromTypeString(columnTypeProperty);
+    assert columnNames.size() == columnTypes.size();
+    numColumns = columnNames.size();
+
+    // No exception for type checking for simplicity
+    // Constructing the row ObjectInspector:
+    // The row consists of some string columns, some Array<Array<int> > columns.
+    List<ObjectInspector> columnOIs = new ArrayList<ObjectInspector>(
+        columnNames.size());
+    for (int c = 0; c < numColumns; c++) {
+      if (columnTypes.get(c).equals(TypeInfoFactory.stringTypeInfo)) {
+        columnOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
+      } else {
+        // Blindly add this as a non settable list of list of integers,
+        // should be sufficient for the test case.
+        // Use the standard list object inspector.
+        columnOIs.add(ObjectInspectorFactory.getStandardListObjectInspector(
+            new CustomNonSettableListObjectInspector1(PrimitiveObjectInspectorFactory.javaIntObjectInspector)));
+      }
+    }
+    // Use non-settable struct object inspector.
+    rowOI = new CustomNonSettableStructObjectInspector1(
+        columnNames, columnOIs);
+
+    // Constructing the row object, etc, which will be reused for all rows.
+    row = new ArrayList<String>(numColumns);
+    for (int c = 0; c < numColumns; c++) {
+      row.add(null);
+    }
+  }
+}
diff --git a/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe4.java b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe4.java
new file mode 100644
index 0000000..281d959
--- /dev/null
+++ b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe4.java
@@ -0,0 +1,66 @@
+package org.apache.hadoop.hive.serde2;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Properties;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+
+public class CustomSerDe4 extends CustomSerDe2 {
+
+    @Override
+    public void initialize(Configuration conf, Properties tbl)
+        throws SerDeException {
+
+      // Read the configuration parameters
+      String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
+      String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
+
+      // The input column can either be a string or a list of integer values.
+      List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
+      List<TypeInfo> columnTypes = TypeInfoUtils
+          .getTypeInfosFromTypeString(columnTypeProperty);
+      assert columnNames.size() == columnTypes.size();
+      numColumns = columnNames.size();
+
+      // No exception for type checking for simplicity
+      // Constructing the row ObjectInspector:
+      // The row consists of string columns, double columns, some union<int, double> columns only.
+      List<ObjectInspector> columnOIs = new ArrayList<ObjectInspector>(
+          columnNames.size());
+      for (int c = 0; c < numColumns; c++) {
+        if (columnTypes.get(c).equals(TypeInfoFactory.stringTypeInfo)) {
+          columnOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
+        }
+        else if (columnTypes.get(c).equals(TypeInfoFactory.doubleTypeInfo)) {
+          columnOIs.add(PrimitiveObjectInspectorFactory.javaDoubleObjectInspector);
+        }
+        else {
+
+          // Blindly add this as a union type containing int and double!
+          // Should be sufficient for the test case.
+         List<ObjectInspector> unionOI =  new ArrayList<ObjectInspector>();
+         unionOI.add(PrimitiveObjectInspectorFactory.javaIntObjectInspector);
+         unionOI.add(PrimitiveObjectInspectorFactory.javaDoubleObjectInspector);
+          columnOIs.add(ObjectInspectorFactory.getStandardUnionObjectInspector(unionOI));
+        }
+      }
+      // StandardList uses ArrayList to store the row.
+      rowOI = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, columnOIs);
+
+      // Constructing the row object, etc, which will be reused for all rows.
+      row = new ArrayList<String>(numColumns);
+      for (int c = 0; c < numColumns; c++) {
+        row.add(null);
+      }
+    }
+
+}
diff --git a/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe5.java b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe5.java
new file mode 100644
index 0000000..169c8ab
--- /dev/null
+++ b/src/itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe5.java
@@ -0,0 +1,63 @@
+package org.apache.hadoop.hive.serde2;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Properties;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+
+public class CustomSerDe5 extends CustomSerDe4 {
+  @Override
+    public void initialize(Configuration conf, Properties tbl)
+        throws SerDeException {
+      // Read the configuration parameters
+      String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
+      String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
+
+      // The input column can either be a string or a list of integer values.
+      List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
+      List<TypeInfo> columnTypes = TypeInfoUtils
+          .getTypeInfosFromTypeString(columnTypeProperty);
+      assert columnNames.size() == columnTypes.size();
+      numColumns = columnNames.size();
+
+      // No exception for type checking for simplicity
+      // Constructing the row ObjectInspector:
+      // The row consists of string columns, double columns, some union<int, double> columns only.
+      List<ObjectInspector> columnOIs = new ArrayList<ObjectInspector>(
+          columnNames.size());
+      for (int c = 0; c < numColumns; c++) {
+        if (columnTypes.get(c).equals(TypeInfoFactory.stringTypeInfo)) {
+          columnOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
+        }
+        else if (columnTypes.get(c).equals(TypeInfoFactory.doubleTypeInfo)) {
+          columnOIs.add(PrimitiveObjectInspectorFactory.javaDoubleObjectInspector);
+        }
+        else {
+
+          // Blindly add this as a union type containing int and double!
+          // Should be sufficient for the test case.
+         List<ObjectInspector> unionOI =  new ArrayList<ObjectInspector>();
+         unionOI.add(PrimitiveObjectInspectorFactory.javaIntObjectInspector);
+         unionOI.add(PrimitiveObjectInspectorFactory.javaDoubleObjectInspector);
+          columnOIs.add(new CustomNonSettableUnionObjectInspector1(unionOI));
+        }
+      }
+      // StandardList uses ArrayList to store the row.
+      rowOI = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, columnOIs);
+
+      // Constructing the row object, etc, which will be reused for all rows.
+      row = new ArrayList<String>(numColumns);
+      for (int c = 0; c < numColumns; c++) {
+        row.add(null);
+      }
+    }
+}
diff --git a/src/itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveCompatibility.java b/src/itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveCompatibility.java
new file mode 100644
index 0000000..e1e7669
--- /dev/null
+++ b/src/itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveCompatibility.java
@@ -0,0 +1,132 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.hcatalog.mapreduce;
+
+import java.io.File;
+import java.io.FileWriter;
+import java.util.Arrays;
+import java.util.Iterator;
+
+import junit.framework.Assert;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hcatalog.common.HCatConstants;
+import org.apache.pig.ExecType;
+import org.apache.pig.PigServer;
+import org.apache.pig.data.Tuple;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/**
+ * @deprecated Use/modify {@link org.apache.hive.hcatalog.mapreduce.TestHCatHiveCompatibility} instead
+ */
+public class TestHCatHiveCompatibility extends HCatBaseTest {
+  private static final String INPUT_FILE_NAME = TEST_DATA_DIR + "/input.data";
+
+  @BeforeClass
+  public static void createInputData() throws Exception {
+    int LOOP_SIZE = 11;
+    File file = new File(INPUT_FILE_NAME);
+    file.deleteOnExit();
+    FileWriter writer = new FileWriter(file);
+    for (int i = 0; i < LOOP_SIZE; i++) {
+      writer.write(i + "\t1\n");
+    }
+    writer.close();
+  }
+
+  @Test
+  public void testUnpartedReadWrite() throws Exception {
+
+    driver.run("drop table if exists junit_unparted_noisd");
+    String createTable = "create table junit_unparted_noisd(a int) stored as RCFILE";
+    Assert.assertEquals(0, driver.run(createTable).getResponseCode());
+
+    // assert that the table created has no hcat instrumentation, and that we're still able to read it.
+    Table table = client.getTable("default", "junit_unparted_noisd");
+    Assert.assertTrue(table.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
+
+    PigServer server = new PigServer(ExecType.LOCAL);
+    logAndRegister(server, "A = load '" + INPUT_FILE_NAME + "' as (a:int);");
+    logAndRegister(server, "store A into 'default.junit_unparted_noisd' using org.apache.hcatalog.pig.HCatStorer();");
+    logAndRegister(server, "B = load 'default.junit_unparted_noisd' using org.apache.hcatalog.pig.HCatLoader();");
+    Iterator<Tuple> itr = server.openIterator("B");
+
+    int i = 0;
+
+    while (itr.hasNext()) {
+      Tuple t = itr.next();
+      Assert.assertEquals(1, t.size());
+      Assert.assertEquals(t.get(0), i);
+      i++;
+    }
+
+    Assert.assertFalse(itr.hasNext());
+    Assert.assertEquals(11, i);
+
+    // assert that the table created still has no hcat instrumentation
+    Table table2 = client.getTable("default", "junit_unparted_noisd");
+    Assert.assertTrue(table2.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
+
+    driver.run("drop table junit_unparted_noisd");
+  }
+
+  @Test
+  public void testPartedRead() throws Exception {
+
+    driver.run("drop table if exists junit_parted_noisd");
+    String createTable = "create table junit_parted_noisd(a int) partitioned by (b string) stored as RCFILE";
+    Assert.assertEquals(0, driver.run(createTable).getResponseCode());
+
+    // assert that the table created has no hcat instrumentation, and that we're still able to read it.
+    Table table = client.getTable("default", "junit_parted_noisd");
+    Assert.assertTrue(table.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
+
+    PigServer server = new PigServer(ExecType.LOCAL);
+    logAndRegister(server, "A = load '" + INPUT_FILE_NAME + "' as (a:int);");
+    logAndRegister(server, "store A into 'default.junit_parted_noisd' using org.apache.hcatalog.pig.HCatStorer('b=42');");
+    logAndRegister(server, "B = load 'default.junit_parted_noisd' using org.apache.hcatalog.pig.HCatLoader();");
+    Iterator<Tuple> itr = server.openIterator("B");
+
+    int i = 0;
+
+    while (itr.hasNext()) {
+      Tuple t = itr.next();
+      Assert.assertEquals(2, t.size()); // Contains explicit field "a" and partition "b".
+      Assert.assertEquals(t.get(0), i);
+      Assert.assertEquals(t.get(1), "42");
+      i++;
+    }
+
+    Assert.assertFalse(itr.hasNext());
+    Assert.assertEquals(11, i);
+
+    // assert that the table created still has no hcat instrumentation
+    Table table2 = client.getTable("default", "junit_parted_noisd");
+    Assert.assertTrue(table2.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
+
+    // assert that there is one partition present, and it had hcat instrumentation inserted when it was created.
+    Partition ptn = client.getPartition("default", "junit_parted_noisd", Arrays.asList("42"));
+
+    Assert.assertNotNull(ptn);
+    Assert.assertTrue(ptn.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
+    driver.run("drop table junit_unparted_noisd");
+  }
+}
diff --git a/src/itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java b/src/itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
new file mode 100644
index 0000000..28c11b8
--- /dev/null
+++ b/src/itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
@@ -0,0 +1,119 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.hcatalog.mapreduce;
+
+import junit.framework.Assert;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.serde2.thrift.test.IntString;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.pig.ExecType;
+import org.apache.pig.PigServer;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.transport.TIOStreamTransport;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.ByteArrayOutputStream;
+import java.util.Iterator;
+
+/**
+ * @deprecated Use/modify {@link org.apache.hive.hcatalog.mapreduce.TestHCatHiveThriftCompatibility} instead
+ */
+public class TestHCatHiveThriftCompatibility extends HCatBaseTest {
+
+  private boolean setUpComplete = false;
+  private Path intStringSeq;
+
+  @Before
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    if (setUpComplete) {
+      return;
+    }
+
+    ByteArrayOutputStream out = new ByteArrayOutputStream();
+    TIOStreamTransport transport = new TIOStreamTransport(out);
+    TBinaryProtocol protocol = new TBinaryProtocol(transport);
+
+    IntString intString = new IntString(1, "one", 1);
+    intString.write(protocol);
+    BytesWritable bytesWritable = new BytesWritable(out.toByteArray());
+
+    intStringSeq = new Path(TEST_DATA_DIR + "/data/intString.seq");
+    LOG.info("Creating data file: " + intStringSeq);
+
+    SequenceFile.Writer seqFileWriter = SequenceFile.createWriter(
+        intStringSeq.getFileSystem(hiveConf), hiveConf, intStringSeq,
+        NullWritable.class, BytesWritable.class);
+    seqFileWriter.append(NullWritable.get(), bytesWritable);
+    seqFileWriter.close();
+
+    setUpComplete = true;
+  }
+
+  /**
+   *  Create a table with no explicit schema and ensure its correctly
+   *  discovered from the thrift struct.
+   */
+  @Test
+  public void testDynamicCols() throws Exception {
+    Assert.assertEquals(0, driver.run("drop table if exists test_thrift").getResponseCode());
+    Assert.assertEquals(0, driver.run(
+        "create external table test_thrift " +
+            "partitioned by (year string) " +
+            "row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' " +
+            "with serdeproperties ( " +
+            "  'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString', " +
+            "  'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') " +
+            "stored as" +
+            "  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'" +
+            "  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'")
+        .getResponseCode());
+    Assert.assertEquals(0,
+        driver.run("alter table test_thrift add partition (year = '2012') location '" +
+            intStringSeq.getParent() + "'").getResponseCode());
+
+    PigServer pigServer = new PigServer(ExecType.LOCAL);
+    pigServer.registerQuery("A = load 'test_thrift' using org.apache.hcatalog.pig.HCatLoader();");
+
+    Schema expectedSchema = new Schema();
+    expectedSchema.add(new Schema.FieldSchema("myint", DataType.INTEGER));
+    expectedSchema.add(new Schema.FieldSchema("mystring", DataType.CHARARRAY));
+    expectedSchema.add(new Schema.FieldSchema("underscore_int", DataType.INTEGER));
+    expectedSchema.add(new Schema.FieldSchema("year", DataType.CHARARRAY));
+
+    Assert.assertEquals(expectedSchema, pigServer.dumpSchema("A"));
+
+    Iterator<Tuple> iterator = pigServer.openIterator("A");
+    Tuple t = iterator.next();
+    Assert.assertEquals(1, t.get(0));
+    Assert.assertEquals("one", t.get(1));
+    Assert.assertEquals(1, t.get(2));
+    Assert.assertEquals("2012", t.get(3));
+
+    Assert.assertFalse(iterator.hasNext());
+  }
+}
diff --git a/src/itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java b/src/itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java
new file mode 100644
index 0000000..61d14f1
--- /dev/null
+++ b/src/itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java
@@ -0,0 +1,278 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.hcatalog.mapreduce;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
+import org.apache.hcatalog.HcatTestUtils;
+import org.apache.hcatalog.common.HCatConstants;
+import org.apache.hcatalog.common.HCatException;
+import org.apache.hcatalog.common.HCatUtil;
+import org.apache.hcatalog.data.DefaultHCatRecord;
+import org.apache.hcatalog.data.schema.HCatFieldSchema;
+import org.apache.hcatalog.data.schema.HCatSchema;
+import org.apache.pig.ExecType;
+import org.apache.pig.PigServer;
+import org.apache.pig.data.Tuple;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+/**
+ * @deprecated Use/modify {@link org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite} instead
+ */
+public class TestSequenceFileReadWrite {
+
+  private File dataDir;
+  private String warehouseDir;
+  private String inputFileName;
+  private Driver driver;
+  private PigServer server;
+  private String[] input;
+  private HiveConf hiveConf;
+
+  @Before
+  public void setup() throws Exception {
+    dataDir = new File(System.getProperty("java.io.tmpdir") + File.separator +
+        TestSequenceFileReadWrite.class.getCanonicalName() + "-" + System.currentTimeMillis());
+    hiveConf = new HiveConf(this.getClass());
+    warehouseDir = new File(dataDir, "warehouse").getAbsolutePath();
+    inputFileName = new File(dataDir, "input.data").getAbsolutePath();
+    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+    hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehouseDir);
+    driver = new Driver(hiveConf);
+    SessionState.start(new CliSessionState(hiveConf));
+
+    if(!(new File(warehouseDir).mkdirs())) {
+      throw new RuntimeException("Could not create " + warehouseDir);
+    }
+
+    int numRows = 3;
+    input = new String[numRows];
+    for (int i = 0; i < numRows; i++) {
+      String col1 = "a" + i;
+      String col2 = "b" + i;
+      input[i] = i + "," + col1 + "," + col2;
+    }
+    HcatTestUtils.createTestDataFile(inputFileName, input);
+    server = new PigServer(ExecType.LOCAL);
+  }
+
+  @After
+  public void teardown() throws IOException {
+    if(dataDir != null) {
+      FileUtils.deleteDirectory(dataDir);
+    }
+  }
+
+  @Test
+  public void testSequenceTableWriteRead() throws Exception {
+    String createTable = "CREATE TABLE demo_table(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
+    driver.run("drop table demo_table");
+    int retCode1 = driver.run(createTable).getResponseCode();
+    assertTrue(retCode1 == 0);
+
+    server.setBatchOn();
+    server.registerQuery("A = load '"
+        + inputFileName
+        + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
+    server.registerQuery("store A into 'demo_table' using org.apache.hcatalog.pig.HCatStorer();");
+    server.executeBatch();
+
+    server.registerQuery("B = load 'demo_table' using org.apache.hcatalog.pig.HCatLoader();");
+    Iterator<Tuple> XIter = server.openIterator("B");
+    int numTuplesRead = 0;
+    while (XIter.hasNext()) {
+      Tuple t = XIter.next();
+      assertEquals(3, t.size());
+      assertEquals(t.get(0).toString(), "" + numTuplesRead);
+      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
+      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
+      numTuplesRead++;
+    }
+    assertEquals(input.length, numTuplesRead);
+  }
+
+  @Test
+  public void testTextTableWriteRead() throws Exception {
+    String createTable = "CREATE TABLE demo_table_1(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
+    driver.run("drop table demo_table_1");
+    int retCode1 = driver.run(createTable).getResponseCode();
+    assertTrue(retCode1 == 0);
+
+    server.setBatchOn();
+    server.registerQuery("A = load '"
+        + inputFileName
+        + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
+    server.registerQuery("store A into 'demo_table_1' using org.apache.hcatalog.pig.HCatStorer();");
+    server.executeBatch();
+
+    server.registerQuery("B = load 'demo_table_1' using org.apache.hcatalog.pig.HCatLoader();");
+    Iterator<Tuple> XIter = server.openIterator("B");
+    int numTuplesRead = 0;
+    while (XIter.hasNext()) {
+      Tuple t = XIter.next();
+      assertEquals(3, t.size());
+      assertEquals(t.get(0).toString(), "" + numTuplesRead);
+      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
+      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
+      numTuplesRead++;
+    }
+    assertEquals(input.length, numTuplesRead);
+  }
+
+  @Test
+  public void testSequenceTableWriteReadMR() throws Exception {
+    String createTable = "CREATE TABLE demo_table_2(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
+    driver.run("drop table demo_table_2");
+    int retCode1 = driver.run(createTable).getResponseCode();
+    assertTrue(retCode1 == 0);
+
+    Configuration conf = new Configuration();
+    conf.set(HCatConstants.HCAT_KEY_HIVE_CONF,
+        HCatUtil.serialize(hiveConf.getAllProperties()));
+    Job job = new Job(conf, "Write-hcat-seq-table");
+    job.setJarByClass(TestSequenceFileReadWrite.class);
+
+    job.setMapperClass(Map.class);
+    job.setOutputKeyClass(NullWritable.class);
+    job.setOutputValueClass(DefaultHCatRecord.class);
+    job.setInputFormatClass(TextInputFormat.class);
+    TextInputFormat.setInputPaths(job, inputFileName);
+
+    HCatOutputFormat.setOutput(job, OutputJobInfo.create(
+        MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_2", null));
+    job.setOutputFormatClass(HCatOutputFormat.class);
+    HCatOutputFormat.setSchema(job, getSchema());
+    job.setNumReduceTasks(0);
+    assertTrue(job.waitForCompletion(true));
+    if (!HCatUtil.isHadoop23()) {
+      new FileOutputCommitterContainer(job, null).commitJob(job);
+    }
+    assertTrue(job.isSuccessful());
+
+    server.setBatchOn();
+    server.registerQuery("C = load 'default.demo_table_2' using org.apache.hcatalog.pig.HCatLoader();");
+    server.executeBatch();
+    Iterator<Tuple> XIter = server.openIterator("C");
+    int numTuplesRead = 0;
+    while (XIter.hasNext()) {
+      Tuple t = XIter.next();
+      assertEquals(3, t.size());
+      assertEquals(t.get(0).toString(), "" + numTuplesRead);
+      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
+      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
+      numTuplesRead++;
+    }
+    assertEquals(input.length, numTuplesRead);
+  }
+
+  @Test
+  public void testTextTableWriteReadMR() throws Exception {
+    String createTable = "CREATE TABLE demo_table_3(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
+    driver.run("drop table demo_table_3");
+    int retCode1 = driver.run(createTable).getResponseCode();
+    assertTrue(retCode1 == 0);
+
+    Configuration conf = new Configuration();
+    conf.set(HCatConstants.HCAT_KEY_HIVE_CONF,
+        HCatUtil.serialize(hiveConf.getAllProperties()));
+    Job job = new Job(conf, "Write-hcat-text-table");
+    job.setJarByClass(TestSequenceFileReadWrite.class);
+
+    job.setMapperClass(Map.class);
+    job.setOutputKeyClass(NullWritable.class);
+    job.setOutputValueClass(DefaultHCatRecord.class);
+    job.setInputFormatClass(TextInputFormat.class);
+    job.setNumReduceTasks(0);
+    TextInputFormat.setInputPaths(job, inputFileName);
+
+    HCatOutputFormat.setOutput(job, OutputJobInfo.create(
+        MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_3", null));
+    job.setOutputFormatClass(HCatOutputFormat.class);
+    HCatOutputFormat.setSchema(job, getSchema());
+    assertTrue(job.waitForCompletion(true));
+    if (!HCatUtil.isHadoop23()) {
+      new FileOutputCommitterContainer(job, null).commitJob(job);
+    }
+    assertTrue(job.isSuccessful());
+
+    server.setBatchOn();
+    server.registerQuery("D = load 'default.demo_table_3' using org.apache.hcatalog.pig.HCatLoader();");
+    server.executeBatch();
+    Iterator<Tuple> XIter = server.openIterator("D");
+    int numTuplesRead = 0;
+    while (XIter.hasNext()) {
+      Tuple t = XIter.next();
+      assertEquals(3, t.size());
+      assertEquals(t.get(0).toString(), "" + numTuplesRead);
+      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
+      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
+      numTuplesRead++;
+    }
+    assertEquals(input.length, numTuplesRead);
+  }
+
+
+  public static class Map extends Mapper<LongWritable, Text, NullWritable, DefaultHCatRecord> {
+
+    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
+      String[] cols = value.toString().split(",");
+      DefaultHCatRecord record = new DefaultHCatRecord(3);
+      record.set(0, Integer.parseInt(cols[0]));
+      record.set(1, cols[1]);
+      record.set(2, cols[2]);
+      context.write(NullWritable.get(), record);
+    }
+  }
+
+  private HCatSchema getSchema() throws HCatException {
+    HCatSchema schema = new HCatSchema(new ArrayList<HCatFieldSchema>());
+    schema.append(new HCatFieldSchema("a0", HCatFieldSchema.Type.INT,
+        ""));
+    schema.append(new HCatFieldSchema("a1",
+        HCatFieldSchema.Type.STRING, ""));
+    schema.append(new HCatFieldSchema("a2",
+        HCatFieldSchema.Type.STRING, ""));
+    return schema;
+  }
+
+}
diff --git a/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java b/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java
new file mode 100644
index 0000000..6bd278e
--- /dev/null
+++ b/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java
@@ -0,0 +1,361 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.hive.hcatalog.hbase;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.ResultScanner;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.pig.ExecType;
+import org.apache.pig.PigServer;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
+import org.junit.Test;
+
+public class TestPigHBaseStorageHandler extends SkeletonHBaseTest {
+
+  private static HiveConf   hcatConf;
+  private static Driver driver;
+  private static String mypath;
+
+  private final byte[] FAMILY     = Bytes.toBytes("testFamily");
+  private final byte[] QUALIFIER1 = Bytes.toBytes("testQualifier1");
+  private final byte[] QUALIFIER2 = Bytes.toBytes("testQualifier2");
+
+  public void Initialize() throws Exception {
+
+    hcatConf = new HiveConf(this.getClass());
+    //hcatConf.set(ConfVars.SEMANTIC_ANALYZER_HOOK.varname,
+    //		HCatSemanticAnalyzer.class.getName());
+    URI fsuri = getFileSystem().getUri();
+    Path whPath = new Path(fsuri.getScheme(), fsuri.getAuthority(),
+        getTestDir());
+    hcatConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+    hcatConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    hcatConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+    hcatConf.set(ConfVars.METASTOREWAREHOUSE.varname, whPath.toString());
+
+    //Add hbase properties
+    for (Map.Entry<String, String> el : getHbaseConf()) {
+      if (el.getKey().startsWith("hbase.")) {
+        hcatConf.set(el.getKey(), el.getValue());
+      }
+    }
+
+    driver = new Driver(hcatConf);
+    SessionState.start(new CliSessionState(hcatConf));
+
+  }
+
+  private void populateHBaseTable(String tName) throws IOException {
+    List<Put> myPuts = generatePuts(tName);
+    HTable table = new HTable(getHbaseConf(), Bytes.toBytes(tName));
+    table.put(myPuts);
+  }
+
+  private List<Put> generatePuts(String tableName) throws IOException {
+
+    List<String> columnFamilies = Arrays.asList("testFamily");
+    List<Put> myPuts;
+    myPuts = new ArrayList<Put>();
+    for (int i = 1; i <=10; i++) {
+      Put put = new Put(Bytes.toBytes(i));
+      put.add(FAMILY, QUALIFIER1, 1, Bytes.toBytes("textA-" + i));
+      put.add(FAMILY, QUALIFIER2, 1, Bytes.toBytes("textB-" + i));
+      myPuts.add(put);
+    }
+    return myPuts;
+  }
+
+  public static void createTestDataFile(String filename) throws IOException {
+    FileWriter writer = null;
+    int LOOP_SIZE = 10;
+    float f = -100.1f;
+    try {
+      File file = new File(filename);
+      file.deleteOnExit();
+      writer = new FileWriter(file);
+
+      for (int i =1; i <= LOOP_SIZE; i++) {
+        writer.write(i+ "\t" +(f+i)+ "\t" + "textB-" + i + "\n");
+      }
+    } finally {
+      if (writer != null) {
+        writer.close();
+      }
+    }
+
+  }
+
+  @Test
+  public void testPigHBaseSchema() throws Exception {
+    Initialize();
+
+    String tableName = newTableName("MyTable");
+    String databaseName = newTableName("MyDatabase");
+    //Table name will be lower case unless specified by hbase.table.name property
+    String hbaseTableName = "testTable";
+    String db_dir = getTestDir() + "/hbasedb";
+
+    String dbQuery = "CREATE DATABASE IF NOT EXISTS " + databaseName + " LOCATION '"
+        + db_dir + "'";
+
+    String deleteQuery = "DROP TABLE "+databaseName+"."+tableName;
+
+    String tableQuery = "CREATE TABLE " + databaseName + "." + tableName
+        + "(key float, testqualifier1 string, testqualifier2 int) STORED BY " +
+        "'org.apache.hadoop.hive.hbase.HBaseStorageHandler'"
+        + " WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,testFamily:testQualifier1,testFamily:testQualifier2')"
+        +  " TBLPROPERTIES ('hbase.table.name'='"+hbaseTableName+"')";
+
+    CommandProcessorResponse responseOne = driver.run(deleteQuery);
+    assertEquals(0, responseOne.getResponseCode());
+
+
+    CommandProcessorResponse responseTwo = driver.run(dbQuery);
+    assertEquals(0, responseTwo.getResponseCode());
+
+
+    CommandProcessorResponse responseThree = driver.run(tableQuery);
+
+    HBaseAdmin hAdmin = new HBaseAdmin(getHbaseConf());
+    boolean doesTableExist = hAdmin.tableExists(hbaseTableName);
+    assertTrue(doesTableExist);
+
+    PigServer server = new PigServer(ExecType.LOCAL,hcatConf.getAllProperties());
+    server.registerQuery("A = load '"+databaseName+"."+tableName+"' using org.apache.hive.hcatalog.pig.HCatLoader();");
+
+    Schema dumpedASchema = server.dumpSchema("A");
+
+    List<FieldSchema> fields = dumpedASchema.getFields();
+    assertEquals(3, fields.size());
+
+    assertEquals(DataType.FLOAT,fields.get(0).type);
+    assertEquals("key",fields.get(0).alias.toLowerCase());
+
+    assertEquals( DataType.CHARARRAY,fields.get(1).type);
+    assertEquals("testQualifier1".toLowerCase(), fields.get(1).alias.toLowerCase());
+
+    assertEquals( DataType.INTEGER,fields.get(2).type);
+    assertEquals("testQualifier2".toLowerCase(), fields.get(2).alias.toLowerCase());
+
+  }
+
+
+  @Test
+  public void testPigFilterProjection() throws Exception {
+    Initialize();
+
+    String tableName = newTableName("MyTable");
+    String databaseName = newTableName("MyDatabase");
+    //Table name will be lower case unless specified by hbase.table.name property
+    String hbaseTableName = (databaseName + "." + tableName).toLowerCase();
+    String db_dir = getTestDir() + "/hbasedb";
+
+    String dbQuery = "CREATE DATABASE IF NOT EXISTS " + databaseName + " LOCATION '"
+        + db_dir + "'";
+
+    String deleteQuery = "DROP TABLE "+databaseName+"."+tableName;
+
+    String tableQuery = "CREATE TABLE " + databaseName + "." + tableName
+        + "(key int, testqualifier1 string, testqualifier2 string) STORED BY " +
+        "'org.apache.hadoop.hive.hbase.HBaseStorageHandler'" +
+        " WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,testFamily:testQualifier1,testFamily:testQualifier2')" +
+        " TBLPROPERTIES ('hbase.table.default.storage.type'='binary')";
+
+    CommandProcessorResponse responseOne = driver.run(deleteQuery);
+    assertEquals(0, responseOne.getResponseCode());
+
+
+    CommandProcessorResponse responseTwo = driver.run(dbQuery);
+    assertEquals(0, responseTwo.getResponseCode());
+
+
+    CommandProcessorResponse responseThree = driver.run(tableQuery);
+
+    HBaseAdmin hAdmin = new HBaseAdmin(getHbaseConf());
+    boolean doesTableExist = hAdmin.tableExists(hbaseTableName);
+    assertTrue(doesTableExist);
+
+    populateHBaseTable(hbaseTableName);
+
+    Configuration conf = new Configuration(getHbaseConf());
+    HTable table = new HTable(conf, hbaseTableName);
+    Scan scan = new Scan();
+    scan.addFamily(Bytes.toBytes("testFamily"));
+    ResultScanner scanner = table.getScanner(scan);
+    int index=1;
+
+    PigServer server = new PigServer(ExecType.LOCAL,hcatConf.getAllProperties());
+    server.registerQuery("A = load '"+databaseName+"."+tableName+"' using org.apache.hive.hcatalog.pig.HCatLoader();");
+    server.registerQuery("B = filter A by key < 5;");
+    server.registerQuery("C = foreach B generate key,testqualifier2;");
+    Iterator<Tuple> itr = server.openIterator("C");
+    //verify if the filter is correct and returns 2 rows and contains 2 columns and the contents match
+    while(itr.hasNext()){
+      Tuple t = itr.next();
+      assertTrue(t.size() == 2);
+      assertTrue(t.get(0).getClass() == Integer.class);
+      assertEquals(index,t.get(0));
+      assertTrue(t.get(1).getClass() == String.class);
+      assertEquals("textB-"+index,t.get(1));
+      index++;
+    }
+    assertEquals(index-1,4);
+  }
+
+  @Test
+  public void testPigPopulation() throws Exception {
+    Initialize();
+
+    String tableName = newTableName("MyTable");
+    String databaseName = newTableName("MyDatabase");
+    //Table name will be lower case unless specified by hbase.table.name property
+    String hbaseTableName = (databaseName + "." + tableName).toLowerCase();
+    String db_dir = getTestDir() + "/hbasedb";
+    String POPTXT_FILE_NAME = db_dir+"testfile.txt";
+    float f = -100.1f;
+
+    String dbQuery = "CREATE DATABASE IF NOT EXISTS " + databaseName + " LOCATION '"
+        + db_dir + "'";
+
+    String deleteQuery = "DROP TABLE "+databaseName+"."+tableName;
+
+    String tableQuery = "CREATE TABLE " + databaseName + "." + tableName
+        + "(key int, testqualifier1 float, testqualifier2 string) STORED BY " +
+        "'org.apache.hadoop.hive.hbase.HBaseStorageHandler'"
+        + " WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,testFamily:testQualifier1,testFamily:testQualifier2')"
+        + " TBLPROPERTIES ('hbase.table.default.storage.type'='binary')";
+
+
+    String selectQuery = "SELECT * from "+databaseName.toLowerCase()+"."+tableName.toLowerCase();
+
+
+    CommandProcessorResponse responseOne = driver.run(deleteQuery);
+    assertEquals(0, responseOne.getResponseCode());
+
+
+    CommandProcessorResponse responseTwo = driver.run(dbQuery);
+    assertEquals(0, responseTwo.getResponseCode());
+
+
+    CommandProcessorResponse responseThree = driver.run(tableQuery);
+
+    HBaseAdmin hAdmin = new HBaseAdmin(getHbaseConf());
+    boolean doesTableExist = hAdmin.tableExists(hbaseTableName);
+    assertTrue(doesTableExist);
+
+
+    createTestDataFile(POPTXT_FILE_NAME);
+
+    PigServer server = new PigServer(ExecType.LOCAL,hcatConf.getAllProperties());
+    server.registerQuery("A = load '"+POPTXT_FILE_NAME+"' using PigStorage() as (key:int, testqualifier1:float, testqualifier2:chararray);");
+    server.registerQuery("B = filter A by (key > 2) AND (key < 8) ;");
+    server.registerQuery("store B into '"+databaseName.toLowerCase()+"."+tableName.toLowerCase()+"' using  org.apache.hive.hcatalog.pig.HCatStorer();");
+    server.registerQuery("C = load '"+databaseName.toLowerCase()+"."+tableName.toLowerCase()+"' using org.apache.hive.hcatalog.pig.HCatLoader();");
+    // Schema should be same
+    Schema dumpedBSchema = server.dumpSchema("C");
+
+    List<FieldSchema> fields = dumpedBSchema.getFields();
+    assertEquals(3, fields.size());
+
+    assertEquals(DataType.INTEGER,fields.get(0).type);
+    assertEquals("key",fields.get(0).alias.toLowerCase());
+
+    assertEquals( DataType.FLOAT,fields.get(1).type);
+    assertEquals("testQualifier1".toLowerCase(), fields.get(1).alias.toLowerCase());
+
+    assertEquals( DataType.CHARARRAY,fields.get(2).type);
+    assertEquals("testQualifier2".toLowerCase(), fields.get(2).alias.toLowerCase());
+
+    //Query the hbase table and check the key is valid and only 5  are present
+    Configuration conf = new Configuration(getHbaseConf());
+    HTable table = new HTable(conf, hbaseTableName);
+    Scan scan = new Scan();
+    scan.addFamily(Bytes.toBytes("testFamily"));
+    byte[] familyNameBytes = Bytes.toBytes("testFamily");
+    ResultScanner scanner = table.getScanner(scan);
+    int index=3;
+    int count=0;
+    for(Result result: scanner) {
+      //key is correct
+      assertEquals(index,Bytes.toInt(result.getRow()));
+      //first column exists
+      assertTrue(result.containsColumn(familyNameBytes,Bytes.toBytes("testQualifier1")));
+      //value is correct
+      assertEquals((index+f),Bytes.toFloat(result.getValue(familyNameBytes,Bytes.toBytes("testQualifier1"))),0);
+
+      //second column exists
+      assertTrue(result.containsColumn(familyNameBytes,Bytes.toBytes("testQualifier2")));
+      //value is correct
+      assertEquals(("textB-"+index).toString(),Bytes.toString(result.getValue(familyNameBytes,Bytes.toBytes("testQualifier2"))));
+      index++;
+      count++;
+    }
+    // 5 rows should be returned
+    assertEquals(count,5);
+
+    //Check if hive returns results correctly
+    driver.run(selectQuery);
+    ArrayList<String> result = new ArrayList<String>();
+    driver.getResults(result);
+    //Query using the hive command line
+    assertEquals(5, result.size());
+    Iterator<String> itr = result.iterator();
+    for(int i = 3; i <= 7; i++) {
+      String tokens[] = itr.next().split("\\s+");
+      assertEquals(i,Integer.parseInt(tokens[0]));
+      assertEquals(i+f,Float.parseFloat(tokens[1]),0);
+      assertEquals(("textB-"+i).toString(),tokens[2]);
+    }
+
+    //delete the table from the database
+    CommandProcessorResponse responseFour = driver.run(deleteQuery);
+    assertEquals(0, responseFour.getResponseCode());
+
+  }
+
+}
diff --git a/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveCompatibility.java b/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveCompatibility.java
new file mode 100644
index 0000000..c5dfa43
--- /dev/null
+++ b/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveCompatibility.java
@@ -0,0 +1,129 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.hive.hcatalog.mapreduce;
+
+import java.io.File;
+import java.io.FileWriter;
+import java.util.Arrays;
+import java.util.Iterator;
+
+import junit.framework.Assert;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hive.hcatalog.common.HCatConstants;
+import org.apache.pig.ExecType;
+import org.apache.pig.PigServer;
+import org.apache.pig.data.Tuple;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestHCatHiveCompatibility extends HCatBaseTest {
+  private static final String INPUT_FILE_NAME = TEST_DATA_DIR + "/input.data";
+
+  @BeforeClass
+  public static void createInputData() throws Exception {
+    int LOOP_SIZE = 11;
+    File file = new File(INPUT_FILE_NAME);
+    file.deleteOnExit();
+    FileWriter writer = new FileWriter(file);
+    for (int i = 0; i < LOOP_SIZE; i++) {
+      writer.write(i + "\t1\n");
+    }
+    writer.close();
+  }
+
+  @Test
+  public void testUnpartedReadWrite() throws Exception {
+
+    driver.run("drop table if exists junit_unparted_noisd");
+    String createTable = "create table junit_unparted_noisd(a int) stored as RCFILE";
+    Assert.assertEquals(0, driver.run(createTable).getResponseCode());
+
+    // assert that the table created has no hcat instrumentation, and that we're still able to read it.
+    Table table = client.getTable("default", "junit_unparted_noisd");
+    Assert.assertTrue(table.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
+
+    PigServer server = new PigServer(ExecType.LOCAL);
+    logAndRegister(server, "A = load '" + INPUT_FILE_NAME + "' as (a:int);");
+    logAndRegister(server, "store A into 'default.junit_unparted_noisd' using org.apache.hive.hcatalog.pig.HCatStorer();");
+    logAndRegister(server, "B = load 'default.junit_unparted_noisd' using org.apache.hive.hcatalog.pig.HCatLoader();");
+    Iterator<Tuple> itr = server.openIterator("B");
+
+    int i = 0;
+
+    while (itr.hasNext()) {
+      Tuple t = itr.next();
+      Assert.assertEquals(1, t.size());
+      Assert.assertEquals(t.get(0), i);
+      i++;
+    }
+
+    Assert.assertFalse(itr.hasNext());
+    Assert.assertEquals(11, i);
+
+    // assert that the table created still has no hcat instrumentation
+    Table table2 = client.getTable("default", "junit_unparted_noisd");
+    Assert.assertTrue(table2.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
+
+    driver.run("drop table junit_unparted_noisd");
+  }
+
+  @Test
+  public void testPartedRead() throws Exception {
+
+    driver.run("drop table if exists junit_parted_noisd");
+    String createTable = "create table junit_parted_noisd(a int) partitioned by (b string) stored as RCFILE";
+    Assert.assertEquals(0, driver.run(createTable).getResponseCode());
+
+    // assert that the table created has no hcat instrumentation, and that we're still able to read it.
+    Table table = client.getTable("default", "junit_parted_noisd");
+    Assert.assertTrue(table.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
+
+    PigServer server = new PigServer(ExecType.LOCAL);
+    logAndRegister(server, "A = load '" + INPUT_FILE_NAME + "' as (a:int);");
+    logAndRegister(server, "store A into 'default.junit_parted_noisd' using org.apache.hive.hcatalog.pig.HCatStorer('b=42');");
+    logAndRegister(server, "B = load 'default.junit_parted_noisd' using org.apache.hive.hcatalog.pig.HCatLoader();");
+    Iterator<Tuple> itr = server.openIterator("B");
+
+    int i = 0;
+
+    while (itr.hasNext()) {
+      Tuple t = itr.next();
+      Assert.assertEquals(2, t.size()); // Contains explicit field "a" and partition "b".
+      Assert.assertEquals(t.get(0), i);
+      Assert.assertEquals(t.get(1), "42");
+      i++;
+    }
+
+    Assert.assertFalse(itr.hasNext());
+    Assert.assertEquals(11, i);
+
+    // assert that the table created still has no hcat instrumentation
+    Table table2 = client.getTable("default", "junit_parted_noisd");
+    Assert.assertTrue(table2.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
+
+    // assert that there is one partition present, and it had hcat instrumentation inserted when it was created.
+    Partition ptn = client.getPartition("default", "junit_parted_noisd", Arrays.asList("42"));
+
+    Assert.assertNotNull(ptn);
+    Assert.assertTrue(ptn.getSd().getInputFormat().equals(HCatConstants.HIVE_RCFILE_IF_CLASS));
+    driver.run("drop table junit_unparted_noisd");
+  }
+}
diff --git a/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java b/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
new file mode 100644
index 0000000..470ff58
--- /dev/null
+++ b/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java
@@ -0,0 +1,116 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.hive.hcatalog.mapreduce;
+
+import junit.framework.Assert;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.serde2.thrift.test.IntString;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.pig.ExecType;
+import org.apache.pig.PigServer;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.transport.TIOStreamTransport;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.ByteArrayOutputStream;
+import java.util.Iterator;
+
+public class TestHCatHiveThriftCompatibility extends HCatBaseTest {
+
+  private boolean setUpComplete = false;
+  private Path intStringSeq;
+
+  @Before
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    if (setUpComplete) {
+      return;
+    }
+
+    ByteArrayOutputStream out = new ByteArrayOutputStream();
+    TIOStreamTransport transport = new TIOStreamTransport(out);
+    TBinaryProtocol protocol = new TBinaryProtocol(transport);
+
+    IntString intString = new IntString(1, "one", 1);
+    intString.write(protocol);
+    BytesWritable bytesWritable = new BytesWritable(out.toByteArray());
+
+    intStringSeq = new Path(TEST_DATA_DIR + "/data/intString.seq");
+    LOG.info("Creating data file: " + intStringSeq);
+
+    SequenceFile.Writer seqFileWriter = SequenceFile.createWriter(
+        intStringSeq.getFileSystem(hiveConf), hiveConf, intStringSeq,
+        NullWritable.class, BytesWritable.class);
+    seqFileWriter.append(NullWritable.get(), bytesWritable);
+    seqFileWriter.close();
+
+    setUpComplete = true;
+  }
+
+  /**
+   *  Create a table with no explicit schema and ensure its correctly
+   *  discovered from the thrift struct.
+   */
+  @Test
+  public void testDynamicCols() throws Exception {
+    Assert.assertEquals(0, driver.run("drop table if exists test_thrift").getResponseCode());
+    Assert.assertEquals(0, driver.run(
+        "create external table test_thrift " +
+            "partitioned by (year string) " +
+            "row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' " +
+            "with serdeproperties ( " +
+            "  'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString', " +
+            "  'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') " +
+            "stored as" +
+            "  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'" +
+            "  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'")
+        .getResponseCode());
+    Assert.assertEquals(0,
+        driver.run("alter table test_thrift add partition (year = '2012') location '" +
+            intStringSeq.getParent() + "'").getResponseCode());
+
+    PigServer pigServer = new PigServer(ExecType.LOCAL);
+    pigServer.registerQuery("A = load 'test_thrift' using org.apache.hive.hcatalog.pig.HCatLoader();");
+
+    Schema expectedSchema = new Schema();
+    expectedSchema.add(new Schema.FieldSchema("myint", DataType.INTEGER));
+    expectedSchema.add(new Schema.FieldSchema("mystring", DataType.CHARARRAY));
+    expectedSchema.add(new Schema.FieldSchema("underscore_int", DataType.INTEGER));
+    expectedSchema.add(new Schema.FieldSchema("year", DataType.CHARARRAY));
+
+    Assert.assertEquals(expectedSchema, pigServer.dumpSchema("A"));
+
+    Iterator<Tuple> iterator = pigServer.openIterator("A");
+    Tuple t = iterator.next();
+    Assert.assertEquals(1, t.get(0));
+    Assert.assertEquals("one", t.get(1));
+    Assert.assertEquals(1, t.get(2));
+    Assert.assertEquals("2012", t.get(3));
+
+    Assert.assertFalse(iterator.hasNext());
+  }
+}
diff --git a/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java b/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
new file mode 100644
index 0000000..e567c23
--- /dev/null
+++ b/src/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
@@ -0,0 +1,275 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.hive.hcatalog.mapreduce;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
+import org.apache.hive.hcatalog.HcatTestUtils;
+import org.apache.hive.hcatalog.common.HCatConstants;
+import org.apache.hive.hcatalog.common.HCatException;
+import org.apache.hive.hcatalog.common.HCatUtil;
+import org.apache.hive.hcatalog.data.DefaultHCatRecord;
+import org.apache.hive.hcatalog.data.schema.HCatFieldSchema;
+import org.apache.hive.hcatalog.data.schema.HCatSchema;
+import org.apache.pig.ExecType;
+import org.apache.pig.PigServer;
+import org.apache.pig.data.Tuple;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+public class TestSequenceFileReadWrite {
+
+  private File dataDir;
+  private String warehouseDir;
+  private String inputFileName;
+  private Driver driver;
+  private PigServer server;
+  private String[] input;
+  private HiveConf hiveConf;
+
+  @Before
+  public void setup() throws Exception {
+    dataDir = new File(System.getProperty("java.io.tmpdir") + File.separator +
+        TestSequenceFileReadWrite.class.getCanonicalName() + "-" + System.currentTimeMillis());
+    hiveConf = new HiveConf(this.getClass());
+    warehouseDir = new File(dataDir, "warehouse").getAbsolutePath();
+    inputFileName = new File(dataDir, "input.data").getAbsolutePath();
+    hiveConf = new HiveConf(this.getClass());
+    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+    hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehouseDir);
+    driver = new Driver(hiveConf);
+    SessionState.start(new CliSessionState(hiveConf));
+
+    if(!(new File(warehouseDir).mkdirs())) {
+      throw new RuntimeException("Could not create " + warehouseDir);
+    }
+
+    int numRows = 3;
+    input = new String[numRows];
+    for (int i = 0; i < numRows; i++) {
+      String col1 = "a" + i;
+      String col2 = "b" + i;
+      input[i] = i + "," + col1 + "," + col2;
+    }
+    HcatTestUtils.createTestDataFile(inputFileName, input);
+    server = new PigServer(ExecType.LOCAL);
+  }
+  @After
+  public void teardown() throws IOException {
+    if(dataDir != null) {
+      FileUtils.deleteDirectory(dataDir);
+    }
+  }
+
+  @Test
+  public void testSequenceTableWriteRead() throws Exception {
+    String createTable = "CREATE TABLE demo_table(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
+    driver.run("drop table demo_table");
+    int retCode1 = driver.run(createTable).getResponseCode();
+    assertTrue(retCode1 == 0);
+
+    server.setBatchOn();
+    server.registerQuery("A = load '"
+        + inputFileName
+        + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
+    server.registerQuery("store A into 'demo_table' using org.apache.hive.hcatalog.pig.HCatStorer();");
+    server.executeBatch();
+
+    server.registerQuery("B = load 'demo_table' using org.apache.hive.hcatalog.pig.HCatLoader();");
+    Iterator<Tuple> XIter = server.openIterator("B");
+    int numTuplesRead = 0;
+    while (XIter.hasNext()) {
+      Tuple t = XIter.next();
+      assertEquals(3, t.size());
+      assertEquals(t.get(0).toString(), "" + numTuplesRead);
+      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
+      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
+      numTuplesRead++;
+    }
+    assertEquals(input.length, numTuplesRead);
+  }
+
+  @Test
+  public void testTextTableWriteRead() throws Exception {
+    String createTable = "CREATE TABLE demo_table_1(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
+    driver.run("drop table demo_table_1");
+    int retCode1 = driver.run(createTable).getResponseCode();
+    assertTrue(retCode1 == 0);
+
+    server.setBatchOn();
+    server.registerQuery("A = load '"
+        + inputFileName
+        + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
+    server.registerQuery("store A into 'demo_table_1' using org.apache.hive.hcatalog.pig.HCatStorer();");
+    server.executeBatch();
+
+    server.registerQuery("B = load 'demo_table_1' using org.apache.hive.hcatalog.pig.HCatLoader();");
+    Iterator<Tuple> XIter = server.openIterator("B");
+    int numTuplesRead = 0;
+    while (XIter.hasNext()) {
+      Tuple t = XIter.next();
+      assertEquals(3, t.size());
+      assertEquals(t.get(0).toString(), "" + numTuplesRead);
+      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
+      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
+      numTuplesRead++;
+    }
+    assertEquals(input.length, numTuplesRead);
+  }
+
+  @Test
+  public void testSequenceTableWriteReadMR() throws Exception {
+    String createTable = "CREATE TABLE demo_table_2(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
+    driver.run("drop table demo_table_2");
+    int retCode1 = driver.run(createTable).getResponseCode();
+    assertTrue(retCode1 == 0);
+
+    Configuration conf = new Configuration();
+    conf.set(HCatConstants.HCAT_KEY_HIVE_CONF,
+        HCatUtil.serialize(hiveConf.getAllProperties()));
+    Job job = new Job(conf, "Write-hcat-seq-table");
+    job.setJarByClass(TestSequenceFileReadWrite.class);
+
+    job.setMapperClass(Map.class);
+    job.setOutputKeyClass(NullWritable.class);
+    job.setOutputValueClass(DefaultHCatRecord.class);
+    job.setInputFormatClass(TextInputFormat.class);
+    TextInputFormat.setInputPaths(job, inputFileName);
+
+    HCatOutputFormat.setOutput(job, OutputJobInfo.create(
+        MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_2", null));
+    job.setOutputFormatClass(HCatOutputFormat.class);
+    HCatOutputFormat.setSchema(job, getSchema());
+    job.setNumReduceTasks(0);
+    assertTrue(job.waitForCompletion(true));
+    if (!HCatUtil.isHadoop23()) {
+      new FileOutputCommitterContainer(job, null).commitJob(job);
+    }
+    assertTrue(job.isSuccessful());
+
+    server.setBatchOn();
+    server.registerQuery("C = load 'default.demo_table_2' using org.apache.hive.hcatalog.pig.HCatLoader();");
+    server.executeBatch();
+    Iterator<Tuple> XIter = server.openIterator("C");
+    int numTuplesRead = 0;
+    while (XIter.hasNext()) {
+      Tuple t = XIter.next();
+      assertEquals(3, t.size());
+      assertEquals(t.get(0).toString(), "" + numTuplesRead);
+      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
+      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
+      numTuplesRead++;
+    }
+    assertEquals(input.length, numTuplesRead);
+  }
+
+  @Test
+  public void testTextTableWriteReadMR() throws Exception {
+    String createTable = "CREATE TABLE demo_table_3(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
+    driver.run("drop table demo_table_3");
+    int retCode1 = driver.run(createTable).getResponseCode();
+    assertTrue(retCode1 == 0);
+
+    Configuration conf = new Configuration();
+    conf.set(HCatConstants.HCAT_KEY_HIVE_CONF,
+        HCatUtil.serialize(hiveConf.getAllProperties()));
+    Job job = new Job(conf, "Write-hcat-text-table");
+    job.setJarByClass(TestSequenceFileReadWrite.class);
+
+    job.setMapperClass(Map.class);
+    job.setOutputKeyClass(NullWritable.class);
+    job.setOutputValueClass(DefaultHCatRecord.class);
+    job.setInputFormatClass(TextInputFormat.class);
+    job.setNumReduceTasks(0);
+    TextInputFormat.setInputPaths(job, inputFileName);
+
+    HCatOutputFormat.setOutput(job, OutputJobInfo.create(
+        MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_3", null));
+    job.setOutputFormatClass(HCatOutputFormat.class);
+    HCatOutputFormat.setSchema(job, getSchema());
+    assertTrue(job.waitForCompletion(true));
+    if (!HCatUtil.isHadoop23()) {
+      new FileOutputCommitterContainer(job, null).commitJob(job);
+    }
+    assertTrue(job.isSuccessful());
+
+    server.setBatchOn();
+    server.registerQuery("D = load 'default.demo_table_3' using org.apache.hive.hcatalog.pig.HCatLoader();");
+    server.executeBatch();
+    Iterator<Tuple> XIter = server.openIterator("D");
+    int numTuplesRead = 0;
+    while (XIter.hasNext()) {
+      Tuple t = XIter.next();
+      assertEquals(3, t.size());
+      assertEquals(t.get(0).toString(), "" + numTuplesRead);
+      assertEquals(t.get(1).toString(), "a" + numTuplesRead);
+      assertEquals(t.get(2).toString(), "b" + numTuplesRead);
+      numTuplesRead++;
+    }
+    assertEquals(input.length, numTuplesRead);
+  }
+
+
+  public static class Map extends Mapper<LongWritable, Text, NullWritable, DefaultHCatRecord> {
+
+    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
+      String[] cols = value.toString().split(",");
+      DefaultHCatRecord record = new DefaultHCatRecord(3);
+      record.set(0, Integer.parseInt(cols[0]));
+      record.set(1, cols[1]);
+      record.set(2, cols[2]);
+      context.write(NullWritable.get(), record);
+    }
+  }
+
+  private HCatSchema getSchema() throws HCatException {
+    HCatSchema schema = new HCatSchema(new ArrayList<HCatFieldSchema>());
+    schema.append(new HCatFieldSchema("a0", HCatFieldSchema.Type.INT,
+        ""));
+    schema.append(new HCatFieldSchema("a1",
+        HCatFieldSchema.Type.STRING, ""));
+    schema.append(new HCatFieldSchema("a2",
+        HCatFieldSchema.Type.STRING, ""));
+    return schema;
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
new file mode 100644
index 0000000..e1107dd
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
@@ -0,0 +1,1143 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.jdbc;
+
+import static org.apache.hadoop.hive.ql.exec.ExplainTask.EXPL_COLUMN_NAME;
+import static org.apache.hadoop.hive.ql.processors.SetProcessor.SET_COLUMN_NAME;
+
+import java.sql.Connection;
+import java.sql.DatabaseMetaData;
+import java.sql.DriverManager;
+import java.sql.DriverPropertyInfo;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.ResultSetMetaData;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.sql.Types;
+import java.util.Arrays;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.conf.HiveConf;
+
+/**
+ * TestJdbcDriver.
+ *
+ */
+public class TestJdbcDriver extends TestCase {
+  private static final String driverName = "org.apache.hadoop.hive.jdbc.HiveDriver";
+  private static final String tableName = "testHiveJdbcDriver_Table";
+  private static final String tableComment = "Simple table";
+  private static final String viewName = "testHiveJdbcDriverView";
+  private static final String viewComment = "Simple view";
+  private static final String partitionedTableName = "testHiveJdbcDriverPartitionedTable";
+  private static final String partitionedColumnName = "partcolabc";
+  private static final String partitionedColumnValue = "20090619";
+  private static final String partitionedTableComment = "Partitioned table";
+  private static final String dataTypeTableName = "testDataTypeTable";
+  private static final String dataTypeTableComment = "Table with many column data types";
+  private final HiveConf conf;
+  private final Path dataFilePath;
+  private final Path dataTypeDataFilePath;
+  private Connection con;
+  private boolean standAloneServer = false;
+
+  public TestJdbcDriver(String name) {
+    super(name);
+    conf = new HiveConf(TestJdbcDriver.class);
+    String dataFileDir = conf.get("test.data.files").replace('\\', '/')
+        .replace("c:", "");
+    dataFilePath = new Path(dataFileDir, "kv1.txt");
+    dataTypeDataFilePath = new Path(dataFileDir, "datatypes.txt");
+    standAloneServer = "true".equals(System
+        .getProperty("test.service.standalone.server"));
+  }
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    Class.forName(driverName);
+    if (standAloneServer) {
+      // get connection
+      con = DriverManager.getConnection("jdbc:hive://localhost:10000/default",
+          "", "");
+    } else {
+      con = DriverManager.getConnection("jdbc:hive://", "", "");
+    }
+    assertNotNull("Connection is null", con);
+    assertFalse("Connection should not be closed", con.isClosed());
+    Statement stmt = con.createStatement();
+    assertNotNull("Statement is null", stmt);
+
+    stmt.executeQuery("set hive.support.concurrency = false");
+
+    // drop table. ignore error.
+    try {
+      stmt.executeQuery("drop table " + tableName);
+    } catch (Exception ex) {
+      fail(ex.toString());
+    }
+
+    // create table
+    ResultSet res = stmt.executeQuery("create table " + tableName
+        + " (under_col int comment 'the under column', value string) comment '"
+        + tableComment + "'");
+    assertFalse(res.next());
+
+    // load data
+    res = stmt.executeQuery("load data local inpath '"
+        + dataFilePath.toString() + "' into table " + tableName);
+    assertFalse(res.next());
+
+    // also initialize a paritioned table to test against.
+
+    // drop table. ignore error.
+    try {
+      stmt.executeQuery("drop table " + partitionedTableName);
+    } catch (Exception ex) {
+      fail(ex.toString());
+    }
+
+    res = stmt.executeQuery("create table " + partitionedTableName
+        + " (under_col int, value string) comment '"+partitionedTableComment
+            +"' partitioned by (" + partitionedColumnName + " STRING)");
+    assertFalse(res.next());
+
+    // load data
+    res = stmt.executeQuery("load data local inpath '"
+        + dataFilePath.toString() + "' into table " + partitionedTableName
+        + " PARTITION (" + partitionedColumnName + "="
+        + partitionedColumnValue + ")");
+    assertFalse(res.next());
+
+    // drop table. ignore error.
+    try {
+      stmt.executeQuery("drop table " + dataTypeTableName);
+    } catch (Exception ex) {
+      fail(ex.toString());
+    }
+
+    res = stmt.executeQuery("create table " + dataTypeTableName
+        + " (c1 int, c2 boolean, c3 double, c4 string,"
+        + " c5 array<int>, c6 map<int,string>, c7 map<string,string>,"
+        + " c8 struct<r:string,s:int,t:double>,"
+        + " c9 tinyint, c10 smallint, c11 float, c12 bigint,"
+        + " c13 array<array<string>>,"
+        + " c14 map<int, map<int,int>>,"
+        + " c15 struct<r:int,s:struct<a:int,b:string>>,"
+        + " c16 array<struct<m:map<string,string>,n:int>>,"
+        + " c17 timestamp, "
+        + " c18 decimal,"
+        + " c19 binary,"
+        + " c20 date) comment'" + dataTypeTableComment
+            +"' partitioned by (dt STRING)");
+    assertFalse(res.next());
+
+    // load data
+    res = stmt.executeQuery("load data local inpath '"
+        + dataTypeDataFilePath.toString() + "' into table " + dataTypeTableName
+        + " PARTITION (dt='20090619')");
+    assertFalse(res.next());
+
+    // drop view. ignore error.
+    try {
+      stmt.executeQuery("drop view " + viewName);
+    } catch (Exception ex) {
+      fail(ex.toString());
+    }
+
+    // create view
+    res = stmt.executeQuery("create view " + viewName + " comment '"+viewComment
+            +"' as select * from "+ tableName);
+    assertFalse(res.next());
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+
+    // drop table
+    Statement stmt = con.createStatement();
+    assertNotNull("Statement is null", stmt);
+    ResultSet res = stmt.executeQuery("drop table " + tableName);
+    assertFalse(res.next());
+    res = stmt.executeQuery("drop table " + partitionedTableName);
+    assertFalse(res.next());
+    res = stmt.executeQuery("drop table " + dataTypeTableName);
+    assertFalse(res.next());
+
+    con.close();
+    assertTrue("Connection should be closed", con.isClosed());
+
+    Exception expectedException = null;
+    try {
+      con.createStatement();
+    } catch (Exception e) {
+      expectedException = e;
+    }
+
+    assertNotNull(
+        "createStatement() on closed connection should throw exception",
+        expectedException);
+  }
+
+  /**
+   * verify 'explain ...' resultset
+   * @throws SQLException
+   */
+  public void testExplainStmt() throws SQLException {
+    Statement stmt = con.createStatement();
+
+    ResultSet res = stmt.executeQuery(
+        "explain select c1, c2, c3, c4, c5 as a, c6, c7, c8, c9, c10, c11, c12, " +
+        "c1*2, sentences(null, null, null) as b from " + dataTypeTableName + " limit 1");
+
+    ResultSetMetaData md = res.getMetaData();
+    assertEquals(md.getColumnCount(), 1); // only one result column
+    assertEquals(md.getColumnLabel(1), EXPL_COLUMN_NAME); // verify the column name
+    //verify that there is data in the resultset
+    assertTrue("Nothing returned explain", res.next());
+  }
+
+  public void testPrepareStatement() {
+
+    String sql = "from (select count(1) from "
+        + tableName
+        + " where   'not?param?not?param' <> 'not_param??not_param' and ?=? "
+        + " and 1=? and 2=? and 3.0=? and 4.0=? and 'test\\'string\"'=? and 5=? and ?=? "
+        + " and date '2012-01-01' = date ?"
+        + " ) t  select '2011-03-25' ddate,'China',true bv, 10 num limit 10";
+
+     ///////////////////////////////////////////////
+    //////////////////// correct testcase
+    //////////////////// executed twice: once with the typed ps setters, once with the generic setObject
+    //////////////////////////////////////////////
+    try {
+      PreparedStatement ps = createPreapredStatementUsingSetXXX(sql);
+      ResultSet res = ps.executeQuery();
+      assertPreparedStatementResultAsExpected(res);
+      ps.close();
+
+      ps = createPreapredStatementUsingSetObject(sql);
+      res = ps.executeQuery();
+      assertPreparedStatementResultAsExpected(res);
+      ps.close();
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.toString());
+    }
+
+     ///////////////////////////////////////////////
+    //////////////////// other failure testcases
+    //////////////////////////////////////////////
+    // set nothing for prepared sql
+    Exception expectedException = null;
+    try {
+      PreparedStatement ps = con.prepareStatement(sql);
+      ps.executeQuery();
+    } catch (Exception e) {
+      expectedException = e;
+    }
+    assertNotNull(
+        "Execute the un-setted sql statement should throw exception",
+        expectedException);
+
+    // set some of parameters for prepared sql, not all of them.
+    expectedException = null;
+    try {
+      PreparedStatement ps = con.prepareStatement(sql);
+      ps.setBoolean(1, true);
+      ps.setBoolean(2, true);
+      ps.executeQuery();
+    } catch (Exception e) {
+      expectedException = e;
+    }
+    assertNotNull(
+        "Execute the invalid setted sql statement should throw exception",
+        expectedException);
+
+    // set the wrong type parameters for prepared sql.
+    expectedException = null;
+    try {
+      PreparedStatement ps = con.prepareStatement(sql);
+
+      // wrong type here
+      ps.setString(1, "wrong");
+
+      assertTrue(true);
+      ResultSet res = ps.executeQuery();
+      if (!res.next()) {
+        throw new Exception("there must be a empty result set");
+      }
+    } catch (Exception e) {
+      expectedException = e;
+    }
+    assertNotNull(
+        "Execute the invalid setted sql statement should throw exception",
+        expectedException);
+
+    // setObject to the yet unknown type java.util.Date
+    expectedException = null;
+    try {
+      PreparedStatement ps = con.prepareStatement(sql);
+      ps.setObject(1, new Date());
+      ps.executeQuery();
+    } catch (Exception e) {
+      expectedException = e;
+    }
+    assertNotNull(
+        "Setting to an unknown type should throw an exception",
+        expectedException);
+
+  }
+
+  private PreparedStatement createPreapredStatementUsingSetObject(String sql) throws SQLException {
+    PreparedStatement ps = con.prepareStatement(sql);
+
+    ps.setObject(1, true); //setBoolean
+    ps.setObject(2, true); //setBoolean
+
+    ps.setObject(3, Short.valueOf("1")); //setShort
+    ps.setObject(4, 2); //setInt
+    ps.setObject(5, 3f); //setFloat
+    ps.setObject(6, Double.valueOf(4)); //setDouble
+    ps.setObject(7, "test'string\""); //setString
+    ps.setObject(8, 5L); //setLong
+    ps.setObject(9, (byte) 1); //setByte
+    ps.setObject(10, (byte) 1); //setByte
+    ps.setString(11, "2012-01-01"); //setString
+
+    ps.setMaxRows(2);
+    return ps;
+  }
+
+  private PreparedStatement createPreapredStatementUsingSetXXX(String sql) throws SQLException {
+    PreparedStatement ps = con.prepareStatement(sql);
+
+    ps.setBoolean(1, true); //setBoolean
+    ps.setBoolean(2, true); //setBoolean
+
+    ps.setShort(3, Short.valueOf("1")); //setShort
+    ps.setInt(4, 2); //setInt
+    ps.setFloat(5, 3f); //setFloat
+    ps.setDouble(6, Double.valueOf(4)); //setDouble
+    ps.setString(7, "test'string\""); //setString
+    ps.setLong(8, 5L); //setLong
+    ps.setByte(9, (byte) 1); //setByte
+    ps.setByte(10, (byte) 1); //setByte
+    ps.setString(11, "2012-01-01"); //setString
+
+    ps.setMaxRows(2);
+    return ps;
+  }
+
+  private void assertPreparedStatementResultAsExpected(ResultSet res ) throws SQLException {
+    assertNotNull(res);
+
+    while (res.next()) {
+      assertEquals("2011-03-25", res.getString("ddate"));
+      assertEquals("10", res.getString("num"));
+      assertEquals((byte) 10, res.getByte("num"));
+      assertEquals("2011-03-25", res.getDate("ddate").toString());
+      assertEquals(Double.valueOf(10).doubleValue(), res.getDouble("num"), 0.1);
+      assertEquals(10, res.getInt("num"));
+      assertEquals(Short.valueOf("10").shortValue(), res.getShort("num"));
+      assertEquals(10L, res.getLong("num"));
+      assertEquals(true, res.getBoolean("bv"));
+      Object o = res.getObject("ddate");
+      assertNotNull(o);
+      o = res.getObject("num");
+      assertNotNull(o);
+    }
+    res.close();
+    assertTrue(true);
+  }
+
+  public final void testSelectAll() throws Exception {
+    doTestSelectAll(tableName, -1, -1); // tests not setting maxRows (return all)
+    doTestSelectAll(tableName, 0, -1); // tests setting maxRows to 0 (return all)
+  }
+
+  public final void testSelectAllPartioned() throws Exception {
+    doTestSelectAll(partitionedTableName, -1, -1); // tests not setting maxRows
+    // (return all)
+    doTestSelectAll(partitionedTableName, 0, -1); // tests setting maxRows to 0
+    // (return all)
+  }
+
+  public final void testSelectAllMaxRows() throws Exception {
+    doTestSelectAll(tableName, 100, -1);
+  }
+
+  public final void testSelectAllFetchSize() throws Exception {
+    doTestSelectAll(tableName, 100, 20);
+  }
+
+  public void testNullType() throws Exception {
+    Statement stmt = con.createStatement();
+    try {
+      ResultSet res = stmt.executeQuery("select null from " + dataTypeTableName);
+      assertTrue(res.next());
+      assertNull(res.getObject(1));
+    } finally {
+      stmt.close();
+    }
+  }
+
+  public void testDataTypes() throws Exception {
+    Statement stmt = con.createStatement();
+
+    ResultSet res = stmt.executeQuery(
+        "select * from " + dataTypeTableName + " order by c1");
+    ResultSetMetaData meta = res.getMetaData();
+
+    // row 1
+    assertTrue(res.next());
+    // skip the last (partitioning) column since it is always non-null
+    for (int i = 1; i < meta.getColumnCount(); i++) {
+      assertNull(res.getObject(i));
+    }
+
+    // row 2
+    assertTrue(res.next());
+    assertEquals(-1, res.getInt(1));
+    assertEquals(false, res.getBoolean(2));
+    assertEquals(-1.1d, res.getDouble(3));
+    assertEquals("", res.getString(4));
+    assertEquals("[]", res.getString(5));
+    assertEquals("{}", res.getString(6));
+    assertEquals("{}", res.getString(7));
+    assertEquals("[null, null, null]", res.getString(8));
+    assertEquals(-1, res.getByte(9));
+    assertEquals(-1, res.getShort(10));
+    assertEquals(-1.0f, res.getFloat(11));
+    assertEquals(-1, res.getLong(12));
+    assertEquals("[]", res.getString(13));
+    assertEquals("{}", res.getString(14));
+    assertEquals("[null, null]", res.getString(15));
+    assertEquals("[]", res.getString(16));
+    assertEquals(null, res.getString(17));
+    assertEquals(null, res.getTimestamp(17));
+    assertEquals(null, res.getBigDecimal(18));
+    assertEquals(null, res.getString(20));
+    assertEquals(null, res.getDate(20));
+
+    // row 3
+    assertTrue(res.next());
+    assertEquals(1, res.getInt(1));
+    assertEquals(true, res.getBoolean(2));
+    assertEquals(1.1d, res.getDouble(3));
+    assertEquals("1", res.getString(4));
+    assertEquals("[1, 2]", res.getString(5));
+    assertEquals("{1=x, 2=y}", res.getString(6));
+    assertEquals("{k=v}", res.getString(7));
+    assertEquals("[a, 9, 2.2]", res.getString(8));
+    assertEquals(1, res.getByte(9));
+    assertEquals(1, res.getShort(10));
+    assertEquals(1.0f, res.getFloat(11));
+    assertEquals(1, res.getLong(12));
+    assertEquals("[[a, b], [c, d]]", res.getString(13));
+    assertEquals("{1={11=12, 13=14}, 2={21=22}}", res.getString(14));
+    assertEquals("[1, [2, x]]", res.getString(15));
+    assertEquals("[[{}, 1], [{c=d, a=b}, 2]]", res.getString(16));
+    assertEquals("2012-04-22 09:00:00.123456789", res.getString(17));
+    assertEquals("2012-04-22 09:00:00.123456789", res.getTimestamp(17).toString());
+    assertEquals("123456789.0123456", res.getBigDecimal(18).toString());
+    assertEquals("2013-01-01", res.getString(20));
+    assertEquals("2013-01-01", res.getDate(20).toString());
+
+    // test getBoolean rules on non-boolean columns
+    assertEquals(true, res.getBoolean(1));
+    assertEquals(true, res.getBoolean(4));
+
+    // no more rows
+    assertFalse(res.next());
+  }
+
+  private void doTestSelectAll(String tableName, int maxRows, int fetchSize) throws Exception {
+    boolean isPartitionTable = tableName.equals(partitionedTableName);
+
+    Statement stmt = con.createStatement();
+    if (maxRows >= 0) {
+      stmt.setMaxRows(maxRows);
+    }
+    if (fetchSize > 0) {
+      stmt.setFetchSize(fetchSize);
+      assertEquals(fetchSize, stmt.getFetchSize());
+    }
+
+    // JDBC says that 0 means return all, which is the default
+    int expectedMaxRows = maxRows < 1 ? 0 : maxRows;
+
+    assertNotNull("Statement is null", stmt);
+    assertEquals("Statement max rows not as expected", expectedMaxRows, stmt
+        .getMaxRows());
+    assertFalse("Statement should not be closed", stmt.isClosed());
+
+    ResultSet res;
+
+    // run some queries
+    res = stmt.executeQuery("select * from " + tableName);
+    assertNotNull("ResultSet is null", res);
+    assertTrue("getResultSet() not returning expected ResultSet", res == stmt
+        .getResultSet());
+    assertEquals("get update count not as expected", 0, stmt.getUpdateCount());
+    int i = 0;
+
+    ResultSetMetaData meta = res.getMetaData();
+    int expectedColCount = isPartitionTable ? 3 : 2;
+    assertEquals(
+      "Unexpected column count", expectedColCount, meta.getColumnCount());
+
+    boolean moreRow = res.next();
+    while (moreRow) {
+      try {
+        i++;
+        assertEquals(res.getInt(1), res.getInt("under_col"));
+        assertEquals(res.getString(1), res.getString("under_col"));
+        assertEquals(res.getString(2), res.getString("value"));
+        if (isPartitionTable) {
+          assertEquals(res.getString(3), partitionedColumnValue);
+          assertEquals(res.getString(3), res.getString(partitionedColumnName));
+        }
+        assertFalse("Last result value was not null", res.wasNull());
+        assertNull("No warnings should be found on ResultSet", res
+            .getWarnings());
+        res.clearWarnings(); // verifying that method is supported
+
+        // System.out.println(res.getString(1) + " " + res.getString(2));
+        assertEquals(
+            "getInt and getString don't align for the same result value",
+            String.valueOf(res.getInt(1)), res.getString(1));
+        assertEquals("Unexpected result found", "val_" + res.getString(1), res
+            .getString(2));
+        moreRow = res.next();
+      } catch (SQLException e) {
+        System.out.println(e.toString());
+        e.printStackTrace();
+        throw new Exception(e.toString());
+      }
+    }
+
+    // supposed to get 500 rows if maxRows isn't set
+    int expectedRowCount = maxRows > 0 ? maxRows : 500;
+    assertEquals("Incorrect number of rows returned", expectedRowCount, i);
+
+    // should have no more rows
+    assertEquals(false, moreRow);
+
+    assertNull("No warnings should be found on statement", stmt.getWarnings());
+    stmt.clearWarnings(); // verifying that method is supported
+
+    assertNull("No warnings should be found on connection", con.getWarnings());
+    con.clearWarnings(); // verifying that method is supported
+
+    stmt.close();
+    assertTrue("Statement should be closed", stmt.isClosed());
+  }
+
+  public void testErrorMessages() throws SQLException {
+    String invalidSyntaxSQLState = "42000";
+
+    // These tests inherently cause exceptions to be written to the test output
+    // logs. This is undesirable, since you it might appear to someone looking
+    // at the test output logs as if something is failing when it isn't. Not
+    // sure
+    // how to get around that.
+    doTestErrorCase("SELECTT * FROM " + tableName,
+        "cannot recognize input near 'SELECTT' '*' 'FROM'",
+        invalidSyntaxSQLState, 40000);
+    doTestErrorCase("SELECT * FROM some_table_that_does_not_exist",
+        "Table not found", "42S02", 10001);
+    doTestErrorCase("drop table some_table_that_does_not_exist",
+        "Table not found", "42S02", 10001);
+    doTestErrorCase("SELECT invalid_column FROM " + tableName,
+        "Invalid table alias or column reference", invalidSyntaxSQLState, 10004);
+    doTestErrorCase("SELECT invalid_function(under_col) FROM " + tableName,
+    "Invalid function", invalidSyntaxSQLState, 10011);
+
+    // TODO: execute errors like this currently don't return good error
+    // codes and messages. This should be fixed.
+    doTestErrorCase(
+        "create table " + tableName + " (key int, value string)",
+        "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask",
+        "08S01", 1);
+  }
+
+  private void doTestErrorCase(String sql, String expectedMessage,
+      String expectedSQLState, int expectedErrorCode) throws SQLException {
+    Statement stmt = con.createStatement();
+    boolean exceptionFound = false;
+    try {
+      stmt.executeQuery(sql);
+    } catch (SQLException e) {
+      assertTrue("Adequate error messaging not found for '" + sql + "': "
+          + e.getMessage(), e.getMessage().contains(expectedMessage));
+      assertEquals("Expected SQLState not found for '" + sql + "'",
+          expectedSQLState, e.getSQLState());
+      assertEquals("Expected error code not found for '" + sql + "'",
+          expectedErrorCode, e.getErrorCode());
+      exceptionFound = true;
+    }
+
+    assertNotNull("Exception should have been thrown for query: " + sql,
+        exceptionFound);
+  }
+
+  public void testShowTables() throws SQLException {
+    Statement stmt = con.createStatement();
+    assertNotNull("Statement is null", stmt);
+
+    ResultSet res = stmt.executeQuery("show tables");
+
+    boolean testTableExists = false;
+    while (res.next()) {
+      assertNotNull("table name is null in result set", res.getString(1));
+      if (tableName.equalsIgnoreCase(res.getString(1))) {
+        testTableExists = true;
+      }
+    }
+
+    assertTrue("table name " + tableName
+        + " not found in SHOW TABLES result set", testTableExists);
+  }
+
+  public void testMetaDataGetTables() throws SQLException {
+    Map<String, Object[]> tests = new HashMap<String, Object[]>();
+    tests.put("test%jdbc%", new Object[]{"testhivejdbcdriver_table"
+            , "testhivejdbcdriverpartitionedtable"
+            , "testhivejdbcdriverview"});
+    tests.put("%jdbcdriver\\_table", new Object[]{"testhivejdbcdriver_table"});
+    tests.put("testhivejdbcdriver\\_table", new Object[]{"testhivejdbcdriver_table"});
+    tests.put("test_ivejdbcdri_er\\_table", new Object[]{"testhivejdbcdriver_table"});
+    tests.put("test_ivejdbcdri_er_table", new Object[]{"testhivejdbcdriver_table"});
+    tests.put("test_ivejdbcdri_er%table", new Object[]{
+        "testhivejdbcdriver_table", "testhivejdbcdriverpartitionedtable" });
+    tests.put("%jdbc%", new Object[]{ "testhivejdbcdriver_table"
+            , "testhivejdbcdriverpartitionedtable"
+            , "testhivejdbcdriverview"});
+    tests.put("", new Object[]{});
+
+    for (String checkPattern: tests.keySet()) {
+      ResultSet rs = (ResultSet)con.getMetaData().getTables("default", null, checkPattern, null);
+      int cnt = 0;
+      while (rs.next()) {
+        String resultTableName = rs.getString("TABLE_NAME");
+        assertEquals("Get by index different from get by name.", rs.getString(3), resultTableName);
+        assertEquals("Excpected a different table.", tests.get(checkPattern)[cnt], resultTableName);
+        String resultTableComment = rs.getString("REMARKS");
+        assertTrue("Missing comment on the table.", resultTableComment.length()>0);
+        String tableType = rs.getString("TABLE_TYPE");
+        if (resultTableName.endsWith("view")) {
+          assertEquals("Expected a tabletype view but got something else.", "VIEW", tableType);
+        }
+        cnt++;
+      }
+      rs.close();
+      assertEquals("Received an incorrect number of tables.", tests.get(checkPattern).length, cnt);
+    }
+
+    // only ask for the views.
+    ResultSet rs = (ResultSet)con.getMetaData().getTables("default", null, null
+            , new String[]{"VIEW"});
+    int cnt=0;
+    while (rs.next()) {
+      cnt++;
+    }
+    rs.close();
+    assertEquals("Incorrect number of views found.", 1, cnt);
+  }
+
+  public void testMetaDataGetCatalogs() throws SQLException {
+    ResultSet rs = (ResultSet)con.getMetaData().getCatalogs();
+    int cnt = 0;
+    while (rs.next()) {
+      String catalogname = rs.getString("TABLE_CAT");
+      assertEquals("Get by index different from get by name", rs.getString(1), catalogname);
+      switch(cnt) {
+        case 0:
+          assertEquals("default", catalogname);
+          break;
+        default:
+          fail("More then one catalog found.");
+          break;
+      }
+      cnt++;
+    }
+    rs.close();
+    assertEquals("Incorrect catalog count", 1, cnt);
+  }
+
+  public void testMetaDataGetSchemas() throws SQLException {
+    ResultSet rs = (ResultSet)con.getMetaData().getSchemas();
+    int cnt = 0;
+    while (rs.next()) {
+      cnt++;
+    }
+    rs.close();
+    assertEquals("Incorrect schema count", 0, cnt);
+  }
+
+  public void testMetaDataGetTableTypes() throws SQLException {
+    ResultSet rs = (ResultSet)con.getMetaData().getTableTypes();
+    Set<String> tabletypes = new HashSet();
+    tabletypes.add("TABLE");
+    tabletypes.add("EXTERNAL TABLE");
+    tabletypes.add("VIEW");
+
+    int cnt = 0;
+    while (rs.next()) {
+      String tabletype = rs.getString("TABLE_TYPE");
+      assertEquals("Get by index different from get by name", rs.getString(1), tabletype);
+      tabletypes.remove(tabletype);
+      cnt++;
+    }
+    rs.close();
+    assertEquals("Incorrect tabletype count.", 0, tabletypes.size());
+    assertTrue("Found less tabletypes then we test for.", cnt >= tabletypes.size());
+  }
+
+  public void testMetaDataGetColumns() throws SQLException {
+    Map<String[], Integer> tests = new HashMap<String[], Integer>();
+    tests.put(new String[]{"testhivejdbcdriver\\_table", null}, 2);
+    tests.put(new String[]{"testhivejdbc%", null}, 7);
+    tests.put(new String[]{"testhiveJDBC%", null}, 7);
+    tests.put(new String[]{"testhiveJDB\\C%", null}, 0);
+    tests.put(new String[]{"%jdbcdriver\\_table", null}, 2);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_col"}, 1);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_COL"}, 1);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_co_"}, 1);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "under_col"}, 1);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "und%"}, 1);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "%"}, 2);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "_%"}, 2);
+
+    for (String[] checkPattern: tests.keySet()) {
+      ResultSet rs = con.getMetaData().getColumns(null, null, checkPattern[0],
+          checkPattern[1]);
+
+      // validate the metadata for the getColumns result set
+      ResultSetMetaData rsmd = rs.getMetaData();
+      assertEquals("TABLE_CAT", rsmd.getColumnName(1));
+
+      int cnt = 0;
+      while (rs.next()) {
+        String columnname = rs.getString("COLUMN_NAME");
+        int ordinalPos = rs.getInt("ORDINAL_POSITION");
+        switch(cnt) {
+          case 0:
+            assertEquals("Wrong column name found", "under_col", columnname);
+            assertEquals("Wrong ordinal position found", ordinalPos, 1);
+            break;
+          case 1:
+            assertEquals("Wrong column name found", "value", columnname);
+            assertEquals("Wrong ordinal position found", ordinalPos, 2);
+            break;
+          default:
+            break;
+        }
+        cnt++;
+      }
+      rs.close();
+      assertEquals("Found less columns then we test for.", tests.get(checkPattern).intValue(), cnt);
+    }
+  }
+
+  /**
+   * Validate the Metadata for the result set of a metadata getColumns call.
+   */
+  public void testMetaDataGetColumnsMetaData() throws SQLException {
+    ResultSet rs = (ResultSet)con.getMetaData().getColumns(null, null
+            , "testhivejdbcdriver\\_table", null);
+
+    ResultSetMetaData rsmd = rs.getMetaData();
+
+    assertEquals("TABLE_CAT", rsmd.getColumnName(1));
+    assertEquals(Types.VARCHAR, rsmd.getColumnType(1));
+    assertEquals(Integer.MAX_VALUE, rsmd.getColumnDisplaySize(1));
+
+    assertEquals("ORDINAL_POSITION", rsmd.getColumnName(17));
+    assertEquals(Types.INTEGER, rsmd.getColumnType(17));
+    assertEquals(11, rsmd.getColumnDisplaySize(17));
+  }
+
+  public void testConversionsBaseResultSet() throws SQLException {
+    ResultSet rs = new HiveMetaDataResultSet(Arrays.asList("key")
+            , Arrays.asList("long")
+            , Arrays.asList(1234, "1234", "abc")) {
+      private int cnt=1;
+      public boolean next() throws SQLException {
+        if (cnt<data.size()) {
+          row = Arrays.asList(data.get(cnt));
+          cnt++;
+          return true;
+        } else {
+          return false;
+        }
+      }
+    };
+
+    while (rs.next()) {
+      String key = rs.getString("key");
+      if ("1234".equals(key)) {
+        assertEquals("Converting a string column into a long failed.", rs.getLong("key"), 1234L);
+        assertEquals("Converting a string column into a int failed.", rs.getInt("key"), 1234);
+      } else if ("abc".equals(key)) {
+        Object result = null;
+        Exception expectedException = null;
+        try {
+          result = rs.getLong("key");
+        } catch (SQLException e) {
+          expectedException = e;
+        }
+        assertTrue("Trying to convert 'abc' into a long should not work.", expectedException!=null);
+        try {
+          result = rs.getInt("key");
+        } catch (SQLException e) {
+          expectedException = e;
+        }
+        assertTrue("Trying to convert 'abc' into a int should not work.", expectedException!=null);
+      }
+    }
+  }
+
+  public void testDescribeTable() throws SQLException {
+    Statement stmt = con.createStatement();
+    assertNotNull("Statement is null", stmt);
+
+    ResultSet res = stmt.executeQuery("describe " + tableName);
+    res.next();
+    assertEquals(true, res.getString(1).contains("under_col"));
+    assertEquals(true, res.getString(2).contains("int"));
+    res.next();
+    assertEquals(true, res.getString(1).contains("value"));
+    assertEquals(true, res.getString(2).contains("string"));
+    assertFalse("More results found than expected", res.next());
+  }
+
+  public void testDatabaseMetaData() throws SQLException {
+    DatabaseMetaData meta = con.getMetaData();
+
+    assertEquals("Hive", meta.getDatabaseProductName());
+    assertEquals("1", meta.getDatabaseProductVersion());
+    assertEquals(DatabaseMetaData.sqlStateSQL99, meta.getSQLStateType());
+    assertNull(meta.getProcedures(null, null, null));
+    assertFalse(meta.supportsCatalogsInTableDefinitions());
+    assertFalse(meta.supportsSchemasInTableDefinitions());
+    assertFalse(meta.supportsSchemasInDataManipulation());
+    assertFalse(meta.supportsMultipleResultSets());
+    assertFalse(meta.supportsStoredProcedures());
+    assertTrue(meta.supportsAlterTableWithAddColumn());
+  }
+
+  public void testResultSetMetaData() throws SQLException {
+    Statement stmt = con.createStatement();
+
+    ResultSet res = stmt.executeQuery(
+        "select c1, c2, c3, c4, c5 as a, c6, c7, c8, c9, c10, c11, c12, " +
+        "c1*2, sentences(null, null, null) as b, c17, c18, c20 from " + dataTypeTableName +
+        " limit 1");
+    ResultSetMetaData meta = res.getMetaData();
+
+    ResultSet colRS = con.getMetaData().getColumns(null, null,
+        dataTypeTableName.toLowerCase(), null);
+
+    assertEquals(17, meta.getColumnCount());
+
+    assertTrue(colRS.next());
+
+    assertEquals("c1", meta.getColumnName(1));
+    assertEquals(Types.INTEGER, meta.getColumnType(1));
+    assertEquals("int", meta.getColumnTypeName(1));
+    assertEquals(11, meta.getColumnDisplaySize(1));
+    assertEquals(10, meta.getPrecision(1));
+    assertEquals(0, meta.getScale(1));
+
+    assertEquals("c1", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.INTEGER, colRS.getInt("DATA_TYPE"));
+    assertEquals("int", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(1), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(1), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c2", meta.getColumnName(2));
+    assertEquals("boolean", meta.getColumnTypeName(2));
+    assertEquals(Types.BOOLEAN, meta.getColumnType(2));
+    assertEquals(1, meta.getColumnDisplaySize(2));
+    assertEquals(1, meta.getPrecision(2));
+    assertEquals(0, meta.getScale(2));
+
+    assertEquals("c2", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.BOOLEAN, colRS.getInt("DATA_TYPE"));
+    assertEquals("boolean", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(2), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(2), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c3", meta.getColumnName(3));
+    assertEquals(Types.DOUBLE, meta.getColumnType(3));
+    assertEquals("double", meta.getColumnTypeName(3));
+    assertEquals(25, meta.getColumnDisplaySize(3));
+    assertEquals(15, meta.getPrecision(3));
+    assertEquals(15, meta.getScale(3));
+
+    assertEquals("c3", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.DOUBLE, colRS.getInt("DATA_TYPE"));
+    assertEquals("double", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(3), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(3), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c4", meta.getColumnName(4));
+    assertEquals(Types.VARCHAR, meta.getColumnType(4));
+    assertEquals("string", meta.getColumnTypeName(4));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(4));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(4));
+    assertEquals(0, meta.getScale(4));
+
+    assertEquals("c4", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
+    assertEquals("string", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(4), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(4), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("a", meta.getColumnName(5));
+    assertEquals(Types.VARCHAR, meta.getColumnType(5));
+    assertEquals("string", meta.getColumnTypeName(5));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(5));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(5));
+    assertEquals(0, meta.getScale(5));
+
+    assertEquals("c5", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
+    assertEquals("array<int>", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(5), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(5), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c6", meta.getColumnName(6));
+    assertEquals(Types.VARCHAR, meta.getColumnType(6));
+    assertEquals("string", meta.getColumnTypeName(6));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(6));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(6));
+    assertEquals(0, meta.getScale(6));
+
+    assertEquals("c6", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
+    assertEquals("map<int,string>", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(6), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(6), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c7", meta.getColumnName(7));
+    assertEquals(Types.VARCHAR, meta.getColumnType(7));
+    assertEquals("string", meta.getColumnTypeName(7));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(7));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(7));
+    assertEquals(0, meta.getScale(7));
+
+    assertEquals("c7", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
+    assertEquals("map<string,string>", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(7), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(7), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c8", meta.getColumnName(8));
+    assertEquals(Types.VARCHAR, meta.getColumnType(8));
+    assertEquals("string", meta.getColumnTypeName(8));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(8));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(8));
+    assertEquals(0, meta.getScale(8));
+
+    assertEquals("c8", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
+    assertEquals("struct<r:string,s:int,t:double>", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(8), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(8), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c9", meta.getColumnName(9));
+    assertEquals(Types.TINYINT, meta.getColumnType(9));
+    assertEquals("tinyint", meta.getColumnTypeName(9));
+    assertEquals(4, meta.getColumnDisplaySize(9));
+    assertEquals(3, meta.getPrecision(9));
+    assertEquals(0, meta.getScale(9));
+
+    assertEquals("c9", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.TINYINT, colRS.getInt("DATA_TYPE"));
+    assertEquals("tinyint", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(9), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(9), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c10", meta.getColumnName(10));
+    assertEquals(Types.SMALLINT, meta.getColumnType(10));
+    assertEquals("smallint", meta.getColumnTypeName(10));
+    assertEquals(6, meta.getColumnDisplaySize(10));
+    assertEquals(5, meta.getPrecision(10));
+    assertEquals(0, meta.getScale(10));
+
+    assertEquals("c10", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.SMALLINT, colRS.getInt("DATA_TYPE"));
+    assertEquals("smallint", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(10), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(10), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c11", meta.getColumnName(11));
+    assertEquals(Types.FLOAT, meta.getColumnType(11));
+    assertEquals("float", meta.getColumnTypeName(11));
+    assertEquals(24, meta.getColumnDisplaySize(11));
+    assertEquals(7, meta.getPrecision(11));
+    assertEquals(7, meta.getScale(11));
+
+    assertEquals("c11", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.FLOAT, colRS.getInt("DATA_TYPE"));
+    assertEquals("float", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(11), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(11), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c12", meta.getColumnName(12));
+    assertEquals(Types.BIGINT, meta.getColumnType(12));
+    assertEquals("bigint", meta.getColumnTypeName(12));
+    assertEquals(20, meta.getColumnDisplaySize(12));
+    assertEquals(19, meta.getPrecision(12));
+    assertEquals(0, meta.getScale(12));
+
+    assertEquals("c12", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.BIGINT, colRS.getInt("DATA_TYPE"));
+    assertEquals("bigint", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(12), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(12), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertEquals("_c12", meta.getColumnName(13));
+    assertEquals(Types.INTEGER, meta.getColumnType(13));
+    assertEquals("int", meta.getColumnTypeName(13));
+    assertEquals(11, meta.getColumnDisplaySize(13));
+    assertEquals(10, meta.getPrecision(13));
+    assertEquals(0, meta.getScale(13));
+
+    assertEquals("b", meta.getColumnName(14));
+    assertEquals(Types.VARCHAR, meta.getColumnType(14));
+    assertEquals("string", meta.getColumnTypeName(14));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(14));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(14));
+    assertEquals(0, meta.getScale(14));
+
+    assertEquals("c17", meta.getColumnName(15));
+    assertEquals(Types.TIMESTAMP, meta.getColumnType(15));
+    assertEquals("timestamp", meta.getColumnTypeName(15));
+    assertEquals(29, meta.getColumnDisplaySize(15));
+    assertEquals(29, meta.getPrecision(15));
+    assertEquals(9, meta.getScale(15));
+
+    assertEquals("c18", meta.getColumnName(16));
+    assertEquals(Types.DECIMAL, meta.getColumnType(16));
+    assertEquals("decimal", meta.getColumnTypeName(16));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(16));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(16));
+    assertEquals(Integer.MAX_VALUE, meta.getScale(16));
+
+    assertEquals("c20", meta.getColumnName(17));
+    assertEquals(Types.DATE, meta.getColumnType(17));
+    assertEquals("date", meta.getColumnTypeName(17));
+    assertEquals(10, meta.getColumnDisplaySize(17));
+    assertEquals(10, meta.getPrecision(17));
+    assertEquals(0, meta.getScale(17));
+
+    for (int i = 1; i <= meta.getColumnCount(); i++) {
+      assertFalse(meta.isAutoIncrement(i));
+      assertFalse(meta.isCurrency(i));
+      assertEquals(ResultSetMetaData.columnNullable, meta.isNullable(i));
+    }
+  }
+
+  // [url] [host] [port] [db]
+  private static final String[][] URL_PROPERTIES = new String[][] {
+      {"jdbc:hive://", "", "", "default"},
+      {"jdbc:hive://localhost:10001/default", "localhost", "10001", "default"},
+      {"jdbc:hive://localhost/notdefault", "localhost", "10000", "notdefault"},
+      {"jdbc:hive://foo:1243", "foo", "1243", "default"}};
+
+  public void testDriverProperties() throws SQLException {
+    HiveDriver driver = new HiveDriver();
+
+    for (String[] testValues : URL_PROPERTIES) {
+      DriverPropertyInfo[] dpi = driver.getPropertyInfo(testValues[0], null);
+      assertEquals("unexpected DriverPropertyInfo array size", 3, dpi.length);
+      assertDpi(dpi[0], "HOST", testValues[1]);
+      assertDpi(dpi[1], "PORT", testValues[2]);
+      assertDpi(dpi[2], "DBNAME", testValues[3]);
+    }
+
+  }
+
+  private static void assertDpi(DriverPropertyInfo dpi, String name,
+      String value) {
+    assertEquals("Invalid DriverPropertyInfo name", name, dpi.name);
+    assertEquals("Invalid DriverPropertyInfo value", value, dpi.value);
+    assertEquals("Invalid DriverPropertyInfo required", false, dpi.required);
+  }
+
+
+  /**
+   * validate schema generated by "set" command
+   * @throws SQLException
+   */
+  public void testSetCommand() throws SQLException {
+    // execute set command
+    String sql = "set -v";
+    Statement stmt = con.createStatement();
+    ResultSet res = stmt.executeQuery(sql);
+
+    // Validate resultset columns
+    ResultSetMetaData md = res.getMetaData() ;
+    assertEquals(1, md.getColumnCount());
+    assertEquals(SET_COLUMN_NAME, md.getColumnLabel(1));
+
+    //check if there is data in the resultset
+    assertTrue("Nothing returned by set -v", res.next());
+
+    res.close();
+    stmt.close();
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestEmbeddedHiveMetaStore.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestEmbeddedHiveMetaStore.java
new file mode 100644
index 0000000..b6b0e6e
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestEmbeddedHiveMetaStore.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.util.StringUtils;
+
+public class TestEmbeddedHiveMetaStore extends TestHiveMetaStore {
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    hiveConf.setBoolean(
+        HiveConf.ConfVars.HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS.varname, true);
+    warehouse = new Warehouse(hiveConf);
+    try {
+      client = new HiveMetaStoreClient(hiveConf, null);
+    } catch (Throwable e) {
+      System.err.println("Unable to open the metastore");
+      System.err.println(StringUtils.stringifyException(e));
+      throw new Exception(e);
+    }
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    try {
+      super.tearDown();
+      client.close();
+    } catch (Throwable e) {
+      System.err.println("Unable to close metastore");
+      System.err.println(StringUtils.stringifyException(e));
+      throw new Exception(e);
+    }
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
new file mode 100644
index 0000000..663abd6
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
@@ -0,0 +1,2679 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import java.sql.Connection;
+import java.sql.PreparedStatement;
+import java.sql.SQLException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import junit.framework.TestCase;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;
+import org.apache.hadoop.hive.metastore.api.ColumnStatistics;
+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;
+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc;
+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;
+import org.apache.hadoop.hive.metastore.api.ConfigValSecurityException;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.InvalidObjectException;
+import org.apache.hadoop.hive.metastore.api.InvalidOperationException;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.metastore.api.Order;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.SerDeInfo;
+import org.apache.hadoop.hive.metastore.api.SkewedInfo;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.metastore.api.Type;
+import org.apache.hadoop.hive.metastore.api.UnknownDBException;
+import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.thrift.TException;
+
+import com.google.common.collect.Lists;
+
+public abstract class TestHiveMetaStore extends TestCase {
+  private static final Log LOG = LogFactory.getLog(TestHiveMetaStore.class);
+  protected static HiveMetaStoreClient client;
+  protected static HiveConf hiveConf;
+  protected static Warehouse warehouse;
+  protected static boolean isThriftClient = false;
+
+  private static final String TEST_DB1_NAME = "testdb1";
+  private static final String TEST_DB2_NAME = "testdb2";
+
+  @Override
+  protected void setUp() throws Exception {
+    hiveConf = new HiveConf(this.getClass());
+    warehouse = new Warehouse(hiveConf);
+
+    // set some values to use for getting conf. vars
+    hiveConf.set("hive.metastore.metrics.enabled","true");
+    hiveConf.set("hive.key1", "value1");
+    hiveConf.set("hive.key2", "http://www.example.com");
+    hiveConf.set("hive.key3", "");
+    hiveConf.set("hive.key4", "0");
+  }
+
+  public void testNameMethods() {
+    Map<String, String> spec = new LinkedHashMap<String, String>();
+    spec.put("ds", "2008-07-01 14:13:12");
+    spec.put("hr", "14");
+    List<String> vals = new ArrayList<String>();
+    for(String v : spec.values()) {
+      vals.add(v);
+    }
+    String partName = "ds=2008-07-01 14%3A13%3A12/hr=14";
+
+    try {
+      List<String> testVals = client.partitionNameToVals(partName);
+      assertTrue("Values from name are incorrect", vals.equals(testVals));
+
+      Map<String, String> testSpec = client.partitionNameToSpec(partName);
+      assertTrue("Spec from name is incorrect", spec.equals(testSpec));
+
+      List<String> emptyVals = client.partitionNameToVals("");
+      assertTrue("Values should be empty", emptyVals.size() == 0);
+
+      Map<String, String> emptySpec =  client.partitionNameToSpec("");
+      assertTrue("Spec should be empty", emptySpec.size() == 0);
+    } catch (Exception e) {
+      assert(false);
+    }
+  }
+
+  /**
+   * tests create table and partition and tries to drop the table without
+   * droppping the partition
+   *
+   * @throws Exception
+   */
+  public void testPartition() throws Exception {
+    partitionTester(client, hiveConf);
+  }
+
+  public static void partitionTester(HiveMetaStoreClient client, HiveConf hiveConf)
+    throws Exception {
+    try {
+      String dbName = "compdb";
+      String tblName = "comptbl";
+      String typeName = "Person";
+      List<String> vals = makeVals("2008-07-01 14:13:12", "14");
+      List<String> vals2 = makeVals("2008-07-01 14:13:12", "15");
+      List<String> vals3 = makeVals("2008-07-02 14:13:12", "15");
+      List<String> vals4 = makeVals("2008-07-03 14:13:12", "151");
+
+      client.dropTable(dbName, tblName);
+      silentDropDatabase(dbName);
+      Database db = new Database();
+      db.setName(dbName);
+      client.createDatabase(db);
+      db = client.getDatabase(dbName);
+      Path dbPath = new Path(db.getLocationUri());
+      FileSystem fs = FileSystem.get(dbPath.toUri(), hiveConf);
+      boolean inheritPerms = hiveConf.getBoolVar(
+          HiveConf.ConfVars.HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS);
+      FsPermission dbPermission = fs.getFileStatus(dbPath).getPermission();
+      if (inheritPerms) {
+         //Set different perms for the database dir for further tests
+         dbPermission = new FsPermission((short)488);
+         fs.setPermission(dbPath, dbPermission);
+      }
+
+      client.dropType(typeName);
+      Type typ1 = new Type();
+      typ1.setName(typeName);
+      typ1.setFields(new ArrayList<FieldSchema>(2));
+      typ1.getFields().add(
+          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+      typ1.getFields().add(
+          new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+      client.createType(typ1);
+
+      Table tbl = new Table();
+      tbl.setDbName(dbName);
+      tbl.setTableName(tblName);
+      StorageDescriptor sd = new StorageDescriptor();
+      tbl.setSd(sd);
+      sd.setCols(typ1.getFields());
+      sd.setCompressed(false);
+      sd.setNumBuckets(1);
+      sd.setParameters(new HashMap<String, String>());
+      sd.getParameters().put("test_param_1", "Use this for comments etc");
+      sd.setBucketCols(new ArrayList<String>(2));
+      sd.getBucketCols().add("name");
+      sd.setSerdeInfo(new SerDeInfo());
+      sd.getSerdeInfo().setName(tbl.getTableName());
+      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+      sd.getSerdeInfo().getParameters()
+          .put(serdeConstants.SERIALIZATION_FORMAT, "1");
+      sd.setSortCols(new ArrayList<Order>());
+      sd.setStoredAsSubDirectories(false);
+
+      //skewed information
+      SkewedInfo skewInfor = new SkewedInfo();
+      skewInfor.setSkewedColNames(Arrays.asList("name"));
+      List<String> skv = Arrays.asList("1");
+      skewInfor.setSkewedColValues(Arrays.asList(skv));
+      Map<List<String>, String> scvlm = new HashMap<List<String>, String>();
+      scvlm.put(skv, "location1");
+      skewInfor.setSkewedColValueLocationMaps(scvlm);
+      sd.setSkewedInfo(skewInfor);
+
+      tbl.setPartitionKeys(new ArrayList<FieldSchema>(2));
+      tbl.getPartitionKeys().add(
+          new FieldSchema("ds", serdeConstants.STRING_TYPE_NAME, ""));
+      tbl.getPartitionKeys().add(
+          new FieldSchema("hr", serdeConstants.STRING_TYPE_NAME, ""));
+
+      client.createTable(tbl);
+
+      if (isThriftClient) {
+        // the createTable() above does not update the location in the 'tbl'
+        // object when the client is a thrift client and the code below relies
+        // on the location being present in the 'tbl' object - so get the table
+        // from the metastore
+        tbl = client.getTable(dbName, tblName);
+      }
+
+      assertEquals(dbPermission, fs.getFileStatus(new Path(tbl.getSd().getLocation()))
+          .getPermission());
+
+      Partition part = makePartitionObject(dbName, tblName, vals, tbl, "/part1");
+      Partition part2 = makePartitionObject(dbName, tblName, vals2, tbl, "/part2");
+      Partition part3 = makePartitionObject(dbName, tblName, vals3, tbl, "/part3");
+      Partition part4 = makePartitionObject(dbName, tblName, vals4, tbl, "/part4");
+
+      // check if the partition exists (it shouldn't)
+      boolean exceptionThrown = false;
+      try {
+        Partition p = client.getPartition(dbName, tblName, vals);
+      } catch(Exception e) {
+        assertEquals("partition should not have existed",
+            NoSuchObjectException.class, e.getClass());
+        exceptionThrown = true;
+      }
+      assertTrue("getPartition() should have thrown NoSuchObjectException", exceptionThrown);
+      Partition retp = client.add_partition(part);
+      assertNotNull("Unable to create partition " + part, retp);
+      assertEquals(dbPermission, fs.getFileStatus(new Path(retp.getSd().getLocation()))
+          .getPermission());
+      Partition retp2 = client.add_partition(part2);
+      assertNotNull("Unable to create partition " + part2, retp2);
+      assertEquals(dbPermission, fs.getFileStatus(new Path(retp2.getSd().getLocation()))
+          .getPermission());
+      Partition retp3 = client.add_partition(part3);
+      assertNotNull("Unable to create partition " + part3, retp3);
+      assertEquals(dbPermission, fs.getFileStatus(new Path(retp3.getSd().getLocation()))
+          .getPermission());
+      Partition retp4 = client.add_partition(part4);
+      assertNotNull("Unable to create partition " + part4, retp4);
+      assertEquals(dbPermission, fs.getFileStatus(new Path(retp4.getSd().getLocation()))
+          .getPermission());
+
+      Partition part_get = client.getPartition(dbName, tblName, part.getValues());
+      if(isThriftClient) {
+        // since we are using thrift, 'part' will not have the create time and
+        // last DDL time set since it does not get updated in the add_partition()
+        // call - likewise part2 and part3 - set it correctly so that equals check
+        // doesn't fail
+        adjust(client, part, dbName, tblName);
+        adjust(client, part2, dbName, tblName);
+        adjust(client, part3, dbName, tblName);
+      }
+      assertTrue("Partitions are not same", part.equals(part_get));
+
+      String partName = "ds=2008-07-01 14%3A13%3A12/hr=14";
+      String part2Name = "ds=2008-07-01 14%3A13%3A12/hr=15";
+      String part3Name ="ds=2008-07-02 14%3A13%3A12/hr=15";
+      String part4Name ="ds=2008-07-03 14%3A13%3A12/hr=151";
+
+      part_get = client.getPartition(dbName, tblName, partName);
+      assertTrue("Partitions are not the same", part.equals(part_get));
+
+      // Test partition listing with a partial spec - ds is specified but hr is not
+      List<String> partialVals = new ArrayList<String>();
+      partialVals.add(vals.get(0));
+      Set<Partition> parts = new HashSet<Partition>();
+      parts.add(part);
+      parts.add(part2);
+
+      List<Partition> partial = client.listPartitions(dbName, tblName, partialVals,
+          (short) -1);
+      assertTrue("Should have returned 2 partitions", partial.size() == 2);
+      assertTrue("Not all parts returned", partial.containsAll(parts));
+
+      Set<String> partNames = new HashSet<String>();
+      partNames.add(partName);
+      partNames.add(part2Name);
+      List<String> partialNames = client.listPartitionNames(dbName, tblName, partialVals,
+          (short) -1);
+      assertTrue("Should have returned 2 partition names", partialNames.size() == 2);
+      assertTrue("Not all part names returned", partialNames.containsAll(partNames));
+
+      partNames.add(part3Name);
+      partNames.add(part4Name);
+      partialVals.clear();
+      partialVals.add("");
+      partialNames = client.listPartitionNames(dbName, tblName, partialVals, (short) -1);
+      assertTrue("Should have returned 4 partition names", partialNames.size() == 4);
+      assertTrue("Not all part names returned", partialNames.containsAll(partNames));
+
+      // Test partition listing with a partial spec - hr is specified but ds is not
+      parts.clear();
+      parts.add(part2);
+      parts.add(part3);
+
+      partialVals.clear();
+      partialVals.add("");
+      partialVals.add(vals2.get(1));
+
+      partial = client.listPartitions(dbName, tblName, partialVals, (short) -1);
+      assertEquals("Should have returned 2 partitions", 2, partial.size());
+      assertTrue("Not all parts returned", partial.containsAll(parts));
+
+      partNames.clear();
+      partNames.add(part2Name);
+      partNames.add(part3Name);
+      partialNames = client.listPartitionNames(dbName, tblName, partialVals,
+          (short) -1);
+      assertEquals("Should have returned 2 partition names", 2, partialNames.size());
+      assertTrue("Not all part names returned", partialNames.containsAll(partNames));
+
+      // Verify escaped partition names don't return partitions
+      exceptionThrown = false;
+      try {
+        String badPartName = "ds=2008-07-01 14%3A13%3A12/hrs=14";
+        client.getPartition(dbName, tblName, badPartName);
+      } catch(NoSuchObjectException e) {
+        exceptionThrown = true;
+      }
+      assertTrue("Bad partition spec should have thrown an exception", exceptionThrown);
+
+      Path partPath = new Path(part.getSd().getLocation());
+
+
+      assertTrue(fs.exists(partPath));
+      client.dropPartition(dbName, tblName, part.getValues(), true);
+      assertFalse(fs.exists(partPath));
+
+      // Test append_partition_by_name
+      client.appendPartition(dbName, tblName, partName);
+      Partition part5 = client.getPartition(dbName, tblName, part.getValues());
+      assertTrue("Append partition by name failed", part5.getValues().equals(vals));;
+      Path part5Path = new Path(part5.getSd().getLocation());
+      assertTrue(fs.exists(part5Path));
+
+      // Test drop_partition_by_name
+      assertTrue("Drop partition by name failed",
+          client.dropPartition(dbName, tblName, partName, true));
+      assertFalse(fs.exists(part5Path));
+
+      // add the partition again so that drop table with a partition can be
+      // tested
+      retp = client.add_partition(part);
+      assertNotNull("Unable to create partition " + part, retp);
+      assertEquals(dbPermission, fs.getFileStatus(new Path(retp.getSd().getLocation()))
+          .getPermission());
+
+      // test add_partitions
+
+      List<String> mvals1 = makeVals("2008-07-04 14:13:12", "14641");
+      List<String> mvals2 = makeVals("2008-07-04 14:13:12", "14642");
+      List<String> mvals3 = makeVals("2008-07-04 14:13:12", "14643");
+      List<String> mvals4 = makeVals("2008-07-04 14:13:12", "14643"); // equal to 3
+      List<String> mvals5 = makeVals("2008-07-04 14:13:12", "14645");
+
+      Exception savedException;
+
+      // add_partitions(empty list) : ok, normal operation
+      client.add_partitions(new ArrayList<Partition>());
+
+      // add_partitions(1,2,3) : ok, normal operation
+      Partition mpart1 = makePartitionObject(dbName, tblName, mvals1, tbl, "/mpart1");
+      Partition mpart2 = makePartitionObject(dbName, tblName, mvals2, tbl, "/mpart2");
+      Partition mpart3 = makePartitionObject(dbName, tblName, mvals3, tbl, "/mpart3");
+      client.add_partitions(Arrays.asList(mpart1,mpart2,mpart3));
+
+      if(isThriftClient) {
+        // do DDL time munging if thrift mode
+        adjust(client, mpart1, dbName, tblName);
+        adjust(client, mpart2, dbName, tblName);
+        adjust(client, mpart3, dbName, tblName);
+      }
+      verifyPartitionsPublished(client, dbName, tblName,
+          Arrays.asList(mvals1.get(0)),
+          Arrays.asList(mpart1,mpart2,mpart3));
+
+      Partition mpart4 = makePartitionObject(dbName, tblName, mvals4, tbl, "/mpart4");
+      Partition mpart5 = makePartitionObject(dbName, tblName, mvals5, tbl, "/mpart5");
+
+      // create dir for /mpart5
+      Path mp5Path = new Path(mpart5.getSd().getLocation());
+      warehouse.mkdirs(mp5Path);
+      assertTrue(fs.exists(mp5Path));
+      assertEquals(dbPermission, fs.getFileStatus(mp5Path).getPermission());
+
+      // add_partitions(5,4) : err = duplicate keyvals on mpart4
+      savedException = null;
+      try {
+        client.add_partitions(Arrays.asList(mpart5,mpart4));
+      } catch (Exception e) {
+        savedException = e;
+      } finally {
+        assertNotNull(savedException);
+      }
+
+      // check that /mpart4 does not exist, but /mpart5 still does.
+      assertTrue(fs.exists(mp5Path));
+      assertFalse(fs.exists(new Path(mpart4.getSd().getLocation())));
+
+      // add_partitions(5) : ok
+      client.add_partitions(Arrays.asList(mpart5));
+
+      if(isThriftClient) {
+        // do DDL time munging if thrift mode
+        adjust(client, mpart5, dbName, tblName);
+      }
+
+      verifyPartitionsPublished(client, dbName, tblName,
+          Arrays.asList(mvals1.get(0)),
+          Arrays.asList(mpart1,mpart2,mpart3,mpart5));
+
+      //// end add_partitions tests
+
+      client.dropTable(dbName, tblName);
+
+      client.dropType(typeName);
+
+      // recreate table as external, drop partition and it should
+      // still exist
+      tbl.setParameters(new HashMap<String, String>());
+      tbl.getParameters().put("EXTERNAL", "TRUE");
+      client.createTable(tbl);
+      retp = client.add_partition(part);
+      assertTrue(fs.exists(partPath));
+      client.dropPartition(dbName, tblName, part.getValues(), true);
+      assertTrue(fs.exists(partPath));
+
+      for (String tableName : client.getTables(dbName, "*")) {
+        client.dropTable(dbName, tableName);
+      }
+
+      client.dropDatabase(dbName);
+
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testPartition() failed.");
+      throw e;
+    }
+  }
+
+  private static void verifyPartitionsPublished(HiveMetaStoreClient client,
+      String dbName, String tblName, List<String> partialSpec,
+      List<Partition> expectedPartitions)
+          throws NoSuchObjectException, MetaException, TException {
+    // Test partition listing with a partial spec
+
+    List<Partition> mpartial = client.listPartitions(dbName, tblName, partialSpec,
+        (short) -1);
+    assertEquals("Should have returned "+expectedPartitions.size()+
+        " partitions, returned " + mpartial.size(),
+        expectedPartitions.size(), mpartial.size());
+    assertTrue("Not all parts returned", mpartial.containsAll(expectedPartitions));
+  }
+
+  private static List<String> makeVals(String ds, String id) {
+    List <String> vals4 = new ArrayList<String>(2);
+    vals4 = new ArrayList<String>(2);
+    vals4.add(ds);
+    vals4.add(id);
+    return vals4;
+  }
+
+  private static Partition makePartitionObject(String dbName, String tblName,
+      List<String> ptnVals, Table tbl, String ptnLocationSuffix) {
+    Partition part4 = new Partition();
+    part4.setDbName(dbName);
+    part4.setTableName(tblName);
+    part4.setValues(ptnVals);
+    part4.setParameters(new HashMap<String, String>());
+    part4.setSd(tbl.getSd().deepCopy());
+    part4.getSd().setSerdeInfo(tbl.getSd().getSerdeInfo().deepCopy());
+    part4.getSd().setLocation(tbl.getSd().getLocation() + ptnLocationSuffix);
+    return part4;
+  }
+
+  public void testListPartitions() throws Throwable {
+    // create a table with multiple partitions
+    String dbName = "compdb";
+    String tblName = "comptbl";
+    String typeName = "Person";
+
+    cleanUp(dbName, tblName, typeName);
+
+    List<List<String>> values = new ArrayList<List<String>>();
+    values.add(makeVals("2008-07-01 14:13:12", "14"));
+    values.add(makeVals("2008-07-01 14:13:12", "15"));
+    values.add(makeVals("2008-07-02 14:13:12", "15"));
+    values.add(makeVals("2008-07-03 14:13:12", "151"));
+
+    createMultiPartitionTableSchema(dbName, tblName, typeName, values);
+
+    List<Partition> partitions = client.listPartitions(dbName, tblName, (short)-1);
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() +
+      " partitions", values.size(), partitions.size());
+
+    partitions = client.listPartitions(dbName, tblName, (short)(values.size()/2));
+
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() / 2 +
+      " partitions",values.size() / 2, partitions.size());
+
+
+    partitions = client.listPartitions(dbName, tblName, (short) (values.size() * 2));
+
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() +
+      " partitions",values.size(), partitions.size());
+
+    cleanUp(dbName, tblName, typeName);
+
+  }
+
+
+
+  public void testListPartitionNames() throws Throwable {
+    // create a table with multiple partitions
+    String dbName = "compdb";
+    String tblName = "comptbl";
+    String typeName = "Person";
+
+    cleanUp(dbName, tblName, typeName);
+
+    List<List<String>> values = new ArrayList<List<String>>();
+    values.add(makeVals("2008-07-01 14:13:12", "14"));
+    values.add(makeVals("2008-07-01 14:13:12", "15"));
+    values.add(makeVals("2008-07-02 14:13:12", "15"));
+    values.add(makeVals("2008-07-03 14:13:12", "151"));
+
+
+
+    createMultiPartitionTableSchema(dbName, tblName, typeName, values);
+
+    List<String> partitions = client.listPartitionNames(dbName, tblName, (short)-1);
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() +
+      " partitions", values.size(), partitions.size());
+
+    partitions = client.listPartitionNames(dbName, tblName, (short)(values.size()/2));
+
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() / 2 +
+      " partitions",values.size() / 2, partitions.size());
+
+
+    partitions = client.listPartitionNames(dbName, tblName, (short) (values.size() * 2));
+
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() +
+      " partitions",values.size(), partitions.size());
+
+    cleanUp(dbName, tblName, typeName);
+
+  }
+
+
+  public void testDropTable() throws Throwable {
+    // create a table with multiple partitions
+    String dbName = "compdb";
+    String tblName = "comptbl";
+    String typeName = "Person";
+
+    cleanUp(dbName, tblName, typeName);
+
+    List<List<String>> values = new ArrayList<List<String>>();
+    values.add(makeVals("2008-07-01 14:13:12", "14"));
+    values.add(makeVals("2008-07-01 14:13:12", "15"));
+    values.add(makeVals("2008-07-02 14:13:12", "15"));
+    values.add(makeVals("2008-07-03 14:13:12", "151"));
+
+    createMultiPartitionTableSchema(dbName, tblName, typeName, values);
+
+    client.dropTable(dbName, tblName);
+    client.dropType(typeName);
+
+    boolean exceptionThrown = false;
+    try {
+      client.getTable(dbName, tblName);
+    } catch(Exception e) {
+      assertEquals("table should not have existed",
+          NoSuchObjectException.class, e.getClass());
+      exceptionThrown = true;
+    }
+    assertTrue("Table " + tblName + " should have been dropped ", exceptionThrown);
+
+  }
+
+  public void testAlterViewParititon() throws Throwable {
+    String dbName = "compdb";
+    String tblName = "comptbl";
+    String viewName = "compView";
+
+    client.dropTable(dbName, tblName);
+    silentDropDatabase(dbName);
+    Database db = new Database();
+    db.setName(dbName);
+    db.setDescription("Alter Partition Test database");
+    client.createDatabase(db);
+
+    ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
+    cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+    cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+
+    Table tbl = new Table();
+    tbl.setDbName(dbName);
+    tbl.setTableName(tblName);
+    StorageDescriptor sd = new StorageDescriptor();
+    tbl.setSd(sd);
+    sd.setCols(cols);
+    sd.setCompressed(false);
+    sd.setParameters(new HashMap<String, String>());
+    sd.setSerdeInfo(new SerDeInfo());
+    sd.getSerdeInfo().setName(tbl.getTableName());
+    sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+    sd.getSerdeInfo().getParameters()
+        .put(serdeConstants.SERIALIZATION_FORMAT, "1");
+    sd.setSortCols(new ArrayList<Order>());
+
+    client.createTable(tbl);
+
+    if (isThriftClient) {
+      // the createTable() above does not update the location in the 'tbl'
+      // object when the client is a thrift client and the code below relies
+      // on the location being present in the 'tbl' object - so get the table
+      // from the metastore
+      tbl = client.getTable(dbName, tblName);
+    }
+
+    ArrayList<FieldSchema> viewCols = new ArrayList<FieldSchema>(1);
+    viewCols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+
+    ArrayList<FieldSchema> viewPartitionCols = new ArrayList<FieldSchema>(1);
+    viewPartitionCols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+
+    Table view = new Table();
+    view.setDbName(dbName);
+    view.setTableName(viewName);
+    view.setTableType(TableType.VIRTUAL_VIEW.name());
+    view.setPartitionKeys(viewPartitionCols);
+    view.setViewOriginalText("SELECT income, name FROM " + tblName);
+    view.setViewExpandedText("SELECT `" + tblName + "`.`income`, `" + tblName +
+        "`.`name` FROM `" + dbName + "`.`" + tblName + "`");
+    StorageDescriptor viewSd = new StorageDescriptor();
+    view.setSd(viewSd);
+    viewSd.setCols(viewCols);
+    viewSd.setCompressed(false);
+    viewSd.setParameters(new HashMap<String, String>());
+    viewSd.setSerdeInfo(new SerDeInfo());
+    viewSd.getSerdeInfo().setParameters(new HashMap<String, String>());
+
+    client.createTable(view);
+
+    if (isThriftClient) {
+      // the createTable() above does not update the location in the 'tbl'
+      // object when the client is a thrift client and the code below relies
+      // on the location being present in the 'tbl' object - so get the table
+      // from the metastore
+      view = client.getTable(dbName, viewName);
+    }
+
+    List<String> vals = new ArrayList<String>(1);
+    vals.add("abc");
+
+    Partition part = new Partition();
+    part.setDbName(dbName);
+    part.setTableName(viewName);
+    part.setValues(vals);
+    part.setParameters(new HashMap<String, String>());
+
+    client.add_partition(part);
+
+    Partition part2 = client.getPartition(dbName, viewName, part.getValues());
+
+    part2.getParameters().put("a", "b");
+
+    client.alter_partition(dbName, viewName, part2);
+
+    Partition part3 = client.getPartition(dbName, viewName, part.getValues());
+    assertEquals("couldn't view alter partition", part3.getParameters().get(
+        "a"), "b");
+
+    client.dropTable(dbName, viewName);
+
+    client.dropTable(dbName, tblName);
+
+    client.dropDatabase(dbName);
+  }
+
+  public void testAlterPartition() throws Throwable {
+
+    try {
+      String dbName = "compdb";
+      String tblName = "comptbl";
+      List<String> vals = new ArrayList<String>(2);
+      vals.add("2008-07-01");
+      vals.add("14");
+
+      client.dropTable(dbName, tblName);
+      silentDropDatabase(dbName);
+      Database db = new Database();
+      db.setName(dbName);
+      db.setDescription("Alter Partition Test database");
+      client.createDatabase(db);
+
+      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
+      cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+      cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+
+      Table tbl = new Table();
+      tbl.setDbName(dbName);
+      tbl.setTableName(tblName);
+      StorageDescriptor sd = new StorageDescriptor();
+      tbl.setSd(sd);
+      sd.setCols(cols);
+      sd.setCompressed(false);
+      sd.setNumBuckets(1);
+      sd.setParameters(new HashMap<String, String>());
+      sd.getParameters().put("test_param_1", "Use this for comments etc");
+      sd.setBucketCols(new ArrayList<String>(2));
+      sd.getBucketCols().add("name");
+      sd.setSerdeInfo(new SerDeInfo());
+      sd.getSerdeInfo().setName(tbl.getTableName());
+      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+      sd.getSerdeInfo().getParameters()
+          .put(serdeConstants.SERIALIZATION_FORMAT, "1");
+      sd.setSortCols(new ArrayList<Order>());
+
+      tbl.setPartitionKeys(new ArrayList<FieldSchema>(2));
+      tbl.getPartitionKeys().add(
+          new FieldSchema("ds", serdeConstants.STRING_TYPE_NAME, ""));
+      tbl.getPartitionKeys().add(
+          new FieldSchema("hr", serdeConstants.INT_TYPE_NAME, ""));
+
+      client.createTable(tbl);
+
+      if (isThriftClient) {
+        // the createTable() above does not update the location in the 'tbl'
+        // object when the client is a thrift client and the code below relies
+        // on the location being present in the 'tbl' object - so get the table
+        // from the metastore
+        tbl = client.getTable(dbName, tblName);
+      }
+
+      Partition part = new Partition();
+      part.setDbName(dbName);
+      part.setTableName(tblName);
+      part.setValues(vals);
+      part.setParameters(new HashMap<String, String>());
+      part.setSd(tbl.getSd());
+      part.getSd().setSerdeInfo(tbl.getSd().getSerdeInfo());
+      part.getSd().setLocation(tbl.getSd().getLocation() + "/part1");
+
+      client.add_partition(part);
+
+      Partition part2 = client.getPartition(dbName, tblName, part.getValues());
+
+      part2.getParameters().put("retention", "10");
+      part2.getSd().setNumBuckets(12);
+      part2.getSd().getSerdeInfo().getParameters().put("abc", "1");
+      client.alter_partition(dbName, tblName, part2);
+
+      Partition part3 = client.getPartition(dbName, tblName, part.getValues());
+      assertEquals("couldn't alter partition", part3.getParameters().get(
+          "retention"), "10");
+      assertEquals("couldn't alter partition", part3.getSd().getSerdeInfo()
+          .getParameters().get("abc"), "1");
+      assertEquals("couldn't alter partition", part3.getSd().getNumBuckets(),
+          12);
+
+      client.dropTable(dbName, tblName);
+
+      client.dropDatabase(dbName);
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testPartition() failed.");
+      throw e;
+    }
+  }
+
+  public void testRenamePartition() throws Throwable {
+
+    try {
+      String dbName = "compdb1";
+      String tblName = "comptbl1";
+      List<String> vals = new ArrayList<String>(2);
+      vals.add("2011-07-11");
+      vals.add("8");
+      String part_path = "/ds=2011-07-11/hr=8";
+      List<String> tmp_vals = new ArrayList<String>(2);
+      tmp_vals.add("tmp_2011-07-11");
+      tmp_vals.add("-8");
+      String part2_path = "/ds=tmp_2011-07-11/hr=-8";
+
+      client.dropTable(dbName, tblName);
+      silentDropDatabase(dbName);
+      Database db = new Database();
+      db.setName(dbName);
+      db.setDescription("Rename Partition Test database");
+      client.createDatabase(db);
+
+      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
+      cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+      cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+
+      Table tbl = new Table();
+      tbl.setDbName(dbName);
+      tbl.setTableName(tblName);
+      StorageDescriptor sd = new StorageDescriptor();
+      tbl.setSd(sd);
+      sd.setCols(cols);
+      sd.setCompressed(false);
+      sd.setNumBuckets(1);
+      sd.setParameters(new HashMap<String, String>());
+      sd.getParameters().put("test_param_1", "Use this for comments etc");
+      sd.setBucketCols(new ArrayList<String>(2));
+      sd.getBucketCols().add("name");
+      sd.setSerdeInfo(new SerDeInfo());
+      sd.getSerdeInfo().setName(tbl.getTableName());
+      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+      sd.getSerdeInfo().getParameters()
+          .put(serdeConstants.SERIALIZATION_FORMAT, "1");
+      sd.setSortCols(new ArrayList<Order>());
+
+      tbl.setPartitionKeys(new ArrayList<FieldSchema>(2));
+      tbl.getPartitionKeys().add(
+          new FieldSchema("ds", serdeConstants.STRING_TYPE_NAME, ""));
+      tbl.getPartitionKeys().add(
+          new FieldSchema("hr", serdeConstants.INT_TYPE_NAME, ""));
+
+      client.createTable(tbl);
+
+      if (isThriftClient) {
+        // the createTable() above does not update the location in the 'tbl'
+        // object when the client is a thrift client and the code below relies
+        // on the location being present in the 'tbl' object - so get the table
+        // from the metastore
+        tbl = client.getTable(dbName, tblName);
+      }
+
+      Partition part = new Partition();
+      part.setDbName(dbName);
+      part.setTableName(tblName);
+      part.setValues(vals);
+      part.setParameters(new HashMap<String, String>());
+      part.setSd(tbl.getSd().deepCopy());
+      part.getSd().setSerdeInfo(tbl.getSd().getSerdeInfo());
+      part.getSd().setLocation(tbl.getSd().getLocation() + "/part1");
+      part.getParameters().put("retention", "10");
+      part.getSd().setNumBuckets(12);
+      part.getSd().getSerdeInfo().getParameters().put("abc", "1");
+
+      client.add_partition(part);
+
+      part.setValues(tmp_vals);
+      client.renamePartition(dbName, tblName, vals, part);
+
+      boolean exceptionThrown = false;
+      try {
+        Partition p = client.getPartition(dbName, tblName, vals);
+      } catch(Exception e) {
+        assertEquals("partition should not have existed",
+            NoSuchObjectException.class, e.getClass());
+        exceptionThrown = true;
+      }
+      assertTrue("Expected NoSuchObjectException", exceptionThrown);
+
+      Partition part3 = client.getPartition(dbName, tblName, tmp_vals);
+      assertEquals("couldn't rename partition", part3.getParameters().get(
+          "retention"), "10");
+      assertEquals("couldn't rename partition", part3.getSd().getSerdeInfo()
+          .getParameters().get("abc"), "1");
+      assertEquals("couldn't rename partition", part3.getSd().getNumBuckets(),
+          12);
+      assertEquals("new partition sd matches", part3.getSd().getLocation(),
+          tbl.getSd().getLocation() + part2_path);
+
+      part.setValues(vals);
+      client.renamePartition(dbName, tblName, tmp_vals, part);
+
+      exceptionThrown = false;
+      try {
+        Partition p = client.getPartition(dbName, tblName, tmp_vals);
+      } catch(Exception e) {
+        assertEquals("partition should not have existed",
+            NoSuchObjectException.class, e.getClass());
+        exceptionThrown = true;
+      }
+      assertTrue("Expected NoSuchObjectException", exceptionThrown);
+
+      part3 = client.getPartition(dbName, tblName, vals);
+      assertEquals("couldn't rename partition", part3.getParameters().get(
+          "retention"), "10");
+      assertEquals("couldn't rename partition", part3.getSd().getSerdeInfo()
+          .getParameters().get("abc"), "1");
+      assertEquals("couldn't rename partition", part3.getSd().getNumBuckets(),
+          12);
+      assertEquals("new partition sd matches", part3.getSd().getLocation(),
+          tbl.getSd().getLocation() + part_path);
+
+      client.dropTable(dbName, tblName);
+
+      client.dropDatabase(dbName);
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testRenamePartition() failed.");
+      throw e;
+    }
+  }
+
+  public void testDatabase() throws Throwable {
+    try {
+      // clear up any existing databases
+      silentDropDatabase(TEST_DB1_NAME);
+      silentDropDatabase(TEST_DB2_NAME);
+
+      Database db = new Database();
+      db.setName(TEST_DB1_NAME);
+      client.createDatabase(db);
+
+      db = client.getDatabase(TEST_DB1_NAME);
+
+      assertEquals("name of returned db is different from that of inserted db",
+          TEST_DB1_NAME, db.getName());
+      assertEquals("location of the returned db is different from that of inserted db",
+          warehouse.getDatabasePath(db).toString(), db.getLocationUri());
+
+      Database db2 = new Database();
+      db2.setName(TEST_DB2_NAME);
+      client.createDatabase(db2);
+
+      db2 = client.getDatabase(TEST_DB2_NAME);
+
+      assertEquals("name of returned db is different from that of inserted db",
+          TEST_DB2_NAME, db2.getName());
+      assertEquals("location of the returned db is different from that of inserted db",
+          warehouse.getDatabasePath(db2).toString(), db2.getLocationUri());
+
+      List<String> dbs = client.getDatabases(".*");
+
+      assertTrue("first database is not " + TEST_DB1_NAME, dbs.contains(TEST_DB1_NAME));
+      assertTrue("second database is not " + TEST_DB2_NAME, dbs.contains(TEST_DB2_NAME));
+
+      client.dropDatabase(TEST_DB1_NAME);
+      client.dropDatabase(TEST_DB2_NAME);
+      silentDropDatabase(TEST_DB1_NAME);
+      silentDropDatabase(TEST_DB2_NAME);
+    } catch (Throwable e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testDatabase() failed.");
+      throw e;
+    }
+  }
+
+  public void testDatabaseLocationWithPermissionProblems() throws Exception {
+
+    // Note: The following test will fail if you are running this test as root. Setting
+    // permission to '0' on the database folder will not preclude root from being able
+    // to create the necessary files.
+
+    if (System.getProperty("user.name").equals("root")) {
+      System.err.println("Skipping test because you are running as root!");
+      return;
+    }
+
+    silentDropDatabase(TEST_DB1_NAME);
+
+    Database db = new Database();
+    db.setName(TEST_DB1_NAME);
+    String dbLocation =
+      HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/test/_testDB_create_";
+    FileSystem fs = FileSystem.get(new Path(dbLocation).toUri(), hiveConf);
+    fs.mkdirs(
+              new Path(HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/test"),
+              new FsPermission((short) 0));
+    db.setLocationUri(dbLocation);
+
+
+    boolean createFailed = false;
+    try {
+      client.createDatabase(db);
+    } catch (MetaException cantCreateDB) {
+      createFailed = true;
+    } finally {
+      // Cleanup
+      if (!createFailed) {
+        try {
+          client.dropDatabase(TEST_DB1_NAME);
+        } catch(Exception e) {
+          System.err.println("Failed to remove database in cleanup: " + e.getMessage());
+        }
+      }
+
+      fs.setPermission(new Path(HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/test"),
+                       new FsPermission((short) 755));
+      fs.delete(new Path(HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/test"), true);
+    }
+
+    assertTrue("Database creation succeeded even with permission problem", createFailed);
+  }
+
+  public void testDatabaseLocation() throws Throwable {
+    try {
+      // clear up any existing databases
+      silentDropDatabase(TEST_DB1_NAME);
+
+      Database db = new Database();
+      db.setName(TEST_DB1_NAME);
+      String dbLocation =
+          HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/_testDB_create_";
+      db.setLocationUri(dbLocation);
+      client.createDatabase(db);
+
+      db = client.getDatabase(TEST_DB1_NAME);
+
+      assertEquals("name of returned db is different from that of inserted db",
+          TEST_DB1_NAME, db.getName());
+      assertEquals("location of the returned db is different from that of inserted db",
+          warehouse.getDnsPath(new Path(dbLocation)).toString(), db.getLocationUri());
+
+      client.dropDatabase(TEST_DB1_NAME);
+      silentDropDatabase(TEST_DB1_NAME);
+
+      boolean objectNotExist = false;
+      try {
+        client.getDatabase(TEST_DB1_NAME);
+      } catch (NoSuchObjectException e) {
+        objectNotExist = true;
+      }
+      assertTrue("Database " + TEST_DB1_NAME + " exists ", objectNotExist);
+
+      db = new Database();
+      db.setName(TEST_DB1_NAME);
+      dbLocation =
+          HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/_testDB_file_";
+      FileSystem fs = FileSystem.get(new Path(dbLocation).toUri(), hiveConf);
+      fs.createNewFile(new Path(dbLocation));
+      fs.deleteOnExit(new Path(dbLocation));
+      db.setLocationUri(dbLocation);
+
+      boolean createFailed = false;
+      try {
+        client.createDatabase(db);
+      } catch (MetaException cantCreateDB) {
+        System.err.println(cantCreateDB.getMessage());
+        createFailed = true;
+      }
+      assertTrue("Database creation succeeded even location exists and is a file", createFailed);
+
+      objectNotExist = false;
+      try {
+        client.getDatabase(TEST_DB1_NAME);
+      } catch (NoSuchObjectException e) {
+        objectNotExist = true;
+      }
+      assertTrue("Database " + TEST_DB1_NAME + " exists when location is specified and is a file",
+          objectNotExist);
+
+    } catch (Throwable e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testDatabaseLocation() failed.");
+      throw e;
+    }
+  }
+
+
+  public void testSimpleTypeApi() throws Exception {
+    try {
+      client.dropType(serdeConstants.INT_TYPE_NAME);
+
+      Type typ1 = new Type();
+      typ1.setName(serdeConstants.INT_TYPE_NAME);
+      boolean ret = client.createType(typ1);
+      assertTrue("Unable to create type", ret);
+
+      Type typ1_2 = client.getType(serdeConstants.INT_TYPE_NAME);
+      assertNotNull(typ1_2);
+      assertEquals(typ1.getName(), typ1_2.getName());
+
+      ret = client.dropType(serdeConstants.INT_TYPE_NAME);
+      assertTrue("unable to drop type integer", ret);
+
+      boolean exceptionThrown = false;
+      try {
+        client.getType(serdeConstants.INT_TYPE_NAME);
+      } catch (NoSuchObjectException e) {
+        exceptionThrown = true;
+      }
+      assertTrue("Expected NoSuchObjectException", exceptionThrown);
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testSimpleTypeApi() failed.");
+      throw e;
+    }
+  }
+
+  // TODO:pc need to enhance this with complex fields and getType_all function
+  public void testComplexTypeApi() throws Exception {
+    try {
+      client.dropType("Person");
+
+      Type typ1 = new Type();
+      typ1.setName("Person");
+      typ1.setFields(new ArrayList<FieldSchema>(2));
+      typ1.getFields().add(
+          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+      typ1.getFields().add(
+          new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+      boolean ret = client.createType(typ1);
+      assertTrue("Unable to create type", ret);
+
+      Type typ1_2 = client.getType("Person");
+      assertNotNull("type Person not found", typ1_2);
+      assertEquals(typ1.getName(), typ1_2.getName());
+      assertEquals(typ1.getFields().size(), typ1_2.getFields().size());
+      assertEquals(typ1.getFields().get(0), typ1_2.getFields().get(0));
+      assertEquals(typ1.getFields().get(1), typ1_2.getFields().get(1));
+
+      client.dropType("Family");
+
+      Type fam = new Type();
+      fam.setName("Family");
+      fam.setFields(new ArrayList<FieldSchema>(2));
+      fam.getFields().add(
+          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+      fam.getFields().add(
+          new FieldSchema("members",
+              MetaStoreUtils.getListType(typ1.getName()), ""));
+
+      ret = client.createType(fam);
+      assertTrue("Unable to create type " + fam.getName(), ret);
+
+      Type fam2 = client.getType("Family");
+      assertNotNull("type Person not found", fam2);
+      assertEquals(fam.getName(), fam2.getName());
+      assertEquals(fam.getFields().size(), fam2.getFields().size());
+      assertEquals(fam.getFields().get(0), fam2.getFields().get(0));
+      assertEquals(fam.getFields().get(1), fam2.getFields().get(1));
+
+      ret = client.dropType("Family");
+      assertTrue("unable to drop type Family", ret);
+
+      ret = client.dropType("Person");
+      assertTrue("unable to drop type Person", ret);
+
+      boolean exceptionThrown = false;
+      try {
+        client.getType("Person");
+      } catch (NoSuchObjectException e) {
+        exceptionThrown = true;
+      }
+      assertTrue("Expected NoSuchObjectException", exceptionThrown);
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testComplexTypeApi() failed.");
+      throw e;
+    }
+  }
+
+  public void testSimpleTable() throws Exception {
+    try {
+      String dbName = "simpdb";
+      String tblName = "simptbl";
+      String tblName2 = "simptbl2";
+      String typeName = "Person";
+
+      client.dropTable(dbName, tblName);
+      silentDropDatabase(dbName);
+
+      Database db = new Database();
+      db.setName(dbName);
+      client.createDatabase(db);
+
+      client.dropType(typeName);
+      Type typ1 = new Type();
+      typ1.setName(typeName);
+      typ1.setFields(new ArrayList<FieldSchema>(2));
+      typ1.getFields().add(
+          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+      typ1.getFields().add(
+          new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+      client.createType(typ1);
+
+      Table tbl = new Table();
+      tbl.setDbName(dbName);
+      tbl.setTableName(tblName);
+      StorageDescriptor sd = new StorageDescriptor();
+      tbl.setSd(sd);
+      sd.setCols(typ1.getFields());
+      sd.setCompressed(false);
+      sd.setNumBuckets(1);
+      sd.setParameters(new HashMap<String, String>());
+      sd.getParameters().put("test_param_1", "Use this for comments etc");
+      sd.setBucketCols(new ArrayList<String>(2));
+      sd.getBucketCols().add("name");
+      sd.setSerdeInfo(new SerDeInfo());
+      sd.getSerdeInfo().setName(tbl.getTableName());
+      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+      sd.getSerdeInfo().getParameters().put(
+          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
+      sd.getSerdeInfo().setSerializationLib(
+          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
+      tbl.setPartitionKeys(new ArrayList<FieldSchema>());
+
+      client.createTable(tbl);
+
+      if (isThriftClient) {
+        // the createTable() above does not update the location in the 'tbl'
+        // object when the client is a thrift client and the code below relies
+        // on the location being present in the 'tbl' object - so get the table
+        // from the metastore
+        tbl = client.getTable(dbName, tblName);
+      }
+
+      Table tbl2 = client.getTable(dbName, tblName);
+      assertNotNull(tbl2);
+      assertEquals(tbl2.getDbName(), dbName);
+      assertEquals(tbl2.getTableName(), tblName);
+      assertEquals(tbl2.getSd().getCols().size(), typ1.getFields().size());
+      assertEquals(tbl2.getSd().isCompressed(), false);
+      assertEquals(tbl2.getSd().getNumBuckets(), 1);
+      assertEquals(tbl2.getSd().getLocation(), tbl.getSd().getLocation());
+      assertNotNull(tbl2.getSd().getSerdeInfo());
+      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+      sd.getSerdeInfo().getParameters().put(
+          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
+
+      tbl2.setTableName(tblName2);
+      tbl2.setParameters(new HashMap<String, String>());
+      tbl2.getParameters().put("EXTERNAL", "TRUE");
+      tbl2.getSd().setLocation(tbl.getSd().getLocation() + "-2");
+
+      List<FieldSchema> fieldSchemas = client.getFields(dbName, tblName);
+      assertNotNull(fieldSchemas);
+      assertEquals(fieldSchemas.size(), tbl.getSd().getCols().size());
+      for (FieldSchema fs : tbl.getSd().getCols()) {
+        assertTrue(fieldSchemas.contains(fs));
+      }
+
+      List<FieldSchema> fieldSchemasFull = client.getSchema(dbName, tblName);
+      assertNotNull(fieldSchemasFull);
+      assertEquals(fieldSchemasFull.size(), tbl.getSd().getCols().size()
+          + tbl.getPartitionKeys().size());
+      for (FieldSchema fs : tbl.getSd().getCols()) {
+        assertTrue(fieldSchemasFull.contains(fs));
+      }
+      for (FieldSchema fs : tbl.getPartitionKeys()) {
+        assertTrue(fieldSchemasFull.contains(fs));
+      }
+
+      client.createTable(tbl2);
+      if (isThriftClient) {
+        tbl2 = client.getTable(tbl2.getDbName(), tbl2.getTableName());
+      }
+
+      Table tbl3 = client.getTable(dbName, tblName2);
+      assertNotNull(tbl3);
+      assertEquals(tbl3.getDbName(), dbName);
+      assertEquals(tbl3.getTableName(), tblName2);
+      assertEquals(tbl3.getSd().getCols().size(), typ1.getFields().size());
+      assertEquals(tbl3.getSd().isCompressed(), false);
+      assertEquals(tbl3.getSd().getNumBuckets(), 1);
+      assertEquals(tbl3.getSd().getLocation(), tbl2.getSd().getLocation());
+      assertEquals(tbl3.getParameters(), tbl2.getParameters());
+
+      fieldSchemas = client.getFields(dbName, tblName2);
+      assertNotNull(fieldSchemas);
+      assertEquals(fieldSchemas.size(), tbl2.getSd().getCols().size());
+      for (FieldSchema fs : tbl2.getSd().getCols()) {
+        assertTrue(fieldSchemas.contains(fs));
+      }
+
+      fieldSchemasFull = client.getSchema(dbName, tblName2);
+      assertNotNull(fieldSchemasFull);
+      assertEquals(fieldSchemasFull.size(), tbl2.getSd().getCols().size()
+          + tbl2.getPartitionKeys().size());
+      for (FieldSchema fs : tbl2.getSd().getCols()) {
+        assertTrue(fieldSchemasFull.contains(fs));
+      }
+      for (FieldSchema fs : tbl2.getPartitionKeys()) {
+        assertTrue(fieldSchemasFull.contains(fs));
+      }
+
+      assertEquals("Use this for comments etc", tbl2.getSd().getParameters()
+          .get("test_param_1"));
+      assertEquals("name", tbl2.getSd().getBucketCols().get(0));
+      assertTrue("Partition key list is not empty",
+          (tbl2.getPartitionKeys() == null)
+              || (tbl2.getPartitionKeys().size() == 0));
+
+      //test get_table_objects_by_name functionality
+      ArrayList<String> tableNames = new ArrayList<String>();
+      tableNames.add(tblName2);
+      tableNames.add(tblName);
+      tableNames.add(tblName2);
+      List<Table> foundTables = client.getTableObjectsByName(dbName, tableNames);
+
+      assertEquals(foundTables.size(), 2);
+      for (Table t: foundTables) {
+        if (t.getTableName().equals(tblName2)) {
+          assertEquals(t.getSd().getLocation(), tbl2.getSd().getLocation());
+        } else {
+          assertEquals(t.getTableName(), tblName);
+          assertEquals(t.getSd().getLocation(), tbl.getSd().getLocation());
+        }
+        assertEquals(t.getSd().getCols().size(), typ1.getFields().size());
+        assertEquals(t.getSd().isCompressed(), false);
+        assertEquals(foundTables.get(0).getSd().getNumBuckets(), 1);
+        assertNotNull(t.getSd().getSerdeInfo());
+        assertEquals(t.getDbName(), dbName);
+      }
+
+      tableNames.add(1, "table_that_doesnt_exist");
+      foundTables = client.getTableObjectsByName(dbName, tableNames);
+      assertEquals(foundTables.size(), 2);
+
+      InvalidOperationException ioe = null;
+      try {
+        foundTables = client.getTableObjectsByName(dbName, null);
+      } catch (InvalidOperationException e) {
+        ioe = e;
+      }
+      assertNotNull(ioe);
+      assertTrue("Table not found", ioe.getMessage().contains("null tables"));
+
+      UnknownDBException udbe = null;
+      try {
+        foundTables = client.getTableObjectsByName("db_that_doesnt_exist", tableNames);
+      } catch (UnknownDBException e) {
+        udbe = e;
+      }
+      assertNotNull(udbe);
+      assertTrue("DB not found", udbe.getMessage().contains("not find database db_that_doesnt_exist"));
+
+      udbe = null;
+      try {
+        foundTables = client.getTableObjectsByName("", tableNames);
+      } catch (UnknownDBException e) {
+        udbe = e;
+      }
+      assertNotNull(udbe);
+      assertTrue("DB not found", udbe.getMessage().contains("is null or empty"));
+
+      FileSystem fs = FileSystem.get((new Path(tbl.getSd().getLocation())).toUri(), hiveConf);
+      client.dropTable(dbName, tblName);
+      assertFalse(fs.exists(new Path(tbl.getSd().getLocation())));
+
+      client.dropTable(dbName, tblName2);
+      assertTrue(fs.exists(new Path(tbl2.getSd().getLocation())));
+
+      client.dropType(typeName);
+      client.dropDatabase(dbName);
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testSimpleTable() failed.");
+      throw e;
+    }
+  }
+
+  public void testColumnStatistics() throws Throwable {
+
+    String dbName = "columnstatstestdb";
+    String tblName = "tbl";
+    String typeName = "Person";
+    String tblOwner = "testowner";
+    int lastAccessed = 6796;
+
+    try {
+      cleanUp(dbName, tblName, typeName);
+      Database db = new Database();
+      db.setName(dbName);
+      client.createDatabase(db);
+      createTableForTestFilter(dbName,tblName, tblOwner, lastAccessed, true);
+
+      // Create a ColumnStatistics Obj
+      String[] colName = new String[]{"income", "name"};
+      double lowValue = 50000.21;
+      double highValue = 1200000.4525;
+      long numNulls = 3;
+      long numDVs = 22;
+      double avgColLen = 50.30;
+      long maxColLen = 102;
+      String[] colType = new String[] {"double", "string"};
+      boolean isTblLevel = true;
+      String partName = null;
+      List<ColumnStatisticsObj> statsObjs = new ArrayList<ColumnStatisticsObj>();
+
+      ColumnStatisticsDesc statsDesc = new ColumnStatisticsDesc();
+      statsDesc.setDbName(dbName);
+      statsDesc.setTableName(tblName);
+      statsDesc.setIsTblLevel(isTblLevel);
+      statsDesc.setPartName(partName);
+
+      ColumnStatisticsObj statsObj = new ColumnStatisticsObj();
+      statsObj.setColName(colName[0]);
+      statsObj.setColType(colType[0]);
+
+      ColumnStatisticsData statsData = new ColumnStatisticsData();
+      DoubleColumnStatsData numericStats = new DoubleColumnStatsData();
+      statsData.setDoubleStats(numericStats);
+
+      statsData.getDoubleStats().setHighValue(highValue);
+      statsData.getDoubleStats().setLowValue(lowValue);
+      statsData.getDoubleStats().setNumDVs(numDVs);
+      statsData.getDoubleStats().setNumNulls(numNulls);
+
+      statsObj.setStatsData(statsData);
+      statsObjs.add(statsObj);
+
+      statsObj = new ColumnStatisticsObj();
+      statsObj.setColName(colName[1]);
+      statsObj.setColType(colType[1]);
+
+      statsData = new ColumnStatisticsData();
+      StringColumnStatsData stringStats = new StringColumnStatsData();
+      statsData.setStringStats(stringStats);
+      statsData.getStringStats().setAvgColLen(avgColLen);
+      statsData.getStringStats().setMaxColLen(maxColLen);
+      statsData.getStringStats().setNumDVs(numDVs);
+      statsData.getStringStats().setNumNulls(numNulls);
+
+      statsObj.setStatsData(statsData);
+      statsObjs.add(statsObj);
+
+      ColumnStatistics colStats = new ColumnStatistics();
+      colStats.setStatsDesc(statsDesc);
+      colStats.setStatsObj(statsObjs);
+
+      // write stats objs persistently
+      client.updateTableColumnStatistics(colStats);
+
+      // retrieve the stats obj that was just written
+      ColumnStatistics colStats2 = client.getTableColumnStatistics(dbName, tblName, colName[0]);
+
+     // compare stats obj to ensure what we get is what we wrote
+      assertNotNull(colStats2);
+      assertEquals(colStats2.getStatsDesc().getDbName(), dbName);
+      assertEquals(colStats2.getStatsDesc().getTableName(), tblName);
+      assertEquals(colStats2.getStatsObj().get(0).getColName(), colName[0]);
+      assertEquals(colStats2.getStatsObj().get(0).getStatsData().getDoubleStats().getLowValue(),
+        lowValue);
+      assertEquals(colStats2.getStatsObj().get(0).getStatsData().getDoubleStats().getHighValue(),
+        highValue);
+      assertEquals(colStats2.getStatsObj().get(0).getStatsData().getDoubleStats().getNumNulls(),
+        numNulls);
+      assertEquals(colStats2.getStatsObj().get(0).getStatsData().getDoubleStats().getNumDVs(),
+        numDVs);
+      assertEquals(colStats2.getStatsDesc().isIsTblLevel(), isTblLevel);
+
+      // test delete column stats; if no col name is passed all column stats associated with the
+      // table is deleted
+      boolean status = client.deleteTableColumnStatistics(dbName, tblName, null);
+      assertTrue(status);
+      // try to query stats for a column for which stats doesn't exist
+      try {
+        colStats2 = client.getTableColumnStatistics(dbName, tblName, colName[1]);
+        assertTrue(true);
+      } catch (NoSuchObjectException e) {
+        System.out.println("Statistics for column=" + colName[1] + " not found");
+      }
+
+      colStats.setStatsDesc(statsDesc);
+      colStats.setStatsObj(statsObjs);
+
+      // update table level column stats
+      client.updateTableColumnStatistics(colStats);
+
+      // query column stats for column whose stats were updated in the previous call
+      colStats2 = client.getTableColumnStatistics(dbName, tblName, colName[0]);
+
+      // partition level column statistics test
+      // create a table with multiple partitions
+      cleanUp(dbName, tblName, typeName);
+
+      List<List<String>> values = new ArrayList<List<String>>();
+      values.add(makeVals("2008-07-01 14:13:12", "14"));
+      values.add(makeVals("2008-07-01 14:13:12", "15"));
+      values.add(makeVals("2008-07-02 14:13:12", "15"));
+      values.add(makeVals("2008-07-03 14:13:12", "151"));
+
+      createMultiPartitionTableSchema(dbName, tblName, typeName, values);
+
+      List<String> partitions = client.listPartitionNames(dbName, tblName, (short)-1);
+
+      partName = partitions.get(0);
+      isTblLevel = false;
+
+      // create a new columnstatistics desc to represent partition level column stats
+      statsDesc = new ColumnStatisticsDesc();
+      statsDesc.setDbName(dbName);
+      statsDesc.setTableName(tblName);
+      statsDesc.setPartName(partName);
+      statsDesc.setIsTblLevel(isTblLevel);
+
+      colStats = new ColumnStatistics();
+      colStats.setStatsDesc(statsDesc);
+      colStats.setStatsObj(statsObjs);
+
+     client.updatePartitionColumnStatistics(colStats);
+
+     colStats2 = client.getPartitionColumnStatistics(dbName, tblName, partName, colName[1]);
+
+     // compare stats obj to ensure what we get is what we wrote
+     assertNotNull(colStats2);
+     assertEquals(colStats2.getStatsDesc().getDbName(), dbName);
+     assertEquals(colStats2.getStatsDesc().getTableName(), tblName);
+     assertEquals(colStats.getStatsDesc().getPartName(), partName);
+     assertEquals(colStats2.getStatsObj().get(0).getColName(), colName[1]);
+     assertEquals(colStats2.getStatsObj().get(0).getStatsData().getStringStats().getMaxColLen(),
+       maxColLen);
+     assertEquals(colStats2.getStatsObj().get(0).getStatsData().getStringStats().getAvgColLen(),
+       avgColLen);
+     assertEquals(colStats2.getStatsObj().get(0).getStatsData().getStringStats().getNumNulls(),
+       numNulls);
+     assertEquals(colStats2.getStatsObj().get(0).getStatsData().getStringStats().getNumDVs(),
+       numDVs);
+     assertEquals(colStats2.getStatsDesc().isIsTblLevel(), isTblLevel);
+
+     // test stats deletion at partition level
+     client.deletePartitionColumnStatistics(dbName, tblName, partName, colName[1]);
+
+     colStats2 = client.getPartitionColumnStatistics(dbName, tblName, partName, colName[0]);
+
+     // test get stats on a column for which stats doesn't exist
+     try {
+       colStats2 = client.getPartitionColumnStatistics(dbName, tblName, partName, colName[1]);
+       assertTrue(true);
+     } catch (NoSuchObjectException e) {
+       System.out.println("Statistics for column=" + colName[1] + " not found");
+     }
+
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testColumnStatistics() failed.");
+      throw e;
+    } finally {
+      cleanUp(dbName, tblName, typeName);
+    }
+  }
+
+  public void testAlterTable() throws Exception {
+    String dbName = "alterdb";
+    String invTblName = "alter-tbl";
+    String tblName = "altertbl";
+
+    try {
+      client.dropTable(dbName, tblName);
+      silentDropDatabase(dbName);
+
+      Database db = new Database();
+      db.setName(dbName);
+      client.createDatabase(db);
+
+      ArrayList<FieldSchema> invCols = new ArrayList<FieldSchema>(2);
+      invCols.add(new FieldSchema("n-ame", serdeConstants.STRING_TYPE_NAME, ""));
+      invCols.add(new FieldSchema("in.come", serdeConstants.INT_TYPE_NAME, ""));
+
+      Table tbl = new Table();
+      tbl.setDbName(dbName);
+      tbl.setTableName(invTblName);
+      StorageDescriptor sd = new StorageDescriptor();
+      tbl.setSd(sd);
+      sd.setCols(invCols);
+      sd.setCompressed(false);
+      sd.setNumBuckets(1);
+      sd.setParameters(new HashMap<String, String>());
+      sd.getParameters().put("test_param_1", "Use this for comments etc");
+      sd.setBucketCols(new ArrayList<String>(2));
+      sd.getBucketCols().add("name");
+      sd.setSerdeInfo(new SerDeInfo());
+      sd.getSerdeInfo().setName(tbl.getTableName());
+      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+      sd.getSerdeInfo().getParameters().put(
+          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
+      boolean failed = false;
+      try {
+        client.createTable(tbl);
+      } catch (InvalidObjectException ex) {
+        failed = true;
+      }
+      if (!failed) {
+        assertTrue("Able to create table with invalid name: " + invTblName,
+            false);
+      }
+
+      // create an invalid table which has wrong column type
+      ArrayList<FieldSchema> invColsInvType = new ArrayList<FieldSchema>(2);
+      invColsInvType.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+      invColsInvType.add(new FieldSchema("income", "xyz", ""));
+      tbl.setTableName(tblName);
+      tbl.getSd().setCols(invColsInvType);
+      boolean failChecker = false;
+      try {
+        client.createTable(tbl);
+      } catch (InvalidObjectException ex) {
+        failChecker = true;
+      }
+      if (!failChecker) {
+        assertTrue("Able to create table with invalid column type: " + invTblName,
+            false);
+      }
+
+      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
+      cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+      cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+
+      // create a valid table
+      tbl.setTableName(tblName);
+      tbl.getSd().setCols(cols);
+      client.createTable(tbl);
+
+      if (isThriftClient) {
+        tbl = client.getTable(tbl.getDbName(), tbl.getTableName());
+      }
+
+      // now try to invalid alter table
+      Table tbl2 = client.getTable(dbName, tblName);
+      failed = false;
+      try {
+        tbl2.setTableName(invTblName);
+        tbl2.getSd().setCols(invCols);
+        client.alter_table(dbName, tblName, tbl2);
+      } catch (InvalidOperationException ex) {
+        failed = true;
+      }
+      if (!failed) {
+        assertTrue("Able to rename table with invalid name: " + invTblName,
+            false);
+      }
+
+      //try an invalid alter table with partition key name
+      Table tbl_pk = client.getTable(tbl.getDbName(), tbl.getTableName());
+      List<FieldSchema> partitionKeys = tbl_pk.getPartitionKeys();
+      for (FieldSchema fs : partitionKeys) {
+        fs.setName("invalid_to_change_name");
+        fs.setComment("can_change_comment");
+      }
+      tbl_pk.setPartitionKeys(partitionKeys);
+      try {
+        client.alter_table(dbName, tblName, tbl_pk);
+      } catch (InvalidOperationException ex) {
+        failed = true;
+      }
+      assertTrue("Should not have succeeded in altering partition key name", failed);
+
+      //try a valid alter table partition key comment
+      failed = false;
+      tbl_pk = client.getTable(tbl.getDbName(), tbl.getTableName());
+      partitionKeys = tbl_pk.getPartitionKeys();
+      for (FieldSchema fs : partitionKeys) {
+        fs.setComment("can_change_comment");
+      }
+      tbl_pk.setPartitionKeys(partitionKeys);
+      try {
+        client.alter_table(dbName, tblName, tbl_pk);
+      } catch (InvalidOperationException ex) {
+        failed = true;
+      }
+      assertFalse("Should not have failed alter table partition comment", failed);
+      Table newT = client.getTable(tbl.getDbName(), tbl.getTableName());
+      assertEquals(partitionKeys, newT.getPartitionKeys());
+
+      // try a valid alter table
+      tbl2.setTableName(tblName + "_renamed");
+      tbl2.getSd().setCols(cols);
+      tbl2.getSd().setNumBuckets(32);
+      client.alter_table(dbName, tblName, tbl2);
+      Table tbl3 = client.getTable(dbName, tbl2.getTableName());
+      assertEquals("Alter table didn't succeed. Num buckets is different ",
+          tbl2.getSd().getNumBuckets(), tbl3.getSd().getNumBuckets());
+      // check that data has moved
+      FileSystem fs = FileSystem.get((new Path(tbl.getSd().getLocation())).toUri(), hiveConf);
+      assertFalse("old table location still exists", fs.exists(new Path(tbl
+          .getSd().getLocation())));
+      assertTrue("data did not move to new location", fs.exists(new Path(tbl3
+          .getSd().getLocation())));
+
+      if (!isThriftClient) {
+        assertEquals("alter table didn't move data correct location", tbl3
+            .getSd().getLocation(), tbl2.getSd().getLocation());
+      }
+
+      // alter table with invalid column type
+      tbl_pk.getSd().setCols(invColsInvType);
+      failed = false;
+      try {
+        client.alter_table(dbName, tbl2.getTableName(), tbl_pk);
+      } catch (InvalidOperationException ex) {
+        failed = true;
+      }
+      assertTrue("Should not have succeeded in altering column", failed);
+
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testSimpleTable() failed.");
+      throw e;
+    } finally {
+      silentDropDatabase(dbName);
+    }
+  }
+
+  public void testComplexTable() throws Exception {
+
+    String dbName = "compdb";
+    String tblName = "comptbl";
+    String typeName = "Person";
+
+    try {
+      client.dropTable(dbName, tblName);
+      silentDropDatabase(dbName);
+      Database db = new Database();
+      db.setName(dbName);
+      client.createDatabase(db);
+
+      client.dropType(typeName);
+      Type typ1 = new Type();
+      typ1.setName(typeName);
+      typ1.setFields(new ArrayList<FieldSchema>(2));
+      typ1.getFields().add(
+          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+      typ1.getFields().add(
+          new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+      client.createType(typ1);
+
+      Table tbl = new Table();
+      tbl.setDbName(dbName);
+      tbl.setTableName(tblName);
+      StorageDescriptor sd = new StorageDescriptor();
+      tbl.setSd(sd);
+      sd.setCols(typ1.getFields());
+      sd.setCompressed(false);
+      sd.setNumBuckets(1);
+      sd.setParameters(new HashMap<String, String>());
+      sd.getParameters().put("test_param_1", "Use this for comments etc");
+      sd.setBucketCols(new ArrayList<String>(2));
+      sd.getBucketCols().add("name");
+      sd.setSerdeInfo(new SerDeInfo());
+      sd.getSerdeInfo().setName(tbl.getTableName());
+      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+      sd.getSerdeInfo().getParameters().put(
+          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "9");
+      sd.getSerdeInfo().setSerializationLib(
+          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
+
+      tbl.setPartitionKeys(new ArrayList<FieldSchema>(2));
+      tbl.getPartitionKeys().add(
+          new FieldSchema("ds",
+              org.apache.hadoop.hive.serde.serdeConstants.DATE_TYPE_NAME, ""));
+      tbl.getPartitionKeys().add(
+          new FieldSchema("hr",
+              org.apache.hadoop.hive.serde.serdeConstants.INT_TYPE_NAME, ""));
+
+      client.createTable(tbl);
+
+      Table tbl2 = client.getTable(dbName, tblName);
+      assertEquals(tbl2.getDbName(), dbName);
+      assertEquals(tbl2.getTableName(), tblName);
+      assertEquals(tbl2.getSd().getCols().size(), typ1.getFields().size());
+      assertFalse(tbl2.getSd().isCompressed());
+      assertFalse(tbl2.getSd().isStoredAsSubDirectories());
+      assertEquals(tbl2.getSd().getNumBuckets(), 1);
+
+      assertEquals("Use this for comments etc", tbl2.getSd().getParameters()
+          .get("test_param_1"));
+      assertEquals("name", tbl2.getSd().getBucketCols().get(0));
+
+      assertNotNull(tbl2.getPartitionKeys());
+      assertEquals(2, tbl2.getPartitionKeys().size());
+      assertEquals(serdeConstants.DATE_TYPE_NAME, tbl2.getPartitionKeys().get(0)
+          .getType());
+      assertEquals(serdeConstants.INT_TYPE_NAME, tbl2.getPartitionKeys().get(1)
+          .getType());
+      assertEquals("ds", tbl2.getPartitionKeys().get(0).getName());
+      assertEquals("hr", tbl2.getPartitionKeys().get(1).getName());
+
+      List<FieldSchema> fieldSchemas = client.getFields(dbName, tblName);
+      assertNotNull(fieldSchemas);
+      assertEquals(fieldSchemas.size(), tbl.getSd().getCols().size());
+      for (FieldSchema fs : tbl.getSd().getCols()) {
+        assertTrue(fieldSchemas.contains(fs));
+      }
+
+      List<FieldSchema> fieldSchemasFull = client.getSchema(dbName, tblName);
+      assertNotNull(fieldSchemasFull);
+      assertEquals(fieldSchemasFull.size(), tbl.getSd().getCols().size()
+          + tbl.getPartitionKeys().size());
+      for (FieldSchema fs : tbl.getSd().getCols()) {
+        assertTrue(fieldSchemasFull.contains(fs));
+      }
+      for (FieldSchema fs : tbl.getPartitionKeys()) {
+        assertTrue(fieldSchemasFull.contains(fs));
+      }
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testComplexTable() failed.");
+      throw e;
+    } finally {
+      client.dropTable(dbName, tblName);
+      boolean ret = client.dropType(typeName);
+      assertTrue("Unable to drop type " + typeName, ret);
+      client.dropDatabase(dbName);
+    }
+  }
+
+  public void testTableDatabase() throws Exception {
+    String dbName = "testDb";
+    String tblName_1 = "testTbl_1";
+    String tblName_2 = "testTbl_2";
+
+    try {
+      silentDropDatabase(dbName);
+
+      Database db = new Database();
+      db.setName(dbName);
+      String dbLocation =
+          HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "_testDB_table_create_";
+      db.setLocationUri(dbLocation);
+      client.createDatabase(db);
+      db = client.getDatabase(dbName);
+
+      Table tbl = new Table();
+      tbl.setDbName(dbName);
+      tbl.setTableName(tblName_1);
+
+      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
+      cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+      cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+
+      StorageDescriptor sd = new StorageDescriptor();
+      sd.setSerdeInfo(new SerDeInfo());
+      sd.getSerdeInfo().setName(tbl.getTableName());
+      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+      sd.setParameters(new HashMap<String, String>());
+      sd.getSerdeInfo().getParameters().put(
+          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "9");
+      sd.getSerdeInfo().setSerializationLib(
+          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
+
+      tbl.setSd(sd);
+      tbl.getSd().setCols(cols);
+      client.createTable(tbl);
+      tbl = client.getTable(dbName, tblName_1);
+
+      Path path = new Path(tbl.getSd().getLocation());
+      System.err.println("Table's location " + path + ", Database's location " + db.getLocationUri());
+      assertEquals("Table location is not a subset of the database location",
+          path.getParent().toString(), db.getLocationUri());
+
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testTableDatabase() failed.");
+      throw e;
+    } finally {
+      silentDropDatabase(dbName);
+    }
+  }
+
+
+  public void testGetConfigValue() {
+
+    String val = "value";
+
+    if (!isThriftClient) {
+      try {
+        assertEquals(client.getConfigValue("hive.key1", val), "value1");
+        assertEquals(client.getConfigValue("hive.key2", val), "http://www.example.com");
+        assertEquals(client.getConfigValue("hive.key3", val), "");
+        assertEquals(client.getConfigValue("hive.key4", val), "0");
+        assertEquals(client.getConfigValue("hive.key5", val), val);
+        assertEquals(client.getConfigValue(null, val), val);
+      } catch (ConfigValSecurityException e) {
+        e.printStackTrace();
+        assert (false);
+      } catch (TException e) {
+        e.printStackTrace();
+        assert (false);
+      }
+    }
+
+    boolean threwException = false;
+    try {
+      // Attempting to get the password should throw an exception
+      client.getConfigValue("javax.jdo.option.ConnectionPassword", "password");
+    } catch (ConfigValSecurityException e) {
+      threwException = true;
+    } catch (TException e) {
+      e.printStackTrace();
+      assert (false);
+    }
+    assert (threwException);
+  }
+
+  private static void adjust(HiveMetaStoreClient client, Partition part,
+      String dbName, String tblName)
+  throws NoSuchObjectException, MetaException, TException {
+    Partition part_get = client.getPartition(dbName, tblName, part.getValues());
+    part.setCreateTime(part_get.getCreateTime());
+    part.putToParameters(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.DDL_TIME, Long.toString(part_get.getCreateTime()));
+  }
+
+  private static void silentDropDatabase(String dbName) throws MetaException, TException {
+    try {
+      for (String tableName : client.getTables(dbName, "*")) {
+        client.dropTable(dbName, tableName);
+      }
+      client.dropDatabase(dbName);
+    } catch (NoSuchObjectException e) {
+    } catch (InvalidOperationException e) {
+    }
+  }
+
+  /**
+   * Tests for list partition by filter functionality.
+   * @throws Exception
+   */
+
+  public void testPartitionFilter() throws Exception {
+    String dbName = "filterdb";
+    String tblName = "filtertbl";
+
+    silentDropDatabase(dbName);
+
+    Database db = new Database();
+    db.setName(dbName);
+    client.createDatabase(db);
+
+    ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
+    cols.add(new FieldSchema("c1", serdeConstants.STRING_TYPE_NAME, ""));
+    cols.add(new FieldSchema("c2", serdeConstants.INT_TYPE_NAME, ""));
+
+    ArrayList<FieldSchema> partCols = new ArrayList<FieldSchema>(3);
+    partCols.add(new FieldSchema("p1", serdeConstants.STRING_TYPE_NAME, ""));
+    partCols.add(new FieldSchema("p2", serdeConstants.STRING_TYPE_NAME, ""));
+    partCols.add(new FieldSchema("p3", serdeConstants.INT_TYPE_NAME, ""));
+
+    Table tbl = new Table();
+    tbl.setDbName(dbName);
+    tbl.setTableName(tblName);
+    StorageDescriptor sd = new StorageDescriptor();
+    tbl.setSd(sd);
+    sd.setCols(cols);
+    sd.setCompressed(false);
+    sd.setNumBuckets(1);
+    sd.setParameters(new HashMap<String, String>());
+    sd.setBucketCols(new ArrayList<String>());
+    sd.setSerdeInfo(new SerDeInfo());
+    sd.getSerdeInfo().setName(tbl.getTableName());
+    sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+    sd.getSerdeInfo().getParameters()
+        .put(serdeConstants.SERIALIZATION_FORMAT, "1");
+    sd.setSortCols(new ArrayList<Order>());
+
+    tbl.setPartitionKeys(partCols);
+    client.createTable(tbl);
+
+    tbl = client.getTable(dbName, tblName);
+
+    add_partition(client, tbl, Lists.newArrayList("p11", "p21", "31"), "part1");
+    add_partition(client, tbl, Lists.newArrayList("p11", "p22", "32"), "part2");
+    add_partition(client, tbl, Lists.newArrayList("p12", "p21", "31"), "part3");
+    add_partition(client, tbl, Lists.newArrayList("p12", "p23", "32"), "part4");
+    add_partition(client, tbl, Lists.newArrayList("p13", "p24", "31"), "part5");
+    add_partition(client, tbl, Lists.newArrayList("p13", "p25", "-33"), "part6");
+
+    // Test equals operator for strings and integers.
+    checkFilter(client, dbName, tblName, "p1 = \"p11\"", 2);
+    checkFilter(client, dbName, tblName, "p1 = \"p12\"", 2);
+    checkFilter(client, dbName, tblName, "p2 = \"p21\"", 2);
+    checkFilter(client, dbName, tblName, "p2 = \"p23\"", 1);
+    checkFilter(client, dbName, tblName, "p3 = 31", 3);
+    checkFilter(client, dbName, tblName, "p3 = 33", 0);
+    checkFilter(client, dbName, tblName, "p3 = -33", 1);
+    checkFilter(client, dbName, tblName, "p1 = \"p11\" and p2=\"p22\"", 1);
+    checkFilter(client, dbName, tblName, "p1 = \"p11\" or p2=\"p23\"", 3);
+    checkFilter(client, dbName, tblName, "p1 = \"p11\" or p1=\"p12\"", 4);
+    checkFilter(client, dbName, tblName, "p1 = \"p11\" or p1=\"p12\"", 4);
+    checkFilter(client, dbName, tblName, "p1 = \"p11\" or p1=\"p12\"", 4);
+    checkFilter(client, dbName, tblName, "p1 = \"p11\" and p3 = 31", 1);
+    checkFilter(client, dbName, tblName, "p3 = -33 or p1 = \"p12\"", 3);
+
+    // Test not-equals operator for strings and integers.
+    checkFilter(client, dbName, tblName, "p1 != \"p11\"", 4);
+    checkFilter(client, dbName, tblName, "p2 != \"p23\"", 5);
+    checkFilter(client, dbName, tblName, "p2 != \"p33\"", 6);
+    checkFilter(client, dbName, tblName, "p3 != 32", 4);
+    checkFilter(client, dbName, tblName, "p3 != 8589934592", 6);
+    checkFilter(client, dbName, tblName, "p1 != \"p11\" and p1 != \"p12\"", 2);
+    checkFilter(client, dbName, tblName, "p1 != \"p11\" and p2 != \"p22\"", 4);
+    checkFilter(client, dbName, tblName, "p1 != \"p11\" or p2 != \"p22\"", 5);
+    checkFilter(client, dbName, tblName, "p1 != \"p12\" and p2 != \"p25\"", 3);
+    checkFilter(client, dbName, tblName, "p1 != \"p12\" or p2 != \"p25\"", 6);
+    checkFilter(client, dbName, tblName, "p3 != -33 or p1 != \"p13\"", 5);
+    checkFilter(client, dbName, tblName, "p1 != \"p11\" and p3 = 31", 2);
+    checkFilter(client, dbName, tblName, "p3 != 31 and p1 = \"p12\"", 1);
+
+    // Test reverse order.
+    checkFilter(client, dbName, tblName, "31 != p3 and p1 = \"p12\"", 1);
+    checkFilter(client, dbName, tblName, "\"p23\" = p2", 1);
+
+    // Test and/or more...
+    checkFilter(client, dbName, tblName,
+        "p1 = \"p11\" or (p1=\"p12\" and p2=\"p21\")", 3);
+    checkFilter(client, dbName, tblName,
+       "p1 = \"p11\" or (p1=\"p12\" and p2=\"p21\") Or " +
+       "(p1=\"p13\" aNd p2=\"p24\")", 4);
+    //test for and or precedence
+    checkFilter(client, dbName, tblName,
+       "p1=\"p12\" and (p2=\"p27\" Or p2=\"p21\")", 1);
+    checkFilter(client, dbName, tblName,
+       "p1=\"p12\" and p2=\"p27\" Or p2=\"p21\"", 2);
+
+    // Test gt/lt/lte/gte/like for strings.
+    checkFilter(client, dbName, tblName, "p1 > \"p12\"", 2);
+    checkFilter(client, dbName, tblName, "p1 >= \"p12\"", 4);
+    checkFilter(client, dbName, tblName, "p1 < \"p12\"", 2);
+    checkFilter(client, dbName, tblName, "p1 <= \"p12\"", 4);
+    checkFilter(client, dbName, tblName, "p1 like \"p1.*\"", 6);
+    checkFilter(client, dbName, tblName, "p2 like \"p.*3\"", 1);
+
+    // Test gt/lt/lte/gte for numbers.
+    checkFilter(client, dbName, tblName, "p3 < 0", 1);
+    checkFilter(client, dbName, tblName, "p3 >= -33", 6);
+    checkFilter(client, dbName, tblName, "p3 > -33", 5);
+    checkFilter(client, dbName, tblName, "p3 > 31 and p3 < 32", 0);
+    checkFilter(client, dbName, tblName, "p3 > 31 or p3 < 31", 3);
+    checkFilter(client, dbName, tblName, "p3 > 30 or p3 < 30", 6);
+    checkFilter(client, dbName, tblName, "p3 >= 31 or p3 < -32", 6);
+    checkFilter(client, dbName, tblName, "p3 >= 32", 2);
+    checkFilter(client, dbName, tblName, "p3 > 32", 0);
+
+    // Test between
+    checkFilter(client, dbName, tblName, "p1 between \"p11\" and \"p12\"", 4);
+    checkFilter(client, dbName, tblName, "p1 not between \"p11\" and \"p12\"", 2);
+    checkFilter(client, dbName, tblName, "p3 not between 0 and 2", 6);
+    checkFilter(client, dbName, tblName, "p3 between 31 and 32", 5);
+    checkFilter(client, dbName, tblName, "p3 between 32 and 31", 0);
+    checkFilter(client, dbName, tblName, "p3 between -32 and 34 and p3 not between 31 and 32", 0);
+    checkFilter(client, dbName, tblName, "p3 between 1 and 3 or p3 not between 1 and 3", 6);
+    checkFilter(client, dbName, tblName,
+        "p3 between 31 and 32 and p1 between \"p12\" and \"p14\"", 3);
+
+    //Test for setting the maximum partition count
+    List<Partition> partitions = client.listPartitionsByFilter(dbName,
+        tblName, "p1 >= \"p12\"", (short) 2);
+    assertEquals("User specified row limit for partitions",
+        2, partitions.size());
+
+    //Negative tests
+    Exception me = null;
+    try {
+      client.listPartitionsByFilter(dbName,
+          tblName, "p3 >= \"p12\"", (short) -1);
+    } catch(MetaException e) {
+      me = e;
+    }
+    assertNotNull(me);
+    assertTrue("Filter on int partition key", me.getMessage().contains(
+          "Filtering is supported only on partition keys of type string"));
+
+    me = null;
+    try {
+      client.listPartitionsByFilter(dbName,
+          tblName, "c1 >= \"p12\"", (short) -1);
+    } catch(MetaException e) {
+      me = e;
+    }
+    assertNotNull(me);
+    assertTrue("Filter on invalid key", me.getMessage().contains(
+          "<c1> is not a partitioning key for the table"));
+
+    me = null;
+    try {
+      client.listPartitionsByFilter(dbName,
+          tblName, "c1 >= ", (short) -1);
+    } catch(MetaException e) {
+      me = e;
+    }
+    assertNotNull(me);
+    assertTrue("Invalid filter string", me.getMessage().contains(
+          "Error parsing partition filter"));
+
+    me = null;
+    try {
+      client.listPartitionsByFilter("invDBName",
+          "invTableName", "p1 = \"p11\"", (short) -1);
+    } catch(NoSuchObjectException e) {
+      me = e;
+    }
+    assertNotNull(me);
+    assertTrue("NoSuchObject exception", me.getMessage().contains(
+          "database/table does not exist"));
+
+    client.dropTable(dbName, tblName);
+    client.dropDatabase(dbName);
+  }
+
+
+  /**
+   * Test filtering on table with single partition
+   * @throws Exception
+   */
+  public void testFilterSinglePartition() throws Exception {
+      String dbName = "filterdb";
+      String tblName = "filtertbl";
+
+      List<String> vals = new ArrayList<String>(1);
+      vals.add("p11");
+      List <String> vals2 = new ArrayList<String>(1);
+      vals2.add("p12");
+      List <String> vals3 = new ArrayList<String>(1);
+      vals3.add("p13");
+
+      silentDropDatabase(dbName);
+
+      Database db = new Database();
+      db.setName(dbName);
+      client.createDatabase(db);
+
+      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
+      cols.add(new FieldSchema("c1", serdeConstants.STRING_TYPE_NAME, ""));
+      cols.add(new FieldSchema("c2", serdeConstants.INT_TYPE_NAME, ""));
+
+      ArrayList<FieldSchema> partCols = new ArrayList<FieldSchema>(1);
+      partCols.add(new FieldSchema("p1", serdeConstants.STRING_TYPE_NAME, ""));
+
+      Table tbl = new Table();
+      tbl.setDbName(dbName);
+      tbl.setTableName(tblName);
+      StorageDescriptor sd = new StorageDescriptor();
+      tbl.setSd(sd);
+      sd.setCols(cols);
+      sd.setCompressed(false);
+      sd.setNumBuckets(1);
+      sd.setParameters(new HashMap<String, String>());
+      sd.setBucketCols(new ArrayList<String>());
+      sd.setSerdeInfo(new SerDeInfo());
+      sd.getSerdeInfo().setName(tbl.getTableName());
+      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+      sd.getSerdeInfo().getParameters()
+          .put(serdeConstants.SERIALIZATION_FORMAT, "1");
+      sd.setSortCols(new ArrayList<Order>());
+
+      tbl.setPartitionKeys(partCols);
+      client.createTable(tbl);
+
+      tbl = client.getTable(dbName, tblName);
+
+      add_partition(client, tbl, vals, "part1");
+      add_partition(client, tbl, vals2, "part2");
+      add_partition(client, tbl, vals3, "part3");
+
+      checkFilter(client, dbName, tblName, "p1 = \"p12\"", 1);
+      checkFilter(client, dbName, tblName, "p1 < \"p12\"", 1);
+      checkFilter(client, dbName, tblName, "p1 > \"p12\"", 1);
+      checkFilter(client, dbName, tblName, "p1 >= \"p12\"", 2);
+      checkFilter(client, dbName, tblName, "p1 <= \"p12\"", 2);
+      checkFilter(client, dbName, tblName, "p1 <> \"p12\"", 2);
+      checkFilter(client, dbName, tblName, "p1 like \"p1.*\"", 3);
+      checkFilter(client, dbName, tblName, "p1 like \"p.*2\"", 1);
+
+      client.dropTable(dbName, tblName);
+      client.dropDatabase(dbName);
+  }
+
+  /**
+   * Test filtering based on the value of the last partition
+   * @throws Exception
+   */
+  public void testFilterLastPartition() throws Exception {
+      String dbName = "filterdb";
+      String tblName = "filtertbl";
+
+      List<String> vals = new ArrayList<String>(2);
+      vals.add("p11");
+      vals.add("p21");
+      List <String> vals2 = new ArrayList<String>(2);
+      vals2.add("p11");
+      vals2.add("p22");
+      List <String> vals3 = new ArrayList<String>(2);
+      vals3.add("p12");
+      vals3.add("p21");
+
+      cleanUp(dbName, tblName, null);
+
+      createDb(dbName);
+
+      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
+      cols.add(new FieldSchema("c1", serdeConstants.STRING_TYPE_NAME, ""));
+      cols.add(new FieldSchema("c2", serdeConstants.INT_TYPE_NAME, ""));
+
+      ArrayList<FieldSchema> partCols = new ArrayList<FieldSchema>(2);
+      partCols.add(new FieldSchema("p1", serdeConstants.STRING_TYPE_NAME, ""));
+      partCols.add(new FieldSchema("p2", serdeConstants.STRING_TYPE_NAME, ""));
+
+      Map<String, String> serdParams = new HashMap<String, String>();
+      serdParams.put(serdeConstants.SERIALIZATION_FORMAT, "1");
+      StorageDescriptor sd = createStorageDescriptor(tblName, partCols, null, serdParams);
+
+      Table tbl = new Table();
+      tbl.setDbName(dbName);
+      tbl.setTableName(tblName);
+      tbl.setSd(sd);
+      tbl.setPartitionKeys(partCols);
+      client.createTable(tbl);
+      tbl = client.getTable(dbName, tblName);
+
+      add_partition(client, tbl, vals, "part1");
+      add_partition(client, tbl, vals2, "part2");
+      add_partition(client, tbl, vals3, "part3");
+
+      checkFilter(client, dbName, tblName, "p2 = \"p21\"", 2);
+      checkFilter(client, dbName, tblName, "p2 < \"p23\"", 3);
+      checkFilter(client, dbName, tblName, "p2 > \"p21\"", 1);
+      checkFilter(client, dbName, tblName, "p2 >= \"p21\"", 3);
+      checkFilter(client, dbName, tblName, "p2 <= \"p21\"", 2);
+      checkFilter(client, dbName, tblName, "p2 <> \"p12\"", 3);
+      checkFilter(client, dbName, tblName, "p2 != \"p12\"", 3);
+      checkFilter(client, dbName, tblName, "p2 like \"p2.*\"", 3);
+      checkFilter(client, dbName, tblName, "p2 like \"p.*2\"", 1);
+
+      try {
+        checkFilter(client, dbName, tblName, "p2 !< 'dd'", 0);
+        fail("Invalid operator not detected");
+      } catch (MetaException e) {
+        // expected exception due to lexer error
+      }
+
+      cleanUp(dbName, tblName, null);
+  }
+
+  private void checkFilter(HiveMetaStoreClient client, String dbName,
+        String tblName, String filter, int expectedCount)
+        throws MetaException, NoSuchObjectException, TException {
+    LOG.debug("Testing filter: " + filter);
+    List<Partition> partitions = client.listPartitionsByFilter(dbName,
+            tblName, filter, (short) -1);
+
+    assertEquals("Partition count expected for filter " + filter,
+            expectedCount, partitions.size());
+  }
+
+  private void add_partition(HiveMetaStoreClient client, Table table,
+      List<String> vals, String location) throws InvalidObjectException,
+        AlreadyExistsException, MetaException, TException {
+
+    Partition part = new Partition();
+    part.setDbName(table.getDbName());
+    part.setTableName(table.getTableName());
+    part.setValues(vals);
+    part.setParameters(new HashMap<String, String>());
+    part.setSd(table.getSd().deepCopy());
+    part.getSd().setSerdeInfo(table.getSd().getSerdeInfo());
+    part.getSd().setLocation(table.getSd().getLocation() + location);
+
+    client.add_partition(part);
+  }
+
+  /**
+   * Tests {@link HiveMetaStoreClient#newSynchronizedClient}.  Does not
+   * actually test multithreading, but does verify that the proxy
+   * at least works correctly.
+   */
+  public void testSynchronized() throws Exception {
+    int currentNumberOfDbs = client.getAllDatabases().size();
+
+    IMetaStoreClient synchronizedClient =
+      HiveMetaStoreClient.newSynchronizedClient(client);
+    List<String> databases = synchronizedClient.getAllDatabases();
+    assertEquals(currentNumberOfDbs, databases.size());
+  }
+
+  public void testTableFilter() throws Exception {
+    try {
+      String dbName = "testTableFilter";
+      String owner1 = "testOwner1";
+      String owner2 = "testOwner2";
+      int lastAccessTime1 = 90;
+      int lastAccessTime2 = 30;
+      String tableName1 = "table1";
+      String tableName2 = "table2";
+      String tableName3 = "table3";
+
+      client.dropTable(dbName, tableName1);
+      client.dropTable(dbName, tableName2);
+      client.dropTable(dbName, tableName3);
+      silentDropDatabase(dbName);
+      Database db = new Database();
+      db.setName(dbName);
+      db.setDescription("Alter Partition Test database");
+      client.createDatabase(db);
+
+      Table table1 = createTableForTestFilter(dbName,tableName1, owner1, lastAccessTime1, true);
+      Table table2 = createTableForTestFilter(dbName,tableName2, owner2, lastAccessTime2, true);
+      Table table3 = createTableForTestFilter(dbName,tableName3, owner1, lastAccessTime2, false);
+
+      List<String> tableNames;
+      String filter;
+      //test owner
+      //owner like ".*Owner.*" and owner like "test.*"
+      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_OWNER +
+          " like \".*Owner.*\" and " +
+          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_OWNER +
+          " like  \"test.*\"";
+      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
+      assertEquals(tableNames.size(), 3);
+      assert(tableNames.contains(table1.getTableName()));
+      assert(tableNames.contains(table2.getTableName()));
+      assert(tableNames.contains(table3.getTableName()));
+
+      //owner = "testOwner1"
+      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_OWNER +
+          " = \"testOwner1\"";
+
+      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
+      assertEquals(2, tableNames.size());
+      assert(tableNames.contains(table1.getTableName()));
+      assert(tableNames.contains(table3.getTableName()));
+
+      //lastAccessTime < 90
+      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_LAST_ACCESS +
+          " < 90";
+
+      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
+      assertEquals(2, tableNames.size());
+      assert(tableNames.contains(table2.getTableName()));
+      assert(tableNames.contains(table3.getTableName()));
+
+      //lastAccessTime > 90
+      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_LAST_ACCESS +
+      " > 90";
+
+      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
+      assertEquals(0, tableNames.size());
+
+      //test params
+      //test_param_2 = "50"
+      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
+          "test_param_2 = \"50\"";
+
+      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
+      assertEquals(2, tableNames.size());
+      assert(tableNames.contains(table1.getTableName()));
+      assert(tableNames.contains(table2.getTableName()));
+
+      //test_param_2 = "75"
+      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
+          "test_param_2 = \"75\"";
+
+      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
+      assertEquals(0, tableNames.size());
+
+      //key_dne = "50"
+      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
+          "key_dne = \"50\"";
+
+      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
+      assertEquals(0, tableNames.size());
+
+      //test_param_1 != "yellow"
+      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
+          "test_param_1 <> \"yellow\"";
+
+      tableNames = client.listTableNamesByFilter(dbName, filter, (short) 2);
+      assertEquals(2, tableNames.size());
+
+      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
+          "test_param_1 != \"yellow\"";
+
+      tableNames = client.listTableNamesByFilter(dbName, filter, (short) 2);
+      assertEquals(2, tableNames.size());
+
+      //owner = "testOwner1" and (lastAccessTime = 30 or test_param_1 = "hi")
+      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_OWNER +
+        " = \"testOwner1\" and (" +
+        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_LAST_ACCESS +
+        " = 30 or " +
+        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
+        "test_param_1 = \"hi\")";
+      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
+
+      assertEquals(2, tableNames.size());
+      assert(tableNames.contains(table1.getTableName()));
+      assert(tableNames.contains(table3.getTableName()));
+
+      //Negative tests
+      Exception me = null;
+      try {
+        filter = "badKey = \"testOwner1\"";
+        tableNames = client.listTableNamesByFilter(dbName, filter, (short) -1);
+      } catch(MetaException e) {
+        me = e;
+      }
+      assertNotNull(me);
+      assertTrue("Bad filter key test", me.getMessage().contains(
+            "Invalid key name in filter"));
+
+      client.dropTable(dbName, tableName1);
+      client.dropTable(dbName, tableName2);
+      client.dropTable(dbName, tableName3);
+      client.dropDatabase(dbName);
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testTableFilter() failed.");
+      throw e;
+    }
+  }
+
+  private Table createTableForTestFilter(String dbName, String tableName, String owner,
+    int lastAccessTime, boolean hasSecondParam) throws Exception {
+
+    ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
+    cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+    cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+
+    Map<String, String> params = new HashMap<String, String>();
+    params.put("sd_param_1", "Use this for comments etc");
+
+    Map<String, String> serdParams = new HashMap<String, String>();
+    serdParams.put(serdeConstants.SERIALIZATION_FORMAT, "1");
+
+    StorageDescriptor sd = createStorageDescriptor(tableName, cols, params, serdParams);
+
+    Map<String, String> partitionKeys = new HashMap<String, String>();
+    partitionKeys.put("ds", serdeConstants.STRING_TYPE_NAME);
+    partitionKeys.put("hr", serdeConstants.INT_TYPE_NAME);
+
+    Map<String, String> tableParams =  new HashMap<String, String>();
+    tableParams.put("test_param_1", "hi");
+    if(hasSecondParam) {
+      tableParams.put("test_param_2", "50");
+    }
+
+    Table tbl = createTable(dbName, tableName, owner, tableParams,
+        partitionKeys, sd, lastAccessTime);
+
+    if (isThriftClient) {
+      // the createTable() above does not update the location in the 'tbl'
+      // object when the client is a thrift client and the code below relies
+      // on the location being present in the 'tbl' object - so get the table
+      // from the metastore
+      tbl = client.getTable(dbName, tableName);
+    }
+    return tbl;
+  }
+  /**
+   * Verify that if another  client, either a metastore Thrift server or  a Hive CLI instance
+   * renames a table recently created by this instance, and hence potentially in its cache, the
+   * current instance still sees the change.
+   * @throws Exception
+   */
+  public void testConcurrentMetastores() throws Exception {
+    String dbName = "concurrentdb";
+    String tblName = "concurrenttbl";
+    String renameTblName = "rename_concurrenttbl";
+
+    try {
+      cleanUp(dbName, tblName, null);
+
+      createDb(dbName);
+
+      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
+      cols.add(new FieldSchema("c1", serdeConstants.STRING_TYPE_NAME, ""));
+      cols.add(new FieldSchema("c2", serdeConstants.INT_TYPE_NAME, ""));
+
+      Map<String, String> params = new HashMap<String, String>();
+      params.put("test_param_1", "Use this for comments etc");
+
+      Map<String, String> serdParams = new HashMap<String, String>();
+      serdParams.put(serdeConstants.SERIALIZATION_FORMAT, "1");
+
+      StorageDescriptor sd =  createStorageDescriptor(tblName, cols, params, serdParams);
+
+      createTable(dbName, tblName, null, null, null, sd, 0);
+
+      // get the table from the client, verify the name is correct
+      Table tbl2 = client.getTable(dbName, tblName);
+
+      assertEquals("Client returned table with different name.", tbl2.getTableName(), tblName);
+
+      // Simulate renaming via another metastore Thrift server or another Hive CLI instance
+      updateTableNameInDB(tblName, renameTblName);
+
+      // get the table from the client again, verify the name has been updated
+      Table tbl3 = client.getTable(dbName, renameTblName);
+
+      assertEquals("Client returned table with different name after rename.",
+          tbl3.getTableName(), renameTblName);
+
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testConcurrentMetastores() failed.");
+      throw e;
+    } finally {
+      silentDropDatabase(dbName);
+    }
+  }
+
+
+
+  /**
+   * This method simulates another Hive metastore renaming a table, by accessing the db and
+   * updating the name.
+   *
+   * Unfortunately, derby cannot be run in two different JVMs simultaneously, but the only way
+   * to rename without having it put in this client's cache is to run a metastore in a separate JVM,
+   * so this simulation is required.
+   * @param oldTableName
+   * @param newTableName
+   * @throws SQLException
+   */
+  private void updateTableNameInDB(String oldTableName, String newTableName) throws SQLException {
+    String connectionStr = HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTORECONNECTURLKEY);
+    int interval= HiveConf.getIntVar(hiveConf, HiveConf.ConfVars.METASTOREINTERVAL);
+    int attempts = HiveConf.getIntVar(hiveConf, HiveConf.ConfVars.METASTOREATTEMPTS);
+
+
+    Utilities.SQLCommand<Void> execUpdate = new Utilities.SQLCommand<Void>() {
+      @Override
+      public Void run(PreparedStatement stmt) throws SQLException {
+        stmt.executeUpdate();
+        return null;
+      }
+    };
+
+    Connection conn = Utilities.connectWithRetry(connectionStr, interval, attempts);
+
+    PreparedStatement updateStmt = Utilities.prepareWithRetry(conn,
+        "UPDATE TBLS SET tbl_name = '" + newTableName + "' WHERE tbl_name = '" + oldTableName + "'",
+        interval, attempts);
+
+    Utilities.executeWithRetry(execUpdate, updateStmt, interval, attempts);
+  }
+
+  private void cleanUp(String dbName, String tableName, String typeName) throws Exception {
+    if(dbName != null && tableName != null) {
+      client.dropTable(dbName, tableName);
+    }
+    if(dbName != null) {
+      silentDropDatabase(dbName);
+    }
+    if(typeName != null) {
+      client.dropType(typeName);
+    }
+  }
+
+  private Database createDb(String dbName) throws Exception {
+    if(null == dbName) { return null; }
+    Database db = new Database();
+    db.setName(dbName);
+    client.createDatabase(db);
+    return db;
+  }
+
+  private Type createType(String typeName, Map<String, String> fields) throws Throwable {
+    Type typ1 = new Type();
+    typ1.setName(typeName);
+    typ1.setFields(new ArrayList<FieldSchema>(fields.size()));
+    for(String fieldName : fields.keySet()) {
+      typ1.getFields().add(
+          new FieldSchema(fieldName, fields.get(fieldName), ""));
+    }
+    client.createType(typ1);
+    return typ1;
+  }
+
+  private Table createTable(String dbName, String tblName, String owner,
+      Map<String,String> tableParams, Map<String, String> partitionKeys,
+      StorageDescriptor sd, int lastAccessTime) throws Exception {
+    Table tbl = new Table();
+    tbl.setDbName(dbName);
+    tbl.setTableName(tblName);
+    if(tableParams != null) {
+      tbl.setParameters(tableParams);
+    }
+
+    if(owner != null) {
+      tbl.setOwner(owner);
+    }
+
+    if(partitionKeys != null) {
+      tbl.setPartitionKeys(new ArrayList<FieldSchema>(partitionKeys.size()));
+      for(String key : partitionKeys.keySet()) {
+        tbl.getPartitionKeys().add(
+            new FieldSchema(key, partitionKeys.get(key), ""));
+      }
+    }
+
+    tbl.setSd(sd);
+    tbl.setLastAccessTime(lastAccessTime);
+
+    client.createTable(tbl);
+    return tbl;
+  }
+
+  private StorageDescriptor createStorageDescriptor(String tableName,
+    List<FieldSchema> cols, Map<String, String> params, Map<String, String> serdParams)  {
+    StorageDescriptor sd = new StorageDescriptor();
+
+    sd.setCols(cols);
+    sd.setCompressed(false);
+    sd.setNumBuckets(1);
+    sd.setParameters(params);
+    sd.setBucketCols(new ArrayList<String>(2));
+    sd.getBucketCols().add("name");
+    sd.setSerdeInfo(new SerDeInfo());
+    sd.getSerdeInfo().setName(tableName);
+    sd.getSerdeInfo().setParameters(serdParams);
+    sd.getSerdeInfo().getParameters()
+        .put(serdeConstants.SERIALIZATION_FORMAT, "1");
+    sd.setSortCols(new ArrayList<Order>());
+
+    return sd;
+  }
+
+  private List<Partition> createPartitions(String dbName, Table tbl,
+      List<List<String>> values)  throws Throwable {
+    int i = 1;
+    List<Partition> partitions = new ArrayList<Partition>();
+    for(List<String> vals : values) {
+      Partition part = makePartitionObject(dbName, tbl.getTableName(), vals, tbl, "/part"+i);
+      i++;
+      // check if the partition exists (it shouldn't)
+      boolean exceptionThrown = false;
+      try {
+        Partition p = client.getPartition(dbName, tbl.getTableName(), vals);
+      } catch(Exception e) {
+        assertEquals("partition should not have existed",
+            NoSuchObjectException.class, e.getClass());
+        exceptionThrown = true;
+      }
+      assertTrue("getPartition() should have thrown NoSuchObjectException", exceptionThrown);
+      Partition retp = client.add_partition(part);
+      assertNotNull("Unable to create partition " + part, retp);
+      partitions.add(retp);
+    }
+    return partitions;
+  }
+
+  private void createMultiPartitionTableSchema(String dbName, String tblName,
+      String typeName, List<List<String>> values)
+      throws Throwable, MetaException, TException, NoSuchObjectException {
+    createDb(dbName);
+
+    Map<String, String> fields = new HashMap<String, String>();
+    fields.put("name", serdeConstants.STRING_TYPE_NAME);
+    fields.put("income", serdeConstants.INT_TYPE_NAME);
+
+    Type typ1 = createType(typeName, fields);
+
+    Map<String , String> partitionKeys = new HashMap<String, String>();
+    partitionKeys.put("ds", serdeConstants.STRING_TYPE_NAME);
+    partitionKeys.put("hr", serdeConstants.STRING_TYPE_NAME);
+
+    Map<String, String> params = new HashMap<String, String>();
+    params.put("test_param_1", "Use this for comments etc");
+
+    Map<String, String> serdParams = new HashMap<String, String>();
+    serdParams.put(serdeConstants.SERIALIZATION_FORMAT, "1");
+
+    StorageDescriptor sd =  createStorageDescriptor(tblName, typ1.getFields(), params, serdParams);
+
+    Table tbl = createTable(dbName, tblName, null, null, partitionKeys, sd, 0);
+
+    if (isThriftClient) {
+      // the createTable() above does not update the location in the 'tbl'
+      // object when the client is a thrift client and the code below relies
+      // on the location being present in the 'tbl' object - so get the table
+      // from the metastore
+      tbl = client.getTable(dbName, tblName);
+    }
+
+    createPartitions(dbName, tbl, values);
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreWithEnvironmentContext.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreWithEnvironmentContext.java
new file mode 100644
index 0000000..7e2c27d
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreWithEnvironmentContext.java
@@ -0,0 +1,222 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.EnvironmentContext;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.SerDeInfo;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.metastore.events.AddPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.AlterTableEvent;
+import org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.CreateTableEvent;
+import org.apache.hadoop.hive.metastore.events.DropDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.DropPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.DropTableEvent;
+import org.apache.hadoop.hive.metastore.events.ListenerEvent;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.mortbay.log.Log;
+
+/**
+ * TestHiveMetaStoreWithEnvironmentContext. Test case for _with_environment_context
+ * calls in {@link org.apache.hadoop.hive.metastore.HiveMetaStore}
+ */
+public class TestHiveMetaStoreWithEnvironmentContext extends TestCase {
+
+  private HiveConf hiveConf;
+  private HiveMetaStoreClient msc;
+  private EnvironmentContext envContext;
+  private final Database db = new Database();
+  private Table table = new Table();
+  private final Partition partition = new Partition();
+
+  private static final String dbName = "hive3252";
+  private static final String tblName = "tmptbl";
+  private static final String renamed = "tmptbl2";
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+
+    System.setProperty("hive.metastore.event.listeners",
+        DummyListener.class.getName());
+
+    int port = MetaStoreUtils.findFreePort();
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+
+    hiveConf = new HiveConf(this.getClass());
+    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+    SessionState.start(new CliSessionState(hiveConf));
+    msc = new HiveMetaStoreClient(hiveConf, null);
+
+    msc.dropDatabase(dbName, true, true);
+
+    Map<String, String> envProperties = new HashMap<String, String>();
+    envProperties.put("hadoop.job.ugi", "test_user");
+    envContext = new EnvironmentContext(envProperties);
+
+    db.setName(dbName);
+
+    Map<String, String> tableParams = new HashMap<String, String>();
+    tableParams.put("a", "string");
+    List<FieldSchema> partitionKeys = new ArrayList<FieldSchema>();
+    partitionKeys.add(new FieldSchema("b", "string", ""));
+
+    List<FieldSchema> cols = new ArrayList<FieldSchema>();
+    cols.add(new FieldSchema("a", "string", ""));
+    cols.add(new FieldSchema("b", "string", ""));
+    StorageDescriptor sd = new StorageDescriptor();
+    sd.setCols(cols);
+    sd.setCompressed(false);
+    sd.setParameters(tableParams);
+    sd.setSerdeInfo(new SerDeInfo());
+    sd.getSerdeInfo().setName(tblName);
+    sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+    sd.getSerdeInfo().getParameters().put(serdeConstants.SERIALIZATION_FORMAT, "1");
+
+    table.setDbName(dbName);
+    table.setTableName(tblName);
+    table.setParameters(tableParams);
+    table.setPartitionKeys(partitionKeys);
+    table.setSd(sd);
+
+    List<String> partValues = new ArrayList<String>();
+    partValues.add("2011");
+    partition.setDbName(dbName);
+    partition.setTableName(tblName);
+    partition.setValues(partValues);
+    partition.setSd(table.getSd().deepCopy());
+    partition.getSd().setSerdeInfo(table.getSd().getSerdeInfo().deepCopy());
+
+    DummyListener.notifyList.clear();
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+  public void testEnvironmentContext() throws Exception {
+    int listSize = 0;
+
+    List<ListenerEvent> notifyList = DummyListener.notifyList;
+    assertEquals(notifyList.size(), listSize);
+    msc.createDatabase(db);
+    listSize++;
+    assertEquals(listSize, notifyList.size());
+    CreateDatabaseEvent dbEvent = (CreateDatabaseEvent)(notifyList.get(listSize - 1));
+    assert dbEvent.getStatus();
+
+    Log.debug("Creating table");
+    msc.createTable(table, envContext);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    CreateTableEvent tblEvent = (CreateTableEvent)(notifyList.get(listSize - 1));
+    assert tblEvent.getStatus();
+    assertEquals(envContext, tblEvent.getEnvironmentContext());
+
+    table = msc.getTable(dbName, tblName);
+
+    Log.debug("Adding partition");
+    partition.getSd().setLocation(table.getSd().getLocation() + "/part1");
+    msc.add_partition(partition, envContext);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    AddPartitionEvent partEvent = (AddPartitionEvent)(notifyList.get(listSize-1));
+    assert partEvent.getStatus();
+    assertEquals(envContext, partEvent.getEnvironmentContext());
+
+    Log.debug("Appending partition");
+    List<String> partVals = new ArrayList<String>();
+    partVals.add("2012");
+    msc.appendPartition(dbName, tblName, partVals, envContext);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    AddPartitionEvent appendPartEvent = (AddPartitionEvent)(notifyList.get(listSize-1));
+    assert appendPartEvent.getStatus();
+    assertEquals(envContext, appendPartEvent.getEnvironmentContext());
+
+    Log.debug("Renaming table");
+    table.setTableName(renamed);
+    msc.alter_table(dbName, tblName, table, envContext);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    AlterTableEvent alterTableEvent = (AlterTableEvent) notifyList.get(listSize-1);
+    assert alterTableEvent.getStatus();
+    assertEquals(envContext, alterTableEvent.getEnvironmentContext());
+
+    Log.debug("Renaming table back");
+    table.setTableName(tblName);
+    msc.alter_table(dbName, renamed, table, envContext);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+
+    Log.debug("Dropping partition");
+    List<String> dropPartVals = new ArrayList<String>();
+    dropPartVals.add("2011");
+    msc.dropPartition(dbName, tblName, dropPartVals, envContext);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    DropPartitionEvent dropPartEvent = (DropPartitionEvent)notifyList.get(listSize - 1);
+    assert dropPartEvent.getStatus();
+    assertEquals(envContext, dropPartEvent.getEnvironmentContext());
+
+    Log.debug("Dropping partition by name");
+    msc.dropPartition(dbName, tblName, "b=2012", true, envContext);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    DropPartitionEvent dropPartByNameEvent = (DropPartitionEvent)notifyList.get(listSize - 1);
+    assert dropPartByNameEvent.getStatus();
+    assertEquals(envContext, dropPartByNameEvent.getEnvironmentContext());
+
+    Log.debug("Dropping table");
+    msc.dropTable(dbName, tblName, true, false, envContext);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    DropTableEvent dropTblEvent = (DropTableEvent)notifyList.get(listSize-1);
+    assert dropTblEvent.getStatus();
+    assertEquals(envContext, dropTblEvent.getEnvironmentContext());
+
+    msc.dropDatabase(dbName);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+
+    DropDatabaseEvent dropDB = (DropDatabaseEvent)notifyList.get(listSize-1);
+    assert dropDB.getStatus();
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaTool.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaTool.java
new file mode 100644
index 0000000..1b688bd
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaTool.java
@@ -0,0 +1,247 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import java.io.ByteArrayOutputStream;
+import java.io.OutputStream;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.HashMap;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.InvalidOperationException;
+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.metastore.api.SerDeInfo;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.metastore.api.Type;
+import org.apache.hadoop.hive.metastore.tools.HiveMetaTool;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils;
+import org.apache.hadoop.util.StringUtils;
+
+public class TestHiveMetaTool extends TestCase {
+
+  private HiveMetaStoreClient client;
+
+  private PrintStream originalOut;
+  private OutputStream os;
+  private PrintStream ps;
+  private String locationUri;
+  private final String dbName = "TestHiveMetaToolDB";
+  private final String typeName = "Person";
+  private final String tblName = "simpleTbl";
+  private final String badTblName = "badSimpleTbl";
+
+
+  private void dropDatabase(String dbName) throws Exception {
+    try {
+      client.dropDatabase(dbName);
+    } catch (NoSuchObjectException e) {
+    } catch (InvalidOperationException e) {
+    } catch (Exception e) {
+      throw e;
+    }
+  }
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+
+    try {
+      HiveConf hiveConf = new HiveConf(HiveMetaTool.class);
+      client = new HiveMetaStoreClient(hiveConf, null);
+
+      // Setup output stream to redirect output to
+      os = new ByteArrayOutputStream();
+      ps = new PrintStream(os);
+
+      // create a dummy database and a couple of dummy tables
+      Database db = new Database();
+      db.setName(dbName);
+      client.dropTable(dbName, tblName);
+      client.dropTable(dbName, badTblName);
+      dropDatabase(dbName);
+      client.createDatabase(db);
+      locationUri = db.getLocationUri();
+      String avroUri = "hdfs://nn.example.com/warehouse/hive/ab.avsc";
+      String badAvroUri = new String("hdfs:/hive");
+
+      client.dropType(typeName);
+      Type typ1 = new Type();
+      typ1.setName(typeName);
+      typ1.setFields(new ArrayList<FieldSchema>(2));
+      typ1.getFields().add(
+          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
+      typ1.getFields().add(
+          new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
+      client.createType(typ1);
+
+      Table tbl = new Table();
+      tbl.setDbName(dbName);
+      tbl.setTableName(tblName);
+      StorageDescriptor sd = new StorageDescriptor();
+      tbl.setSd(sd);
+      sd.setCols(typ1.getFields());
+      sd.setCompressed(false);
+      sd.setNumBuckets(1);
+      sd.setParameters(new HashMap<String, String>());
+      sd.getParameters().put("test_param_1", "Use this for comments etc");
+      sd.setBucketCols(new ArrayList<String>(2));
+      sd.getBucketCols().add("name");
+      sd.setSerdeInfo(new SerDeInfo());
+      sd.getSerdeInfo().setName(tbl.getTableName());
+      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+      sd.getSerdeInfo().getParameters().put(
+          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
+      sd.getParameters().put(AvroSerdeUtils.SCHEMA_URL, avroUri);
+      sd.getSerdeInfo().setSerializationLib(
+          org.apache.hadoop.hive.serde2.avro.AvroSerDe.class.getName());
+      tbl.setPartitionKeys(new ArrayList<FieldSchema>());
+      client.createTable(tbl);
+
+      //create a table with bad avro uri
+      tbl = new Table();
+      tbl.setDbName(dbName);
+      tbl.setTableName(badTblName);
+      sd = new StorageDescriptor();
+      tbl.setSd(sd);
+      sd.setCols(typ1.getFields());
+      sd.setCompressed(false);
+      sd.setNumBuckets(1);
+      sd.setParameters(new HashMap<String, String>());
+      sd.getParameters().put("test_param_1", "Use this for comments etc");
+      sd.setBucketCols(new ArrayList<String>(2));
+      sd.getBucketCols().add("name");
+      sd.setSerdeInfo(new SerDeInfo());
+      sd.getSerdeInfo().setName(tbl.getTableName());
+      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+      sd.getSerdeInfo().getParameters().put(
+          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
+      sd.getParameters().put(AvroSerdeUtils.SCHEMA_URL, badAvroUri);
+      sd.getSerdeInfo().setSerializationLib(
+          org.apache.hadoop.hive.serde2.avro.AvroSerDe.class.getName());
+      tbl.setPartitionKeys(new ArrayList<FieldSchema>());
+      client.createTable(tbl);
+      client.close();
+    } catch (Exception e) {
+      System.err.println("Unable to setup the hive metatool test");
+      System.err.println(StringUtils.stringifyException(e));
+      throw new Exception(e);
+    }
+  }
+
+  private void redirectOutputStream() {
+
+    originalOut = System.out;
+    System.setOut(ps);
+
+  }
+
+  private void restoreOutputStream() {
+
+    System.setOut(originalOut);
+  }
+
+  public void testListFSRoot() throws Exception {
+
+    redirectOutputStream();
+    String[] args = new String[1];
+    args[0] = new String("-listFSRoot");
+
+    try {
+      HiveMetaTool.main(args);
+      String out = os.toString();
+      boolean b = out.contains(locationUri);
+      assertTrue(b);
+    } finally {
+      restoreOutputStream();
+      System.out.println("Completed testListFSRoot");
+    }
+  }
+
+  public void testExecuteJDOQL() throws Exception {
+
+    redirectOutputStream();
+    String[] args = new String[2];
+    args[0] = new String("-executeJDOQL");
+    args[1] = new String("select locationUri from org.apache.hadoop.hive.metastore.model.MDatabase");
+
+    try {
+      HiveMetaTool.main(args);
+      String out = os.toString();
+      boolean b = out.contains(locationUri);
+      assertTrue(b);
+    } finally {
+      restoreOutputStream();
+      System.out.println("Completed testExecuteJDOQL");
+    }
+  }
+
+  public void testUpdateFSRootLocation() throws Exception {
+
+    redirectOutputStream();
+    String oldLocationUri = "hdfs://nn.example.com/";
+    String newLocationUri = "hdfs://nn-ha-uri/";
+    String[] args = new String[5];
+    args[0] = new String("-updateLocation");
+    args[1] = new String(newLocationUri);
+    args[2] = new String(oldLocationUri);
+    args[3] = new String("-tablePropKey");
+    args[4] = new String("avro.schema.url");
+
+    try {
+      // perform HA upgrade
+      HiveMetaTool.main(args);
+      String out = os.toString();
+      boolean b = out.contains(newLocationUri);
+      restoreOutputStream();
+      assertTrue(b);
+
+      //restore the original HDFS root
+      args[1] = new String(oldLocationUri);
+      args[2] = new String(newLocationUri);
+      redirectOutputStream();
+      HiveMetaTool.main(args);
+      restoreOutputStream();
+    } finally {
+      restoreOutputStream();
+      System.out.println("Completed testUpdateFSRootLocation..");
+    }
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    try {
+      client.dropTable(dbName, tblName);
+      client.dropTable(dbName, badTblName);
+      dropDatabase(dbName);
+      super.tearDown();
+      client.close();
+    } catch (Throwable e) {
+      System.err.println("Unable to close metastore");
+      System.err.println(StringUtils.stringifyException(e));
+      throw new Exception(e);
+    }
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMarkPartition.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMarkPartition.java
new file mode 100644
index 0000000..57a5e6b
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMarkPartition.java
@@ -0,0 +1,107 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.metastore.api.InvalidPartitionException;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.metastore.api.PartitionEventType;
+import org.apache.hadoop.hive.metastore.api.UnknownDBException;
+import org.apache.hadoop.hive.metastore.api.UnknownPartitionException;
+import org.apache.hadoop.hive.metastore.api.UnknownTableException;
+import org.apache.hadoop.hive.ql.CommandNeedRetryException;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.thrift.TException;
+
+public class TestMarkPartition extends TestCase{
+
+  protected HiveConf hiveConf;
+  private Driver driver;
+
+  @Override
+  protected void setUp() throws Exception {
+
+    super.setUp();
+    System.setProperty("hive.metastore.event.clean.freq", "2");
+    System.setProperty("hive.metastore.event.expiry.duration", "5");
+    hiveConf = new HiveConf(this.getClass());
+    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+    SessionState.start(new CliSessionState(hiveConf));
+
+  }
+
+  public void testMarkingPartitionSet() throws CommandNeedRetryException, MetaException,
+  TException, NoSuchObjectException, UnknownDBException, UnknownTableException,
+  InvalidPartitionException, UnknownPartitionException, InterruptedException {
+    HiveMetaStoreClient msc = new HiveMetaStoreClient(hiveConf, null);
+    driver = new Driver(hiveConf);
+    driver.run("drop database if exists hive2215 cascade");
+    driver.run("create database hive2215");
+    driver.run("use hive2215");
+    driver.run("drop table if exists tmptbl");
+    driver.run("create table tmptbl (a string) partitioned by (b string)");
+    driver.run("alter table tmptbl add partition (b='2011')");
+    Map<String,String> kvs = new HashMap<String, String>();
+    kvs.put("b", "'2011'");
+    msc.markPartitionForEvent("hive2215", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
+    assert msc.isPartitionMarkedForEvent("hive2215", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
+    Thread.sleep(10000);
+    assert !msc.isPartitionMarkedForEvent("hive2215", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
+
+    kvs.put("b", "'2012'");
+    assert !msc.isPartitionMarkedForEvent("hive2215", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
+    try{
+      msc.markPartitionForEvent("hive2215", "tmptbl2", kvs, PartitionEventType.LOAD_DONE);
+      assert false;
+    } catch(Exception e){
+      assert e instanceof UnknownTableException;
+    }
+    try{
+      msc.isPartitionMarkedForEvent("hive2215", "tmptbl2", kvs, PartitionEventType.LOAD_DONE);
+      assert false;
+    } catch(Exception e){
+      assert e instanceof UnknownTableException;
+    }
+    kvs.put("a", "'2012'");
+    try{
+      msc.isPartitionMarkedForEvent("hive2215", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
+      assert false;
+    } catch(Exception e){
+      assert e instanceof InvalidPartitionException;
+    }
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    driver.run("drop database if exists hive2215 cascade");
+    super.tearDown();
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java
new file mode 100644
index 0000000..7576f39
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java
@@ -0,0 +1,55 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+
+public class TestMarkPartitionRemote extends TestMarkPartition {
+
+  private static class RunMS implements Runnable {
+
+    private final int port;
+
+    public RunMS(int port) {
+      this.port = port;
+    }
+    @Override
+    public void run() {
+      try {
+        HiveMetaStore.main(new String[] { String.valueOf(port) });
+      } catch (Throwable e) {
+        e.printStackTrace(System.err);
+        assert false;
+      }
+    }
+
+  }
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    int port = MetaStoreUtils.findFreePort();
+    Thread t = new Thread(new RunMS(port));
+    t.setDaemon(true);
+    t.start();
+    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    Thread.sleep(30000);
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreAuthorization.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreAuthorization.java
new file mode 100644
index 0000000..a6a038a
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreAuthorization.java
@@ -0,0 +1,125 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import java.io.IOException;
+import java.net.ServerSocket;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.shims.ShimLoader;
+
+
+public class TestMetaStoreAuthorization extends TestCase {
+  protected HiveConf conf = new HiveConf();
+
+  private int port;
+
+  public void setup() throws Exception {
+    port = findFreePort();
+    System.setProperty(HiveConf.ConfVars.METASTORE_AUTHORIZATION_STORAGE_AUTH_CHECKS.varname,
+        "true");
+    conf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    conf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    conf.setIntVar(ConfVars.METASTORE_CLIENT_CONNECT_RETRY_DELAY, 60);
+  }
+
+  public void testIsWritable() throws Exception {
+    setup();
+    conf = new HiveConf(this.getClass());
+    String testDir = System.getProperty("test.warehouse.dir", "/tmp");
+    Path testDirPath = new Path(testDir);
+    FileSystem fs = testDirPath.getFileSystem(conf);
+    Path top = new Path(testDirPath, "_foobarbaz12_");
+    try {
+      fs.mkdirs(top);
+
+      Warehouse wh = new Warehouse(conf);
+      FsPermission writePerm = FsPermission.createImmutable((short)0777);
+      FsPermission noWritePerm = FsPermission.createImmutable((short)0555);
+
+      fs.setPermission(top, writePerm);
+      assertTrue("Expected " + top + " to be writable", wh.isWritable(top));
+
+      fs.setPermission(top, noWritePerm);
+      assertTrue("Expected " + top + " to be not writable", !wh.isWritable(top));
+    } finally {
+      fs.delete(top, true);
+    }
+  }
+
+  public void testMetaStoreAuthorization() throws Exception {
+    setup();
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+    HiveMetaStoreClient client = new HiveMetaStoreClient(conf);
+
+    FileSystem fs = null;
+    String dbName = "simpdb";
+    Database db1 = null;
+    Path p = null;
+    try {
+      try {
+        db1 = client.getDatabase(dbName);
+        client.dropDatabase(dbName);
+      } catch (NoSuchObjectException noe) {}
+      if (db1 != null) {
+        p = new Path(db1.getLocationUri());
+        fs = p.getFileSystem(conf);
+        fs.delete(p, true);
+      }
+      db1 = new Database();
+      db1.setName(dbName);
+      client.createDatabase(db1);
+      Database db = client.getDatabase(dbName);
+
+      assertTrue("Databases do not match", db1.getName().equals(db.getName()));
+      p = new Path(db.getLocationUri());
+      if (fs == null) {
+        fs = p.getFileSystem(conf);
+      }
+      fs.setPermission(p.getParent(), FsPermission.createImmutable((short)0555));
+      try {
+        client.dropDatabase(dbName);
+        throw new Exception("Expected dropDatabase call to fail");
+      } catch (MetaException me) {
+      }
+      fs.setPermission(p.getParent(), FsPermission.createImmutable((short)0755));
+      client.dropDatabase(dbName);
+    } finally {
+      if (p != null) {
+        fs.delete(p, true);
+      }
+    }
+  }
+
+  private int findFreePort() throws IOException {
+    ServerSocket socket= new ServerSocket(0);
+    int port = socket.getLocalPort();
+    socket.close();
+    return port;
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreConnectionUrlHook.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreConnectionUrlHook.java
new file mode 100644
index 0000000..91a2888
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreConnectionUrlHook.java
@@ -0,0 +1,62 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.session.SessionState;
+
+/**
+ * TestMetaStoreConnectionUrlHook
+ * Verifies that when an instance of an implementation of RawStore is initialized, the connection
+ * URL has already been updated by any metastore connect URL hooks.
+ */
+public class TestMetaStoreConnectionUrlHook extends TestCase {
+  private HiveConf hiveConf;
+
+  @Override
+  protected void setUp() throws Exception {
+
+    super.setUp();
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+  public void testUrlHook() throws Exception {
+    hiveConf = new HiveConf(this.getClass());
+    hiveConf.setVar(HiveConf.ConfVars.METASTORECONNECTURLHOOK,
+        DummyJdoConnectionUrlHook.class.getName());
+    hiveConf.setVar(HiveConf.ConfVars.METASTORECONNECTURLKEY,
+        DummyJdoConnectionUrlHook.initialUrl);
+    hiveConf.setVar(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL,
+        DummyRawStoreForJdoConnection.class.getName());
+    hiveConf.setBoolean("hive.metastore.checkForDefaultDb", true);
+    SessionState.start(new CliSessionState(hiveConf));
+
+    // Instantiating the HMSHandler with hive.metastore.checkForDefaultDb will cause it to
+    // initialize an instance of the DummyRawStoreForJdoConnection
+    HiveMetaStore.HMSHandler hms = new HiveMetaStore.HMSHandler(
+        "test_metastore_connection_url_hook_hms_handler", hiveConf);
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEndFunctionListener.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEndFunctionListener.java
new file mode 100644
index 0000000..e2c860c
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEndFunctionListener.java
@@ -0,0 +1,142 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.shims.ShimLoader;
+/**
+ * TestMetaStoreEventListener. Test case for
+ * {@link org.apache.hadoop.hive.metastore.MetaStoreEndFunctionListener}
+ */
+public class TestMetaStoreEndFunctionListener extends TestCase {
+  private HiveConf hiveConf;
+  private HiveMetaStoreClient msc;
+  private Driver driver;
+
+  @Override
+  protected void setUp() throws Exception {
+
+    super.setUp();
+    System.setProperty("hive.metastore.event.listeners",
+        DummyListener.class.getName());
+    System.setProperty("hive.metastore.pre.event.listeners",
+        DummyPreListener.class.getName());
+    System.setProperty("hive.metastore.end.function.listeners",
+        DummyEndFunctionListener.class.getName());
+    int port = MetaStoreUtils.findFreePort();
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+    hiveConf = new HiveConf(this.getClass());
+    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+    SessionState.start(new CliSessionState(hiveConf));
+    msc = new HiveMetaStoreClient(hiveConf, null);
+    driver = new Driver(hiveConf);
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+  public void testEndFunctionListener() throws Exception {
+    /* Objective here is to ensure that when exceptions are thrown in HiveMetaStore in API methods
+     * they bubble up and are stored in the MetaStoreEndFunctionContext objects
+     */
+    String dbName = "hive3524";
+    String tblName = "tmptbl";
+    int listSize = 0;
+
+    driver.run("create database " + dbName);
+
+    try {
+      msc.getDatabase("UnknownDB");
+    }
+    catch (Exception e) {
+    }
+    listSize = DummyEndFunctionListener.funcNameList.size();
+    String func_name = DummyEndFunctionListener.funcNameList.get(listSize-1);
+    MetaStoreEndFunctionContext context = DummyEndFunctionListener.contextList.get(listSize-1);
+    assertEquals(func_name,"get_database");
+    assertFalse(context.isSuccess());
+    Exception e = context.getException();
+    assertTrue((e!=null));
+    assertTrue((e instanceof NoSuchObjectException));
+    assertEquals(context.getInputTableName(), null);
+
+    driver.run("use " + dbName);
+    driver.run(String.format("create table %s (a string) partitioned by (b string)", tblName));
+    String tableName = "Unknown";
+    try {
+      msc.getTable(dbName, tableName);
+    }
+    catch (Exception e1) {
+    }
+    listSize = DummyEndFunctionListener.funcNameList.size();
+    func_name = DummyEndFunctionListener.funcNameList.get(listSize-1);
+    context = DummyEndFunctionListener.contextList.get(listSize-1);
+    assertEquals(func_name,"get_table");
+    assertFalse(context.isSuccess());
+    e = context.getException();
+    assertTrue((e!=null));
+    assertTrue((e instanceof NoSuchObjectException));
+    assertEquals(context.getInputTableName(), tableName);
+
+    try {
+      msc.getPartition("hive3524", tblName, "b=2012");
+    }
+    catch (Exception e2) {
+    }
+    listSize = DummyEndFunctionListener.funcNameList.size();
+    func_name = DummyEndFunctionListener.funcNameList.get(listSize-1);
+    context = DummyEndFunctionListener.contextList.get(listSize-1);
+    assertEquals(func_name,"get_partition_by_name");
+    assertFalse(context.isSuccess());
+    e = context.getException();
+    assertTrue((e!=null));
+    assertTrue((e instanceof NoSuchObjectException));
+    assertEquals(context.getInputTableName(), tblName);
+    try {
+      driver.run("drop table Unknown");
+    }
+    catch (Exception e4) {
+    }
+    listSize = DummyEndFunctionListener.funcNameList.size();
+    func_name = DummyEndFunctionListener.funcNameList.get(listSize-1);
+    context = DummyEndFunctionListener.contextList.get(listSize-1);
+    assertEquals(func_name,"get_table");
+    assertFalse(context.isSuccess());
+    e = context.getException();
+    assertTrue((e!=null));
+    assertTrue((e instanceof NoSuchObjectException));
+    assertEquals(context.getInputTableName(), "Unknown");
+
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
new file mode 100644
index 0000000..4951c94
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
@@ -0,0 +1,362 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.PartitionEventType;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.metastore.events.AddPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.AlterPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.AlterTableEvent;
+import org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.CreateTableEvent;
+import org.apache.hadoop.hive.metastore.events.DropDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.DropPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.DropTableEvent;
+import org.apache.hadoop.hive.metastore.events.ListenerEvent;
+import org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent;
+import org.apache.hadoop.hive.metastore.events.PreAddPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.PreAlterTableEvent;
+import org.apache.hadoop.hive.metastore.events.PreCreateDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.PreCreateTableEvent;
+import org.apache.hadoop.hive.metastore.events.PreDropDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.PreDropTableEvent;
+import org.apache.hadoop.hive.metastore.events.PreEventContext;
+import org.apache.hadoop.hive.metastore.events.PreLoadPartitionDoneEvent;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.shims.ShimLoader;
+
+/**
+ * TestMetaStoreEventListener. Test case for
+ * {@link org.apache.hadoop.hive.metastore.MetaStoreEventListener} and
+ * {@link org.apache.hadoop.hive.metastore.MetaStorePreEventListener}
+ */
+public class TestMetaStoreEventListener extends TestCase {
+  private HiveConf hiveConf;
+  private HiveMetaStoreClient msc;
+  private Driver driver;
+
+  private static final String dbName = "hive2038";
+  private static final String tblName = "tmptbl";
+  private static final String renamed = "tmptbl2";
+
+  @Override
+  protected void setUp() throws Exception {
+
+    super.setUp();
+
+    System.setProperty("hive.metastore.event.listeners",
+        DummyListener.class.getName());
+    System.setProperty("hive.metastore.pre.event.listeners",
+        DummyPreListener.class.getName());
+
+    int port = MetaStoreUtils.findFreePort();
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+
+    hiveConf = new HiveConf(this.getClass());
+    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+    SessionState.start(new CliSessionState(hiveConf));
+    msc = new HiveMetaStoreClient(hiveConf, null);
+    driver = new Driver(hiveConf);
+
+    driver.run("drop database if exists " + dbName + " cascade");
+
+    DummyListener.notifyList.clear();
+    DummyPreListener.notifyList.clear();
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+  private void validateCreateDb(Database expectedDb, Database actualDb) {
+    assertEquals(expectedDb.getName(), actualDb.getName());
+    assertEquals(expectedDb.getLocationUri(), actualDb.getLocationUri());
+  }
+
+  private void validateTable(Table expectedTable, Table actualTable) {
+    assertEquals(expectedTable.getTableName(), actualTable.getTableName());
+    assertEquals(expectedTable.getDbName(), actualTable.getDbName());
+    assertEquals(expectedTable.getSd().getLocation(), actualTable.getSd().getLocation());
+  }
+
+  private void validateCreateTable(Table expectedTable, Table actualTable) {
+    validateTable(expectedTable, actualTable);
+  }
+
+  private void validateAddPartition(Partition expectedPartition, Partition actualPartition) {
+    assertEquals(expectedPartition, actualPartition);
+  }
+
+  private void validateTableInAddPartition(Table expectedTable, Table actualTable) {
+    assertEquals(expectedTable, actualTable);
+  }
+
+  private void validatePartition(Partition expectedPartition, Partition actualPartition) {
+    assertEquals(expectedPartition.getValues(), actualPartition.getValues());
+    assertEquals(expectedPartition.getDbName(), actualPartition.getDbName());
+    assertEquals(expectedPartition.getTableName(), actualPartition.getTableName());
+  }
+
+  private void validateAlterPartition(Partition expectedOldPartition,
+      Partition expectedNewPartition, String actualOldPartitionDbName,
+      String actualOldPartitionTblName,List<String> actualOldPartitionValues,
+      Partition actualNewPartition) {
+    assertEquals(expectedOldPartition.getValues(), actualOldPartitionValues);
+    assertEquals(expectedOldPartition.getDbName(), actualOldPartitionDbName);
+    assertEquals(expectedOldPartition.getTableName(), actualOldPartitionTblName);
+
+    validatePartition(expectedNewPartition, actualNewPartition);
+  }
+
+  private void validateAlterTable(Table expectedOldTable, Table expectedNewTable,
+      Table actualOldTable, Table actualNewTable) {
+    validateTable(expectedOldTable, actualOldTable);
+    validateTable(expectedNewTable, actualNewTable);
+  }
+
+  private void validateAlterTableColumns(Table expectedOldTable, Table expectedNewTable,
+      Table actualOldTable, Table actualNewTable) {
+    validateAlterTable(expectedOldTable, expectedNewTable, actualOldTable, actualNewTable);
+
+    assertEquals(expectedOldTable.getSd().getCols(), actualOldTable.getSd().getCols());
+    assertEquals(expectedNewTable.getSd().getCols(), actualNewTable.getSd().getCols());
+  }
+
+  private void validateLoadPartitionDone(String expectedTableName,
+      Map<String,String> expectedPartitionName, String actualTableName,
+      Map<String,String> actualPartitionName) {
+    assertEquals(expectedPartitionName, actualPartitionName);
+    assertEquals(expectedTableName, actualTableName);
+  }
+
+  private void validateDropPartition(Partition expectedPartition, Partition actualPartition) {
+    validatePartition(expectedPartition, actualPartition);
+  }
+
+  private void validateTableInDropPartition(Table expectedTable, Table actualTable) {
+    validateTable(expectedTable, actualTable);
+  }
+
+  private void validateDropTable(Table expectedTable, Table actualTable) {
+    validateTable(expectedTable, actualTable);
+  }
+
+  private void validateDropDb(Database expectedDb, Database actualDb) {
+    assertEquals(expectedDb, actualDb);
+  }
+
+  public void testListener() throws Exception {
+    int listSize = 0;
+
+    List<ListenerEvent> notifyList = DummyListener.notifyList;
+    assertEquals(notifyList.size(), listSize);
+    List<PreEventContext> preNotifyList = DummyPreListener.notifyList;
+    assertEquals(preNotifyList.size(), listSize);
+
+    driver.run("create database " + dbName);
+    listSize++;
+    Database db = msc.getDatabase(dbName);
+    assertEquals(listSize, notifyList.size());
+    assertEquals(listSize, preNotifyList.size());
+
+    CreateDatabaseEvent dbEvent = (CreateDatabaseEvent)(notifyList.get(listSize - 1));
+    assert dbEvent.getStatus();
+    validateCreateDb(db, dbEvent.getDatabase());
+
+    PreCreateDatabaseEvent preDbEvent = (PreCreateDatabaseEvent)(preNotifyList.get(listSize - 1));
+    validateCreateDb(db, preDbEvent.getDatabase());
+
+    driver.run("use " + dbName);
+    driver.run(String.format("create table %s (a string) partitioned by (b string)", tblName));
+    listSize++;
+    Table tbl = msc.getTable(dbName, tblName);
+    assertEquals(notifyList.size(), listSize);
+    assertEquals(preNotifyList.size(), listSize);
+
+    CreateTableEvent tblEvent = (CreateTableEvent)(notifyList.get(listSize - 1));
+    assert tblEvent.getStatus();
+    validateCreateTable(tbl, tblEvent.getTable());
+
+    PreCreateTableEvent preTblEvent = (PreCreateTableEvent)(preNotifyList.get(listSize - 1));
+    validateCreateTable(tbl, preTblEvent.getTable());
+
+    driver.run("alter table tmptbl add partition (b='2011')");
+    listSize++;
+    Partition part = msc.getPartition("hive2038", "tmptbl", "b=2011");
+    assertEquals(notifyList.size(), listSize);
+    assertEquals(preNotifyList.size(), listSize);
+
+    AddPartitionEvent partEvent = (AddPartitionEvent)(notifyList.get(listSize-1));
+    assert partEvent.getStatus();
+    validateAddPartition(part, partEvent.getPartition());
+    validateTableInAddPartition(tbl, partEvent.getTable());
+
+    PreAddPartitionEvent prePartEvent = (PreAddPartitionEvent)(preNotifyList.get(listSize-1));
+    validateAddPartition(part, prePartEvent.getPartition());
+
+    driver.run(String.format("alter table %s touch partition (%s)", tblName, "b='2011'"));
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    assertEquals(preNotifyList.size(), listSize);
+
+    //the partition did not change,
+    // so the new partition should be similar to the original partition
+    Partition origP = msc.getPartition(dbName, tblName, "b=2011");
+
+    AlterPartitionEvent alterPartEvent = (AlterPartitionEvent)notifyList.get(listSize - 1);
+    assert alterPartEvent.getStatus();
+    validateAlterPartition(origP, origP, alterPartEvent.getOldPartition().getDbName(),
+        alterPartEvent.getOldPartition().getTableName(),
+        alterPartEvent.getOldPartition().getValues(), alterPartEvent.getNewPartition());
+
+    PreAlterPartitionEvent preAlterPartEvent =
+        (PreAlterPartitionEvent)preNotifyList.get(listSize - 1);
+    validateAlterPartition(origP, origP, preAlterPartEvent.getDbName(),
+        preAlterPartEvent.getTableName(), preAlterPartEvent.getNewPartition().getValues(),
+        preAlterPartEvent.getNewPartition());
+
+    List<String> part_vals = new ArrayList<String>();
+    part_vals.add("c=2012");
+    Partition newPart = msc.appendPartition(dbName, tblName, part_vals);
+
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    assertEquals(preNotifyList.size(), listSize);
+
+    AddPartitionEvent appendPartEvent =
+        (AddPartitionEvent)(notifyList.get(listSize-1));
+    validateAddPartition(newPart, appendPartEvent.getPartition());
+
+    PreAddPartitionEvent preAppendPartEvent =
+        (PreAddPartitionEvent)(preNotifyList.get(listSize-1));
+    validateAddPartition(newPart, preAppendPartEvent.getPartition());
+
+    driver.run(String.format("alter table %s rename to %s", tblName, renamed));
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    assertEquals(preNotifyList.size(), listSize);
+
+    Table renamedTable = msc.getTable(dbName, renamed);
+
+    AlterTableEvent alterTableE = (AlterTableEvent) notifyList.get(listSize-1);
+    assert alterTableE.getStatus();
+    validateAlterTable(tbl, renamedTable, alterTableE.getOldTable(), alterTableE.getNewTable());
+
+    PreAlterTableEvent preAlterTableE = (PreAlterTableEvent) preNotifyList.get(listSize-1);
+    validateAlterTable(tbl, renamedTable, preAlterTableE.getOldTable(),
+        preAlterTableE.getNewTable());
+
+    //change the table name back
+    driver.run(String.format("alter table %s rename to %s", renamed, tblName));
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    assertEquals(preNotifyList.size(), listSize);
+
+    driver.run(String.format("alter table %s ADD COLUMNS (c int)", tblName));
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    assertEquals(preNotifyList.size(), listSize);
+
+    Table altTable = msc.getTable(dbName, tblName);
+
+    alterTableE = (AlterTableEvent) notifyList.get(listSize-1);
+    assert alterTableE.getStatus();
+    validateAlterTableColumns(tbl, altTable, alterTableE.getOldTable(), alterTableE.getNewTable());
+
+    preAlterTableE = (PreAlterTableEvent) preNotifyList.get(listSize-1);
+    validateAlterTableColumns(tbl, altTable, preAlterTableE.getOldTable(),
+        preAlterTableE.getNewTable());
+
+    Map<String,String> kvs = new HashMap<String, String>(1);
+    kvs.put("b", "2011");
+    msc.markPartitionForEvent("hive2038", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    assertEquals(preNotifyList.size(), listSize);
+
+    LoadPartitionDoneEvent partMarkEvent = (LoadPartitionDoneEvent)notifyList.get(listSize - 1);
+    assert partMarkEvent.getStatus();
+    validateLoadPartitionDone("tmptbl", kvs, partMarkEvent.getTable().getTableName(),
+        partMarkEvent.getPartitionName());
+
+    PreLoadPartitionDoneEvent prePartMarkEvent =
+        (PreLoadPartitionDoneEvent)preNotifyList.get(listSize - 1);
+    validateLoadPartitionDone("tmptbl", kvs, prePartMarkEvent.getTableName(),
+        prePartMarkEvent.getPartitionName());
+
+    driver.run(String.format("alter table %s drop partition (b='2011')", tblName));
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    assertEquals(preNotifyList.size(), listSize);
+
+    DropPartitionEvent dropPart = (DropPartitionEvent)notifyList.get(listSize - 1);
+    assert dropPart.getStatus();
+    validateDropPartition(part, dropPart.getPartition());
+    validateTableInDropPartition(tbl, dropPart.getTable());
+
+    PreDropPartitionEvent preDropPart = (PreDropPartitionEvent)preNotifyList.get(listSize - 1);
+    validateDropPartition(part, preDropPart.getPartition());
+    validateTableInDropPartition(tbl, preDropPart.getTable());
+
+    driver.run("drop table " + tblName);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    assertEquals(preNotifyList.size(), listSize);
+
+    DropTableEvent dropTbl = (DropTableEvent)notifyList.get(listSize-1);
+    assert dropTbl.getStatus();
+    validateDropTable(tbl, dropTbl.getTable());
+
+    PreDropTableEvent preDropTbl = (PreDropTableEvent)preNotifyList.get(listSize-1);
+    validateDropTable(tbl, preDropTbl.getTable());
+
+    driver.run("drop database " + dbName);
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+    assertEquals(preNotifyList.size(), listSize);
+
+    DropDatabaseEvent dropDB = (DropDatabaseEvent)notifyList.get(listSize-1);
+    assert dropDB.getStatus();
+    validateDropDb(db, dropDB.getDatabase());
+
+    PreDropDatabaseEvent preDropDB = (PreDropDatabaseEvent)preNotifyList.get(listSize-1);
+    assert dropDB.getStatus();
+    validateDropDb(db, preDropDB.getDatabase());
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListenerOnlyOnCommit.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListenerOnlyOnCommit.java
new file mode 100644
index 0000000..6a14982
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListenerOnlyOnCommit.java
@@ -0,0 +1,104 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import java.util.List;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.events.ListenerEvent;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.shims.ShimLoader;
+
+/**
+ * Ensure that the status of MetaStore events depend on the RawStore's commit status.
+ */
+public class TestMetaStoreEventListenerOnlyOnCommit extends TestCase {
+
+  private HiveConf hiveConf;
+  private HiveMetaStoreClient msc;
+  private Driver driver;
+
+  @Override
+  protected void setUp() throws Exception {
+
+    super.setUp();
+
+    DummyRawStoreControlledCommit.setCommitSucceed(true);
+
+    System.setProperty(HiveConf.ConfVars.METASTORE_EVENT_LISTENERS.varname,
+            DummyListener.class.getName());
+    System.setProperty(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL.varname,
+            DummyRawStoreControlledCommit.class.getName());
+
+    int port = MetaStoreUtils.findFreePort();
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+
+    hiveConf = new HiveConf(this.getClass());
+    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+    SessionState.start(new CliSessionState(hiveConf));
+    msc = new HiveMetaStoreClient(hiveConf, null);
+    driver = new Driver(hiveConf);
+
+    DummyListener.notifyList.clear();
+  }
+
+  public void testEventStatus() throws Exception {
+    int listSize = 0;
+    List<ListenerEvent> notifyList = DummyListener.notifyList;
+    assertEquals(notifyList.size(), listSize);
+
+    driver.run("CREATE DATABASE tmpDb");
+    listSize += 1;
+    notifyList = DummyListener.notifyList;
+    assertEquals(notifyList.size(), listSize);
+    assertTrue(DummyListener.getLastEvent().getStatus());
+
+    driver.run("CREATE TABLE unittest_TestMetaStoreEventListenerOnlyOnCommit (id INT) " +
+                "PARTITIONED BY (ds STRING)");
+    listSize += 1;
+    notifyList = DummyListener.notifyList;
+    assertEquals(notifyList.size(), listSize);
+    assertTrue(DummyListener.getLastEvent().getStatus());
+
+    driver.run("ALTER TABLE unittest_TestMetaStoreEventListenerOnlyOnCommit " +
+                "ADD PARTITION(ds='foo1')");
+    listSize += 1;
+    notifyList = DummyListener.notifyList;
+    assertEquals(notifyList.size(), listSize);
+    assertTrue(DummyListener.getLastEvent().getStatus());
+
+    DummyRawStoreControlledCommit.setCommitSucceed(false);
+
+    driver.run("ALTER TABLE unittest_TestMetaStoreEventListenerOnlyOnCommit " +
+                "ADD PARTITION(ds='foo2')");
+    listSize += 1;
+    notifyList = DummyListener.notifyList;
+    assertEquals(notifyList.size(), listSize);
+    assertFalse(DummyListener.getLastEvent().getStatus());
+
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreInitListener.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreInitListener.java
new file mode 100644
index 0000000..42232fd
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreInitListener.java
@@ -0,0 +1,69 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.shims.ShimLoader;
+
+/**
+ * TestMetaStoreInitListener. Test case for
+ * {@link org.apache.hadoop.hive.metastore.MetaStoreInitListener}
+ */
+public class TestMetaStoreInitListener extends TestCase {
+  private HiveConf hiveConf;
+  private HiveMetaStoreClient msc;
+  private Driver driver;
+
+  @Override
+  protected void setUp() throws Exception {
+
+    super.setUp();
+    System.setProperty("hive.metastore.init.hooks",
+        DummyMetaStoreInitListener.class.getName());
+    int port = MetaStoreUtils.findFreePort();
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+    hiveConf = new HiveConf(this.getClass());
+    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+    SessionState.start(new CliSessionState(hiveConf));
+    msc = new HiveMetaStoreClient(hiveConf, null);
+    driver = new Driver(hiveConf);
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+  public void testMetaStoreInitListener() throws Exception {
+    // DummyMataStoreInitListener's onInit will be called at HMSHandler
+    // initialization, and set this to true
+    assertTrue(DummyMetaStoreInitListener.wasCalled);
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreListenersError.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreListenersError.java
new file mode 100644
index 0000000..d074028
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreListenersError.java
@@ -0,0 +1,84 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import junit.framework.Assert;
+import junit.framework.TestCase;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.shims.ShimLoader;
+
+/**
+ * Test for unwrapping InvocationTargetException, which is thrown from
+ * constructor of listener class
+ */
+public class TestMetaStoreListenersError extends TestCase {
+
+  public void testInitListenerException() throws Throwable {
+
+    System.setProperty("hive.metastore.init.hooks", ErrorInitListener.class.getName());
+    int port = MetaStoreUtils.findFreePort();
+    try {
+      HiveMetaStore.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+    } catch (Throwable throwable) {
+      Assert.assertEquals(MetaException.class, throwable.getClass());
+      Assert.assertEquals(
+          "Failed to instantiate listener named: " +
+              "org.apache.hadoop.hive.metastore.TestMetaStoreListenersError$ErrorInitListener, " +
+              "reason: java.lang.IllegalArgumentException: exception on constructor",
+          throwable.getMessage());
+    }
+  }
+
+  public void testEventListenerException() throws Throwable {
+
+    System.setProperty("hive.metastore.init.hooks", "");
+    System.setProperty("hive.metastore.event.listeners", ErrorEventListener.class.getName());
+    int port = MetaStoreUtils.findFreePort();
+    try {
+      HiveMetaStore.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+    } catch (Throwable throwable) {
+      Assert.assertEquals(MetaException.class, throwable.getClass());
+      Assert.assertEquals(
+          "Failed to instantiate listener named: " +
+              "org.apache.hadoop.hive.metastore.TestMetaStoreListenersError$ErrorEventListener, " +
+              "reason: java.lang.IllegalArgumentException: exception on constructor",
+          throwable.getMessage());
+    }
+  }
+
+  public static class ErrorInitListener extends MetaStoreInitListener {
+
+    public ErrorInitListener(Configuration config) {
+      super(config);
+      throw new IllegalArgumentException("exception on constructor");
+    }
+
+    public void onInit(MetaStoreInitContext context) throws MetaException {
+    }
+  }
+
+  public static class ErrorEventListener extends MetaStoreEventListener {
+
+    public ErrorEventListener(Configuration config) {
+      super(config);
+      throw new IllegalArgumentException("exception on constructor");
+    }
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
new file mode 100644
index 0000000..3efec79
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
@@ -0,0 +1,188 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.metastore;
+
+import java.io.File;
+import java.lang.reflect.Field;
+import java.util.Random;
+
+import junit.framework.TestCase;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
+import org.apache.hadoop.hive.ql.session.SessionState;
+
+public class TestMetastoreVersion extends TestCase {
+
+  protected HiveConf hiveConf;
+  private Driver driver;
+  private String metaStoreRoot;
+  private String testMetastoreDB;
+  Random randomNum = new Random();
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    Field defDb = HiveMetaStore.HMSHandler.class.getDeclaredField("createDefaultDB");
+    defDb.setAccessible(true);
+    defDb.setBoolean(null, false);
+    hiveConf = new HiveConf(this.getClass());
+    System.setProperty("hive.metastore.event.listeners",
+        DummyListener.class.getName());
+    System.setProperty("hive.metastore.pre.event.listeners",
+        DummyPreListener.class.getName());
+    testMetastoreDB = System.getProperty("java.io.tmpdir") +
+    File.separator + "test_metastore-" + randomNum.nextInt();
+    System.setProperty(HiveConf.ConfVars.METASTORECONNECTURLKEY.varname,
+        "jdbc:derby:" + testMetastoreDB + ";create=true");
+    metaStoreRoot = System.getProperty("test.tmp.dir");
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    File metaStoreDir = new File(testMetastoreDB);
+    if (metaStoreDir.exists()) {
+      FileUtils.deleteDirectory(metaStoreDir);
+    }
+  }
+
+  /***
+   * Test config defaults
+   */
+  public void testDefaults() {
+    hiveConf = new HiveConf(this.getClass());
+    assertFalse(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION));
+    assertTrue(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_AUTO_CREATE_SCHEMA));
+    assertFalse(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_FIXED_DATASTORE));
+  }
+
+  /***
+   * Test schema verification property
+   * @throws Exception
+   */
+  public void testVersionRestriction () throws Exception {
+    System.setProperty(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION.toString(), "true");
+    hiveConf = new HiveConf(this.getClass());
+    assertTrue(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION));
+    assertFalse(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_AUTO_CREATE_SCHEMA));
+    assertTrue(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_FIXED_DATASTORE));
+
+    SessionState.start(new CliSessionState(hiveConf));
+    driver = new Driver(hiveConf);
+    // driver execution should fail since the schema didn't get created
+    CommandProcessorResponse proc = driver.run("show tables");
+    assertFalse(proc.getResponseCode() == 0);
+   }
+
+  /***
+   * Test that with no verification, hive populates the schema and version correctly
+   * @throws Exception
+   */
+  public void testMetastoreVersion () throws Exception {
+    // let the schema and version be auto created
+    System.setProperty(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION.toString(), "false");
+    hiveConf = new HiveConf(this.getClass());
+    SessionState.start(new CliSessionState(hiveConf));
+    driver = new Driver(hiveConf);
+    driver.run("show tables");
+
+    // correct version stored by Metastore during startup
+    assertEquals(MetaStoreSchemaInfo.getHiveSchemaVersion(), getVersion(hiveConf));
+    setVersion(hiveConf, "foo");
+    assertEquals("foo", getVersion(hiveConf));
+  }
+
+  /***
+   * Test that with verification enabled, hive works when the correct schema is already populated
+   * @throws Exception
+   */
+  public void testVersionMatching () throws Exception {
+    System.setProperty(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION.toString(), "false");
+    hiveConf = new HiveConf(this.getClass());
+    SessionState.start(new CliSessionState(hiveConf));
+    driver = new Driver(hiveConf);
+    driver.run("show tables");
+
+    hiveConf.setBoolVar(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION, true);
+    setVersion(hiveConf, MetaStoreSchemaInfo.getHiveSchemaVersion());
+    driver = new Driver(hiveConf);
+    CommandProcessorResponse proc = driver.run("show tables");
+    assertTrue(proc.getResponseCode() == 0);
+  }
+
+  /**
+   * Store garbage version in metastore and verify that hive fails when verification is on
+   * @throws Exception
+   */
+  public void testVersionMisMatch () throws Exception {
+    System.setProperty(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION.toString(), "false");
+    hiveConf = new HiveConf(this.getClass());
+    SessionState.start(new CliSessionState(hiveConf));
+    driver = new Driver(hiveConf);
+    driver.run("show tables");
+
+    System.setProperty(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION.toString(), "true");
+    hiveConf = new HiveConf(this.getClass());
+    setVersion(hiveConf, "fooVersion");
+    SessionState.start(new CliSessionState(hiveConf));
+    driver = new Driver(hiveConf);
+    CommandProcessorResponse proc = driver.run("show tables");
+    assertFalse(proc.getResponseCode() == 0);
+  }
+
+  //  write the given version to metastore
+  private String getVersion(HiveConf conf) throws HiveMetaException {
+    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, conf, "derby");
+    return getMetaStoreVersion();
+  }
+
+  //  write the given version to metastore
+  private void setVersion(HiveConf conf, String version) throws HiveMetaException {
+    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, conf, "derby");
+    setMetaStoreVersion(version, "setVersion test");
+  }
+
+  // Load the version stored in the metastore db
+  public String getMetaStoreVersion() throws HiveMetaException {
+    ObjectStore objStore = new ObjectStore();
+    objStore.setConf(hiveConf);
+    try {
+      return objStore.getMetaStoreSchemaVersion();
+    } catch (MetaException e) {
+      throw new HiveMetaException("Failed to get version", e);
+    }
+  }
+
+  // Store the given version and comment in the metastore
+  public void setMetaStoreVersion(String newVersion, String comment) throws HiveMetaException {
+    ObjectStore objStore = new ObjectStore();
+    objStore.setConf(hiveConf);
+    try {
+      objStore.setMetaStoreSchemaVersion(newVersion, comment);
+    } catch (MetaException e) {
+      throw new HiveMetaException("Failed to set version", e);
+    }
+  }
+
+
+}
+
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestPartitionNameWhitelistValidation.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestPartitionNameWhitelistValidation.java
new file mode 100644
index 0000000..61d08cd
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestPartitionNameWhitelistValidation.java
@@ -0,0 +1,146 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import junit.framework.Assert;
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.junit.Test;
+
+// Validate the metastore client call validatePartitionNameCharacters to ensure it throws
+// an exception if partition fields contain Unicode characters or commas
+
+public class TestPartitionNameWhitelistValidation extends TestCase {
+
+  private static final String partitionValidationPattern = "[\\x20-\\x7E&&[^,]]*";
+
+  private HiveConf hiveConf;
+  private HiveMetaStoreClient msc;
+  private Driver driver;
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    System.setProperty(HiveConf.ConfVars.METASTORE_PARTITION_NAME_WHITELIST_PATTERN.varname,
+        partitionValidationPattern);
+    int port = MetaStoreUtils.findFreePort();
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+    hiveConf = new HiveConf(this.getClass());
+    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    SessionState.start(new CliSessionState(hiveConf));
+    msc = new HiveMetaStoreClient(hiveConf, null);
+    driver = new Driver(hiveConf);
+  }
+
+  // Runs an instance of DisallowUnicodePreEventListener
+  // Returns whether or not it succeeded
+  private boolean runValidation(List<String> partVals) {
+
+    try {
+      msc.validatePartitionNameCharacters(partVals);
+    } catch (Exception e) {
+      return false;
+    }
+
+    return true;
+ }
+
+  // Sample data
+  private List<String> getPartValsWithUnicode() {
+
+    List<String> partVals = new ArrayList<String>();
+    partVals.add("klâwen");
+    partVals.add("tägelîch");
+
+    return partVals;
+
+  }
+
+  private List<String> getPartValsWithCommas() {
+
+    List<String> partVals = new ArrayList<String>();
+    partVals.add("a,b");
+    partVals.add("c,d,e,f");
+
+    return partVals;
+
+  }
+
+  private List<String> getPartValsWithValidCharacters() {
+
+    List<String> partVals = new ArrayList<String>();
+    partVals.add("part1");
+    partVals.add("part2");
+
+    return partVals;
+
+  }
+
+  @Test
+  public void testAddPartitionWithCommas() {
+
+    Assert.assertFalse("Add a partition with commas in name",
+        runValidation(getPartValsWithCommas()));
+  }
+
+  @Test
+  public void testAddPartitionWithUnicode() {
+
+    Assert.assertFalse("Add a partition with unicode characters in name",
+        runValidation(getPartValsWithUnicode()));
+  }
+
+  @Test
+  public void testAddPartitionWithValidPartVal() {
+
+    Assert.assertTrue("Add a partition with unicode characters in name",
+        runValidation(getPartValsWithValidCharacters()));
+  }
+
+  @Test
+  public void testAppendPartitionWithUnicode() {
+
+    Assert.assertFalse("Append a partition with unicode characters in name",
+        runValidation(getPartValsWithUnicode()));
+  }
+
+  @Test
+  public void testAppendPartitionWithCommas() {
+
+    Assert.assertFalse("Append a partition with unicode characters in name",
+        runValidation(getPartValsWithCommas()));
+  }
+
+  @Test
+  public void testAppendPartitionWithValidCharacters() {
+
+    Assert.assertTrue("Append a partition with no unicode characters in name",
+        runValidation(getPartValsWithValidCharacters()));
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRawStoreTxn.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRawStoreTxn.java
new file mode 100644
index 0000000..0b87077
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRawStoreTxn.java
@@ -0,0 +1,101 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import java.lang.reflect.Method;
+import java.util.Arrays;
+import java.util.List;
+
+import javax.jdo.JDOException;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.conf.HiveConf;
+
+public class TestRawStoreTxn extends TestCase {
+
+  public static class DummyRawStoreWithCommitError extends DummyRawStoreForJdoConnection {
+    private static int callCount = 0;
+
+    @Override
+    /***
+     * Throw exception on first try
+     */
+    public boolean commitTransaction() {
+      callCount++;
+      if (callCount == 1 ) {
+        throw new JDOException ("Failed for call count " + callCount);
+      } else {
+        return true;
+      }
+    }
+  }
+
+  private ObjectStore objStore;
+  private HiveConf hiveConf;
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    hiveConf = new HiveConf();
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+  /***
+   * Check annotations of the restricted methods
+   * @throws Exception
+   */
+  public void testCheckNoRetryMethods() throws Exception {
+    List<String> nonExecMethods =
+      Arrays.asList("commitTransaction", "commitTransaction");
+
+    RawStore rawStore = RetryingRawStore.getProxy(hiveConf, new Configuration(hiveConf),
+          hiveConf.getVar(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL), 1);
+    for (Method rawStoreMethod : RawStore.class.getMethods()) {
+      if (nonExecMethods.contains(rawStoreMethod.getName())) {
+        assertNotNull(rawStoreMethod.getAnnotation(RawStore.CanNotRetry.class));
+      }
+    }
+  }
+
+  /***
+   * Invoke commit and verify it doesn't get retried
+   * @throws Exception
+   */
+  public void testVerifyNoRetryMethods() throws Exception {
+    hiveConf.setVar(HiveConf.ConfVars.METASTORECONNECTURLKEY,
+        DummyJdoConnectionUrlHook.newUrl);;
+    hiveConf.setVar(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL,
+        DummyRawStoreWithCommitError.class.getName());
+    RawStore rawStore = RetryingRawStore.getProxy(hiveConf, new Configuration(hiveConf),
+        DummyRawStoreWithCommitError.class.getName(), 1);
+    try {
+      rawStore.commitTransaction();
+      fail("Commit should fail due to no retry");
+    } catch (JDOException e) {
+      // Excepted JDOException
+    }
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStore.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStore.java
new file mode 100644
index 0000000..491d093
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStore.java
@@ -0,0 +1,57 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.shims.ShimLoader;
+
+
+public class TestRemoteHiveMetaStore extends TestHiveMetaStore {
+  private static boolean isServerStarted = false;
+
+  public TestRemoteHiveMetaStore() {
+    super();
+    isThriftClient = true;
+  }
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+
+    if (isServerStarted) {
+      assertNotNull("Unable to connect to the MetaStore server", client);
+      return;
+    }
+
+    int port = MetaStoreUtils.findFreePort();
+    System.out.println("Starting MetaStore Server on port " + port);
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+    isServerStarted = true;
+
+    // This is default case with setugi off for both client and server
+    createClient(false, port);
+  }
+
+  protected void createClient(boolean setugi, int port) throws Exception {
+    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    hiveConf.setBoolVar(ConfVars.METASTORE_EXECUTE_SET_UGI,setugi);
+    client = new HiveMetaStoreClient(hiveConf);
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreIpAddress.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreIpAddress.java
new file mode 100644
index 0000000..7600e99
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreIpAddress.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.util.StringUtils;
+
+/**
+ *
+ * TestRemoteHiveMetaStoreIpAddress.
+ *
+ * Test which checks that the remote Hive metastore stores the proper IP address using
+ * IpAddressListener
+ */
+public class TestRemoteHiveMetaStoreIpAddress extends TestCase {
+  private static boolean isServerStarted = false;
+  private static HiveConf hiveConf;
+  private static HiveMetaStoreClient msc;
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    hiveConf = new HiveConf(this.getClass());
+
+    if (isServerStarted) {
+      assertNotNull("Unable to connect to the MetaStore server", msc);
+      return;
+    }
+
+    int port = MetaStoreUtils.findFreePort();
+    System.out.println("Starting MetaStore Server on port " + port);
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+    isServerStarted = true;
+
+    // This is default case with setugi off for both client and server
+    createClient(port);
+  }
+
+  public void testIpAddress() throws Exception {
+    try {
+
+      Database db = new Database();
+      db.setName("testIpAddressIp");
+      msc.createDatabase(db);
+      msc.dropDatabase(db.getName());
+    } catch (Exception e) {
+      System.err.println(StringUtils.stringifyException(e));
+      System.err.println("testIpAddress() failed.");
+      throw e;
+    }
+  }
+
+  protected void createClient(int port) throws Exception {
+    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    msc = new HiveMetaStoreClient(hiveConf);
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteUGIHiveMetaStoreIpAddress.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteUGIHiveMetaStoreIpAddress.java
new file mode 100644
index 0000000..8658262
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteUGIHiveMetaStoreIpAddress.java
@@ -0,0 +1,28 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.metastore;
+
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+
+public class TestRemoteUGIHiveMetaStoreIpAddress extends TestRemoteHiveMetaStoreIpAddress {
+  public TestRemoteUGIHiveMetaStoreIpAddress() {
+    super();
+    System.setProperty(ConfVars.METASTORE_EXECUTE_SET_UGI.varname, "true");
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRetryingHMSHandler.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRetryingHMSHandler.java
new file mode 100644
index 0000000..e059255
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRetryingHMSHandler.java
@@ -0,0 +1,116 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import junit.framework.Assert;
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.Order;
+import org.apache.hadoop.hive.metastore.api.SerDeInfo;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.shims.ShimLoader;
+
+/**
+ * TestRetryingHMSHandler. Test case for
+ * {@link org.apache.hadoop.hive.metastore.RetryingHMSHandler}
+ */
+public class TestRetryingHMSHandler extends TestCase {
+  private HiveConf hiveConf;
+  private HiveMetaStoreClient msc;
+
+  @Override
+  protected void setUp() throws Exception {
+
+    super.setUp();
+    System.setProperty("hive.metastore.pre.event.listeners",
+        AlternateFailurePreListener.class.getName());
+    int port = MetaStoreUtils.findFreePort();
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+    hiveConf = new HiveConf(this.getClass());
+    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    hiveConf.setIntVar(HiveConf.ConfVars.HMSHANDLERATTEMPTS, 2);
+    hiveConf.setIntVar(HiveConf.ConfVars.HMSHANDLERINTERVAL, 0);
+    hiveConf.setBoolVar(HiveConf.ConfVars.HMSHANDLERFORCERELOADCONF, false);
+    msc = new HiveMetaStoreClient(hiveConf, null);
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+  // Create a database and a table in that database.  Because the AlternateFailurePreListener is
+  // being used each attempt to create something should require two calls by the RetryingHMSHandler
+  public void testRetryingHMSHandler() throws Exception {
+    String dbName = "hive4159";
+    String tblName = "tmptbl";
+
+    Database db = new Database();
+    db.setName(dbName);
+    msc.createDatabase(db);
+
+    Assert.assertEquals(2, AlternateFailurePreListener.getCallCount());
+
+    ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
+    cols.add(new FieldSchema("c1", serdeConstants.STRING_TYPE_NAME, ""));
+    cols.add(new FieldSchema("c2", serdeConstants.INT_TYPE_NAME, ""));
+
+    Map<String, String> params = new HashMap<String, String>();
+    params.put("test_param_1", "Use this for comments etc");
+
+    Map<String, String> serdParams = new HashMap<String, String>();
+    serdParams.put(serdeConstants.SERIALIZATION_FORMAT, "1");
+
+    StorageDescriptor sd = new StorageDescriptor();
+
+    sd.setCols(cols);
+    sd.setCompressed(false);
+    sd.setNumBuckets(1);
+    sd.setParameters(params);
+    sd.setBucketCols(new ArrayList<String>(2));
+    sd.getBucketCols().add("name");
+    sd.setSerdeInfo(new SerDeInfo());
+    sd.getSerdeInfo().setName(tblName);
+    sd.getSerdeInfo().setParameters(serdParams);
+    sd.getSerdeInfo().getParameters()
+        .put(serdeConstants.SERIALIZATION_FORMAT, "1");
+    sd.setSortCols(new ArrayList<Order>());
+
+    Table tbl = new Table();
+    tbl.setDbName(dbName);
+    tbl.setTableName(tblName);
+    tbl.setSd(sd);
+    tbl.setLastAccessTime(0);
+
+    msc.createTable(tbl);
+
+    Assert.assertEquals(4, AlternateFailurePreListener.getCallCount());
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnBothClientServer.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnBothClientServer.java
new file mode 100644
index 0000000..98708a6
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnBothClientServer.java
@@ -0,0 +1,31 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+
+public class TestSetUGIOnBothClientServer extends TestRemoteHiveMetaStore{
+
+  public TestSetUGIOnBothClientServer() {
+    super();
+    isThriftClient = true;
+    // This will turn on setugi on both client and server processes of the test.
+    System.setProperty(ConfVars.METASTORE_EXECUTE_SET_UGI.varname, "true");
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyClient.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyClient.java
new file mode 100644
index 0000000..2c6d567
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyClient.java
@@ -0,0 +1,28 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+public class TestSetUGIOnOnlyClient extends TestRemoteHiveMetaStore{
+
+  @Override
+  protected void createClient(boolean setugi, int port) throws Exception {
+    // turn it on for client.
+    super.createClient(true, port);
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyServer.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyServer.java
new file mode 100644
index 0000000..6c3fbf6
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyServer.java
@@ -0,0 +1,28 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+public class TestSetUGIOnOnlyServer extends TestSetUGIOnBothClientServer {
+
+  @Override
+  protected void createClient(boolean setugi, int port) throws Exception {
+    // It is turned on for both client and server because of super class. Turn it off for client.
+    super.createClient(false, port);
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/BaseTestQueries.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/BaseTestQueries.java
new file mode 100644
index 0000000..40674cf
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/BaseTestQueries.java
@@ -0,0 +1,49 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql;
+
+import java.io.File;
+
+import junit.framework.TestCase;
+
+/**
+ * Base class for testing queries.
+ */
+public abstract class BaseTestQueries extends TestCase {
+
+  protected final String inpDir = System
+      .getProperty("hive.root") + "/ql/src/test/queries/clientpositive";
+  protected final String resDir = System
+      .getProperty("hive.root") + "/ql/src/test/results/clientpositive";
+  protected final String logDir = System
+      .getProperty("build.dir") + "/junit-qfile-results/clientpositive";
+
+  /**
+   * Create a file for each test name in the inpDir.
+   * @param testNames
+   * @return files corresponding to each test name
+   */
+  protected File[] setupQFiles(String[] testNames) {
+    File[] qfiles = new File[testNames.length];
+    for (int i = 0; i < testNames.length; i++) {
+      qfiles[i] = new File(inpDir, testNames[i]);
+    }
+    return qfiles;
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java
new file mode 100644
index 0000000..9384a35
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java
@@ -0,0 +1,116 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileReader;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Suite for testing location. e.g. if "alter table alter partition
+ * location" is run, do the partitions end up in the correct location.
+ *
+ *  This is a special case of the regular queries as paths are typically
+ *  ignored.
+ */
+public class TestLocationQueries extends BaseTestQueries {
+
+  public TestLocationQueries() {
+    File logDirFile = new File(logDir);
+    if (!(logDirFile.exists() || logDirFile.mkdirs())) {
+      fail("Could not create " + logDir);
+    }
+  }
+
+  /**
+   * Our own checker - validate the location of the partition.
+   */
+  public static class CheckResults extends QTestUtil {
+    private final String locationSubdir;
+
+    /**
+     * Validate only that the location is correct.
+     * @return non-zero if it failed
+     */
+    @Override
+    public int checkCliDriverResults(String tname) throws Exception {
+      File logFile = new File(logDir, tname + ".out");
+
+      int failedCount = 0;
+      FileReader fr = new FileReader(logFile);
+      BufferedReader in = new BufferedReader(fr);
+      try {
+        String line;
+        int locationCount = 0;
+        Pattern p = Pattern.compile("location:([^,)]+)");
+        while((line = in.readLine()) != null) {
+          Matcher m = p.matcher(line);
+          if (m.find()) {
+            File f = new File(m.group(1));
+            if (!f.getName().equals(locationSubdir)) {
+              failedCount++;
+            }
+            locationCount++;
+          }
+        }
+        // we always have to find at least one location, otw the test is useless
+        if (locationCount == 0) {
+          return Integer.MAX_VALUE;
+        }
+      } finally {
+        in.close();
+      }
+
+      return failedCount;
+    }
+
+    public CheckResults(String outDir, String logDir, boolean miniMr,
+        String hadoopVer, String locationSubdir)
+      throws Exception
+    {
+      super(outDir, logDir, miniMr, hadoopVer);
+      this.locationSubdir = locationSubdir;
+    }
+  }
+
+  /**
+   * Verify that the location of the partition is valid. In this case
+   * the path should end in "parta" and not "dt=a" (the default).
+   *
+   */
+  public void testAlterTablePartitionLocation_alter5() throws Exception {
+    String[] testNames = new String[] {"alter5.q"};
+
+    File[] qfiles = setupQFiles(testNames);
+
+    QTestUtil[] qt = new QTestUtil[qfiles.length];
+    for (int i = 0; i < qfiles.length; i++) {
+      qt[i] = new CheckResults(resDir, logDir, false, "0.20", "parta");
+      qt[i].addFile(qfiles[i]);
+      qt[i].clearTestSideEffects();
+    }
+
+    boolean success = QTestUtil.queryListRunnerSingleThreaded(qfiles, qt);
+    if (!success) {
+      fail("One or more queries failed");
+    }
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestMTQueries.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestMTQueries.java
new file mode 100644
index 0000000..378de03
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestMTQueries.java
@@ -0,0 +1,47 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql;
+
+import java.io.File;
+
+/**
+ * Suite for testing running of queries in multi-threaded mode.
+ */
+public class TestMTQueries extends BaseTestQueries {
+
+  public TestMTQueries() {
+    File logDirFile = new File(logDir);
+    if (!(logDirFile.exists() || logDirFile.mkdirs())) {
+      fail("Could not create " + logDir);
+    }
+  }
+
+  public void testMTQueries1() throws Exception {
+    String[] testNames = new String[] {"join1.q", "join2.q", "groupby1.q",
+        "groupby2.q", "join3.q", "input1.q", "input19.q"};
+
+    File[] qfiles = setupQFiles(testNames);
+
+    QTestUtil[] qts = QTestUtil.queryListRunnerSetup(qfiles, resDir, logDir);
+    boolean success = QTestUtil.queryListRunnerMultiThreaded(qfiles, qts);
+    if (!success) {
+      fail("One or more queries failed");
+    }
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/history/TestHiveHistory.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
new file mode 100644
index 0000000..8beef09
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
@@ -0,0 +1,232 @@
+package org.apache.hadoop.hive.ql.history;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.PrintStream;
+import java.io.UnsupportedEncodingException;
+import java.lang.reflect.Proxy;
+import java.util.LinkedList;
+import java.util.Map;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.common.LogUtils;
+import org.apache.hadoop.hive.common.LogUtils.LogInitializationException;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.history.HiveHistory.Keys;
+import org.apache.hadoop.hive.ql.history.HiveHistory.QueryInfo;
+import org.apache.hadoop.hive.ql.history.HiveHistory.TaskInfo;
+import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
+import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.tools.LineageInfo;
+import org.apache.hadoop.mapred.TextInputFormat;
+
+/**
+ * TestHiveHistory.
+ *
+ */
+public class TestHiveHistory extends TestCase {
+
+  static HiveConf conf;
+
+  private static String tmpdir = System.getProperty("test.tmp.dir");
+  private static Path tmppath = new Path(tmpdir);
+  private static Hive db;
+  private static FileSystem fs;
+  /*
+   * intialize the tables
+   */
+
+  @Override
+  protected void setUp() {
+    try {
+      conf = new HiveConf(HiveHistory.class);
+      SessionState.start(conf);
+
+      fs = FileSystem.get(conf);
+      if (fs.exists(tmppath) && !fs.getFileStatus(tmppath).isDir()) {
+        throw new RuntimeException(tmpdir + " exists but is not a directory");
+      }
+
+      if (!fs.exists(tmppath)) {
+        if (!fs.mkdirs(tmppath)) {
+          throw new RuntimeException("Could not make scratch directory "
+              + tmpdir);
+        }
+      }
+
+      conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
+
+      // copy the test files into hadoop if required.
+      int i = 0;
+      Path[] hadoopDataFile = new Path[2];
+      String[] testFiles = {"kv1.txt", "kv2.txt"};
+      String testFileDir = new Path(conf.get("test.data.files")).toUri().getPath();
+      for (String oneFile : testFiles) {
+        Path localDataFile = new Path(testFileDir, oneFile);
+        hadoopDataFile[i] = new Path(tmppath, oneFile);
+        fs.copyFromLocalFile(false, true, localDataFile, hadoopDataFile[i]);
+        i++;
+      }
+
+      // load the test files into tables
+      i = 0;
+      db = Hive.get(conf);
+      String[] srctables = {"src", "src2"};
+      LinkedList<String> cols = new LinkedList<String>();
+      cols.add("key");
+      cols.add("value");
+      for (String src : srctables) {
+        db.dropTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, src, true, true);
+        db.createTable(src, cols, null, TextInputFormat.class,
+            IgnoreKeyTextOutputFormat.class);
+        db.loadTable(hadoopDataFile[i], src, false, false);
+        i++;
+      }
+
+    } catch (Throwable e) {
+      e.printStackTrace();
+      throw new RuntimeException("Encountered throwable");
+    }
+  }
+
+  /**
+   * Check history file output for this query.
+   */
+  public void testSimpleQuery() {
+    new LineageInfo();
+    try {
+
+      // NOTE: It is critical to do this here so that log4j is reinitialized
+      // before any of the other core hive classes are loaded
+      try {
+        LogUtils.initHiveLog4j();
+      } catch (LogInitializationException e) {
+      }
+      HiveConf hconf = new HiveConf(SessionState.class);
+      hconf.setBoolVar(ConfVars.HIVE_SESSION_HISTORY_ENABLED, true);
+      CliSessionState ss = new CliSessionState(hconf);
+      ss.in = System.in;
+      try {
+        ss.out = new PrintStream(System.out, true, "UTF-8");
+        ss.err = new PrintStream(System.err, true, "UTF-8");
+      } catch (UnsupportedEncodingException e) {
+        System.exit(3);
+      }
+
+      SessionState.start(ss);
+
+      String cmd = "select a.key from src a";
+      Driver d = new Driver(conf);
+      int ret = d.run(cmd).getResponseCode();
+      if (ret != 0) {
+        fail("Failed");
+      }
+      HiveHistoryViewer hv = new HiveHistoryViewer(SessionState.get()
+          .getHiveHistory().getHistFileName());
+      Map<String, QueryInfo> jobInfoMap = hv.getJobInfoMap();
+      Map<String, TaskInfo> taskInfoMap = hv.getTaskInfoMap();
+      if (jobInfoMap.size() != 1) {
+        fail("jobInfo Map size not 1");
+      }
+
+      if (taskInfoMap.size() != 1) {
+        fail("jobInfo Map size not 1");
+      }
+
+      cmd = (String) jobInfoMap.keySet().toArray()[0];
+      QueryInfo ji = jobInfoMap.get(cmd);
+
+      if (!ji.hm.get(Keys.QUERY_NUM_TASKS.name()).equals("1")) {
+        fail("Wrong number of tasks");
+      }
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail("Failed");
+    }
+  }
+
+  public void testQueryloglocParentDirNotExist() throws Exception {
+    String parentTmpDir = tmpdir + "/HIVE2654";
+    Path parentDirPath = new Path(parentTmpDir);
+    try {
+      fs.delete(parentDirPath, true);
+    } catch (Exception e) {
+    }
+    try {
+      String actualDir = parentTmpDir + "/test";
+      HiveConf conf = new HiveConf(SessionState.class);
+      conf.set(HiveConf.ConfVars.HIVEHISTORYFILELOC.toString(), actualDir);
+      SessionState ss = new CliSessionState(conf);
+      HiveHistory hiveHistory = new HiveHistoryImpl(ss);
+      Path actualPath = new Path(actualDir);
+      if (!fs.exists(actualPath)) {
+        fail("Query location path is not exist :" + actualPath.toString());
+      }
+    } finally {
+      try {
+        fs.delete(parentDirPath, true);
+      } catch (Exception e) {
+      }
+    }
+  }
+
+  /**
+   * Check if HiveHistoryImpl class is returned when hive history is enabled
+   * @throws Exception
+   */
+  public void testHiveHistoryConfigEnabled() throws Exception {
+      HiveConf conf = new HiveConf(SessionState.class);
+      conf.setBoolVar(ConfVars.HIVE_SESSION_HISTORY_ENABLED, true);
+      SessionState ss = new CliSessionState(conf);
+      SessionState.start(ss);
+      HiveHistory hHistory = ss.getHiveHistory();
+      assertEquals("checking hive history class when history is enabled",
+          hHistory.getClass(), HiveHistoryImpl.class);
+  }
+  /**
+   * Check if HiveHistory class is a Proxy class when hive history is disabled
+   * @throws Exception
+   */
+  public void testHiveHistoryConfigDisabled() throws Exception {
+    HiveConf conf = new HiveConf(SessionState.class);
+    conf.setBoolVar(ConfVars.HIVE_SESSION_HISTORY_ENABLED, false);
+    SessionState ss = new CliSessionState(conf);
+    SessionState.start(ss);
+    HiveHistory hHistory = ss.getHiveHistory();
+    assertTrue("checking hive history class when history is disabled",
+        hHistory.getClass() != HiveHistoryImpl.class);
+    System.err.println("hHistory.getClass" + hHistory.getClass());
+    assertTrue("verifying proxy class is used when history is disabled",
+        Proxy.isProxyClass(hHistory.getClass()));
+
+  }
+
+
+
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java
new file mode 100644
index 0000000..3027ef4
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java
@@ -0,0 +1,57 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.metadata;
+
+import java.util.Map;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
+import org.apache.hadoop.hive.ql.session.SessionState;
+
+public class TestSemanticAnalyzerHookLoading extends TestCase {
+
+  public void testHookLoading() throws Exception{
+
+    HiveConf conf = new HiveConf(this.getClass());
+    conf.set(ConfVars.SEMANTIC_ANALYZER_HOOK.varname, DummySemanticAnalyzerHook.class.getName());
+    conf.set(ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+    SessionState.start(conf);
+    Driver driver = new Driver(conf);
+
+    driver.run("drop table testDL");
+    CommandProcessorResponse resp = driver.run("create table testDL (a int) as select * from tbl2");
+    assertEquals(40000, resp.getResponseCode());
+
+    resp = driver.run("create table testDL (a int)");
+    assertEquals(0, resp.getResponseCode());
+    assertNull(resp.getErrorMessage());
+
+    Map<String,String> params = Hive.get(conf).getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, "testDL").getParameters();
+
+    assertEquals(DummyCreateTableHook.class.getName(),params.get("createdBy"));
+    assertEquals("Open Source rocks!!", params.get("Message"));
+
+    driver.run("drop table testDL");
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestAuthorizationPreEventListener.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestAuthorizationPreEventListener.java
new file mode 100644
index 0000000..d2595dd
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestAuthorizationPreEventListener.java
@@ -0,0 +1,303 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.security;
+
+import java.io.IOException;
+import java.net.ServerSocket;
+import java.util.ArrayList;
+import java.util.List;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.AuthCallContext;
+import org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.shims.ShimLoader;
+
+/**
+ * TestAuthorizationPreEventListener. Test case for
+ * {@link org.apache.hadoop.hive.metastore.AuthorizationPreEventListener} and
+ * {@link org.apache.hadoop.hive.metastore.MetaStorePreEventListener}
+ */
+public class TestAuthorizationPreEventListener extends TestCase {
+  private HiveConf clientHiveConf;
+  private HiveMetaStoreClient msc;
+  private Driver driver;
+
+  @Override
+  protected void setUp() throws Exception {
+
+    super.setUp();
+
+    int port = MetaStoreUtils.findFreePort();
+
+    System.setProperty(HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS.varname,
+        AuthorizationPreEventListener.class.getName());
+    System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHORIZATION_MANAGER.varname,
+        DummyHiveMetastoreAuthorizationProvider.class.getName());
+    System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHENTICATOR_MANAGER.varname,
+        HadoopDefaultMetastoreAuthenticator.class.getName());
+
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+
+    clientHiveConf = new HiveConf(this.getClass());
+
+    clientHiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    clientHiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    clientHiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+
+    clientHiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    clientHiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+
+
+    SessionState.start(new CliSessionState(clientHiveConf));
+    msc = new HiveMetaStoreClient(clientHiveConf, null);
+    driver = new Driver(clientHiveConf);
+  }
+
+  private static String getFreeAvailablePort() throws IOException {
+    ServerSocket socket = new ServerSocket(0);
+    socket.setReuseAddress(true);
+    int port = socket.getLocalPort();
+    socket.close();
+    return "" + port;
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+  private void validateCreateDb(Database expectedDb, Database actualDb) {
+    assertEquals(expectedDb.getName(), actualDb.getName());
+    assertEquals(expectedDb.getLocationUri(), actualDb.getLocationUri());
+  }
+
+  private void validateTable(Table expectedTable, Table actualTable) {
+    assertEquals(expectedTable.getTableName(), actualTable.getTableName());
+    assertEquals(expectedTable.getDbName(), actualTable.getDbName());
+
+    // We won't try to be too strict in checking this because we're comparing
+    // table create intents with observed tables created.
+    // If it does have a location though, we will compare, as with external tables
+    if ((actualTable.getSd() != null) && (actualTable.getSd().getLocation() != null)){
+      assertEquals(expectedTable.getSd().getLocation(), actualTable.getSd().getLocation());
+    }
+  }
+
+  private void validateCreateTable(Table expectedTable, Table actualTable) {
+    validateTable(expectedTable, actualTable);
+  }
+
+  private void validateAddPartition(Partition expectedPartition, Partition actualPartition) {
+    validatePartition(expectedPartition,actualPartition);
+  }
+
+  private void validatePartition(Partition expectedPartition, Partition actualPartition) {
+    assertEquals(expectedPartition.getValues(),
+        actualPartition.getValues());
+    assertEquals(expectedPartition.getDbName(),
+        actualPartition.getDbName());
+    assertEquals(expectedPartition.getTableName(),
+        actualPartition.getTableName());
+
+    // assertEquals(expectedPartition.getSd().getLocation(),
+    //     actualPartition.getSd().getLocation());
+    // we don't compare locations, because the location can still be empty in
+    // the pre-event listener before it is created.
+
+    assertEquals(expectedPartition.getSd().getInputFormat(),
+        actualPartition.getSd().getInputFormat());
+    assertEquals(expectedPartition.getSd().getOutputFormat(),
+        actualPartition.getSd().getOutputFormat());
+    assertEquals(expectedPartition.getSd().getSerdeInfo(),
+        actualPartition.getSd().getSerdeInfo());
+
+  }
+
+  private void validateAlterPartition(Partition expectedOldPartition,
+      Partition expectedNewPartition, String actualOldPartitionDbName,
+      String actualOldPartitionTblName,List<String> actualOldPartitionValues,
+      Partition actualNewPartition) {
+    assertEquals(expectedOldPartition.getValues(), actualOldPartitionValues);
+    assertEquals(expectedOldPartition.getDbName(), actualOldPartitionDbName);
+    assertEquals(expectedOldPartition.getTableName(), actualOldPartitionTblName);
+
+    validatePartition(expectedNewPartition, actualNewPartition);
+  }
+
+  private void validateAlterTable(Table expectedOldTable, Table expectedNewTable,
+      Table actualOldTable, Table actualNewTable) {
+    validateTable(expectedOldTable, actualOldTable);
+    validateTable(expectedNewTable, actualNewTable);
+  }
+
+  private void validateDropPartition(Partition expectedPartition, Partition actualPartition) {
+    validatePartition(expectedPartition, actualPartition);
+  }
+
+  private void validateDropTable(Table expectedTable, Table actualTable) {
+    validateTable(expectedTable, actualTable);
+  }
+
+  private void validateDropDb(Database expectedDb, Database actualDb) {
+    assertEquals(expectedDb, actualDb);
+  }
+
+  public void testListener() throws Exception {
+    String dbName = "hive3705";
+    String tblName = "tmptbl";
+    String renamed = "tmptbl2";
+    int listSize = 0;
+
+    List<AuthCallContext> authCalls = DummyHiveMetastoreAuthorizationProvider.authCalls;
+    assertEquals(authCalls.size(),listSize);
+
+    driver.run("create database " + dbName);
+    listSize++;
+    Database db = msc.getDatabase(dbName);
+
+    Database dbFromEvent = (Database)assertAndExtractSingleObjectFromEvent(listSize, authCalls,
+        DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.DB);
+    validateCreateDb(db,dbFromEvent);
+
+    driver.run("use " + dbName);
+    driver.run(String.format("create table %s (a string) partitioned by (b string)", tblName));
+    listSize++;
+    Table tbl = msc.getTable(dbName, tblName);
+
+    Table tblFromEvent = (
+        (org.apache.hadoop.hive.ql.metadata.Table)
+        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
+            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.TABLE))
+            .getTTable();
+    validateCreateTable(tbl, tblFromEvent);
+
+    driver.run("alter table tmptbl add partition (b='2011')");
+    listSize++;
+    Partition part = msc.getPartition("hive3705", "tmptbl", "b=2011");
+
+    Partition ptnFromEvent = (
+        (org.apache.hadoop.hive.ql.metadata.Partition)
+        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
+            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION))
+            .getTPartition();
+    validateAddPartition(part,ptnFromEvent);
+
+    driver.run(String.format("alter table %s touch partition (%s)", tblName, "b='2011'"));
+    listSize++;
+
+    //the partition did not change,
+    // so the new partition should be similar to the original partition
+    Partition modifiedP = msc.getPartition(dbName, tblName, "b=2011");
+
+    Partition ptnFromEventAfterAlter = (
+        (org.apache.hadoop.hive.ql.metadata.Partition)
+        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
+            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION))
+            .getTPartition();
+
+    validateAlterPartition(part, modifiedP, ptnFromEventAfterAlter.getDbName(),
+        ptnFromEventAfterAlter.getTableName(), ptnFromEventAfterAlter.getValues(),
+        ptnFromEventAfterAlter);
+
+
+    List<String> part_vals = new ArrayList<String>();
+    part_vals.add("c=2012");
+    Partition newPart = msc.appendPartition(dbName, tblName, part_vals);
+
+    listSize++;
+
+    Partition newPtnFromEvent = (
+        (org.apache.hadoop.hive.ql.metadata.Partition)
+        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
+            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION))
+            .getTPartition();
+    validateAddPartition(newPart,newPtnFromEvent);
+
+
+    driver.run(String.format("alter table %s rename to %s", tblName, renamed));
+    listSize++;
+
+    Table renamedTable = msc.getTable(dbName, renamed);
+    Table renamedTableFromEvent = (
+        (org.apache.hadoop.hive.ql.metadata.Table)
+        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
+            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.TABLE))
+            .getTTable();
+
+    validateAlterTable(tbl, renamedTable, renamedTableFromEvent,
+        renamedTable);
+    assertFalse(tbl.getTableName().equals(renamedTable.getTableName()));
+
+
+    //change the table name back
+    driver.run(String.format("alter table %s rename to %s", renamed, tblName));
+    listSize++;
+
+    driver.run(String.format("alter table %s drop partition (b='2011')", tblName));
+    listSize++;
+
+    Partition ptnFromDropPartition = (
+        (org.apache.hadoop.hive.ql.metadata.Partition)
+        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
+            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION))
+            .getTPartition();
+
+    validateDropPartition(modifiedP, ptnFromDropPartition);
+
+    driver.run("drop table " + tblName);
+    listSize++;
+    Table tableFromDropTableEvent = (
+        (org.apache.hadoop.hive.ql.metadata.Table)
+        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
+            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.TABLE))
+            .getTTable();
+
+
+    validateDropTable(tbl, tableFromDropTableEvent);
+
+    driver.run("drop database " + dbName);
+    listSize++;
+    Database dbFromDropDatabaseEvent =
+        (Database)assertAndExtractSingleObjectFromEvent(listSize, authCalls,
+        DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.DB);
+
+    validateDropDb(db, dbFromDropDatabaseEvent);
+  }
+
+  public Object assertAndExtractSingleObjectFromEvent(int listSize,
+      List<AuthCallContext> authCalls,
+      DummyHiveMetastoreAuthorizationProvider.AuthCallContextType callType) {
+    assertEquals(listSize, authCalls.size());
+    assertEquals(1,authCalls.get(listSize-1).authObjects.size());
+
+    assertEquals(callType,authCalls.get(listSize-1).type);
+    return (authCalls.get(listSize-1).authObjects.get(0));
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestClientSideAuthorizationProvider.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestClientSideAuthorizationProvider.java
new file mode 100644
index 0000000..48e8b23
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestClientSideAuthorizationProvider.java
@@ -0,0 +1,214 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.security;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
+import org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.security.UserGroupInformation;
+
+/**
+ * TestClientSideAuthorizationProvider : Simple base test for client side
+ * Authorization Providers. By default, tests DefaultHiveAuthorizationProvider
+ */
+public class TestClientSideAuthorizationProvider extends TestCase {
+  protected HiveConf clientHiveConf;
+  protected HiveMetaStoreClient msc;
+  protected Driver driver;
+  protected UserGroupInformation ugi;
+
+
+  protected String getAuthorizationProvider(){
+    return DefaultHiveAuthorizationProvider.class.getName();
+  }
+
+
+  @Override
+  protected void setUp() throws Exception {
+
+    super.setUp();
+
+    int port = MetaStoreUtils.findFreePort();
+
+    // Turn off metastore-side authorization
+    System.setProperty(HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS.varname,
+        "");
+
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+
+    clientHiveConf = new HiveConf(this.getClass());
+
+    // Turn on client-side authorization
+    clientHiveConf.setBoolVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED,true);
+    clientHiveConf.set(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER.varname,
+        getAuthorizationProvider());
+    clientHiveConf.set(HiveConf.ConfVars.HIVE_AUTHENTICATOR_MANAGER.varname,
+        InjectableDummyAuthenticator.class.getName());
+    clientHiveConf.set(HiveConf.ConfVars.HIVE_AUTHORIZATION_TABLE_OWNER_GRANTS.varname, "");
+
+    clientHiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    clientHiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    clientHiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+
+    clientHiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    clientHiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+
+    ugi = ShimLoader.getHadoopShims().getUGIForConf(clientHiveConf);
+
+    SessionState.start(new CliSessionState(clientHiveConf));
+    msc = new HiveMetaStoreClient(clientHiveConf, null);
+    driver = new Driver(clientHiveConf);
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+  private void validateCreateDb(Database expectedDb, String dbName) {
+    assertEquals(expectedDb.getName().toLowerCase(), dbName.toLowerCase());
+  }
+
+  private void validateCreateTable(Table expectedTable, String tblName, String dbName) {
+    assertNotNull(expectedTable);
+    assertEquals(expectedTable.getTableName().toLowerCase(),tblName.toLowerCase());
+    assertEquals(expectedTable.getDbName().toLowerCase(),dbName.toLowerCase());
+  }
+
+  protected String getTestDbName(){
+    return "smp_cl_db";
+  }
+
+  protected String getTestTableName(){
+    return "smp_cl_tbl";
+  }
+
+  public void testSimplePrivileges() throws Exception {
+    String dbName = getTestDbName();
+    String tblName = getTestTableName();
+
+    String userName = ugi.getUserName();
+
+    CommandProcessorResponse ret = driver.run("create database " + dbName);
+    assertEquals(0,ret.getResponseCode());
+    Database db = msc.getDatabase(dbName);
+    String dbLocn = db.getLocationUri();
+
+    validateCreateDb(db,dbName);
+    disallowCreateInDb(dbName, userName, dbLocn);
+
+    driver.run("use " + dbName);
+    ret = driver.run(
+        String.format("create table %s (a string) partitioned by (b string)", tblName));
+
+    // failure from not having permissions to create table
+    assertNoPrivileges(ret);
+
+    allowCreateInDb(dbName, userName, dbLocn);
+
+    driver.run("use " + dbName);
+    ret = driver.run(
+        String.format("create table %s (a string) partitioned by (b string)", tblName));
+
+    assertEquals(0,ret.getResponseCode()); // now it succeeds.
+    Table tbl = msc.getTable(dbName, tblName);
+
+    validateCreateTable(tbl,tblName, dbName);
+
+    String fakeUser = "mal";
+    List<String> fakeGroupNames = new ArrayList<String>();
+    fakeGroupNames.add("groupygroup");
+
+    InjectableDummyAuthenticator.injectUserName(fakeUser);
+    InjectableDummyAuthenticator.injectGroupNames(fakeGroupNames);
+    InjectableDummyAuthenticator.injectMode(true);
+
+    ret = driver.run(
+        String.format("create table %s (a string) partitioned by (b string)", tblName+"mal"));
+
+    assertNoPrivileges(ret);
+
+    disallowCreateInTbl(tbl.getTableName(), userName, tbl.getSd().getLocation());
+    ret = driver.run("alter table "+tblName+" add partition (b='2011')");
+    assertNoPrivileges(ret);
+
+    InjectableDummyAuthenticator.injectMode(false);
+    allowCreateInTbl(tbl.getTableName(), userName, tbl.getSd().getLocation());
+
+    ret = driver.run("alter table "+tblName+" add partition (b='2011')");
+    assertEquals(0,ret.getResponseCode());
+
+    allowDropOnTable(tblName, userName, tbl.getSd().getLocation());
+    allowDropOnDb(dbName,userName,db.getLocationUri());
+    driver.run("drop database if exists "+getTestDbName()+" cascade");
+
+  }
+
+  protected void allowCreateInTbl(String tableName, String userName, String location)
+      throws Exception{
+    driver.run("grant create on table "+tableName+" to user "+userName);
+  }
+
+  protected void disallowCreateInTbl(String tableName, String userName, String location)
+      throws Exception {
+    // nothing needed here by default
+  }
+
+
+  protected void allowCreateInDb(String dbName, String userName, String location)
+      throws Exception {
+    driver.run("grant create on database "+dbName+" to user "+userName);
+  }
+
+  protected void disallowCreateInDb(String dbName, String userName, String location)
+      throws Exception {
+    // nothing needed here by default
+  }
+
+  protected void allowDropOnTable(String tblName, String userName, String location)
+      throws Exception {
+    driver.run("grant drop on table "+tblName+" to user "+userName);
+  }
+
+  protected void allowDropOnDb(String dbName, String userName, String location)
+      throws Exception {
+    driver.run("grant drop on database "+dbName+" to user "+userName);
+  }
+
+  protected void assertNoPrivileges(CommandProcessorResponse ret){
+    assertNotNull(ret);
+    assertFalse(0 == ret.getResponseCode());
+    assertTrue(ret.getErrorMessage().indexOf("No privilege") != -1);
+  }
+
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMetastoreAuthorizationProvider.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMetastoreAuthorizationProvider.java
new file mode 100644
index 0000000..f19e3df
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMetastoreAuthorizationProvider.java
@@ -0,0 +1,287 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.security;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.SerDeInfo;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
+import org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener;
+import org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.security.UserGroupInformation;
+
+/**
+ * TestHiveMetastoreAuthorizationProvider. Test case for
+ * HiveMetastoreAuthorizationProvider, and by default,
+ * for DefaultHiveMetaStoreAuthorizationProvider
+ * using {@link org.apache.hadoop.hive.metastore.AuthorizationPreEventListener}
+ * and {@link org.apache.hadoop.hive.}
+ *
+ * Note that while we do use the hive driver to test, that is mostly for test
+ * writing ease, and it has the same effect as using a metastore client directly
+ * because we disable hive client-side authorization for this test, and only
+ * turn on server-side auth.
+ *
+ * This test is also intended to be extended to provide tests for other
+ * authorization providers like StorageBasedAuthorizationProvider
+ */
+public class TestMetastoreAuthorizationProvider extends TestCase {
+  protected HiveConf clientHiveConf;
+  protected HiveMetaStoreClient msc;
+  protected Driver driver;
+  protected UserGroupInformation ugi;
+
+
+  protected String getAuthorizationProvider(){
+    return DefaultHiveMetastoreAuthorizationProvider.class.getName();
+  }
+
+
+  @Override
+  protected void setUp() throws Exception {
+
+    super.setUp();
+
+    int port = MetaStoreUtils.findFreePort();
+
+    // Turn on metastore-side authorization
+    System.setProperty(HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS.varname,
+        AuthorizationPreEventListener.class.getName());
+    System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHORIZATION_MANAGER.varname,
+        getAuthorizationProvider());
+    System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHENTICATOR_MANAGER.varname,
+        InjectableDummyAuthenticator.class.getName());
+    System.setProperty(HiveConf.ConfVars.HIVE_AUTHORIZATION_TABLE_OWNER_GRANTS.varname, "");
+
+
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+
+    clientHiveConf = new HiveConf(this.getClass());
+
+    // Turn off client-side authorization
+    clientHiveConf.setBoolVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED,false);
+
+    clientHiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    clientHiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
+    clientHiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+
+    clientHiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
+    clientHiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+
+    ugi = ShimLoader.getHadoopShims().getUGIForConf(clientHiveConf);
+
+    SessionState.start(new CliSessionState(clientHiveConf));
+    msc = new HiveMetaStoreClient(clientHiveConf, null);
+    driver = new Driver(clientHiveConf);
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+  private void validateCreateDb(Database expectedDb, String dbName) {
+    assertEquals(expectedDb.getName().toLowerCase(), dbName.toLowerCase());
+  }
+
+  private void validateCreateTable(Table expectedTable, String tblName, String dbName) {
+    assertNotNull(expectedTable);
+    assertEquals(expectedTable.getTableName().toLowerCase(),tblName.toLowerCase());
+    assertEquals(expectedTable.getDbName().toLowerCase(),dbName.toLowerCase());
+  }
+
+  protected String getTestDbName(){
+    return "smp_ms_db";
+  }
+
+  protected String getTestTableName(){
+    return "smp_ms_tbl";
+  }
+
+  public void testSimplePrivileges() throws Exception {
+    String dbName = getTestDbName();
+    String tblName = getTestTableName();
+    String userName = ugi.getUserName();
+
+    CommandProcessorResponse ret = driver.run("create database " + dbName);
+    assertEquals(0,ret.getResponseCode());
+    Database db = msc.getDatabase(dbName);
+    String dbLocn = db.getLocationUri();
+
+    validateCreateDb(db,dbName);
+    disallowCreateInDb(dbName, userName, dbLocn);
+
+    driver.run("use " + dbName);
+    ret = driver.run(
+        String.format("create table %s (a string) partitioned by (b string)", tblName));
+
+    assertEquals(1,ret.getResponseCode());
+    // failure from not having permissions to create table
+
+    ArrayList<FieldSchema> fields = new ArrayList<FieldSchema>(2);
+    fields.add(new FieldSchema("a", serdeConstants.STRING_TYPE_NAME, ""));
+
+    Table ttbl = new Table();
+    ttbl.setDbName(dbName);
+    ttbl.setTableName(tblName);
+    StorageDescriptor sd = new StorageDescriptor();
+    ttbl.setSd(sd);
+    sd.setCols(fields);
+    sd.setParameters(new HashMap<String, String>());
+    sd.getParameters().put("test_param_1", "Use this for comments etc");
+    sd.setSerdeInfo(new SerDeInfo());
+    sd.getSerdeInfo().setName(ttbl.getTableName());
+    sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+    sd.getSerdeInfo().getParameters().put(
+        org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
+    sd.getSerdeInfo().setSerializationLib(
+        org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
+    ttbl.setPartitionKeys(new ArrayList<FieldSchema>());
+
+    MetaException me = null;
+    try {
+      msc.createTable(ttbl);
+    } catch (MetaException e){
+      me = e;
+    }
+    assertNoPrivileges(me);
+
+    allowCreateInDb(dbName, userName, dbLocn);
+
+    driver.run("use " + dbName);
+    ret = driver.run(
+        String.format("create table %s (a string) partitioned by (b string)", tblName));
+
+    assertEquals(0,ret.getResponseCode()); // now it succeeds.
+    Table tbl = msc.getTable(dbName, tblName);
+
+    validateCreateTable(tbl,tblName, dbName);
+
+    String fakeUser = "mal";
+    List<String> fakeGroupNames = new ArrayList<String>();
+    fakeGroupNames.add("groupygroup");
+
+    InjectableDummyAuthenticator.injectUserName(fakeUser);
+    InjectableDummyAuthenticator.injectGroupNames(fakeGroupNames);
+    InjectableDummyAuthenticator.injectMode(true);
+
+    ret = driver.run(
+        String.format("create table %s (a string) partitioned by (b string)", tblName+"mal"));
+
+    assertEquals(1,ret.getResponseCode());
+
+    ttbl.setTableName(tblName+"mal");
+    me = null;
+    try {
+      msc.createTable(ttbl);
+    } catch (MetaException e){
+      me = e;
+    }
+    assertNoPrivileges(me);
+
+    disallowCreateInTbl(tbl.getTableName(), userName, tbl.getSd().getLocation());
+    ret = driver.run("alter table "+tblName+" add partition (b='2011')");
+    assertEquals(1,ret.getResponseCode());
+
+    List<String> ptnVals = new ArrayList<String>();
+    ptnVals.add("b=2011");
+    Partition tpart = new Partition();
+    tpart.setDbName(dbName);
+    tpart.setTableName(tblName);
+    tpart.setValues(ptnVals);
+    tpart.setParameters(new HashMap<String, String>());
+    tpart.setSd(tbl.getSd().deepCopy());
+    tpart.getSd().setSerdeInfo(tbl.getSd().getSerdeInfo().deepCopy());
+    tpart.getSd().setLocation(tbl.getSd().getLocation() + "/tpart");
+
+    me = null;
+    try {
+      msc.add_partition(tpart);
+    } catch (MetaException e){
+      me = e;
+    }
+    assertNoPrivileges(me);
+
+    InjectableDummyAuthenticator.injectMode(false);
+    allowCreateInTbl(tbl.getTableName(), userName, tbl.getSd().getLocation());
+
+    ret = driver.run("alter table "+tblName+" add partition (b='2011')");
+    assertEquals(0,ret.getResponseCode());
+
+    allowDropOnTable(tblName, userName, tbl.getSd().getLocation());
+    allowDropOnDb(dbName,userName,db.getLocationUri());
+    driver.run("drop database if exists "+getTestDbName()+" cascade");
+
+  }
+
+  protected void allowCreateInTbl(String tableName, String userName, String location)
+      throws Exception{
+    driver.run("grant create on table "+tableName+" to user "+userName);
+  }
+
+  protected void disallowCreateInTbl(String tableName, String userName, String location)
+      throws Exception {
+    driver.run("revoke create on table "+tableName+" from user "+userName);
+  }
+
+
+  protected void allowCreateInDb(String dbName, String userName, String location)
+      throws Exception {
+    driver.run("grant create on database "+dbName+" to user "+userName);
+  }
+
+  protected void disallowCreateInDb(String dbName, String userName, String location)
+      throws Exception {
+    driver.run("revoke create on database "+dbName+" from user "+userName);
+  }
+
+  protected void allowDropOnTable(String tblName, String userName, String location)
+      throws Exception {
+    driver.run("grant drop on table "+tblName+" to user "+userName);
+  }
+
+  protected void allowDropOnDb(String dbName, String userName, String location)
+      throws Exception {
+    driver.run("grant drop on database "+dbName+" to user "+userName);
+  }
+
+  protected void assertNoPrivileges(MetaException me){
+    assertNotNull(me);
+    assertTrue(me.getMessage().indexOf("No privilege") != -1);
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedClientSideAuthorizationProvider.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedClientSideAuthorizationProvider.java
new file mode 100644
index 0000000..61dc211
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedClientSideAuthorizationProvider.java
@@ -0,0 +1,102 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.security;
+
+import java.net.URI;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
+import org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
+
+/**
+ * TestStorageBasedClientSideAuthorizationProvider : Overrides
+ * TestClientSideAuthorizationProvider to test StorageBasedAuthorizationProvider
+ * on the client side.
+ */
+
+public class TestStorageBasedClientSideAuthorizationProvider extends
+    TestClientSideAuthorizationProvider {
+
+  @Override
+  protected String getAuthorizationProvider(){
+    return StorageBasedAuthorizationProvider.class.getName();
+  }
+
+  @Override
+  protected void allowCreateInDb(String dbName, String userName, String location)
+      throws Exception {
+    setPermissions(location,"-rwxr--r--");
+  }
+
+  @Override
+  protected void disallowCreateInDb(String dbName, String userName, String location)
+      throws Exception {
+    setPermissions(location,"-r--r--r--");
+  }
+
+  @Override
+  protected void allowCreateInTbl(String tableName, String userName, String location)
+      throws Exception{
+    setPermissions(location,"-rwxr--r--");
+  }
+
+
+  @Override
+  protected void disallowCreateInTbl(String tableName, String userName, String location)
+      throws Exception {
+    setPermissions(location,"-r--r--r--");
+  }
+
+  @Override
+  protected void allowDropOnTable(String tblName, String userName, String location)
+      throws Exception {
+    setPermissions(location,"-rwxr--r--");
+  }
+
+  @Override
+  protected void allowDropOnDb(String dbName, String userName, String location)
+      throws Exception {
+    setPermissions(location,"-rwxr--r--");
+  }
+
+  private void setPermissions(String locn, String permissions) throws Exception {
+    FileSystem fs = FileSystem.get(new URI(locn), clientHiveConf);
+    fs.setPermission(new Path(locn), FsPermission.valueOf(permissions));
+  }
+
+  @Override
+  protected void assertNoPrivileges(CommandProcessorResponse ret){
+    assertNotNull(ret);
+    assertFalse(0 == ret.getResponseCode());
+    assertTrue(ret.getErrorMessage().indexOf("not permitted") != -1);
+  }
+
+
+  @Override
+  protected String getTestDbName(){
+    return super.getTestDbName() + "_SBAP";
+  }
+
+  @Override
+  protected String getTestTableName(){
+    return super.getTestTableName() + "_SBAP";
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationProvider.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationProvider.java
new file mode 100644
index 0000000..223f155
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationProvider.java
@@ -0,0 +1,105 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.security;
+
+import java.net.URI;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
+
+/**
+ * TestStorageBasedMetastoreAuthorizationProvider. Test case for
+ * StorageBasedAuthorizationProvider, by overriding methods defined in
+ * TestMetastoreAuthorizationProvider
+ *
+ * Note that while we do use the hive driver to test, that is mostly for test
+ * writing ease, and it has the same effect as using a metastore client directly
+ * because we disable hive client-side authorization for this test, and only
+ * turn on server-side auth.
+ */
+public class TestStorageBasedMetastoreAuthorizationProvider extends
+    TestMetastoreAuthorizationProvider {
+
+  @Override
+  protected String getAuthorizationProvider(){
+    return StorageBasedAuthorizationProvider.class.getName();
+  }
+
+  @Override
+  protected void allowCreateInDb(String dbName, String userName, String location)
+      throws Exception {
+    setPermissions(location,"-rwxr--r--");
+  }
+
+  @Override
+  protected void disallowCreateInDb(String dbName, String userName, String location)
+      throws Exception {
+    setPermissions(location,"-r--r--r--");
+  }
+
+  @Override
+  protected void allowCreateInTbl(String tableName, String userName, String location)
+      throws Exception{
+    setPermissions(location,"-rwxr--r--");
+  }
+
+
+  @Override
+  protected void disallowCreateInTbl(String tableName, String userName, String location)
+      throws Exception {
+    setPermissions(location,"-r--r--r--");
+  }
+
+  @Override
+  protected void allowDropOnTable(String tblName, String userName, String location)
+      throws Exception {
+    setPermissions(location,"-rwxr--r--");
+  }
+
+  @Override
+  protected void allowDropOnDb(String dbName, String userName, String location)
+      throws Exception {
+    setPermissions(location,"-rwxr--r--");
+  }
+
+  private void setPermissions(String locn, String permissions) throws Exception {
+    FileSystem fs = FileSystem.get(new URI(locn), clientHiveConf);
+    fs.setPermission(new Path(locn), FsPermission.valueOf(permissions));
+  }
+
+  @Override
+  protected void assertNoPrivileges(MetaException me){
+    assertNotNull(me);
+    assertTrue(me.getMessage().indexOf("not permitted") != -1);
+  }
+
+  @Override
+  protected String getTestDbName(){
+    return super.getTestDbName() + "_SBAP";
+  }
+
+  @Override
+  protected String getTestTableName(){
+    return super.getTestTableName() + "_SBAP";
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/serde2/TestSerdeWithFieldComments.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/serde2/TestSerdeWithFieldComments.java
new file mode 100644
index 0000000..bb96a89
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/serde2/TestSerdeWithFieldComments.java
@@ -0,0 +1,72 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.serde2;
+
+import junit.framework.TestCase;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.StructField;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+public class TestSerdeWithFieldComments extends TestCase {
+
+  private StructField mockedStructField(String name, String oiTypeName,
+                                        String comment) {
+    StructField m = mock(StructField.class);
+    when(m.getFieldName()).thenReturn(name);
+
+    ObjectInspector oi = mock(ObjectInspector.class);
+    when(oi.getTypeName()).thenReturn(oiTypeName);
+
+    when(m.getFieldObjectInspector()).thenReturn(oi);
+    when(m.getFieldComment()).thenReturn(comment);
+
+    return m;
+  }
+
+  public void testFieldComments() throws MetaException, SerDeException {
+    StructObjectInspector mockSOI = mock(StructObjectInspector.class);
+    when(mockSOI.getCategory()).thenReturn(ObjectInspector.Category.STRUCT);
+    List fieldRefs = new ArrayList<StructField>();
+    // Add field with a comment...
+    fieldRefs.add(mockedStructField("first", "type name 1", "this is a comment"));
+    // ... and one without
+    fieldRefs.add(mockedStructField("second", "type name 2", null));
+
+    when(mockSOI.getAllStructFieldRefs()).thenReturn(fieldRefs);
+
+    Deserializer mockDe = mock(Deserializer.class);
+    when(mockDe.getObjectInspector()).thenReturn(mockSOI);
+    List<FieldSchema> result =
+        MetaStoreUtils.getFieldsFromDeserializer("testTable", mockDe);
+
+    assertEquals(2, result.size());
+    assertEquals("first", result.get(0).getName());
+    assertEquals("this is a comment", result.get(0).getComment());
+    assertEquals("second", result.get(1).getName());
+    assertEquals("from deserializer", result.get(1).getComment());
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/serde2/dynamic_type/TestDynamicSerDe.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/serde2/dynamic_type/TestDynamicSerDe.java
new file mode 100644
index 0000000..1e326ff
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/serde2/dynamic_type/TestDynamicSerDe.java
@@ -0,0 +1,861 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.serde2.dynamic_type;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+import java.util.Random;
+import java.util.Map.Entry;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol;
+import org.apache.hadoop.io.BytesWritable;
+
+/**
+ * TestDynamicSerDe.
+ *
+ */
+public class TestDynamicSerDe extends TestCase {
+
+  public static HashMap<String, String> makeHashMap(String... params) {
+    HashMap<String, String> r = new HashMap<String, String>();
+    for (int i = 0; i < params.length; i += 2) {
+      r.put(params[i], params[i + 1]);
+    }
+    return r;
+  }
+
+  public void testDynamicSerDe() throws Throwable {
+    try {
+
+      // Try to construct an object
+      ArrayList<String> bye = new ArrayList<String>();
+      bye.add("firstString");
+      bye.add("secondString");
+      HashMap<String, Integer> another = new HashMap<String, Integer>();
+      another.put("firstKey", 1);
+      another.put("secondKey", 2);
+      ArrayList<Object> struct = new ArrayList<Object>();
+      struct.add(Integer.valueOf(234));
+      struct.add(bye);
+      struct.add(another);
+      struct.add(Integer.valueOf(-234));
+      struct.add(Double.valueOf(1.0));
+      struct.add(Double.valueOf(-2.5));
+
+      // All protocols
+      ArrayList<String> protocols = new ArrayList<String>();
+      ArrayList<Boolean> isBinaries = new ArrayList<Boolean>();
+      ArrayList<HashMap<String, String>> additionalParams = new ArrayList<HashMap<String, String>>();
+
+      protocols
+          .add(org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.class
+          .getName());
+      isBinaries.add(true);
+      additionalParams.add(makeHashMap("serialization.sort.order", "++++++"));
+      protocols
+          .add(org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.class
+          .getName());
+      isBinaries.add(true);
+      additionalParams.add(makeHashMap("serialization.sort.order", "------"));
+
+      protocols.add(org.apache.thrift.protocol.TBinaryProtocol.class.getName());
+      isBinaries.add(true);
+      additionalParams.add(null);
+
+      protocols.add(org.apache.thrift.protocol.TJSONProtocol.class.getName());
+      isBinaries.add(false);
+      additionalParams.add(null);
+
+      // TSimpleJSONProtocol does not support deserialization.
+      // protocols.add(org.apache.thrift.protocol.TSimpleJSONProtocol.class.getName());
+      // isBinaries.add(false);
+      // additionalParams.add(null);
+
+      // TCTLSeparatedProtocol is not done yet.
+      protocols
+          .add(org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
+          .getName());
+      isBinaries.add(false);
+      additionalParams.add(null);
+
+      System.out.println("input struct = " + struct);
+
+      for (int pp = 0; pp < protocols.size(); pp++) {
+
+        String protocol = protocols.get(pp);
+        boolean isBinary = isBinaries.get(pp);
+
+        System.out.println("Testing protocol: " + protocol);
+        Properties schema = new Properties();
+        schema.setProperty(serdeConstants.SERIALIZATION_FORMAT, protocol);
+        schema.setProperty(
+            org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
+            "test");
+        schema
+            .setProperty(
+                serdeConstants.SERIALIZATION_DDL,
+                "struct test { i32 _hello, list<string> 2bye, map<string,i32> another, i32 nhello, double d, double nd}");
+        schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
+            .getClass().toString());
+        HashMap<String, String> p = additionalParams.get(pp);
+        if (p != null) {
+          for (Entry<String, String> e : p.entrySet()) {
+            schema.setProperty(e.getKey(), e.getValue());
+          }
+        }
+
+        DynamicSerDe serde = new DynamicSerDe();
+        serde.initialize(new Configuration(), schema);
+
+        // Try getObjectInspector
+        ObjectInspector oi = serde.getObjectInspector();
+        System.out.println("TypeName = " + oi.getTypeName());
+
+        // Try to serialize
+        BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
+        System.out.println("bytes =" + hexString(bytes));
+
+        if (!isBinary) {
+          System.out.println("bytes in text ="
+              + new String(bytes.get(), 0, bytes.getSize()));
+        }
+
+        // Try to deserialize
+        Object o = serde.deserialize(bytes);
+        System.out.println("o class = " + o.getClass());
+        List<?> olist = (List<?>) o;
+        System.out.println("o size = " + olist.size());
+        System.out.println("o[0] class = " + olist.get(0).getClass());
+        System.out.println("o[1] class = " + olist.get(1).getClass());
+        System.out.println("o[2] class = " + olist.get(2).getClass());
+        System.out.println("o = " + o);
+
+        assertEquals(struct, o);
+      }
+
+    } catch (Throwable e) {
+      e.printStackTrace();
+      throw e;
+    }
+
+  }
+
+  public String hexString(BytesWritable bytes) {
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < bytes.getSize(); i++) {
+      byte b = bytes.get()[i];
+      int v = (b < 0 ? 256 + b : b);
+      sb.append(String.format("x%02x", v));
+    }
+    return sb.toString();
+  }
+
+  private void testTBinarySortableProtocol(Object[] structs, String ddl,
+      boolean ascending) throws Throwable {
+    int fields = ((List) structs[structs.length - 1]).size();
+    String order = "";
+    for (int i = 0; i < fields; i++) {
+      order = order + (ascending ? "+" : "-");
+    }
+
+    Properties schema = new Properties();
+    schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
+        org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.class
+        .getName());
+    schema.setProperty(
+        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME, "test");
+    schema.setProperty(serdeConstants.SERIALIZATION_DDL, ddl);
+    schema.setProperty(serdeConstants.SERIALIZATION_LIB, DynamicSerDe.class
+        .getName());
+    schema.setProperty(serdeConstants.SERIALIZATION_SORT_ORDER, order);
+
+    DynamicSerDe serde = new DynamicSerDe();
+    serde.initialize(new Configuration(), schema);
+
+    ObjectInspector oi = serde.getObjectInspector();
+
+    // Try to serialize
+    BytesWritable bytes[] = new BytesWritable[structs.length];
+    for (int i = 0; i < structs.length; i++) {
+      bytes[i] = new BytesWritable();
+      BytesWritable s = (BytesWritable) serde.serialize(structs[i], oi);
+      bytes[i].set(s);
+      if (i > 0) {
+        int compareResult = bytes[i - 1].compareTo(bytes[i]);
+        if ((compareResult < 0 && !ascending)
+            || (compareResult > 0 && ascending)) {
+          System.out.println("Test failed in "
+              + (ascending ? "ascending" : "descending") + " order.");
+          System.out.println("serialized data of " + structs[i - 1] + " = "
+              + hexString(bytes[i - 1]));
+          System.out.println("serialized data of " + structs[i] + " = "
+              + hexString(bytes[i]));
+          fail("Sort order of serialized " + structs[i - 1] + " and "
+              + structs[i] + " are reversed!");
+        }
+      }
+    }
+
+    // Try to deserialize
+    Object[] deserialized = new Object[structs.length];
+    for (int i = 0; i < structs.length; i++) {
+      deserialized[i] = serde.deserialize(bytes[i]);
+      if (!structs[i].equals(deserialized[i])) {
+        System.out.println("structs[i] = " + structs[i]);
+        System.out.println("deserialized[i] = " + deserialized[i]);
+        System.out.println("serialized[i] = " + hexString(bytes[i]));
+        assertEquals(structs[i], deserialized[i]);
+      }
+    }
+  }
+
+  static int compare(Object a, Object b) {
+    if (a == null && b == null) {
+      return 0;
+    }
+    if (a == null) {
+      return -1;
+    }
+    if (b == null) {
+      return 1;
+    }
+    if (a instanceof List) {
+      List la = (List) a;
+      List lb = (List) b;
+      assert (la.size() == lb.size());
+      for (int i = 0; i < la.size(); i++) {
+        int r = compare(la.get(i), lb.get(i));
+        if (r != 0) {
+          return r;
+        }
+      }
+      return 0;
+    } else if (a instanceof Number) {
+      Number na = (Number) a;
+      Number nb = (Number) b;
+      if (na.doubleValue() < nb.doubleValue()) {
+        return -1;
+      }
+      if (na.doubleValue() > nb.doubleValue()) {
+        return 1;
+      }
+      return 0;
+    } else if (a instanceof String) {
+      String sa = (String) a;
+      String sb = (String) b;
+      return sa.compareTo(sb);
+    }
+    return 0;
+  }
+
+  private void sort(Object[] structs) {
+    for (int i = 0; i < structs.length; i++) {
+      for (int j = i + 1; j < structs.length; j++) {
+        if (compare(structs[i], structs[j]) > 0) {
+          Object t = structs[i];
+          structs[i] = structs[j];
+          structs[j] = t;
+        }
+      }
+    }
+  }
+
+  public void testTBinarySortableProtocol() throws Throwable {
+    try {
+
+      System.out.println("Beginning Test testTBinarySortableProtocol:");
+
+      int num = 100;
+      Random r = new Random(1234);
+      Object structs[] = new Object[num];
+      String ddl;
+
+      // Test double
+      for (int i = 0; i < num; i++) {
+        ArrayList<Object> struct = new ArrayList<Object>();
+        if (i == 0) {
+          struct.add(null);
+        } else {
+          struct.add(Double.valueOf((r.nextDouble() - 0.5) * 10));
+        }
+        structs[i] = struct;
+      }
+      sort(structs);
+      ddl = "struct test { double hello}";
+      System.out.println("Testing " + ddl);
+      testTBinarySortableProtocol(structs, ddl, true);
+      testTBinarySortableProtocol(structs, ddl, false);
+
+      // Test integer
+      for (int i = 0; i < num; i++) {
+        ArrayList<Object> struct = new ArrayList<Object>();
+        if (i == 0) {
+          struct.add(null);
+        } else {
+          struct.add((int) ((r.nextDouble() - 0.5) * 1.5 * Integer.MAX_VALUE));
+        }
+        structs[i] = struct;
+      }
+      sort(structs);
+      // Null should be smaller than any other value, so put a null at the front
+      // end
+      // to test whether that is held.
+      ((List) structs[0]).set(0, null);
+      ddl = "struct test { i32 hello}";
+      System.out.println("Testing " + ddl);
+      testTBinarySortableProtocol(structs, ddl, true);
+      testTBinarySortableProtocol(structs, ddl, false);
+
+      // Test long
+      for (int i = 0; i < num; i++) {
+        ArrayList<Object> struct = new ArrayList<Object>();
+        if (i == 0) {
+          struct.add(null);
+        } else {
+          struct.add((long) ((r.nextDouble() - 0.5) * 1.5 * Long.MAX_VALUE));
+        }
+        structs[i] = struct;
+      }
+      sort(structs);
+      // Null should be smaller than any other value, so put a null at the front
+      // end
+      // to test whether that is held.
+      ((List) structs[0]).set(0, null);
+      ddl = "struct test { i64 hello}";
+      System.out.println("Testing " + ddl);
+      testTBinarySortableProtocol(structs, ddl, true);
+      testTBinarySortableProtocol(structs, ddl, false);
+
+      // Test string
+      for (int i = 0; i < num; i++) {
+        ArrayList<Object> struct = new ArrayList<Object>();
+        if (i == 0) {
+          struct.add(null);
+        } else {
+          struct.add(String.valueOf((r.nextDouble() - 0.5) * 1000));
+        }
+        structs[i] = struct;
+      }
+      sort(structs);
+      // Null should be smaller than any other value, so put a null at the front
+      // end
+      // to test whether that is held.
+      ((List) structs[0]).set(0, null);
+      ddl = "struct test { string hello}";
+      System.out.println("Testing " + ddl);
+      testTBinarySortableProtocol(structs, ddl, true);
+      testTBinarySortableProtocol(structs, ddl, false);
+
+      // Test string + double
+      for (int i = 0; i < num; i++) {
+        ArrayList<Object> struct = new ArrayList<Object>();
+        if (i % 9 == 0) {
+          struct.add(null);
+        } else {
+          struct.add("str" + (i / 5));
+        }
+        if (i % 7 == 0) {
+          struct.add(null);
+        } else {
+          struct.add(Double.valueOf((r.nextDouble() - 0.5) * 10));
+        }
+        structs[i] = struct;
+      }
+      sort(structs);
+      // Null should be smaller than any other value, so put a null at the front
+      // end
+      // to test whether that is held.
+      ((List) structs[0]).set(0, null);
+      ddl = "struct test { string hello, double another}";
+      System.out.println("Testing " + ddl);
+      testTBinarySortableProtocol(structs, ddl, true);
+      testTBinarySortableProtocol(structs, ddl, false);
+
+      System.out.println("Test testTBinarySortableProtocol passed!");
+    } catch (Throwable e) {
+      e.printStackTrace();
+      throw e;
+    }
+  }
+
+  public void testConfigurableTCTLSeparated() throws Throwable {
+    try {
+
+      // Try to construct an object
+      ArrayList<String> bye = new ArrayList<String>();
+      bye.add("firstString");
+      bye.add("secondString");
+      LinkedHashMap<String, Integer> another = new LinkedHashMap<String, Integer>();
+      another.put("firstKey", 1);
+      another.put("secondKey", 2);
+      ArrayList<Object> struct = new ArrayList<Object>();
+      struct.add(Integer.valueOf(234));
+      struct.add(bye);
+      struct.add(another);
+
+      Properties schema = new Properties();
+      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
+          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
+          .getName());
+      schema.setProperty(
+          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
+          "test");
+      schema
+          .setProperty(serdeConstants.SERIALIZATION_DDL,
+          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
+      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
+          .getClass().toString());
+
+      schema.setProperty(serdeConstants.FIELD_DELIM, "9");
+      schema.setProperty(serdeConstants.COLLECTION_DELIM, "1");
+      schema.setProperty(serdeConstants.LINE_DELIM, "2");
+      schema.setProperty(serdeConstants.MAPKEY_DELIM, "4");
+
+      DynamicSerDe serde = new DynamicSerDe();
+      serde.initialize(new Configuration(), schema);
+
+      TCTLSeparatedProtocol prot = (TCTLSeparatedProtocol) serde.oprot_;
+      assertTrue(prot.getPrimarySeparator().equals("\u0009"));
+
+      ObjectInspector oi = serde.getObjectInspector();
+
+      // Try to serialize
+      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
+
+      hexString(bytes);
+
+      String compare = "234" + "\u0009" + "firstString" + "\u0001"
+          + "secondString" + "\u0009" + "firstKey" + "\u0004" + "1" + "\u0001"
+          + "secondKey" + "\u0004" + "2";
+
+      System.out.println("bytes in text ="
+          + new String(bytes.get(), 0, bytes.getSize()) + ">");
+      System.out.println("compare to    =" + compare + ">");
+
+      assertTrue(compare.equals(new String(bytes.get(), 0, bytes.getSize())));
+
+      // Try to deserialize
+      Object o = serde.deserialize(bytes);
+      System.out.println("o class = " + o.getClass());
+      List<?> olist = (List<?>) o;
+      System.out.println("o size = " + olist.size());
+      System.out.println("o[0] class = " + olist.get(0).getClass());
+      System.out.println("o[1] class = " + olist.get(1).getClass());
+      System.out.println("o[2] class = " + olist.get(2).getClass());
+      System.out.println("o = " + o);
+
+      assertEquals(o, struct);
+
+    } catch (Throwable e) {
+      e.printStackTrace();
+      throw e;
+    }
+
+  }
+
+  /**
+   * Tests a single null list within a struct with return nulls on.
+   */
+
+  public void testNulls1() throws Throwable {
+    try {
+
+      // Try to construct an object
+      ArrayList<String> bye = null;
+      HashMap<String, Integer> another = new HashMap<String, Integer>();
+      another.put("firstKey", 1);
+      another.put("secondKey", 2);
+      ArrayList<Object> struct = new ArrayList<Object>();
+      struct.add(Integer.valueOf(234));
+      struct.add(bye);
+      struct.add(another);
+
+      Properties schema = new Properties();
+      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
+          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
+          .getName());
+      schema.setProperty(
+          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
+          "test");
+      schema
+          .setProperty(serdeConstants.SERIALIZATION_DDL,
+          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
+      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
+          .getClass().toString());
+      schema.setProperty(TCTLSeparatedProtocol.ReturnNullsKey, "true");
+
+      DynamicSerDe serde = new DynamicSerDe();
+      serde.initialize(new Configuration(), schema);
+
+      ObjectInspector oi = serde.getObjectInspector();
+
+      // Try to serialize
+      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
+
+      hexString(bytes);
+
+      // Try to deserialize
+      Object o = serde.deserialize(bytes);
+      assertEquals(struct, o);
+
+    } catch (Throwable e) {
+      e.printStackTrace();
+      throw e;
+    }
+
+  }
+
+  /**
+   * Tests all elements of a struct being null with return nulls on.
+   */
+
+  public void testNulls2() throws Throwable {
+    try {
+
+      // Try to construct an object
+      ArrayList<String> bye = null;
+      HashMap<String, Integer> another = null;
+      ArrayList<Object> struct = new ArrayList<Object>();
+      struct.add(null);
+      struct.add(bye);
+      struct.add(another);
+
+      Properties schema = new Properties();
+      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
+          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
+          .getName());
+      schema.setProperty(
+          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
+          "test");
+      schema
+          .setProperty(serdeConstants.SERIALIZATION_DDL,
+          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
+      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
+          .getClass().toString());
+      schema.setProperty(TCTLSeparatedProtocol.ReturnNullsKey, "true");
+
+      DynamicSerDe serde = new DynamicSerDe();
+      serde.initialize(new Configuration(), schema);
+
+      ObjectInspector oi = serde.getObjectInspector();
+
+      // Try to serialize
+      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
+
+      hexString(bytes);
+
+      // Try to deserialize
+      Object o = serde.deserialize(bytes);
+      List<?> olist = (List<?>) o;
+
+      assertTrue(olist.size() == 3);
+      assertEquals(null, olist.get(0));
+      assertEquals(null, olist.get(1));
+      assertEquals(null, olist.get(2));
+
+      // assertEquals(o, struct); Cannot do this because types of null lists are
+      // wrong.
+
+    } catch (Throwable e) {
+      e.printStackTrace();
+      throw e;
+    }
+
+  }
+
+  /**
+   * Tests map and list being empty with return nulls on.
+   */
+
+  public void testNulls3() throws Throwable {
+    try {
+
+      // Try to construct an object
+      ArrayList<String> bye = new ArrayList<String>();
+      HashMap<String, Integer> another = null;
+      ArrayList<Object> struct = new ArrayList<Object>();
+      struct.add(null);
+      struct.add(bye);
+      struct.add(another);
+
+      Properties schema = new Properties();
+      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
+          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
+          .getName());
+      schema.setProperty(
+          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
+          "test");
+      schema
+          .setProperty(serdeConstants.SERIALIZATION_DDL,
+          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
+      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
+          .getClass().toString());
+
+      schema.setProperty(TCTLSeparatedProtocol.ReturnNullsKey, "true");
+      DynamicSerDe serde = new DynamicSerDe();
+      serde.initialize(new Configuration(), schema);
+
+      ObjectInspector oi = serde.getObjectInspector();
+
+      // Try to serialize
+      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
+
+      hexString(bytes);
+
+      // Try to deserialize
+      Object o = serde.deserialize(bytes);
+      List<?> olist = (List<?>) o;
+
+      assertTrue(olist.size() == 3);
+      assertEquals(null, olist.get(0));
+      assertEquals(0, ((List<?>) olist.get(1)).size());
+      assertEquals(null, olist.get(2));
+
+      // assertEquals(o, struct); Cannot do this because types of null lists are
+      // wrong.
+
+    } catch (Throwable e) {
+      e.printStackTrace();
+      throw e;
+    }
+
+  }
+
+  /**
+   * Tests map and list null/empty with return nulls *off*.
+   */
+
+  public void testNulls4() throws Throwable {
+    try {
+
+      // Try to construct an object
+      ArrayList<String> bye = new ArrayList<String>();
+      HashMap<String, Integer> another = null;
+      ArrayList<Object> struct = new ArrayList<Object>();
+      struct.add(null);
+      struct.add(bye);
+      struct.add(another);
+
+      Properties schema = new Properties();
+      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
+          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
+          .getName());
+      schema.setProperty(
+          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
+          "test");
+      schema
+          .setProperty(serdeConstants.SERIALIZATION_DDL,
+          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
+      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
+          .getClass().toString());
+
+      schema.setProperty(TCTLSeparatedProtocol.ReturnNullsKey, "false");
+      DynamicSerDe serde = new DynamicSerDe();
+      serde.initialize(new Configuration(), schema);
+
+      ObjectInspector oi = serde.getObjectInspector();
+
+      // Try to serialize
+      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
+
+      hexString(bytes);
+
+      // Try to deserialize
+      Object o = serde.deserialize(bytes);
+      List<?> olist = (List<?>) o;
+
+      assertTrue(olist.size() == 3);
+      assertEquals(new Integer(0), (Integer) olist.get(0));
+      List<?> num1 = (List<?>) olist.get(1);
+      assertTrue(num1.size() == 0);
+      Map<?, ?> num2 = (Map<?, ?>) olist.get(2);
+      assertTrue(num2.size() == 0);
+
+      // assertEquals(o, struct); Cannot do this because types of null lists are
+      // wrong.
+
+    } catch (Throwable e) {
+      e.printStackTrace();
+      throw e;
+    }
+
+  }
+
+  /**
+   * Tests map and list null/empty with return nulls *off*.
+   */
+
+  public void testStructsinStructs() throws Throwable {
+    try {
+
+      Properties schema = new Properties();
+      // schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
+      // org.apache.thrift.protocol.TJSONProtocol.class.getName());
+      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
+          org.apache.thrift.protocol.TBinaryProtocol.class.getName());
+      schema.setProperty(
+          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
+          "test");
+      schema.setProperty(
+          serdeConstants.SERIALIZATION_DDL,
+      "struct inner { i32 field1, string field2 },struct  test {inner foo,  i32 hello, list<string> bye, map<string,i32> another}");
+      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
+          .getClass().toString());
+
+      //
+      // construct object of above type
+      //
+
+      // construct the inner struct
+      ArrayList<Object> innerStruct = new ArrayList<Object>();
+      innerStruct.add(new Integer(22));
+      innerStruct.add(new String("hello world"));
+
+      // construct outer struct
+      ArrayList<String> bye = new ArrayList<String>();
+      bye.add("firstString");
+      bye.add("secondString");
+      HashMap<String, Integer> another = new HashMap<String, Integer>();
+      another.put("firstKey", 1);
+      another.put("secondKey", 2);
+
+      ArrayList<Object> struct = new ArrayList<Object>();
+
+      struct.add(innerStruct);
+      struct.add(Integer.valueOf(234));
+      struct.add(bye);
+      struct.add(another);
+
+      DynamicSerDe serde = new DynamicSerDe();
+      serde.initialize(new Configuration(), schema);
+
+      ObjectInspector oi = serde.getObjectInspector();
+
+      // Try to serialize
+      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
+
+      // Try to deserialize
+      Object o = serde.deserialize(bytes);
+      List<?> olist = (List<?>) o;
+
+      assertEquals(4, olist.size());
+      assertEquals(innerStruct, olist.get(0));
+      assertEquals(new Integer(234), olist.get(1));
+      assertEquals(bye, olist.get(2));
+      assertEquals(another, olist.get(3));
+
+    } catch (Throwable e) {
+      e.printStackTrace();
+      throw e;
+    }
+
+  }
+
+  public void testSkip() throws Throwable {
+    try {
+
+      // Try to construct an object
+      ArrayList<String> bye = new ArrayList<String>();
+      bye.add("firstString");
+      bye.add("secondString");
+      LinkedHashMap<String, Integer> another = new LinkedHashMap<String, Integer>();
+      another.put("firstKey", 1);
+      another.put("secondKey", 2);
+      ArrayList<Object> struct = new ArrayList<Object>();
+      struct.add(Integer.valueOf(234));
+      struct.add(bye);
+      struct.add(another);
+
+      Properties schema = new Properties();
+      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
+          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
+          .getName());
+      schema.setProperty(
+          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
+          "test");
+      schema
+          .setProperty(serdeConstants.SERIALIZATION_DDL,
+          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
+      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
+          .getClass().toString());
+
+      schema.setProperty(serdeConstants.FIELD_DELIM, "9");
+      schema.setProperty(serdeConstants.COLLECTION_DELIM, "1");
+      schema.setProperty(serdeConstants.LINE_DELIM, "2");
+      schema.setProperty(serdeConstants.MAPKEY_DELIM, "4");
+
+      DynamicSerDe serde = new DynamicSerDe();
+      serde.initialize(new Configuration(), schema);
+
+      TCTLSeparatedProtocol prot = (TCTLSeparatedProtocol) serde.oprot_;
+      assertTrue(prot.getPrimarySeparator().equals("\u0009"));
+
+      ObjectInspector oi = serde.getObjectInspector();
+
+      // Try to serialize
+      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
+
+      hexString(bytes);
+
+      String compare = "234" + "\u0009" + "firstString" + "\u0001"
+          + "secondString" + "\u0009" + "firstKey" + "\u0004" + "1" + "\u0001"
+          + "secondKey" + "\u0004" + "2";
+
+      System.out.println("bytes in text ="
+          + new String(bytes.get(), 0, bytes.getSize()) + ">");
+      System.out.println("compare to    =" + compare + ">");
+
+      assertTrue(compare.equals(new String(bytes.get(), 0, bytes.getSize())));
+
+      schema
+          .setProperty(serdeConstants.SERIALIZATION_DDL,
+          "struct test { i32 hello, skip list<string> bye, map<string,i32> another}");
+
+      serde.initialize(new Configuration(), schema);
+
+      // Try to deserialize
+      Object o = serde.deserialize(bytes);
+      System.out.println("o class = " + o.getClass());
+      List<?> olist = (List<?>) o;
+      System.out.println("o size = " + olist.size());
+      System.out.println("o = " + o);
+
+      assertEquals(null, olist.get(1));
+
+      // set the skipped field to null
+      struct.set(1, null);
+
+      assertEquals(o, struct);
+
+    } catch (Throwable e) {
+      e.printStackTrace();
+      throw e;
+    }
+
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/service/TestHiveServer.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/service/TestHiveServer.java
new file mode 100644
index 0000000..6188b19
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/service/TestHiveServer.java
@@ -0,0 +1,424 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.service;
+
+import java.util.List;
+import java.util.Properties;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.ServerUtils;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.Schema;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.protocol.TProtocol;
+import org.apache.thrift.transport.TSocket;
+import org.apache.thrift.transport.TTransport;
+
+/**
+ * TestHiveServer.
+ *
+ */
+public class TestHiveServer extends TestCase {
+
+  private HiveInterface client;
+  private static final String host = "localhost";
+  private static final int port = 10000;
+  private final Path dataFilePath;
+
+  private static String tableName = "testhivedrivertable";
+  private final HiveConf conf;
+  private boolean standAloneServer = false;
+  private TTransport transport;
+  private final String invalidPath;
+
+  public TestHiveServer(String name) {
+    super(name);
+    conf = new HiveConf(TestHiveServer.class);
+    String dataFileDir = conf.get("test.data.files").replace('\\', '/')
+        .replace("c:", "");
+    invalidPath = dataFileDir+"/invalidpath/";
+    dataFilePath = new Path(dataFileDir, "kv1.txt");
+    // See data/conf/hive-site.xml
+    String paramStr = System.getProperty("test.service.standalone.server");
+    if (paramStr != null && paramStr.equals("true")) {
+      standAloneServer = true;
+    }
+  }
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+
+    if (standAloneServer) {
+      try {
+        transport = new TSocket(host, port);
+        TProtocol protocol = new TBinaryProtocol(transport);
+        client = new HiveClient(protocol);
+        transport.open();
+      } catch (Throwable e) {
+        e.printStackTrace();
+      }
+    } else {
+      client = new HiveServer.HiveServerHandler();
+    }
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    super.tearDown();
+    if (standAloneServer) {
+      try {
+        client.clean();
+      } catch (Exception e) {
+        e.printStackTrace();
+      }
+      transport.close();
+    }
+  }
+
+  public void testExecute() throws Exception {
+    try {
+      client.execute("set hive.support.concurrency = false");
+      client.execute("drop table " + tableName);
+    } catch (Exception ex) {
+    }
+
+    try {
+      client.execute("create table " + tableName + " (num int)");
+      client.execute("load data local inpath '" + dataFilePath.toString()
+          + "' into table " + tableName);
+      client.execute("select count(1) as cnt from " + tableName);
+      String row = client.fetchOne();
+      assertEquals(row, "500");
+
+      Schema hiveSchema = client.getSchema();
+      List<FieldSchema> listFields = hiveSchema.getFieldSchemas();
+      assertEquals(listFields.size(), 1);
+      assertEquals(listFields.get(0).getName(), "cnt");
+      assertEquals(listFields.get(0).getType(), "bigint");
+
+      Schema thriftSchema = client.getThriftSchema();
+      List<FieldSchema> listThriftFields = thriftSchema.getFieldSchemas();
+      assertEquals(listThriftFields.size(), 1);
+      assertEquals(listThriftFields.get(0).getName(), "cnt");
+      assertEquals(listThriftFields.get(0).getType(), "i64");
+
+      client.execute("drop table " + tableName);
+    } catch (Throwable t) {
+      t.printStackTrace();
+    }
+  }
+
+  public void notestExecute() throws Exception {
+    try {
+      client.execute("set hive.support.concurrency = false");
+      client.execute("drop table " + tableName);
+    } catch (Exception ex) {
+    }
+
+    client.execute("create table " + tableName + " (num int)");
+    client.execute("load data local inpath '" + dataFilePath.toString()
+        + "' into table " + tableName);
+    client.execute("select count(1) from " + tableName);
+    String row = client.fetchOne();
+    assertEquals(row, "500");
+    client.execute("drop table " + tableName);
+    transport.close();
+  }
+
+  public void testNonHiveCommand() throws Exception {
+    try {
+      client.execute("set hive.support.concurrency = false");
+      client.execute("drop table " + tableName);
+    } catch (Exception ex) {
+    }
+
+    client.execute("create table " + tableName + " (num int)");
+    client.execute("load data local inpath '" + dataFilePath.toString()
+        + "' into table " + tableName);
+
+    // Command not part of HiveQL - verify no results
+    client.execute("SET hive.mapred.mode = nonstrict");
+
+    Schema schema = client.getSchema();
+    assertEquals(schema.getFieldSchemasSize(), 0);
+    assertEquals(schema.getPropertiesSize(), 0);
+
+    Schema thriftschema = client.getThriftSchema();
+    assertEquals(thriftschema.getFieldSchemasSize(), 0);
+    assertEquals(thriftschema.getPropertiesSize(), 0);
+
+    try {
+      String ret = client.fetchOne();
+      assertTrue(false);
+    } catch (HiveServerException e) {
+      assertEquals(e.getErrorCode(), 0);
+    }
+    assertEquals(client.fetchN(10).size(), 0);
+    assertEquals(client.fetchAll().size(), 0);
+
+    // Execute Hive query and fetch
+    client.execute("select * from " + tableName + " limit 10");
+    client.fetchOne();
+
+    // Re-execute command not part of HiveQL - verify still no results
+    client.execute("SET hive.mapred.mode = nonstrict");
+
+    schema = client.getSchema();
+    assertEquals(schema.getFieldSchemasSize(), 0);
+    assertEquals(schema.getPropertiesSize(), 0);
+
+    thriftschema = client.getThriftSchema();
+    assertEquals(thriftschema.getFieldSchemasSize(), 0);
+    assertEquals(thriftschema.getPropertiesSize(), 0);
+
+    try {
+      String ret = client.fetchOne();
+      assertTrue(false);
+    } catch (HiveServerException e) {
+      assertEquals(e.getErrorCode(), 0);
+    }
+    assertEquals(client.fetchN(10).size(), 0);
+    assertEquals(client.fetchAll().size(), 0);
+
+    // Cleanup
+    client.execute("drop table " + tableName);
+  }
+
+  /**
+   * Test metastore call.
+   */
+  public void testMetastore() throws Exception {
+    try {
+      client.execute("set hive.support.concurrency = false");
+      client.execute("drop table " + tableName);
+    } catch (Exception ex) {
+    }
+
+    client.execute("create table " + tableName + " (num int)");
+    List<String> tabs = client.get_tables("default", tableName);
+    assertEquals(tabs.get(0), tableName);
+    client.execute("drop table " + tableName);
+  }
+
+  /**
+   * Test cluster status retrieval.
+   */
+  public void testGetClusterStatus() throws Exception {
+    HiveClusterStatus clusterStatus = client.getClusterStatus();
+    assertNotNull(clusterStatus);
+    assertTrue(clusterStatus.getTaskTrackers() >= 0);
+    assertTrue(clusterStatus.getMapTasks() >= 0);
+    assertTrue(clusterStatus.getReduceTasks() >= 0);
+    assertTrue(clusterStatus.getMaxMapTasks() >= 0);
+    assertTrue(clusterStatus.getMaxReduceTasks() >= 0);
+    assertTrue(clusterStatus.getState() == JobTrackerState.INITIALIZING
+        || clusterStatus.getState() == JobTrackerState.RUNNING);
+  }
+
+  /**
+   *
+   */
+  public void testFetch() throws Exception {
+    // create and populate a table with 500 rows.
+    try {
+      client.execute("set hive.support.concurrency = false");
+      client.execute("drop table " + tableName);
+    } catch (Exception ex) {
+    }
+    client.execute("create table " + tableName + " (key int, value string)");
+    client.execute("load data local inpath '" + dataFilePath.toString()
+        + "' into table " + tableName);
+
+    try {
+      // fetchAll test
+      client.execute("select key, value from " + tableName);
+      assertEquals(client.fetchAll().size(), 500);
+      assertEquals(client.fetchAll().size(), 0);
+
+      // fetchOne test
+      client.execute("select key, value from " + tableName);
+      for (int i = 0; i < 500; i++) {
+        try {
+          String str = client.fetchOne();
+        } catch (HiveServerException e) {
+          assertTrue(false);
+        }
+      }
+      try {
+        client.fetchOne();
+      } catch (HiveServerException e) {
+        assertEquals(e.getErrorCode(), 0);
+      }
+
+      // fetchN test
+      client.execute("select key, value from " + tableName);
+      assertEquals(client.fetchN(499).size(), 499);
+      assertEquals(client.fetchN(499).size(), 1);
+      assertEquals(client.fetchN(499).size(), 0);
+    } catch (Throwable e) {
+      e.printStackTrace();
+    }
+  }
+
+  public void testDynamicSerde() throws Exception {
+    try {
+      client.execute("set hive.support.concurrency = false");
+      client.execute("drop table " + tableName);
+    } catch (Exception ex) {
+    }
+
+    client.execute("create table " + tableName + " (key int, value string)");
+    client.execute("load data local inpath '" + dataFilePath.toString()
+        + "' into table " + tableName);
+    // client.execute("select key, count(1) from " + tableName +
+    // " where key > 10 group by key");
+    String sql = "select key, value from " + tableName + " where key > 10";
+    client.execute(sql);
+
+    // Instantiate DynamicSerDe
+    DynamicSerDe ds = new DynamicSerDe();
+    Properties dsp = new Properties();
+    dsp.setProperty(serdeConstants.SERIALIZATION_FORMAT,
+        org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
+            .getName());
+    dsp.setProperty(
+        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
+        "result");
+    String serDDL = new String("struct result { ");
+    List<FieldSchema> schema = client.getThriftSchema().getFieldSchemas();
+    for (int pos = 0; pos < schema.size(); pos++) {
+      if (pos != 0) {
+        serDDL = serDDL.concat(",");
+      }
+      serDDL = serDDL.concat(schema.get(pos).getType());
+      serDDL = serDDL.concat(" ");
+      serDDL = serDDL.concat(schema.get(pos).getName());
+    }
+    serDDL = serDDL.concat("}");
+
+    dsp.setProperty(serdeConstants.SERIALIZATION_DDL, serDDL);
+    dsp.setProperty(serdeConstants.SERIALIZATION_LIB, ds.getClass().toString());
+    dsp.setProperty(serdeConstants.FIELD_DELIM, "9");
+    ds.initialize(new Configuration(), dsp);
+
+    String row = client.fetchOne();
+    Object o = ds.deserialize(new BytesWritable(row.getBytes()));
+
+    assertEquals(o.getClass().toString(), "class java.util.ArrayList");
+    List<?> lst = (List<?>) o;
+    assertEquals(lst.get(0), 238);
+
+    // TODO: serde doesn't like underscore -- struct result { string _c0}
+    sql = "select count(1) as c from " + tableName;
+    client.execute(sql);
+    row = client.fetchOne();
+
+    serDDL = new String("struct result { ");
+    schema = client.getThriftSchema().getFieldSchemas();
+    for (int pos = 0; pos < schema.size(); pos++) {
+      if (pos != 0) {
+        serDDL = serDDL.concat(",");
+      }
+      serDDL = serDDL.concat(schema.get(pos).getType());
+      serDDL = serDDL.concat(" ");
+      serDDL = serDDL.concat(schema.get(pos).getName());
+    }
+    serDDL = serDDL.concat("}");
+
+    dsp.setProperty(serdeConstants.SERIALIZATION_DDL, serDDL);
+    // Need a new DynamicSerDe instance - re-initialization is not supported.
+    ds = new DynamicSerDe();
+    ds.initialize(new Configuration(), dsp);
+    o = ds.deserialize(new BytesWritable(row.getBytes()));
+  }
+
+  public void testAddJarShouldFailIfJarNotExist() throws Exception {
+    boolean queryExecutionFailed = false;
+    try {
+      client.execute("add jar " + invalidPath + "sample.jar");
+    } catch (Exception e) {
+      queryExecutionFailed = true;
+    }
+    if (!queryExecutionFailed) {
+      fail("It should throw exception since jar does not exist");
+    }
+  }
+
+  public void testAddFileShouldFailIfFileNotExist() throws Exception {
+    boolean queryExecutionFailed = false;
+    try {
+      client.execute("add file " + invalidPath + "sample.txt");
+    } catch (Exception e) {
+      queryExecutionFailed = true;
+    }
+    if (!queryExecutionFailed) {
+      fail("It should throw exception since file does not exist");
+    }
+  }
+
+  public void testAddArchiveShouldFailIfFileNotExist() throws Exception {
+    boolean queryExecutionFailed = false;
+    try {
+      client.execute("add archive " + invalidPath + "sample.zip");
+    } catch (Exception e) {
+      queryExecutionFailed = true;
+    }
+    if (!queryExecutionFailed) {
+      fail("It should trow exception since archive does not exist");
+    }
+  }
+
+  public void testScratchDirShouldNotClearWhileStartup() throws Exception {
+    FileSystem fs = FileSystem.get(conf);
+    Path scratchDirPath = new Path(HiveConf.getVar(conf,
+        HiveConf.ConfVars.SCRATCHDIR));
+    boolean fileExists = fs.exists(scratchDirPath);
+    if (!fileExists) {
+      fileExists = fs.mkdirs(scratchDirPath);
+    }
+    ServerUtils.cleanUpScratchDir(conf);
+    assertTrue("Scratch dir is not available after startup", fs.exists(scratchDirPath));
+  }
+
+  public void testScratchDirShouldClearWhileStartup() throws Exception {
+    FileSystem fs = FileSystem.get(conf);
+    Path scratchDirPath = new Path(HiveConf.getVar(conf,
+        HiveConf.ConfVars.SCRATCHDIR));
+    boolean fileExists = fs.exists(scratchDirPath);
+    if (!fileExists) {
+      fileExists = fs.mkdirs(scratchDirPath);
+    }
+    try {
+      conf.setBoolVar(HiveConf.ConfVars.HIVE_START_CLEANUP_SCRATCHDIR, true);
+      ServerUtils.cleanUpScratchDir(conf);
+    } finally {
+      conf.setBoolVar(HiveConf.ConfVars.HIVE_START_CLEANUP_SCRATCHDIR, false);
+    }
+    assertFalse("Scratch dir is available after startup", fs.exists(scratchDirPath));
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestDBTokenStore.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestDBTokenStore.java
new file mode 100644
index 0000000..a60ad70
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestDBTokenStore.java
@@ -0,0 +1,94 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import java.io.IOException;
+import java.util.List;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.thrift.DelegationTokenStore.TokenStoreException;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
+import org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport;
+import org.junit.Assert;
+
+public class TestDBTokenStore extends TestCase{
+
+  public void testDBTokenStore() throws TokenStoreException, MetaException, IOException {
+
+    DelegationTokenStore ts = new DBTokenStore();
+    ts.setStore(new HMSHandler("Test handler"));
+    assertEquals(0, ts.getMasterKeys().length);
+    assertEquals(false,ts.removeMasterKey(-1));
+    try{
+      ts.updateMasterKey(-1, "non-existent-key");
+      fail("Updated non-existent key.");
+    } catch (TokenStoreException e) {
+      assertTrue(e.getCause() instanceof NoSuchObjectException);
+    }
+    int keySeq = ts.addMasterKey("key1Data");
+    int keySeq2 = ts.addMasterKey("key2Data");
+    int keySeq2same = ts.addMasterKey("key2Data");
+    assertEquals("keys sequential", keySeq + 1, keySeq2);
+    assertEquals("keys sequential", keySeq + 2, keySeq2same);
+    assertEquals("expected number of keys", 3, ts.getMasterKeys().length);
+    assertTrue(ts.removeMasterKey(keySeq));
+    assertTrue(ts.removeMasterKey(keySeq2same));
+    assertEquals("expected number of keys", 1, ts.getMasterKeys().length);
+    assertEquals("key2Data",ts.getMasterKeys()[0]);
+    ts.updateMasterKey(keySeq2, "updatedData");
+    assertEquals("updatedData",ts.getMasterKeys()[0]);
+    assertTrue(ts.removeMasterKey(keySeq2));
+
+    // tokens
+    assertEquals(0, ts.getAllDelegationTokenIdentifiers().size());
+    DelegationTokenIdentifier tokenId = new DelegationTokenIdentifier(
+        new Text("owner"), new Text("renewer"), new Text("realUser"));
+    assertNull(ts.getToken(tokenId));
+    assertFalse(ts.removeToken(tokenId));
+    DelegationTokenInformation tokenInfo = new DelegationTokenInformation(
+        99, "password".getBytes());
+    assertTrue(ts.addToken(tokenId, tokenInfo));
+    assertFalse(ts.addToken(tokenId, tokenInfo));
+    DelegationTokenInformation tokenInfoRead = ts.getToken(tokenId);
+    assertEquals(tokenInfo.getRenewDate(), tokenInfoRead.getRenewDate());
+    assertNotSame(tokenInfo, tokenInfoRead);
+    Assert.assertArrayEquals(HiveDelegationTokenSupport
+        .encodeDelegationTokenInformation(tokenInfo),
+        HiveDelegationTokenSupport
+            .encodeDelegationTokenInformation(tokenInfoRead));
+
+    List<DelegationTokenIdentifier> allIds = ts
+        .getAllDelegationTokenIdentifiers();
+    assertEquals(1, allIds.size());
+    Assert.assertEquals(TokenStoreDelegationTokenSecretManager
+        .encodeWritable(tokenId),
+        TokenStoreDelegationTokenSecretManager.encodeWritable(allIds
+            .get(0)));
+
+    assertTrue(ts.removeToken(tokenId));
+    assertEquals(0, ts.getAllDelegationTokenIdentifiers().size());
+    assertNull(ts.getToken(tokenId));
+    ts.close();
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java
new file mode 100644
index 0000000..7ac7ebc
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java
@@ -0,0 +1,418 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.net.InetAddress;
+import java.net.NetworkInterface;
+import java.net.ServerSocket;
+import java.security.PrivilegedExceptionAction;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Enumeration;
+import java.util.List;
+import java.util.Map;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.HiveMetaStore;
+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.security.SaslRpcServer;
+import org.apache.hadoop.security.SaslRpcServer.AuthMethod;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.UserGroupInformation.AuthenticationMethod;
+import org.apache.hadoop.security.authorize.AuthorizationException;
+import org.apache.hadoop.security.authorize.ProxyUsers;
+import org.apache.hadoop.security.token.SecretManager.InvalidToken;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
+import org.apache.hadoop.security.token.delegation.DelegationKey;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.thrift.transport.TSaslServerTransport;
+import org.apache.thrift.transport.TTransportException;
+import org.apache.thrift.transport.TTransportFactory;
+
+public class TestHadoop20SAuthBridge extends TestCase {
+
+  /**
+   * set to true when metastore token manager has intitialized token manager
+   * through call to HadoopThriftAuthBridge20S.Server.startDelegationTokenSecretManager
+   */
+  static volatile boolean isMetastoreTokenManagerInited;
+
+  private static class MyHadoopThriftAuthBridge20S extends HadoopThriftAuthBridge20S {
+    @Override
+    public Server createServer(String keytabFile, String principalConf)
+    throws TTransportException {
+      //Create a Server that doesn't interpret any Kerberos stuff
+      return new Server();
+    }
+
+    static class Server extends HadoopThriftAuthBridge20S.Server {
+      public Server() throws TTransportException {
+        super();
+      }
+      @Override
+      public TTransportFactory createTransportFactory(Map<String, String> saslProps)
+      throws TTransportException {
+        TSaslServerTransport.Factory transFactory =
+          new TSaslServerTransport.Factory();
+        transFactory.addServerDefinition(AuthMethod.DIGEST.getMechanismName(),
+            null, SaslRpcServer.SASL_DEFAULT_REALM,
+            saslProps,
+            new SaslDigestCallbackHandler(secretManager));
+
+        return new TUGIAssumingTransportFactory(transFactory, realUgi);
+      }
+      static DelegationTokenStore TOKEN_STORE = new MemoryTokenStore();
+
+      @Override
+      protected DelegationTokenStore getTokenStore(Configuration conf) throws IOException {
+        return TOKEN_STORE;
+      }
+
+      @Override
+      public void startDelegationTokenSecretManager(Configuration conf, Object hms)
+      throws IOException{
+        super.startDelegationTokenSecretManager(conf, hms);
+        isMetastoreTokenManagerInited = true;
+      }
+
+    }
+  }
+
+
+  private HiveConf conf;
+
+  private void configureSuperUserIPAddresses(Configuration conf,
+      String superUserShortName) throws IOException {
+    List<String> ipList = new ArrayList<String>();
+    Enumeration<NetworkInterface> netInterfaceList = NetworkInterface
+        .getNetworkInterfaces();
+    while (netInterfaceList.hasMoreElements()) {
+      NetworkInterface inf = netInterfaceList.nextElement();
+      Enumeration<InetAddress> addrList = inf.getInetAddresses();
+      while (addrList.hasMoreElements()) {
+        InetAddress addr = addrList.nextElement();
+        ipList.add(addr.getHostAddress());
+      }
+    }
+    StringBuilder builder = new StringBuilder();
+    for (String ip : ipList) {
+      builder.append(ip);
+      builder.append(',');
+    }
+    builder.append("127.0.1.1,");
+    builder.append(InetAddress.getLocalHost().getCanonicalHostName());
+    conf.setStrings(ProxyUsers.getProxySuperuserIpConfKey(superUserShortName),
+        builder.toString());
+  }
+
+  public void setup() throws Exception {
+    isMetastoreTokenManagerInited = false;
+    int port = findFreePort();
+    System.setProperty(HiveConf.ConfVars.METASTORE_USE_THRIFT_SASL.varname,
+        "true");
+    System.setProperty(HiveConf.ConfVars.METASTOREURIS.varname,
+        "thrift://localhost:" + port);
+    System.setProperty(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, new Path(
+        System.getProperty("test.build.data", "/tmp")).toString());
+    conf = new HiveConf(TestHadoop20SAuthBridge.class);
+    MetaStoreUtils.startMetaStore(port, new MyHadoopThriftAuthBridge20S());
+  }
+
+  /**
+   * Test delegation token store/load from shared store.
+   * @throws Exception
+   */
+  public void testDelegationTokenSharedStore() throws Exception {
+    UserGroupInformation clientUgi = UserGroupInformation.getCurrentUser();
+
+    TokenStoreDelegationTokenSecretManager tokenManager =
+        new TokenStoreDelegationTokenSecretManager(0, 60*60*1000, 60*60*1000, 0,
+            MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE);
+    // initializes current key
+    tokenManager.startThreads();
+    tokenManager.stopThreads();
+
+    String tokenStrForm = tokenManager.getDelegationToken(clientUgi.getShortUserName());
+    Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
+    t.decodeFromUrlString(tokenStrForm);
+
+    //check whether the username in the token is what we expect
+    DelegationTokenIdentifier d = new DelegationTokenIdentifier();
+    d.readFields(new DataInputStream(new ByteArrayInputStream(
+        t.getIdentifier())));
+    assertTrue("Usernames don't match",
+        clientUgi.getShortUserName().equals(d.getUser().getShortUserName()));
+
+    DelegationTokenInformation tokenInfo = MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE
+        .getToken(d);
+    assertNotNull("token not in store", tokenInfo);
+    assertFalse("duplicate token add",
+        MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.addToken(d, tokenInfo));
+
+    // check keys are copied from token store when token is loaded
+    TokenStoreDelegationTokenSecretManager anotherManager =
+        new TokenStoreDelegationTokenSecretManager(0, 0, 0, 0,
+            MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE);
+   assertEquals("master keys empty on init", 0,
+        anotherManager.getAllKeys().length);
+    assertNotNull("token loaded",
+        anotherManager.retrievePassword(d));
+    anotherManager.renewToken(t, clientUgi.getShortUserName());
+    assertEquals("master keys not loaded from store",
+        MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.getMasterKeys().length,
+        anotherManager.getAllKeys().length);
+
+    // cancel the delegation token
+    tokenManager.cancelDelegationToken(tokenStrForm);
+    assertNull("token not removed from store after cancel",
+        MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.getToken(d));
+    assertFalse("token removed (again)",
+        MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.removeToken(d));
+    try {
+      anotherManager.retrievePassword(d);
+      fail("InvalidToken expected after cancel");
+    } catch (InvalidToken ex) {
+      // expected
+    }
+
+    // token expiration
+    MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.addToken(d,
+        new DelegationTokenInformation(0, t.getPassword()));
+    assertNotNull(MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.getToken(d));
+    anotherManager.removeExpiredTokens();
+    assertNull("Expired token not removed",
+        MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.getToken(d));
+
+    // key expiration - create an already expired key
+    anotherManager.startThreads(); // generates initial key
+    anotherManager.stopThreads();
+    DelegationKey expiredKey = new DelegationKey(-1, 0, anotherManager.getAllKeys()[0].getKey());
+    anotherManager.logUpdateMasterKey(expiredKey); // updates key with sequence number
+    assertTrue("expired key not in allKeys",
+        anotherManager.reloadKeys().containsKey(expiredKey.getKeyId()));
+    anotherManager.rollMasterKeyExt();
+    assertFalse("Expired key not removed",
+        anotherManager.reloadKeys().containsKey(expiredKey.getKeyId()));
+  }
+
+  public void testSaslWithHiveMetaStore() throws Exception {
+    setup();
+    UserGroupInformation clientUgi = UserGroupInformation.getCurrentUser();
+    obtainTokenAndAddIntoUGI(clientUgi, null);
+    obtainTokenAndAddIntoUGI(clientUgi, "tokenForFooTablePartition");
+  }
+
+  public void testMetastoreProxyUser() throws Exception {
+    setup();
+
+    final String proxyUserName = "proxyUser";
+    //set the configuration up such that proxyUser can act on
+    //behalf of all users belonging to the group foo_bar_group (
+    //a dummy group)
+    String[] groupNames =
+      new String[] { "foo_bar_group" };
+    setGroupsInConf(groupNames, proxyUserName);
+
+    final UserGroupInformation delegationTokenUser =
+      UserGroupInformation.getCurrentUser();
+
+    final UserGroupInformation proxyUserUgi =
+      UserGroupInformation.createRemoteUser(proxyUserName);
+    String tokenStrForm = proxyUserUgi.doAs(new PrivilegedExceptionAction<String>() {
+      public String run() throws Exception {
+        try {
+          //Since the user running the test won't belong to a non-existent group
+          //foo_bar_group, the call to getDelegationTokenStr will fail
+          return getDelegationTokenStr(delegationTokenUser, proxyUserUgi);
+        } catch (AuthorizationException ae) {
+          return null;
+        }
+      }
+    });
+    assertTrue("Expected the getDelegationToken call to fail",
+        tokenStrForm == null);
+
+    //set the configuration up such that proxyUser can act on
+    //behalf of all users belonging to the real group(s) that the
+    //user running the test belongs to
+    setGroupsInConf(UserGroupInformation.getCurrentUser().getGroupNames(),
+        proxyUserName);
+    tokenStrForm = proxyUserUgi.doAs(new PrivilegedExceptionAction<String>() {
+      public String run() throws Exception {
+        try {
+          //Since the user running the test belongs to the group
+          //obtained above the call to getDelegationTokenStr will succeed
+          return getDelegationTokenStr(delegationTokenUser, proxyUserUgi);
+        } catch (AuthorizationException ae) {
+          return null;
+        }
+      }
+    });
+    assertTrue("Expected the getDelegationToken call to not fail",
+        tokenStrForm != null);
+    Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
+    t.decodeFromUrlString(tokenStrForm);
+    //check whether the username in the token is what we expect
+    DelegationTokenIdentifier d = new DelegationTokenIdentifier();
+    d.readFields(new DataInputStream(new ByteArrayInputStream(
+        t.getIdentifier())));
+    assertTrue("Usernames don't match",
+        delegationTokenUser.getShortUserName().equals(d.getUser().getShortUserName()));
+
+  }
+
+  private void setGroupsInConf(String[] groupNames, String proxyUserName)
+  throws IOException {
+   conf.set(
+      ProxyUsers.getProxySuperuserGroupConfKey(proxyUserName),
+      StringUtils.join(",", Arrays.asList(groupNames)));
+    configureSuperUserIPAddresses(conf, proxyUserName);
+    ProxyUsers.refreshSuperUserGroupsConfiguration(conf);
+  }
+
+  private String getDelegationTokenStr(UserGroupInformation ownerUgi,
+      UserGroupInformation realUgi) throws Exception {
+    //obtain a token by directly invoking the metastore operation(without going
+    //through the thrift interface). Obtaining a token makes the secret manager
+    //aware of the user and that it gave the token to the user
+    //also set the authentication method explicitly to KERBEROS. Since the
+    //metastore checks whether the authentication method is KERBEROS or not
+    //for getDelegationToken, and the testcases don't use
+    //kerberos, this needs to be done
+
+    waitForMetastoreTokenInit();
+
+    HadoopThriftAuthBridge20S.Server.authenticationMethod
+                             .set(AuthenticationMethod.KERBEROS);
+    HadoopThriftAuthBridge20S.Server.remoteAddress.set(InetAddress.getLocalHost());
+    return
+        HiveMetaStore.getDelegationToken(ownerUgi.getShortUserName(),
+            realUgi.getShortUserName());
+  }
+
+  /**
+   * Wait for metastore to have initialized token manager
+   * This does not have to be done in other metastore test cases as they
+   * use metastore client which will retry few times on failure
+   * @throws InterruptedException
+   */
+  private void waitForMetastoreTokenInit() throws InterruptedException {
+    int waitAttempts = 30;
+    while(waitAttempts > 0 && !isMetastoreTokenManagerInited){
+      Thread.sleep(1000);
+      waitAttempts--;
+    }
+  }
+
+  private void obtainTokenAndAddIntoUGI(UserGroupInformation clientUgi,
+      String tokenSig) throws Exception {
+    String tokenStrForm = getDelegationTokenStr(clientUgi, clientUgi);
+    Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
+    t.decodeFromUrlString(tokenStrForm);
+
+    //check whether the username in the token is what we expect
+    DelegationTokenIdentifier d = new DelegationTokenIdentifier();
+    d.readFields(new DataInputStream(new ByteArrayInputStream(
+        t.getIdentifier())));
+    assertTrue("Usernames don't match",
+        clientUgi.getShortUserName().equals(d.getUser().getShortUserName()));
+
+    if (tokenSig != null) {
+      conf.set("hive.metastore.token.signature", tokenSig);
+      t.setService(new Text(tokenSig));
+    }
+    //add the token to the clientUgi for securely talking to the metastore
+    clientUgi.addToken(t);
+    //Create the metastore client as the clientUgi. Doing so this
+    //way will give the client access to the token that was added earlier
+    //in the clientUgi
+    HiveMetaStoreClient hiveClient =
+      clientUgi.doAs(new PrivilegedExceptionAction<HiveMetaStoreClient>() {
+        public HiveMetaStoreClient run() throws Exception {
+          HiveMetaStoreClient hiveClient =
+            new HiveMetaStoreClient(conf);
+          return hiveClient;
+        }
+      });
+
+    assertTrue("Couldn't connect to metastore", hiveClient != null);
+
+    //try out some metastore operations
+    createDBAndVerifyExistence(hiveClient);
+
+    //check that getDelegationToken fails since we are not authenticating
+    //over kerberos
+    boolean pass = false;
+    try {
+      hiveClient.getDelegationToken(clientUgi.getUserName());
+    } catch (MetaException ex) {
+      pass = true;
+    }
+    assertTrue("Expected the getDelegationToken call to fail", pass == true);
+    hiveClient.close();
+
+    //Now cancel the delegation token
+    HiveMetaStore.cancelDelegationToken(tokenStrForm);
+
+    //now metastore connection should fail
+    hiveClient =
+      clientUgi.doAs(new PrivilegedExceptionAction<HiveMetaStoreClient>() {
+        public HiveMetaStoreClient run() {
+          try {
+            HiveMetaStoreClient hiveClient =
+              new HiveMetaStoreClient(conf);
+            return hiveClient;
+          } catch (MetaException e) {
+            return null;
+          }
+        }
+      });
+    assertTrue("Expected metastore operations to fail", hiveClient == null);
+  }
+
+  private void createDBAndVerifyExistence(HiveMetaStoreClient client)
+  throws Exception {
+    String dbName = "simpdb";
+    Database db = new Database();
+    db.setName(dbName);
+    client.createDatabase(db);
+    Database db1 = client.getDatabase(dbName);
+    client.dropDatabase(dbName);
+    assertTrue("Databases do not match", db1.getName().equals(db.getName()));
+  }
+
+  private int findFreePort() throws IOException {
+    ServerSocket socket= new ServerSocket(0);
+    int port = socket.getLocalPort();
+    socket.close();
+    return port;
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java
new file mode 100644
index 0000000..83a80b4
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java
@@ -0,0 +1,181 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.List;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
+import org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.ZooKeeper;
+import org.apache.zookeeper.data.ACL;
+import org.apache.zookeeper.data.Stat;
+import org.junit.Assert;
+
+public class TestZooKeeperTokenStore extends TestCase {
+
+  private MiniZooKeeperCluster zkCluster = null;
+  private ZooKeeper zkClient = null;
+  private int zkPort = -1;
+  private ZooKeeperTokenStore ts;
+  // connect timeout large enough for slower test environments
+  private final int connectTimeoutMillis = 30000;
+
+  @Override
+  protected void setUp() throws Exception {
+    File zkDataDir = new File(System.getProperty("test.tmp.dir"));
+    if (this.zkCluster != null) {
+      throw new IOException("Cluster already running");
+    }
+    this.zkCluster = new MiniZooKeeperCluster();
+    this.zkPort = this.zkCluster.startup(zkDataDir);
+
+    this.zkClient = ZooKeeperTokenStore.createConnectedClient("localhost:" + zkPort, 3000,
+        connectTimeoutMillis);
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    this.zkClient.close();
+    if (ts != null) {
+      ts.close();
+    }
+    this.zkCluster.shutdown();
+    this.zkCluster = null;
+  }
+
+  private Configuration createConf(String zkPath) {
+    Configuration conf = new Configuration();
+    conf.set(
+        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_CONNECT_STR,
+        "localhost:" + this.zkPort);
+    conf.set(
+        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ZNODE,
+        zkPath);
+    conf.setLong(
+        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_CONNECT_TIMEOUTMILLIS,
+        connectTimeoutMillis);
+    return conf;
+  }
+
+  public void testTokenStorage() throws Exception {
+    String ZK_PATH = "/zktokenstore-testTokenStorage";
+    ts = new ZooKeeperTokenStore();
+    ts.setConf(createConf(ZK_PATH));
+
+    int keySeq = ts.addMasterKey("key1Data");
+    byte[] keyBytes = zkClient.getData(
+        ZK_PATH
+            + "/keys/"
+            + String.format(ZooKeeperTokenStore.ZK_SEQ_FORMAT,
+                keySeq), false, null);
+    assertNotNull(keyBytes);
+    assertEquals(new String(keyBytes), "key1Data");
+
+    int keySeq2 = ts.addMasterKey("key2Data");
+    assertEquals("keys sequential", keySeq + 1, keySeq2);
+    assertEquals("expected number keys", 2, ts.getMasterKeys().length);
+
+    ts.removeMasterKey(keySeq);
+    assertEquals("expected number keys", 1, ts.getMasterKeys().length);
+
+    // tokens
+    DelegationTokenIdentifier tokenId = new DelegationTokenIdentifier(
+        new Text("owner"), new Text("renewer"), new Text("realUser"));
+    DelegationTokenInformation tokenInfo = new DelegationTokenInformation(
+        99, "password".getBytes());
+    ts.addToken(tokenId, tokenInfo);
+    DelegationTokenInformation tokenInfoRead = ts.getToken(tokenId);
+    assertEquals(tokenInfo.getRenewDate(), tokenInfoRead.getRenewDate());
+    assertNotSame(tokenInfo, tokenInfoRead);
+    Assert.assertArrayEquals(HiveDelegationTokenSupport
+        .encodeDelegationTokenInformation(tokenInfo),
+        HiveDelegationTokenSupport
+            .encodeDelegationTokenInformation(tokenInfoRead));
+
+    List<DelegationTokenIdentifier> allIds = ts
+        .getAllDelegationTokenIdentifiers();
+    assertEquals(1, allIds.size());
+    Assert.assertEquals(TokenStoreDelegationTokenSecretManager
+        .encodeWritable(tokenId),
+        TokenStoreDelegationTokenSecretManager.encodeWritable(allIds
+            .get(0)));
+
+    assertTrue(ts.removeToken(tokenId));
+    assertEquals(0, ts.getAllDelegationTokenIdentifiers().size());
+  }
+
+  public void testAclNoAuth() throws Exception {
+    String ZK_PATH = "/zktokenstore-testAclNoAuth";
+    Configuration conf = createConf(ZK_PATH);
+    conf.set(
+        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ACL,
+        "ip:127.0.0.1:r");
+
+    ts = new ZooKeeperTokenStore();
+    try {
+      ts.setConf(conf);
+      fail("expected ACL exception");
+    } catch (DelegationTokenStore.TokenStoreException e) {
+      assertEquals(e.getCause().getClass(),
+          KeeperException.NoAuthException.class);
+    }
+  }
+
+  public void testAclInvalid() throws Exception {
+    String ZK_PATH = "/zktokenstore-testAclInvalid";
+    String aclString = "sasl:hive/host@TEST.DOMAIN:cdrwa, fail-parse-ignored";
+    Configuration conf = createConf(ZK_PATH);
+    conf.set(
+        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ACL,
+        aclString);
+
+    List<ACL> aclList = ZooKeeperTokenStore.parseACLs(aclString);
+    assertEquals(1, aclList.size());
+
+    ts = new ZooKeeperTokenStore();
+    try {
+      ts.setConf(conf);
+      fail("expected ACL exception");
+    } catch (DelegationTokenStore.TokenStoreException e) {
+      assertEquals(e.getCause().getClass(),
+          KeeperException.InvalidACLException.class);
+    }
+  }
+
+  public void testAclPositive() throws Exception {
+    String ZK_PATH = "/zktokenstore-testAcl";
+    Configuration conf = createConf(ZK_PATH);
+    conf.set(
+        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ACL,
+        "world:anyone:cdrwa,ip:127.0.0.1:cdrwa");
+    ts = new ZooKeeperTokenStore();
+    ts.setConf(conf);
+    List<ACL> acl = zkClient.getACL(ZK_PATH, new Stat());
+    assertEquals(2, acl.size());
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hive/beeline/TestSchemaTool.java b/src/itests/hive-unit/src/test/java/org/apache/hive/beeline/TestSchemaTool.java
new file mode 100644
index 0000000..71c7756
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hive/beeline/TestSchemaTool.java
@@ -0,0 +1,383 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.beeline;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.util.Random;
+
+import junit.framework.TestCase;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.HiveMetaException;
+import org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo;
+import org.apache.hive.beeline.HiveSchemaHelper;
+import org.apache.hive.beeline.HiveSchemaHelper.NestedScriptParser;
+import org.apache.hive.beeline.HiveSchemaTool;
+
+public class TestSchemaTool extends TestCase {
+  private HiveSchemaTool schemaTool;
+  private HiveConf hiveConf;
+  private String testMetastoreDB;
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    testMetastoreDB = System.getProperty("java.io.tmpdir") +
+        File.separator + "test_metastore-" + new Random().nextInt();
+    System.setProperty(HiveConf.ConfVars.METASTORECONNECTURLKEY.varname,
+        "jdbc:derby:" + testMetastoreDB + ";create=true");
+    hiveConf = new HiveConf(this.getClass());
+    schemaTool = new HiveSchemaTool(System.getProperty("test.tmp.dir"), hiveConf, "derby");
+    System.setProperty("beeLine.system.exit", "true");
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    File metaStoreDir = new File(testMetastoreDB);
+    if (metaStoreDir.exists()) {
+      FileUtils.deleteDirectory(metaStoreDir);
+    }
+  }
+
+  /**
+   * Test dryrun of schema initialization
+   * @throws Exception
+   */
+  public void testSchemaInitDryRun() throws Exception {
+    schemaTool.setDryRun(true);
+    schemaTool.doInit("0.7.0");
+    schemaTool.setDryRun(false);
+    try {
+      schemaTool.verifySchemaVersion();
+    } catch (HiveMetaException e) {
+      // The connection should fail since it the dry run
+      return;
+    }
+    fail("Dry run shouldn't create actual metastore");
+  }
+
+  /**
+   * Test dryrun of schema upgrade
+   * @throws Exception
+   */
+  public void testSchemaUpgradeDryRun() throws Exception {
+    schemaTool.doInit("0.7.0");
+
+    schemaTool.setDryRun(true);
+    schemaTool.doUpgrade("0.7.0");
+    schemaTool.setDryRun(false);
+    try {
+      schemaTool.verifySchemaVersion();
+    } catch (HiveMetaException e) {
+      // The connection should fail since it the dry run
+      return;
+    }
+    fail("Dry run shouldn't upgrade metastore schema");
+  }
+
+  /**
+   * Test schema initialization
+   * @throws Exception
+   */
+  public void testSchemaInit() throws Exception {
+    schemaTool.doInit(MetaStoreSchemaInfo.getHiveSchemaVersion());
+    schemaTool.verifySchemaVersion();
+    }
+
+  /**
+   * Test schema upgrade
+   * @throws Exception
+   */
+  public void testSchemaUpgrade() throws Exception {
+    boolean foundException = false;
+    // Initialize 0.7.0 schema
+    schemaTool.doInit("0.7.0");
+    // verify that driver fails due to older version schema
+    try {
+      schemaTool.verifySchemaVersion();
+    } catch (HiveMetaException e) {
+      // Expected to fail due to old schema
+      foundException = true;
+    }
+    if (!foundException) {
+      throw new Exception("Hive operations shouldn't pass with older version schema");
+    }
+
+    // upgrade schema from 0.7.0 to latest
+    schemaTool.doUpgrade("0.7.0");
+    // verify that driver works fine with latest schema
+    schemaTool.verifySchemaVersion();
+  }
+
+  /**
+   * Test script formatting
+   * @throws Exception
+   */
+  public void testScripts() throws Exception {
+    String testScript[] = {
+        "-- this is a comment",
+      "DROP TABLE IF EXISTS fooTab;",
+      "/*!1234 this is comment code like mysql */;",
+      "CREATE TABLE fooTab(id INTEGER);",
+      "DROP TABLE footab;",
+      "-- ending comment"
+    };
+    String resultScript[] = {
+      "DROP TABLE IF EXISTS fooTab",
+      "/*!1234 this is comment code like mysql */",
+      "CREATE TABLE fooTab(id INTEGER)",
+      "DROP TABLE footab",
+    };
+    String expectedSQL = StringUtils.join(resultScript, System.getProperty("line.separator")) +
+        System.getProperty("line.separator");
+    File testScriptFile = generateTestScript(testScript);
+    String flattenedSql = HiveSchemaTool.buildCommand(
+        HiveSchemaHelper.getDbCommandParser("derby"),
+        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
+
+    assertEquals(expectedSQL, flattenedSql);
+  }
+
+  /**
+   * Test nested script formatting
+   * @throws Exception
+   */
+  public void testNestedScriptsForDerby() throws Exception {
+    String childTab1 = "childTab1";
+    String childTab2 = "childTab2";
+    String parentTab = "fooTab";
+
+    String childTestScript1[] = {
+      "-- this is a comment ",
+      "DROP TABLE IF EXISTS " + childTab1 + ";",
+      "CREATE TABLE " + childTab1 + "(id INTEGER);",
+      "DROP TABLE " + childTab1 + ";"
+    };
+    String childTestScript2[] = {
+        "-- this is a comment",
+        "DROP TABLE IF EXISTS " + childTab2 + ";",
+        "CREATE TABLE " + childTab2 + "(id INTEGER);",
+        "-- this is also a comment",
+        "DROP TABLE " + childTab2 + ";"
+    };
+
+    String parentTestScript[] = {
+        " -- this is a comment",
+        "DROP TABLE IF EXISTS " + parentTab + ";",
+        " -- this is another comment ",
+        "CREATE TABLE " + parentTab + "(id INTEGER);",
+        "RUN '" + generateTestScript(childTestScript1).getName() + "';",
+        "DROP TABLE " + parentTab + ";",
+        "RUN '" + generateTestScript(childTestScript2).getName() + "';",
+        "--ending comment ",
+      };
+
+    File testScriptFile = generateTestScript(parentTestScript);
+    String flattenedSql = HiveSchemaTool.buildCommand(
+        HiveSchemaHelper.getDbCommandParser("derby"),
+        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
+    assertFalse(flattenedSql.contains("RUN"));
+    assertFalse(flattenedSql.contains("comment"));
+    assertTrue(flattenedSql.contains(childTab1));
+    assertTrue(flattenedSql.contains(childTab2));
+    assertTrue(flattenedSql.contains(parentTab));
+  }
+
+  /**
+   * Test nested script formatting
+   * @throws Exception
+   */
+  public void testNestedScriptsForMySQL() throws Exception {
+    String childTab1 = "childTab1";
+    String childTab2 = "childTab2";
+    String parentTab = "fooTab";
+
+    String childTestScript1[] = {
+      "/* this is a comment code */",
+      "DROP TABLE IF EXISTS " + childTab1 + ";",
+      "CREATE TABLE " + childTab1 + "(id INTEGER);",
+      "DROP TABLE " + childTab1 + ";"
+    };
+    String childTestScript2[] = {
+        "/* this is a special exec code */;",
+        "DROP TABLE IF EXISTS " + childTab2 + ";",
+        "CREATE TABLE " + childTab2 + "(id INTEGER);",
+        "-- this is a comment",
+        "DROP TABLE " + childTab2 + ";"
+    };
+
+    String parentTestScript[] = {
+        " -- this is a comment",
+        "DROP TABLE IF EXISTS " + parentTab + ";",
+        " /* this is special exec code */;",
+        "CREATE TABLE " + parentTab + "(id INTEGER);",
+        "SOURCE " + generateTestScript(childTestScript1).getName() + ";",
+        "DROP TABLE " + parentTab + ";",
+        "SOURCE " + generateTestScript(childTestScript2).getName() + ";",
+        "--ending comment ",
+      };
+
+    File testScriptFile = generateTestScript(parentTestScript);
+    String flattenedSql = HiveSchemaTool.buildCommand(
+        HiveSchemaHelper.getDbCommandParser("mysql"),
+        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
+    assertFalse(flattenedSql.contains("RUN"));
+    assertFalse(flattenedSql.contains("comment"));
+    assertTrue(flattenedSql.contains(childTab1));
+    assertTrue(flattenedSql.contains(childTab2));
+    assertTrue(flattenedSql.contains(parentTab));
+  }
+
+  /**
+   * Test script formatting
+   * @throws Exception
+   */
+  public void testScriptWithDelimiter() throws Exception {
+    String testScript[] = {
+        "-- this is a comment",
+      "DROP TABLE IF EXISTS fooTab;",
+      "DELIMITER $$",
+      "/*!1234 this is comment code like mysql */$$",
+      "CREATE TABLE fooTab(id INTEGER)$$",
+      "CREATE PROCEDURE fooProc()",
+      "SELECT * FROM fooTab;",
+      "CALL barProc();",
+      "END PROCEDURE$$",
+      "DELIMITER ;",
+      "DROP TABLE footab;",
+      "-- ending comment"
+    };
+    String resultScript[] = {
+      "DROP TABLE IF EXISTS fooTab",
+      "/*!1234 this is comment code like mysql */",
+      "CREATE TABLE fooTab(id INTEGER)",
+      "CREATE PROCEDURE fooProc()" + " " +
+      "SELECT * FROM fooTab;" + " " +
+      "CALL barProc();" + " " +
+      "END PROCEDURE",
+      "DROP TABLE footab",
+    };
+    String expectedSQL = StringUtils.join(resultScript, System.getProperty("line.separator")) +
+        System.getProperty("line.separator");
+    File testScriptFile = generateTestScript(testScript);
+    NestedScriptParser testDbParser = HiveSchemaHelper.getDbCommandParser("mysql");
+    String flattenedSql = HiveSchemaTool.buildCommand(testDbParser,
+        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
+
+    assertEquals(expectedSQL, flattenedSql);
+  }
+
+  /**
+   * Test script formatting
+   * @throws Exception
+   */
+  public void testScriptMultiRowComment() throws Exception {
+    String testScript[] = {
+        "-- this is a comment",
+      "DROP TABLE IF EXISTS fooTab;",
+      "DELIMITER $$",
+      "/*!1234 this is comment code like mysql */$$",
+      "CREATE TABLE fooTab(id INTEGER)$$",
+      "DELIMITER ;",
+      "/* multiline comment started ",
+      " * multiline comment continue",
+      " * multiline comment ended */",
+      "DROP TABLE footab;",
+      "-- ending comment"
+    };
+    String parsedScript[] = {
+      "DROP TABLE IF EXISTS fooTab",
+      "/*!1234 this is comment code like mysql */",
+      "CREATE TABLE fooTab(id INTEGER)",
+      "DROP TABLE footab",
+    };
+
+    String expectedSQL = StringUtils.join(parsedScript, System.getProperty("line.separator")) +
+        System.getProperty("line.separator");
+    File testScriptFile = generateTestScript(testScript);
+    NestedScriptParser testDbParser = HiveSchemaHelper.getDbCommandParser("mysql");
+    String flattenedSql = HiveSchemaTool.buildCommand(testDbParser,
+        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
+
+    assertEquals(expectedSQL, flattenedSql);
+  }
+
+  /**
+   * Test nested script formatting
+   * @throws Exception
+   */
+  public void testNestedScriptsForOracle() throws Exception {
+    String childTab1 = "childTab1";
+    String childTab2 = "childTab2";
+    String parentTab = "fooTab";
+
+    String childTestScript1[] = {
+      "-- this is a comment ",
+      "DROP TABLE IF EXISTS " + childTab1 + ";",
+      "CREATE TABLE " + childTab1 + "(id INTEGER);",
+      "DROP TABLE " + childTab1 + ";"
+    };
+    String childTestScript2[] = {
+        "-- this is a comment",
+        "DROP TABLE IF EXISTS " + childTab2 + ";",
+        "CREATE TABLE " + childTab2 + "(id INTEGER);",
+        "-- this is also a comment",
+        "DROP TABLE " + childTab2 + ";"
+    };
+
+    String parentTestScript[] = {
+        " -- this is a comment",
+        "DROP TABLE IF EXISTS " + parentTab + ";",
+        " -- this is another comment ",
+        "CREATE TABLE " + parentTab + "(id INTEGER);",
+        "@" + generateTestScript(childTestScript1).getName() + ";",
+        "DROP TABLE " + parentTab + ";",
+        "@" + generateTestScript(childTestScript2).getName() + ";",
+        "--ending comment ",
+      };
+
+    File testScriptFile = generateTestScript(parentTestScript);
+    String flattenedSql = HiveSchemaTool.buildCommand(
+        HiveSchemaHelper.getDbCommandParser("oracle"),
+        testScriptFile.getParentFile().getPath(), testScriptFile.getName());
+    assertFalse(flattenedSql.contains("@"));
+    assertFalse(flattenedSql.contains("comment"));
+    assertTrue(flattenedSql.contains(childTab1));
+    assertTrue(flattenedSql.contains(childTab2));
+    assertTrue(flattenedSql.contains(parentTab));
+  }
+
+  private File generateTestScript(String [] stmts) throws IOException {
+    File testScriptFile = File.createTempFile("schematest", ".sql");
+    testScriptFile.deleteOnExit();
+    FileWriter fstream = new FileWriter(testScriptFile.getPath());
+    BufferedWriter out = new BufferedWriter(fstream);
+    for (String line: stmts) {
+      out.write(line);
+      out.newLine();
+    }
+    out.close();
+    return testScriptFile;
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java b/src/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
new file mode 100644
index 0000000..b05d9af
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
@@ -0,0 +1,1745 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.jdbc;
+
+import static org.apache.hadoop.hive.ql.exec.ExplainTask.EXPL_COLUMN_NAME;
+import static org.apache.hadoop.hive.ql.processors.SetProcessor.SET_COLUMN_NAME;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+import java.sql.Connection;
+import java.sql.DatabaseMetaData;
+import java.sql.DriverManager;
+import java.sql.DriverPropertyInfo;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.ResultSetMetaData;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.sql.Types;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+import java.util.Set;
+import java.util.regex.Pattern;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.TableType;
+import org.apache.hive.common.util.HiveVersionInfo;
+import org.apache.hive.jdbc.Utils.JdbcConnectionParams;
+import org.apache.hive.service.cli.operation.ClassicTableTypeMapping;
+import org.apache.hive.service.cli.operation.ClassicTableTypeMapping.ClassicTableTypes;
+import org.apache.hive.service.cli.operation.HiveTableTypeMapping;
+import org.apache.hive.service.cli.operation.TableTypeMappingFactory.TableTypeMappings;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+
+/**
+ * TestJdbcDriver2
+ *
+ */
+public class TestJdbcDriver2 {
+  private static final String driverName = "org.apache.hive.jdbc.HiveDriver";
+  private static final String tableName = "testHiveJdbcDriver_Table";
+  private static final String tableComment = "Simple table";
+  private static final String viewName = "testHiveJdbcDriverView";
+  private static final String viewComment = "Simple view";
+  private static final String partitionedTableName = "testHiveJdbcDriverPartitionedTable";
+  private static final String partitionedColumnName = "partcolabc";
+  private static final String partitionedColumnValue = "20090619";
+  private static final String partitionedTableComment = "Partitioned table";
+  private static final String dataTypeTableName = "testDataTypeTable";
+  private static final String dataTypeTableComment = "Table with many column data types";
+  private final HiveConf conf;
+  private final Path dataFilePath;
+  private final Path dataTypeDataFilePath;
+  private Connection con;
+  private static boolean standAloneServer = false;
+  private static final float floatCompareDelta = 0.0001f;
+
+  public TestJdbcDriver2() {
+    conf = new HiveConf(TestJdbcDriver2.class);
+    String dataFileDir = conf.get("test.data.files").replace('\\', '/')
+        .replace("c:", "");
+    dataFilePath = new Path(dataFileDir, "kv1.txt");
+    dataTypeDataFilePath = new Path(dataFileDir, "datatypes.txt");
+    standAloneServer = "true".equals(System
+        .getProperty("test.service.standalone.server"));
+  }
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws SQLException, ClassNotFoundException{
+    Class.forName(driverName);
+    Connection con1 = getConnection();
+
+    Statement stmt1 = con1.createStatement();
+    assertNotNull("Statement is null", stmt1);
+
+    stmt1.execute("set hive.support.concurrency = false");
+
+    DatabaseMetaData metadata = con1.getMetaData();
+
+    // Drop databases created by other test cases
+    ResultSet databaseRes = metadata.getSchemas();
+
+    while(databaseRes.next()){
+      String db = databaseRes.getString(1);
+      if(!db.equals("default")){
+        System.err.println("Dropping database " + db);
+        stmt1.execute("DROP DATABASE " + db + " CASCADE");
+      }
+    }
+    stmt1.close();
+    con1.close();
+  }
+
+  @Before
+  public void setUp() throws Exception {
+    con = getConnection();
+
+    Statement stmt = con.createStatement();
+    assertNotNull("Statement is null", stmt);
+
+    stmt.execute("set hive.support.concurrency = false");
+
+    // drop table. ignore error.
+    try {
+      stmt.execute("drop table " + tableName);
+    } catch (Exception ex) {
+      fail(ex.toString());
+    }
+
+    // create table
+    stmt.execute("create table " + tableName
+        + " (under_col int comment 'the under column', value string) comment '"
+        + tableComment + "'");
+
+    // load data
+    stmt.execute("load data local inpath '"
+        + dataFilePath.toString() + "' into table " + tableName);
+
+    // also initialize a paritioned table to test against.
+
+    // drop table. ignore error.
+    try {
+      stmt.execute("drop table " + partitionedTableName);
+    } catch (Exception ex) {
+      fail(ex.toString());
+    }
+
+    stmt.execute("create table " + partitionedTableName
+        + " (under_col int, value string) comment '"+partitionedTableComment
+        +"' partitioned by (" + partitionedColumnName + " STRING)");
+
+    // load data
+    stmt.execute("load data local inpath '"
+        + dataFilePath.toString() + "' into table " + partitionedTableName
+        + " PARTITION (" + partitionedColumnName + "="
+        + partitionedColumnValue + ")");
+
+    // drop table. ignore error.
+    try {
+      stmt.execute("drop table " + dataTypeTableName);
+    } catch (Exception ex) {
+      fail(ex.toString());
+    }
+
+    stmt.execute("create table " + dataTypeTableName
+        + " (c1 int, c2 boolean, c3 double, c4 string,"
+        + " c5 array<int>, c6 map<int,string>, c7 map<string,string>,"
+        + " c8 struct<r:string,s:int,t:double>,"
+        + " c9 tinyint, c10 smallint, c11 float, c12 bigint,"
+        + " c13 array<array<string>>,"
+        + " c14 map<int, map<int,int>>,"
+        + " c15 struct<r:int,s:struct<a:int,b:string>>,"
+        + " c16 array<struct<m:map<string,string>,n:int>>,"
+        + " c17 timestamp, "
+        + " c18 decimal, "
+        + " c19 binary, "
+        + " c20 date,"
+        + " c21 varchar(20)"
+        + ") comment'" + dataTypeTableComment
+        +"' partitioned by (dt STRING)");
+
+    stmt.execute("load data local inpath '"
+        + dataTypeDataFilePath.toString() + "' into table " + dataTypeTableName
+        + " PARTITION (dt='20090619')");
+
+    // drop view. ignore error.
+    try {
+      stmt.execute("drop view " + viewName);
+    } catch (Exception ex) {
+      fail(ex.toString());
+    }
+
+    // create view
+    stmt.execute("create view " + viewName + " comment '"+viewComment
+        +"' as select * from "+ tableName);
+  }
+
+  private static Connection getConnection() throws SQLException {
+    Connection con1;
+    if (standAloneServer) {
+      // get connection
+      con1 = DriverManager.getConnection("jdbc:hive2://localhost:10000/default",
+          "", "");
+    } else {
+      con1 = DriverManager.getConnection("jdbc:hive2://", "", "");
+    }
+    assertNotNull("Connection is null", con1);
+    assertFalse("Connection should not be closed", con1.isClosed());
+
+    return con1;
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    // drop table
+    Statement stmt = con.createStatement();
+    assertNotNull("Statement is null", stmt);
+    stmt.execute("drop table " + tableName);
+    stmt.execute("drop table " + partitionedTableName);
+    stmt.execute("drop table " + dataTypeTableName);
+
+    con.close();
+    assertTrue("Connection should be closed", con.isClosed());
+
+    Exception expectedException = null;
+    try {
+      con.createStatement();
+    } catch (Exception e) {
+      expectedException = e;
+    }
+
+    assertNotNull(
+        "createStatement() on closed connection should throw exception",
+        expectedException);
+  }
+
+  @Test
+  public void testBadURL() throws Exception {
+    checkBadUrl("jdbc:hive2://localhost:10000;principal=test");
+    checkBadUrl("jdbc:hive2://localhost:10000;" +
+        "principal=hive/HiveServer2Host@YOUR-REALM.COM");
+    checkBadUrl("jdbc:hive2://localhost:10000test");
+  }
+
+
+  private void checkBadUrl(String url) throws SQLException {
+    try{
+      DriverManager.getConnection(url, "", "");
+      fail("should have thrown IllegalArgumentException but did not ");
+    }catch(IllegalArgumentException i){
+      assertTrue(i.getMessage().contains("Bad URL format. Hostname not found "
+          + " in authority part of the url"));
+    }
+  }
+
+  @Test
+  public void testParentReferences() throws Exception {
+    /* Test parent references from Statement */
+    Statement s = this.con.createStatement();
+    ResultSet rs = s.executeQuery("SELECT * FROM " + dataTypeTableName);
+
+    rs.close();
+    s.close();
+
+    assertTrue(s.getConnection() == this.con);
+    assertTrue(rs.getStatement() == s);
+
+    /* Test parent references from PreparedStatement */
+    PreparedStatement ps = this.con.prepareStatement("SELECT * FROM " + dataTypeTableName);
+    rs = ps.executeQuery();
+
+    rs.close();
+    ps.close();
+
+    assertTrue(ps.getConnection() == this.con);
+    assertTrue(rs.getStatement() == ps);
+
+    /* Test DatabaseMetaData queries which do not have a parent Statement */
+    DatabaseMetaData md = this.con.getMetaData();
+
+    assertTrue(md.getConnection() == this.con);
+
+    rs = md.getCatalogs();
+    assertNull(rs.getStatement());
+    rs.close();
+
+    rs = md.getColumns(null, null, null, null);
+    assertNull(rs.getStatement());
+    rs.close();
+
+    rs = md.getFunctions(null, null, null);
+    assertNull(rs.getStatement());
+    rs.close();
+
+    rs = md.getImportedKeys(null, null, null);
+    assertNull(rs.getStatement());
+    rs.close();
+
+    rs = md.getPrimaryKeys(null, null, null);
+    assertNull(rs.getStatement());
+    rs.close();
+
+    rs = md.getProcedureColumns(null, null, null, null);
+    assertNull(rs.getStatement());
+    rs.close();
+
+    rs = md.getProcedures(null, null, null);
+    assertNull(rs.getStatement());
+    rs.close();
+
+    rs = md.getSchemas();
+    assertNull(rs.getStatement());
+    rs.close();
+
+    rs = md.getTableTypes();
+    assertNull(rs.getStatement());
+    rs.close();
+
+    rs = md.getTables(null, null, null, null);
+    assertNull(rs.getStatement());
+    rs.close();
+
+    rs = md.getTypeInfo();
+    assertNull(rs.getStatement());
+    rs.close();
+  }
+
+  @Test
+  public void testDataTypes2() throws Exception {
+    Statement stmt = con.createStatement();
+
+    ResultSet res = stmt.executeQuery(
+        "select c5, c1 from " + dataTypeTableName + " order by c1");
+    ResultSetMetaData meta = res.getMetaData();
+
+    // row 1
+    assertTrue(res.next());
+    // skip the last (partitioning) column since it is always non-null
+    for (int i = 1; i < meta.getColumnCount(); i++) {
+      assertNull(res.getObject(i));
+    }
+
+  }
+
+  @Test
+  public void testErrorDiag() throws SQLException {
+    Statement stmt = con.createStatement();
+
+    // verify syntax error
+    try {
+      ResultSet res = stmt.executeQuery("select from " + dataTypeTableName);
+    } catch (SQLException e) {
+      assertEquals("42000", e.getSQLState());
+    }
+
+    // verify table not fuond error
+    try {
+      ResultSet res = stmt.executeQuery("select * from nonTable");
+    } catch (SQLException e) {
+      assertEquals("42S02", e.getSQLState());
+    }
+
+    // verify invalid column error
+    try {
+      ResultSet res = stmt.executeQuery("select zzzz from " + dataTypeTableName);
+    } catch (SQLException e) {
+      assertEquals("42000", e.getSQLState());
+    }
+
+  }
+
+  /**
+   * verify 'explain ...' resultset
+   * @throws SQLException
+   */
+  @Test
+  public void testExplainStmt() throws SQLException {
+    Statement stmt = con.createStatement();
+
+    ResultSet res = stmt.executeQuery(
+        "explain select c1, c2, c3, c4, c5 as a, c6, c7, c8, c9, c10, c11, c12, " +
+            "c1*2, sentences(null, null, null) as b from " + dataTypeTableName + " limit 1");
+
+    ResultSetMetaData md = res.getMetaData();
+    // only one result column
+    assertEquals(md.getColumnCount(), 1);
+    // verify the column name
+    assertEquals(md.getColumnLabel(1), EXPL_COLUMN_NAME);
+    //verify that there is data in the resultset
+    assertTrue("Nothing returned explain", res.next());
+  }
+
+  @Test
+  public void testPrepareStatement() {
+
+    String sql = "from (select count(1) from "
+        + tableName
+        + " where   'not?param?not?param' <> 'not_param??not_param' and ?=? "
+        + " and 1=? and 2=? and 3.0=? and 4.0=? and 'test\\'string\"'=? and 5=? and ?=? "
+        + " and date '2012-01-01' = date ?"
+        + " ) t  select '2011-03-25' ddate,'China',true bv, 10 num limit 10";
+
+    ///////////////////////////////////////////////
+    //////////////////// correct testcase
+    //////////////////////////////////////////////
+    try {
+      PreparedStatement ps = con.prepareStatement(sql);
+
+      ps.setBoolean(1, true);
+      ps.setBoolean(2, true);
+
+      ps.setShort(3, Short.valueOf("1"));
+      ps.setInt(4, 2);
+      ps.setFloat(5, 3f);
+      ps.setDouble(6, Double.valueOf(4));
+      ps.setString(7, "test'string\"");
+      ps.setLong(8, 5L);
+      ps.setByte(9, (byte) 1);
+      ps.setByte(10, (byte) 1);
+      ps.setString(11, "2012-01-01");
+
+      ps.setMaxRows(2);
+
+      assertTrue(true);
+
+      ResultSet res = ps.executeQuery();
+      assertNotNull(res);
+
+      while (res.next()) {
+        assertEquals("2011-03-25", res.getString("ddate"));
+        assertEquals("10", res.getString("num"));
+        assertEquals((byte) 10, res.getByte("num"));
+        assertEquals("2011-03-25", res.getDate("ddate").toString());
+        assertEquals(Double.valueOf(10).doubleValue(), res.getDouble("num"), 0.1);
+        assertEquals(10, res.getInt("num"));
+        assertEquals(Short.valueOf("10").shortValue(), res.getShort("num"));
+        assertEquals(10L, res.getLong("num"));
+        assertEquals(true, res.getBoolean("bv"));
+        Object o = res.getObject("ddate");
+        assertNotNull(o);
+        o = res.getObject("num");
+        assertNotNull(o);
+      }
+      res.close();
+      assertTrue(true);
+
+      ps.close();
+      assertTrue(true);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.toString());
+    }
+
+    ///////////////////////////////////////////////
+    //////////////////// other failure testcases
+    //////////////////////////////////////////////
+    // set nothing for prepared sql
+    Exception expectedException = null;
+    try {
+      PreparedStatement ps = con.prepareStatement(sql);
+      ps.executeQuery();
+    } catch (Exception e) {
+      expectedException = e;
+    }
+    assertNotNull(
+        "Execute the un-setted sql statement should throw exception",
+        expectedException);
+
+    // set some of parameters for prepared sql, not all of them.
+    expectedException = null;
+    try {
+      PreparedStatement ps = con.prepareStatement(sql);
+      ps.setBoolean(1, true);
+      ps.setBoolean(2, true);
+      ps.executeQuery();
+    } catch (Exception e) {
+      expectedException = e;
+    }
+    assertNotNull(
+        "Execute the invalid setted sql statement should throw exception",
+        expectedException);
+
+    // set the wrong type parameters for prepared sql.
+    expectedException = null;
+    try {
+      PreparedStatement ps = con.prepareStatement(sql);
+
+      // wrong type here
+      ps.setString(1, "wrong");
+
+      assertTrue(true);
+      ResultSet res = ps.executeQuery();
+      if (!res.next()) {
+        throw new Exception("there must be a empty result set");
+      }
+    } catch (Exception e) {
+      expectedException = e;
+    }
+    assertNotNull(
+        "Execute the invalid setted sql statement should throw exception",
+        expectedException);
+  }
+
+  /**
+   * Execute non-select statements using execute() and executeUpdated() APIs
+   * of PreparedStatement interface
+   * @throws Exception
+   */
+  @Test
+  public void testExecutePreparedStatement() throws Exception {
+    String key = "testKey";
+    String val1 = "val1";
+    String val2 = "val2";
+    PreparedStatement ps = con.prepareStatement("set " + key + " = ?");
+
+    // execute() of Prepared statement
+    ps.setString(1, val1);
+    ps.execute();
+    verifyConfValue(key, val1);
+
+    // executeUpdate() of Prepared statement
+    ps.clearParameters();
+    ps.setString(1, val2);
+    ps.executeUpdate();
+    verifyConfValue(key, val2);
+  }
+
+  /**
+   * Execute "set x" and extract value from key=val format result
+   * Verify the extracted value
+   * @param stmt
+   * @return
+   * @throws Exception
+   */
+  private void verifyConfValue(String key, String expectedVal) throws Exception {
+    Statement stmt = con.createStatement();
+    ResultSet res = stmt.executeQuery("set " + key);
+    assertTrue(res.next());
+    String resultValues[] = res.getString(1).split("="); // "key = 'val'"
+    assertEquals("Result not in key = val format", 2, resultValues.length);
+    String result = resultValues[1].substring(1, resultValues[1].length() -1); // remove '
+    assertEquals("Conf value should be set by execute()", expectedVal, result);
+  }
+
+  @Test
+  public final void testSelectAll() throws Exception {
+    doTestSelectAll(tableName, -1, -1); // tests not setting maxRows (return all)
+    doTestSelectAll(tableName, 0, -1); // tests setting maxRows to 0 (return all)
+  }
+
+  @Test
+  public final void testSelectAllPartioned() throws Exception {
+    doTestSelectAll(partitionedTableName, -1, -1); // tests not setting maxRows
+    // (return all)
+    doTestSelectAll(partitionedTableName, 0, -1); // tests setting maxRows to 0
+    // (return all)
+  }
+
+  @Test
+  public final void testSelectAllMaxRows() throws Exception {
+    doTestSelectAll(tableName, 100, -1);
+  }
+
+  @Test
+  public final void testSelectAllFetchSize() throws Exception {
+    doTestSelectAll(tableName, 100, 20);
+  }
+
+  @Test
+  public void testNullType() throws Exception {
+    Statement stmt = con.createStatement();
+    try {
+      ResultSet res = stmt.executeQuery("select null from " + dataTypeTableName);
+      assertTrue(res.next());
+      assertNull(res.getObject(1));
+    } finally {
+      stmt.close();
+    }
+  }
+
+  // executeQuery should always throw a SQLException,
+  // when it executes a non-ResultSet query (like create)
+  @Test
+  public void testExecuteQueryException() throws Exception {
+    Statement stmt = con.createStatement();
+    try {
+      stmt.executeQuery("create table test_t2 (under_col int, value string)");
+      fail("Expecting SQLException");
+    }
+    catch (SQLException e) {
+      System.out.println("Caught an expected SQLException: " + e.getMessage());
+    }
+    finally {
+      stmt.close();
+    }
+  }
+
+  private void checkResultSetExpected(Statement stmt, List<String> setupQueries, String testQuery,
+      boolean isExpectedResultSet) throws Exception {
+    boolean hasResultSet;
+    // execute the setup queries
+    for(String setupQuery: setupQueries) {
+      try {
+        stmt.execute(setupQuery);
+      } catch (Exception e) {
+        failWithExceptionMsg(e);
+      }
+    }
+    // execute the test query
+    try {
+      hasResultSet = stmt.execute(testQuery);
+      assertEquals(hasResultSet, isExpectedResultSet);
+    }
+    catch(Exception e) {
+      failWithExceptionMsg(e);
+    }
+  }
+
+  private void failWithExceptionMsg(Exception e) {
+    e.printStackTrace();
+    fail(e.toString());
+  }
+
+  @Test
+  public void testNullResultSet() throws Exception {
+    List<String> setupQueries = new ArrayList<String>();
+    String testQuery;
+    Statement stmt = con.createStatement();
+
+    // -select- should return a ResultSet
+    try {
+      stmt.executeQuery("select * from " + tableName);
+      System.out.println("select: success");
+    } catch(SQLException e) {
+      failWithExceptionMsg(e);
+    }
+
+    // -create- should not return a ResultSet
+    setupQueries.add("drop table test_t1");
+    testQuery = "create table test_t1 (under_col int, value string)";
+    checkResultSetExpected(stmt, setupQueries, testQuery, false);
+    setupQueries.clear();
+
+    // -create table as select- should not return a ResultSet
+    setupQueries.add("drop table test_t1");
+    testQuery = "create table test_t1 as select * from " + tableName;
+    checkResultSetExpected(stmt, setupQueries, testQuery, false);
+    setupQueries.clear();
+
+    // -insert table as select- should not return a ResultSet
+    setupQueries.add("drop table test_t1");
+    setupQueries.add("create table test_t1 (under_col int, value string)");
+    testQuery = "insert into table test_t1 select under_col, value from "  + tableName;
+    checkResultSetExpected(stmt, setupQueries, testQuery, false);
+    setupQueries.clear();
+
+    stmt.close();
+  }
+
+  @Test
+  public void testCloseResultSet() throws Exception {
+    Statement stmt = con.createStatement();
+
+    // execute query, ignore exception if any
+    ResultSet res = stmt.executeQuery("select * from " + tableName);
+    // close ResultSet, ignore exception if any
+    res.close();
+    // A statement should be open even after ResultSet#close
+    assertFalse(stmt.isClosed());
+    // A Statement#cancel after ResultSet#close should be a no-op
+    try {
+      stmt.cancel();
+    } catch(SQLException e) {
+      failWithExceptionMsg(e);
+    }
+    stmt.close();
+
+    stmt = con.createStatement();
+    // execute query, ignore exception if any
+    res = stmt.executeQuery("select * from " + tableName);
+    // close ResultSet, ignore exception if any
+    res.close();
+    // A Statement#execute after ResultSet#close should be fine too
+    try {
+      stmt.executeQuery("select * from " + tableName);
+    } catch(SQLException e) {
+      failWithExceptionMsg(e);
+    }
+    // A Statement#close after ResultSet#close should close the statement
+    stmt.close();
+    assertTrue(stmt.isClosed());
+  }
+
+  @Test
+  public void testDataTypes() throws Exception {
+    Statement stmt = con.createStatement();
+
+    ResultSet res = stmt.executeQuery(
+        "select * from " + dataTypeTableName + " order by c1");
+    ResultSetMetaData meta = res.getMetaData();
+
+    // row 1
+    assertTrue(res.next());
+    // skip the last (partitioning) column since it is always non-null
+    for (int i = 1; i < meta.getColumnCount(); i++) {
+      assertNull(res.getObject(i));
+    }
+    // getXXX returns 0 for numeric types, false for boolean and null for other
+    assertEquals(0, res.getInt(1));
+    assertEquals(false, res.getBoolean(2));
+    assertEquals(0d, res.getDouble(3), floatCompareDelta);
+    assertEquals(null, res.getString(4));
+    assertEquals(null, res.getString(5));
+    assertEquals(null, res.getString(6));
+    assertEquals(null, res.getString(7));
+    assertEquals(null, res.getString(8));
+    assertEquals(0, res.getByte(9));
+    assertEquals(0, res.getShort(10));
+    assertEquals(0f, res.getFloat(11), floatCompareDelta);
+    assertEquals(0L, res.getLong(12));
+    assertEquals(null, res.getString(13));
+    assertEquals(null, res.getString(14));
+    assertEquals(null, res.getString(15));
+    assertEquals(null, res.getString(16));
+    assertEquals(null, res.getString(17));
+    assertEquals(null, res.getString(18));
+    assertEquals(null, res.getString(19));
+    assertEquals(null, res.getString(20));
+    assertEquals(null, res.getDate(20));
+    assertEquals(null, res.getString(21));
+
+    // row 2
+    assertTrue(res.next());
+    assertEquals(-1, res.getInt(1));
+    assertEquals(false, res.getBoolean(2));
+    assertEquals(-1.1d, res.getDouble(3), floatCompareDelta);
+    assertEquals("", res.getString(4));
+    assertEquals("[]", res.getString(5));
+    assertEquals("{}", res.getString(6));
+    assertEquals("{}", res.getString(7));
+    assertEquals("{\"r\":null,\"s\":null,\"t\":null}", res.getString(8));
+    assertEquals(-1, res.getByte(9));
+    assertEquals(-1, res.getShort(10));
+    assertEquals(-1.0f, res.getFloat(11), floatCompareDelta);
+    assertEquals(-1, res.getLong(12));
+    assertEquals("[]", res.getString(13));
+    assertEquals("{}", res.getString(14));
+    assertEquals("{\"r\":null,\"s\":null}", res.getString(15));
+    assertEquals("[]", res.getString(16));
+    assertEquals(null, res.getString(17));
+    assertEquals(null, res.getTimestamp(17));
+    assertEquals(null, res.getBigDecimal(18));
+    assertEquals(null, res.getString(19));
+    assertEquals(null, res.getString(20));
+    assertEquals(null, res.getDate(20));
+    assertEquals(null, res.getString(21));
+
+    // row 3
+    assertTrue(res.next());
+    assertEquals(1, res.getInt(1));
+    assertEquals(true, res.getBoolean(2));
+    assertEquals(1.1d, res.getDouble(3), floatCompareDelta);
+    assertEquals("1", res.getString(4));
+    assertEquals("[1,2]", res.getString(5));
+    assertEquals("{1:\"x\",2:\"y\"}", res.getString(6));
+    assertEquals("{\"k\":\"v\"}", res.getString(7));
+    assertEquals("{\"r\":\"a\",\"s\":9,\"t\":2.2}", res.getString(8));
+    assertEquals(1, res.getByte(9));
+    assertEquals(1, res.getShort(10));
+    assertEquals(1.0f, res.getFloat(11), floatCompareDelta);
+    assertEquals(1, res.getLong(12));
+    assertEquals("[[\"a\",\"b\"],[\"c\",\"d\"]]", res.getString(13));
+    assertEquals("{1:{11:12,13:14},2:{21:22}}", res.getString(14));
+    assertEquals("{\"r\":1,\"s\":{\"a\":2,\"b\":\"x\"}}", res.getString(15));
+    assertEquals("[{\"m\":{},\"n\":1},{\"m\":{\"a\":\"b\",\"c\":\"d\"},\"n\":2}]", res.getString(16));
+    assertEquals("2012-04-22 09:00:00.123456789", res.getString(17));
+    assertEquals("2012-04-22 09:00:00.123456789", res.getTimestamp(17).toString());
+    assertEquals("123456789.0123456", res.getBigDecimal(18).toString());
+    assertEquals("abcd", res.getString(19));
+    assertEquals("2013-01-01", res.getString(20));
+    assertEquals("2013-01-01", res.getDate(20).toString());
+    assertEquals("abc123", res.getString(21));
+
+    // test getBoolean rules on non-boolean columns
+    assertEquals(true, res.getBoolean(1));
+    assertEquals(true, res.getBoolean(4));
+
+    // test case sensitivity
+    assertFalse(meta.isCaseSensitive(1));
+    assertFalse(meta.isCaseSensitive(2));
+    assertFalse(meta.isCaseSensitive(3));
+    assertTrue(meta.isCaseSensitive(4));
+
+    // no more rows
+    assertFalse(res.next());
+  }
+
+  private void doTestSelectAll(String tableName, int maxRows, int fetchSize) throws Exception {
+    boolean isPartitionTable = tableName.equals(partitionedTableName);
+
+    Statement stmt = con.createStatement();
+    if (maxRows >= 0) {
+      stmt.setMaxRows(maxRows);
+    }
+    if (fetchSize > 0) {
+      stmt.setFetchSize(fetchSize);
+      assertEquals(fetchSize, stmt.getFetchSize());
+    }
+
+    // JDBC says that 0 means return all, which is the default
+    int expectedMaxRows = maxRows < 1 ? 0 : maxRows;
+
+    assertNotNull("Statement is null", stmt);
+    assertEquals("Statement max rows not as expected", expectedMaxRows, stmt
+        .getMaxRows());
+    assertFalse("Statement should not be closed", stmt.isClosed());
+
+    ResultSet res;
+
+    // run some queries
+    res = stmt.executeQuery("select * from " + tableName);
+    assertNotNull("ResultSet is null", res);
+    assertTrue("getResultSet() not returning expected ResultSet", res == stmt
+        .getResultSet());
+    assertEquals("get update count not as expected", 0, stmt.getUpdateCount());
+    int i = 0;
+
+    ResultSetMetaData meta = res.getMetaData();
+    int expectedColCount = isPartitionTable ? 3 : 2;
+    assertEquals(
+        "Unexpected column count", expectedColCount, meta.getColumnCount());
+
+    boolean moreRow = res.next();
+    while (moreRow) {
+      try {
+        i++;
+        assertEquals(res.getInt(1), res.getInt("under_col"));
+        assertEquals(res.getString(1), res.getString("under_col"));
+        assertEquals(res.getString(2), res.getString("value"));
+        if (isPartitionTable) {
+          assertEquals(res.getString(3), partitionedColumnValue);
+          assertEquals(res.getString(3), res.getString(partitionedColumnName));
+        }
+        assertFalse("Last result value was not null", res.wasNull());
+        assertNull("No warnings should be found on ResultSet", res
+            .getWarnings());
+        res.clearWarnings(); // verifying that method is supported
+
+        // System.out.println(res.getString(1) + " " + res.getString(2));
+        assertEquals(
+            "getInt and getString don't align for the same result value",
+            String.valueOf(res.getInt(1)), res.getString(1));
+        assertEquals("Unexpected result found", "val_" + res.getString(1), res
+            .getString(2));
+        moreRow = res.next();
+      } catch (SQLException e) {
+        System.out.println(e.toString());
+        e.printStackTrace();
+        throw new Exception(e.toString());
+      }
+    }
+
+    // supposed to get 500 rows if maxRows isn't set
+    int expectedRowCount = maxRows > 0 ? maxRows : 500;
+    assertEquals("Incorrect number of rows returned", expectedRowCount, i);
+
+    // should have no more rows
+    assertEquals(false, moreRow);
+
+    assertNull("No warnings should be found on statement", stmt.getWarnings());
+    stmt.clearWarnings(); // verifying that method is supported
+
+    assertNull("No warnings should be found on connection", con.getWarnings());
+    con.clearWarnings(); // verifying that method is supported
+
+    stmt.close();
+    assertTrue("Statement should be closed", stmt.isClosed());
+  }
+
+  @Test
+  public void testErrorMessages() throws SQLException {
+    String invalidSyntaxSQLState = "42000";
+
+    // These tests inherently cause exceptions to be written to the test output
+    // logs. This is undesirable, since you it might appear to someone looking
+    // at the test output logs as if something is failing when it isn't. Not
+    // sure
+    // how to get around that.
+    doTestErrorCase("SELECTT * FROM " + tableName,
+        "cannot recognize input near 'SELECTT' '*' 'FROM'",
+        invalidSyntaxSQLState, 40000);
+    doTestErrorCase("SELECT * FROM some_table_that_does_not_exist",
+        "Table not found", "42S02", 10001);
+    doTestErrorCase("drop table some_table_that_does_not_exist",
+        "Table not found", "42S02", 10001);
+    doTestErrorCase("SELECT invalid_column FROM " + tableName,
+        "Invalid table alias or column reference", invalidSyntaxSQLState, 10004);
+    doTestErrorCase("SELECT invalid_function(under_col) FROM " + tableName,
+        "Invalid function", invalidSyntaxSQLState, 10011);
+
+    // TODO: execute errors like this currently don't return good error
+    // codes and messages. This should be fixed.
+    doTestErrorCase(
+        "create table " + tableName + " (key int, value string)",
+        "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask",
+        "08S01", 1);
+  }
+
+  private void doTestErrorCase(String sql, String expectedMessage,
+      String expectedSQLState, int expectedErrorCode) throws SQLException {
+    Statement stmt = con.createStatement();
+    boolean exceptionFound = false;
+    try {
+      stmt.execute(sql);
+    } catch (SQLException e) {
+      assertTrue("Adequate error messaging not found for '" + sql + "': "
+          + e.getMessage(), e.getMessage().contains(expectedMessage));
+      assertEquals("Expected SQLState not found for '" + sql + "'",
+          expectedSQLState, e.getSQLState());
+      assertEquals("Expected error code not found for '" + sql + "'",
+          expectedErrorCode, e.getErrorCode());
+      exceptionFound = true;
+    }
+
+    assertNotNull("Exception should have been thrown for query: " + sql,
+        exceptionFound);
+  }
+
+  @Test
+  public void testShowTables() throws SQLException {
+    Statement stmt = con.createStatement();
+    assertNotNull("Statement is null", stmt);
+
+    ResultSet res = stmt.executeQuery("show tables");
+
+    boolean testTableExists = false;
+    while (res.next()) {
+      assertNotNull("table name is null in result set", res.getString(1));
+      if (tableName.equalsIgnoreCase(res.getString(1))) {
+        testTableExists = true;
+      }
+    }
+
+    assertTrue("table name " + tableName
+        + " not found in SHOW TABLES result set", testTableExists);
+  }
+
+  @Test
+  public void testMetaDataGetTables() throws SQLException {
+    getTablesTest(ClassicTableTypes.TABLE.toString(), ClassicTableTypes.VIEW.toString());
+  }
+
+  @Test
+  public  void testMetaDataGetTablesHive() throws SQLException {
+    Statement stmt = con.createStatement();
+    stmt.execute("set " + HiveConf.ConfVars.HIVE_SERVER2_TABLE_TYPE_MAPPING.varname +
+        " = " + TableTypeMappings.HIVE.toString());
+    getTablesTest(TableType.MANAGED_TABLE.toString(), TableType.VIRTUAL_VIEW.toString());
+  }
+
+  @Test
+  public  void testMetaDataGetTablesClassic() throws SQLException {
+    Statement stmt = con.createStatement();
+    stmt.execute("set " + HiveConf.ConfVars.HIVE_SERVER2_TABLE_TYPE_MAPPING.varname +
+        " = " + TableTypeMappings.CLASSIC.toString());
+    stmt.close();
+    getTablesTest(ClassicTableTypes.TABLE.toString(), ClassicTableTypes.VIEW.toString());
+  }
+
+  /**
+   * Test the type returned for pre-created table type table and view type
+   * table
+   * @param tableTypeName expected table type
+   * @param viewTypeName expected view type
+   * @throws SQLException
+   */
+  private void getTablesTest(String tableTypeName, String viewTypeName) throws SQLException {
+    Map<String, Object[]> tests = new HashMap<String, Object[]>();
+    tests.put("test%jdbc%", new Object[]{"testhivejdbcdriver_table"
+        , "testhivejdbcdriverpartitionedtable"
+        , "testhivejdbcdriverview"});
+    tests.put("%jdbcdriver\\_table", new Object[]{"testhivejdbcdriver_table"});
+    tests.put("testhivejdbcdriver\\_table", new Object[]{"testhivejdbcdriver_table"});
+    tests.put("test_ivejdbcdri_er\\_table", new Object[]{"testhivejdbcdriver_table"});
+    tests.put("test_ivejdbcdri_er_table", new Object[]{"testhivejdbcdriver_table"});
+    tests.put("test_ivejdbcdri_er%table", new Object[]{
+        "testhivejdbcdriver_table", "testhivejdbcdriverpartitionedtable" });
+    tests.put("%jdbc%", new Object[]{ "testhivejdbcdriver_table"
+        , "testhivejdbcdriverpartitionedtable"
+        , "testhivejdbcdriverview"});
+    tests.put("", new Object[]{});
+
+    for (String checkPattern: tests.keySet()) {
+      ResultSet rs = (ResultSet)con.getMetaData().getTables("default", null, checkPattern, null);
+      ResultSetMetaData resMeta = rs.getMetaData();
+      assertEquals(5, resMeta.getColumnCount());
+      assertEquals("TABLE_CAT", resMeta.getColumnName(1));
+      assertEquals("TABLE_SCHEM", resMeta.getColumnName(2));
+      assertEquals("TABLE_NAME", resMeta.getColumnName(3));
+      assertEquals("TABLE_TYPE", resMeta.getColumnName(4));
+      assertEquals("REMARKS", resMeta.getColumnName(5));
+
+      int cnt = 0;
+      while (rs.next()) {
+        String resultTableName = rs.getString("TABLE_NAME");
+        assertEquals("Get by index different from get by name.", rs.getString(3), resultTableName);
+        assertEquals("Excpected a different table.", tests.get(checkPattern)[cnt], resultTableName);
+        String resultTableComment = rs.getString("REMARKS");
+        assertTrue("Missing comment on the table.", resultTableComment.length()>0);
+        String tableType = rs.getString("TABLE_TYPE");
+        if (resultTableName.endsWith("view")) {
+          assertEquals("Expected a tabletype view but got something else.", viewTypeName, tableType);
+        } else {
+          assertEquals("Expected a tabletype table but got something else.", tableTypeName, tableType);
+        }
+        cnt++;
+      }
+      rs.close();
+      assertEquals("Received an incorrect number of tables.", tests.get(checkPattern).length, cnt);
+    }
+
+    // only ask for the views.
+    ResultSet rs = (ResultSet)con.getMetaData().getTables("default", null, null
+        , new String[]{viewTypeName});
+    int cnt=0;
+    while (rs.next()) {
+      cnt++;
+    }
+    rs.close();
+    assertEquals("Incorrect number of views found.", 1, cnt);
+  }
+
+  @Test
+  public void testMetaDataGetCatalogs() throws SQLException {
+    ResultSet rs = (ResultSet)con.getMetaData().getCatalogs();
+    ResultSetMetaData resMeta = rs.getMetaData();
+    assertEquals(1, resMeta.getColumnCount());
+    assertEquals("TABLE_CAT", resMeta.getColumnName(1));
+
+    assertFalse(rs.next());
+  }
+
+  @Test
+  public void testMetaDataGetSchemas() throws SQLException {
+    ResultSet rs = (ResultSet)con.getMetaData().getSchemas();
+    ResultSetMetaData resMeta = rs.getMetaData();
+    assertEquals(2, resMeta.getColumnCount());
+    assertEquals("TABLE_SCHEM", resMeta.getColumnName(1));
+    assertEquals("TABLE_CATALOG", resMeta.getColumnName(2));
+
+    assertTrue(rs.next());
+    assertEquals("default", rs.getString(1));
+
+    assertFalse(rs.next());
+    rs.close();
+  }
+
+  // test default table types returned in
+  // Connection.getMetaData().getTableTypes()
+  @Test
+  public void testMetaDataGetTableTypes() throws SQLException {
+    metaDataGetTableTypeTest(new ClassicTableTypeMapping().getTableTypeNames());
+  }
+
+  // test default table types returned in
+  // Connection.getMetaData().getTableTypes() when type config is set to "HIVE"
+  @Test
+  public void testMetaDataGetHiveTableTypes() throws SQLException {
+    Statement stmt = con.createStatement();
+    stmt.execute("set " + HiveConf.ConfVars.HIVE_SERVER2_TABLE_TYPE_MAPPING.varname +
+        " = " + TableTypeMappings.HIVE.toString());
+    stmt.close();
+    metaDataGetTableTypeTest(new HiveTableTypeMapping().getTableTypeNames());
+  }
+
+  // test default table types returned in
+  // Connection.getMetaData().getTableTypes() when type config is set to "CLASSIC"
+  @Test
+  public void testMetaDataGetClassicTableTypes() throws SQLException {
+    Statement stmt = con.createStatement();
+    stmt.execute("set " + HiveConf.ConfVars.HIVE_SERVER2_TABLE_TYPE_MAPPING.varname +
+        " = " + TableTypeMappings.CLASSIC.toString());
+    stmt.close();
+    metaDataGetTableTypeTest(new ClassicTableTypeMapping().getTableTypeNames());
+  }
+
+  /**
+   * Test if Connection.getMetaData().getTableTypes() returns expected
+   *  tabletypes
+   * @param tabletypes expected table types
+   * @throws SQLException
+   */
+  private void metaDataGetTableTypeTest(Set<String> tabletypes)
+      throws SQLException {
+    ResultSet rs = (ResultSet)con.getMetaData().getTableTypes();
+
+    int cnt = 0;
+    while (rs.next()) {
+      String tabletype = rs.getString("TABLE_TYPE");
+      assertEquals("Get by index different from get by name", rs.getString(1), tabletype);
+      tabletypes.remove(tabletype);
+      cnt++;
+    }
+    rs.close();
+    assertEquals("Incorrect tabletype count.", 0, tabletypes.size());
+    assertTrue("Found less tabletypes then we test for.", cnt >= tabletypes.size());
+  }
+
+  @Test
+  public void testMetaDataGetColumns() throws SQLException {
+    Map<String[], Integer> tests = new HashMap<String[], Integer>();
+    tests.put(new String[]{"testhivejdbcdriver\\_table", null}, 2);
+    tests.put(new String[]{"testhivejdbc%", null}, 7);
+    tests.put(new String[]{"testhiveJDBC%", null}, 7);
+    tests.put(new String[]{"%jdbcdriver\\_table", null}, 2);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_col"}, 1);
+    //    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_COL"}, 1);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_co_"}, 1);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "under_col"}, 1);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "und%"}, 1);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "%"}, 2);
+    tests.put(new String[]{"%jdbcdriver\\_table%", "_%"}, 2);
+
+    for (String[] checkPattern: tests.keySet()) {
+      ResultSet rs = con.getMetaData().getColumns(null, null, checkPattern[0],
+          checkPattern[1]);
+
+      // validate the metadata for the getColumns result set
+      ResultSetMetaData rsmd = rs.getMetaData();
+      assertEquals("TABLE_CAT", rsmd.getColumnName(1));
+
+      int cnt = 0;
+      while (rs.next()) {
+        String columnname = rs.getString("COLUMN_NAME");
+        int ordinalPos = rs.getInt("ORDINAL_POSITION");
+        switch(cnt) {
+        case 0:
+          assertEquals("Wrong column name found", "under_col", columnname);
+          assertEquals("Wrong ordinal position found", ordinalPos, 1);
+          break;
+        case 1:
+          assertEquals("Wrong column name found", "value", columnname);
+          assertEquals("Wrong ordinal position found", ordinalPos, 2);
+          break;
+        default:
+          break;
+        }
+        cnt++;
+      }
+      rs.close();
+      assertEquals("Found less columns then we test for.", tests.get(checkPattern).intValue(), cnt);
+    }
+  }
+
+  /**
+   * Validate the Metadata for the result set of a metadata getColumns call.
+   */
+  @Test
+  public void testMetaDataGetColumnsMetaData() throws SQLException {
+    ResultSet rs = (ResultSet)con.getMetaData().getColumns(null, null
+        , "testhivejdbcdriver\\_table", null);
+
+    ResultSetMetaData rsmd = rs.getMetaData();
+
+    assertEquals("TABLE_CAT", rsmd.getColumnName(1));
+    assertEquals(Types.VARCHAR, rsmd.getColumnType(1));
+    assertEquals(Integer.MAX_VALUE, rsmd.getColumnDisplaySize(1));
+
+    assertEquals("ORDINAL_POSITION", rsmd.getColumnName(17));
+    assertEquals(Types.INTEGER, rsmd.getColumnType(17));
+    assertEquals(11, rsmd.getColumnDisplaySize(17));
+  }
+
+  /*
+  public void testConversionsBaseResultSet() throws SQLException {
+    ResultSet rs = new HiveMetaDataResultSet(Arrays.asList("key")
+            , Arrays.asList("long")
+            , Arrays.asList(1234, "1234", "abc")) {
+      private int cnt=1;
+      public boolean next() throws SQLException {
+        if (cnt<data.size()) {
+          row = Arrays.asList(data.get(cnt));
+          cnt++;
+          return true;
+        } else {
+          return false;
+        }
+      }
+    };
+
+    while (rs.next()) {
+      String key = rs.getString("key");
+      if ("1234".equals(key)) {
+        assertEquals("Converting a string column into a long failed.", rs.getLong("key"), 1234L);
+        assertEquals("Converting a string column into a int failed.", rs.getInt("key"), 1234);
+      } else if ("abc".equals(key)) {
+        Object result = null;
+        Exception expectedException = null;
+        try {
+          result = rs.getLong("key");
+        } catch (SQLException e) {
+          expectedException = e;
+        }
+        assertTrue("Trying to convert 'abc' into a long should not work.", expectedException!=null);
+        try {
+          result = rs.getInt("key");
+        } catch (SQLException e) {
+          expectedException = e;
+        }
+        assertTrue("Trying to convert 'abc' into a int should not work.", expectedException!=null);
+      }
+    }
+  }
+   */
+
+  @Test
+  public void testDescribeTable() throws SQLException {
+    Statement stmt = con.createStatement();
+    assertNotNull("Statement is null", stmt);
+
+    ResultSet res = stmt.executeQuery("describe " + tableName);
+
+    res.next();
+    assertEquals("Column name 'under_col' not found", "under_col", res.getString(1).trim());
+    assertEquals("Column type 'under_col' for column under_col not found", "int", res
+        .getString(2).trim());
+    res.next();
+    assertEquals("Column name 'value' not found", "value", res.getString(1).trim());
+    assertEquals("Column type 'string' for column key not found", "string", res
+        .getString(2).trim());
+
+    assertFalse("More results found than expected", res.next());
+  }
+
+  @Test
+  public void testDatabaseMetaData() throws SQLException {
+    DatabaseMetaData meta = con.getMetaData();
+
+    assertEquals("Hive", meta.getDatabaseProductName());
+    assertEquals(HiveVersionInfo.getVersion(), meta.getDatabaseProductVersion());
+    assertEquals(System.getProperty("hive.version"), meta.getDatabaseProductVersion());
+    assertTrue("verifying hive version pattern. got " + meta.getDatabaseProductVersion(),
+        Pattern.matches("\\d+\\.\\d+\\.\\d+.*", meta.getDatabaseProductVersion()) );
+
+    assertEquals(DatabaseMetaData.sqlStateSQL99, meta.getSQLStateType());
+    assertFalse(meta.supportsCatalogsInTableDefinitions());
+    assertFalse(meta.supportsSchemasInTableDefinitions());
+    assertFalse(meta.supportsSchemasInDataManipulation());
+    assertFalse(meta.supportsMultipleResultSets());
+    assertFalse(meta.supportsStoredProcedures());
+    assertTrue(meta.supportsAlterTableWithAddColumn());
+  }
+
+  @Test
+  public void testResultSetMetaData() throws SQLException {
+    Statement stmt = con.createStatement();
+
+    ResultSet res = stmt.executeQuery(
+        "select c1, c2, c3, c4, c5 as a, c6, c7, c8, c9, c10, c11, c12, " +
+            "c1*2, sentences(null, null, null) as b, c17, c18, c20, c21 from " + dataTypeTableName +
+        " limit 1");
+    ResultSetMetaData meta = res.getMetaData();
+
+    ResultSet colRS = con.getMetaData().getColumns(null, null,
+        dataTypeTableName.toLowerCase(), null);
+
+    assertEquals(18, meta.getColumnCount());
+
+    assertTrue(colRS.next());
+
+    assertEquals("c1", meta.getColumnName(1));
+    assertEquals(Types.INTEGER, meta.getColumnType(1));
+    assertEquals("int", meta.getColumnTypeName(1));
+    assertEquals(11, meta.getColumnDisplaySize(1));
+    assertEquals(10, meta.getPrecision(1));
+    assertEquals(0, meta.getScale(1));
+
+    assertEquals("c1", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.INTEGER, colRS.getInt("DATA_TYPE"));
+    assertEquals("int", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(1), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(1), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c2", meta.getColumnName(2));
+    assertEquals("boolean", meta.getColumnTypeName(2));
+    assertEquals(Types.BOOLEAN, meta.getColumnType(2));
+    assertEquals(1, meta.getColumnDisplaySize(2));
+    assertEquals(1, meta.getPrecision(2));
+    assertEquals(0, meta.getScale(2));
+
+    assertEquals("c2", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.BOOLEAN, colRS.getInt("DATA_TYPE"));
+    assertEquals("boolean", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getScale(2), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c3", meta.getColumnName(3));
+    assertEquals(Types.DOUBLE, meta.getColumnType(3));
+    assertEquals("double", meta.getColumnTypeName(3));
+    assertEquals(25, meta.getColumnDisplaySize(3));
+    assertEquals(15, meta.getPrecision(3));
+    assertEquals(15, meta.getScale(3));
+
+    assertEquals("c3", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.DOUBLE, colRS.getInt("DATA_TYPE"));
+    assertEquals("double", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(3), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(3), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c4", meta.getColumnName(4));
+    assertEquals(Types.VARCHAR, meta.getColumnType(4));
+    assertEquals("string", meta.getColumnTypeName(4));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(4));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(4));
+    assertEquals(0, meta.getScale(4));
+
+    assertEquals("c4", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
+    assertEquals("string", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(4), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(4), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("a", meta.getColumnName(5));
+    assertEquals(Types.VARCHAR, meta.getColumnType(5));
+    assertEquals("string", meta.getColumnTypeName(5));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(5));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(5));
+    assertEquals(0, meta.getScale(5));
+
+    assertEquals("c5", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
+    assertEquals("array<int>", colRS.getString("TYPE_NAME").toLowerCase());
+
+    assertTrue(colRS.next());
+
+    assertEquals("c6", meta.getColumnName(6));
+    assertEquals(Types.VARCHAR, meta.getColumnType(6));
+    assertEquals("string", meta.getColumnTypeName(6));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(6));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(6));
+    assertEquals(0, meta.getScale(6));
+
+    assertEquals("c6", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
+    assertEquals("map<int,string>", colRS.getString("TYPE_NAME").toLowerCase());
+
+    assertTrue(colRS.next());
+
+    assertEquals("c7", meta.getColumnName(7));
+    assertEquals(Types.VARCHAR, meta.getColumnType(7));
+    assertEquals("string", meta.getColumnTypeName(7));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(7));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(7));
+    assertEquals(0, meta.getScale(7));
+
+    assertEquals("c7", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
+    assertEquals("map<string,string>", colRS.getString("TYPE_NAME").toLowerCase());
+
+    assertTrue(colRS.next());
+
+    assertEquals("c8", meta.getColumnName(8));
+    assertEquals(Types.VARCHAR, meta.getColumnType(8));
+    assertEquals("string", meta.getColumnTypeName(8));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(8));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(8));
+    assertEquals(0, meta.getScale(8));
+
+    assertEquals("c8", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
+    assertEquals("struct<r:string,s:int,t:double>", colRS.getString("TYPE_NAME").toLowerCase());
+
+    assertTrue(colRS.next());
+
+    assertEquals("c9", meta.getColumnName(9));
+    assertEquals(Types.TINYINT, meta.getColumnType(9));
+    assertEquals("tinyint", meta.getColumnTypeName(9));
+    assertEquals(4, meta.getColumnDisplaySize(9));
+    assertEquals(3, meta.getPrecision(9));
+    assertEquals(0, meta.getScale(9));
+
+    assertEquals("c9", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.TINYINT, colRS.getInt("DATA_TYPE"));
+    assertEquals("tinyint", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(9), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(9), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c10", meta.getColumnName(10));
+    assertEquals(Types.SMALLINT, meta.getColumnType(10));
+    assertEquals("smallint", meta.getColumnTypeName(10));
+    assertEquals(6, meta.getColumnDisplaySize(10));
+    assertEquals(5, meta.getPrecision(10));
+    assertEquals(0, meta.getScale(10));
+
+    assertEquals("c10", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.SMALLINT, colRS.getInt("DATA_TYPE"));
+    assertEquals("smallint", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(10), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(10), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c11", meta.getColumnName(11));
+    assertEquals(Types.FLOAT, meta.getColumnType(11));
+    assertEquals("float", meta.getColumnTypeName(11));
+    assertEquals(24, meta.getColumnDisplaySize(11));
+    assertEquals(7, meta.getPrecision(11));
+    assertEquals(7, meta.getScale(11));
+
+    assertEquals("c11", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.FLOAT, colRS.getInt("DATA_TYPE"));
+    assertEquals("float", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(11), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(11), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertTrue(colRS.next());
+
+    assertEquals("c12", meta.getColumnName(12));
+    assertEquals(Types.BIGINT, meta.getColumnType(12));
+    assertEquals("bigint", meta.getColumnTypeName(12));
+    assertEquals(20, meta.getColumnDisplaySize(12));
+    assertEquals(19, meta.getPrecision(12));
+    assertEquals(0, meta.getScale(12));
+
+    assertEquals("c12", colRS.getString("COLUMN_NAME"));
+    assertEquals(Types.BIGINT, colRS.getInt("DATA_TYPE"));
+    assertEquals("bigint", colRS.getString("TYPE_NAME").toLowerCase());
+    assertEquals(meta.getPrecision(12), colRS.getInt("COLUMN_SIZE"));
+    assertEquals(meta.getScale(12), colRS.getInt("DECIMAL_DIGITS"));
+
+    assertEquals("_c12", meta.getColumnName(13));
+    assertEquals(Types.INTEGER, meta.getColumnType(13));
+    assertEquals("int", meta.getColumnTypeName(13));
+    assertEquals(11, meta.getColumnDisplaySize(13));
+    assertEquals(10, meta.getPrecision(13));
+    assertEquals(0, meta.getScale(13));
+
+    assertEquals("b", meta.getColumnName(14));
+    assertEquals(Types.VARCHAR, meta.getColumnType(14));
+    assertEquals("string", meta.getColumnTypeName(14));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(14));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(14));
+    assertEquals(0, meta.getScale(14));
+
+    assertEquals("c17", meta.getColumnName(15));
+    assertEquals(Types.TIMESTAMP, meta.getColumnType(15));
+    assertEquals("timestamp", meta.getColumnTypeName(15));
+    assertEquals(29, meta.getColumnDisplaySize(15));
+    assertEquals(29, meta.getPrecision(15));
+    assertEquals(9, meta.getScale(15));
+
+    assertEquals("c18", meta.getColumnName(16));
+    assertEquals(Types.DECIMAL, meta.getColumnType(16));
+    assertEquals("decimal", meta.getColumnTypeName(16));
+    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(16));
+    assertEquals(Integer.MAX_VALUE, meta.getPrecision(16));
+    assertEquals(Integer.MAX_VALUE, meta.getScale(16));
+
+    assertEquals("c20", meta.getColumnName(17));
+    assertEquals(Types.DATE, meta.getColumnType(17));
+    assertEquals("date", meta.getColumnTypeName(17));
+    assertEquals(10, meta.getColumnDisplaySize(17));
+    assertEquals(10, meta.getPrecision(17));
+    assertEquals(0, meta.getScale(17));
+
+    assertEquals("c21", meta.getColumnName(18));
+    assertEquals(Types.VARCHAR, meta.getColumnType(18));
+    assertEquals("varchar", meta.getColumnTypeName(18));
+    // varchar columns should have correct display size/precision
+    assertEquals(20, meta.getColumnDisplaySize(18));
+    assertEquals(20, meta.getPrecision(18));
+    assertEquals(0, meta.getScale(18));
+
+    for (int i = 1; i <= meta.getColumnCount(); i++) {
+      assertFalse(meta.isAutoIncrement(i));
+      assertFalse(meta.isCurrency(i));
+      assertEquals(ResultSetMetaData.columnNullable, meta.isNullable(i));
+    }
+  }
+
+  // [url] [host] [port] [db]
+  private static final String[][] URL_PROPERTIES = new String[][] {
+    // binary mode
+    {"jdbc:hive2://", "", "", "default"},
+    {"jdbc:hive2://localhost:10001/default", "localhost", "10001", "default"},
+    {"jdbc:hive2://localhost/notdefault", "localhost", "10000", "notdefault"},
+    {"jdbc:hive2://foo:1243", "foo", "1243", "default"},
+
+    // http mode
+    {"jdbc:hive2://server:10002/db;user=foo;password=bar?" +
+        "hive.server2.transport.mode=http;" +
+        "hive.server2.thrift.http.path=hs2",
+        "server", "10002", "db"},
+  };
+
+  @Test
+  public void testDriverProperties() throws SQLException {
+    HiveDriver driver = new HiveDriver();
+    for (String[] testValues : URL_PROPERTIES) {
+      DriverPropertyInfo[] dpi = driver.getPropertyInfo(testValues[0], null);
+      assertEquals("unexpected DriverPropertyInfo array size", 3, dpi.length);
+      assertDpi(dpi[0], "HOST", testValues[1]);
+      assertDpi(dpi[1], "PORT", testValues[2]);
+      assertDpi(dpi[2], "DBNAME", testValues[3]);
+    }
+  }
+
+  private static final String[][] HTTP_URL_PROPERTIES = new String[][] {
+    {"jdbc:hive2://server:10002/db;" +
+        "user=foo;password=bar?" +
+        "hive.server2.transport.mode=http;" +
+        "hive.server2.thrift.http.path=hs2", "server", "10002", "db", "http", "hs2"},
+        {"jdbc:hive2://server:10000/testdb;" +
+            "user=foo;password=bar?" +
+            "hive.server2.transport.mode=binary;" +
+            "hive.server2.thrift.http.path=", "server", "10000", "testdb", "binary", ""},
+  };
+
+  @Test
+  public void testParseUrlHttpMode() throws SQLException {
+    HiveDriver driver = new HiveDriver();
+    for (String[] testValues : HTTP_URL_PROPERTIES) {
+      JdbcConnectionParams params = Utils.parseURL(testValues[0]);
+      assertEquals(params.getHost(), testValues[1]);
+      assertEquals(params.getPort(), Integer.parseInt(testValues[2]));
+      assertEquals(params.getDbName(), testValues[3]);
+      assertEquals(params.getHiveConfs().get("hive.server2.transport.mode"), testValues[4]);
+      assertEquals(params.getHiveConfs().get("hive.server2.thrift.http.path"), testValues[5]);
+    }
+  }
+
+  private static void assertDpi(DriverPropertyInfo dpi, String name,
+      String value) {
+    assertEquals("Invalid DriverPropertyInfo name", name, dpi.name);
+    assertEquals("Invalid DriverPropertyInfo value", value, dpi.value);
+    assertEquals("Invalid DriverPropertyInfo required", false, dpi.required);
+  }
+
+
+  /**
+   * validate schema generated by "set" command
+   * @throws SQLException
+   */
+  @Test
+  public void testSetCommand() throws SQLException {
+    // execute set command
+    String sql = "set -v";
+    Statement stmt = con.createStatement();
+    ResultSet res = stmt.executeQuery(sql);
+
+    // Validate resultset columns
+    ResultSetMetaData md = res.getMetaData() ;
+    assertEquals(1, md.getColumnCount());
+    assertEquals(SET_COLUMN_NAME, md.getColumnLabel(1));
+
+    //check if there is data in the resultset
+    assertTrue("Nothing returned by set -v", res.next());
+
+    res.close();
+    stmt.close();
+  }
+
+  /**
+   * Validate error on closed resultset
+   * @throws SQLException
+   */
+  @Test
+  public void testPostClose() throws SQLException {
+    Statement stmt = con.createStatement();
+    ResultSet res = stmt.executeQuery("select * from " + tableName);
+    assertNotNull("ResultSet is null", res);
+    res.close();
+    try { res.getInt(1); fail("Expected SQLException"); }
+    catch (SQLException e) { }
+    try { res.getMetaData(); fail("Expected SQLException"); }
+    catch (SQLException e) { }
+    try { res.setFetchSize(10); fail("Expected SQLException"); }
+    catch (SQLException e) { }
+  }
+
+  /*
+   * The JDBC spec says when you have duplicate column names,
+   * the first one should be returned.
+   */
+  @Test
+  public void testDuplicateColumnNameOrder() throws SQLException {
+    Statement stmt = con.createStatement();
+    ResultSet rs = stmt.executeQuery("SELECT 1 AS a, 2 AS a from " + tableName);
+    assertTrue(rs.next());
+    assertEquals(1, rs.getInt("a"));
+  }
+
+
+  /**
+   * Test bad args to getXXX()
+   * @throws SQLException
+   */
+  @Test
+  public void testOutOfBoundCols() throws SQLException {
+    Statement stmt = con.createStatement();
+
+    ResultSet res = stmt.executeQuery(
+        "select * from " + tableName);
+
+    // row 1
+    assertTrue(res.next());
+
+    try {
+      res.getInt(200);
+    } catch (SQLException e) {
+    }
+
+    try {
+      res.getInt("zzzz");
+    } catch (SQLException e) {
+    }
+
+  }
+
+  /**
+   * Verify selecting using builtin UDFs
+   * @throws SQLException
+   */
+  @Test
+  public void testBuiltInUDFCol() throws SQLException {
+    Statement stmt = con.createStatement();
+    ResultSet res = stmt.executeQuery("select c12, bin(c12) from " + dataTypeTableName
+        + " where c1=1");
+    ResultSetMetaData md = res.getMetaData();
+    assertEquals(md.getColumnCount(), 2); // only one result column
+    assertEquals(md.getColumnLabel(2), "_c1" ); // verify the system generated column name
+    assertTrue(res.next());
+    assertEquals(res.getLong(1), 1);
+    assertEquals(res.getString(2), "1");
+    res.close();
+  }
+
+  /**
+   * Verify selecting named expression columns
+   * @throws SQLException
+   */
+  @Test
+  public void testExprCol() throws SQLException {
+    Statement stmt = con.createStatement();
+    ResultSet res = stmt.executeQuery("select c1+1 as col1, length(c4) as len from " + dataTypeTableName
+        + " where c1=1");
+    ResultSetMetaData md = res.getMetaData();
+    assertEquals(md.getColumnCount(), 2); // only one result column
+    assertEquals(md.getColumnLabel(1), "col1" ); // verify the column name
+    assertEquals(md.getColumnLabel(2), "len" ); // verify the column name
+    assertTrue(res.next());
+    assertEquals(res.getInt(1), 2);
+    assertEquals(res.getInt(2), 1);
+    res.close();
+  }
+
+  /**
+   * test getProcedureColumns()
+   * @throws SQLException
+   */
+  @Test
+  public void testProcCols() throws SQLException {
+    DatabaseMetaData dbmd = con.getMetaData();
+    assertNotNull(dbmd);
+    // currently getProcedureColumns always returns an empty resultset for Hive
+    ResultSet res = dbmd.getProcedureColumns(null, null, null, null);
+    ResultSetMetaData md = res.getMetaData();
+    assertEquals(md.getColumnCount(), 20);
+    assertFalse(res.next());
+  }
+
+  /**
+   * test testProccedures()
+   * @throws SQLException
+   */
+  @Test
+  public void testProccedures() throws SQLException {
+    DatabaseMetaData dbmd = con.getMetaData();
+    assertNotNull(dbmd);
+    // currently testProccedures always returns an empty resultset for Hive
+    ResultSet res = dbmd.getProcedures(null, null, null);
+    ResultSetMetaData md = res.getMetaData();
+    assertEquals(md.getColumnCount(), 9);
+    assertFalse(res.next());
+  }
+
+  /**
+   * test getPrimaryKeys()
+   * @throws SQLException
+   */
+  @Test
+  public void testPrimaryKeys() throws SQLException {
+    DatabaseMetaData dbmd = con.getMetaData();
+    assertNotNull(dbmd);
+    // currently getPrimaryKeys always returns an empty resultset for Hive
+    ResultSet res = dbmd.getPrimaryKeys(null, null, null);
+    ResultSetMetaData md = res.getMetaData();
+    assertEquals(md.getColumnCount(), 6);
+    assertFalse(res.next());
+  }
+
+  /**
+   * test getImportedKeys()
+   * @throws SQLException
+   */
+  @Test
+  public void testImportedKeys() throws SQLException {
+    DatabaseMetaData dbmd = con.getMetaData();
+    assertNotNull(dbmd);
+    // currently getImportedKeys always returns an empty resultset for Hive
+    ResultSet res = dbmd.getImportedKeys(null, null, null);
+    ResultSetMetaData md = res.getMetaData();
+    assertEquals(md.getColumnCount(), 14);
+    assertFalse(res.next());
+  }
+
+  /**
+   * If the Driver implementation understands the URL, it will return a Connection object;
+   * otherwise it returns null
+   */
+  @Test
+  public void testInvalidURL() throws Exception {
+    HiveDriver driver = new HiveDriver();
+    Connection conn = driver.connect("jdbc:derby://localhost:10000/default", new Properties());
+    assertNull(conn);
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hive/service/auth/TestCustomAuthentication.java b/src/itests/hive-unit/src/test/java/org/apache/hive/service/auth/TestCustomAuthentication.java
new file mode 100644
index 0000000..ece54a8
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hive/service/auth/TestCustomAuthentication.java
@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hive.service.auth;
+
+import junit.framework.Assert;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hive.service.server.HiveServer2;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import javax.security.sasl.AuthenticationException;
+import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.SQLException;
+import java.util.HashMap;
+import java.util.Map;
+
+public class TestCustomAuthentication {
+
+  private static HiveServer2 hiveserver2;
+  private static HiveConf hiveConf;
+  private static byte[] hiveConfBackup;
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    hiveConf = new HiveConf();
+    ByteArrayOutputStream baos = new ByteArrayOutputStream();
+    hiveConf.writeXml(baos);
+    baos.close();
+    hiveConfBackup = baos.toByteArray();
+    hiveConf.set("hive.server2.authentication", "CUSTOM");
+    hiveConf.set("hive.server2.custom.authentication.class",
+        "org.apache.hive.service.auth.TestCustomAuthentication$SimpleAuthenticationProviderImpl");
+    FileOutputStream fos = new FileOutputStream(new File(hiveConf.getHiveSiteLocation().toURI()));
+    hiveConf.writeXml(fos);
+    fos.close();
+    hiveserver2 = new HiveServer2();
+    hiveserver2.init(hiveConf);
+    hiveserver2.start();
+    Thread.sleep(1000);
+    System.out.println("hiveServer2 start ......");
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    if(hiveConf != null && hiveConfBackup != null) {
+      FileOutputStream fos = new FileOutputStream(new File(hiveConf.getHiveSiteLocation().toURI()));
+      fos.write(hiveConfBackup);
+      fos.close();
+    }
+    if (hiveserver2 != null) {
+      hiveserver2.stop();
+      hiveserver2 = null;
+    }
+    Thread.sleep(1000);
+    System.out.println("hiveServer2 stop ......");
+  }
+
+  @Test
+  public void testCustomAuthentication() throws Exception {
+
+    String url = "jdbc:hive2://localhost:10000/default";
+    Class.forName("org.apache.hive.jdbc.HiveDriver");
+
+    try {
+      DriverManager.getConnection(url, "wronguser", "pwd");
+      Assert.fail("Expected Exception");
+    } catch(SQLException e) {
+      Assert.assertNotNull(e.getMessage());
+      Assert.assertTrue(e.getMessage(), e.getMessage().contains("Peer indicated failure: Error validating the login"));
+    }
+
+    Connection connection = DriverManager.getConnection(url, "hiveuser", "hive");
+    connection.close();
+
+    System.out.println(">>> PASSED testCustomAuthentication");
+  }
+
+  public static class SimpleAuthenticationProviderImpl implements PasswdAuthenticationProvider {
+
+    private Map<String, String> userMap = new HashMap<String, String>();
+
+    public SimpleAuthenticationProviderImpl() {
+      init();
+    }
+
+    private void init(){
+      userMap.put("hiveuser","hive");
+    }
+
+    @Override
+    public void Authenticate(String user, String password) throws AuthenticationException {
+
+      if(!userMap.containsKey(user)){
+        throw new AuthenticationException("Invalid user : "+user);
+      }
+      if(!userMap.get(user).equals(password)){
+        throw new AuthenticationException("Invalid passwd : "+password);
+      }
+    }
+  }
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java b/src/itests/hive-unit/src/test/java/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java
new file mode 100644
index 0000000..ebda296
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java
@@ -0,0 +1,59 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.service.cli;
+
+import org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService;
+import org.apache.hive.service.cli.thrift.ThriftCLIService;
+import org.apache.hive.service.cli.thrift.ThriftCLIServiceClient;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+
+/**
+ * TestEmbeddedThriftBinaryCLIService.
+ *
+ */
+public class TestEmbeddedThriftBinaryCLIService extends CLIServiceTest {
+  protected static ThriftCLIService service;
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    service = new EmbeddedThriftBinaryCLIService();
+    client = new ThriftCLIServiceClient(service);
+  }
+
+  /* (non-Javadoc)
+   * @see org.apache.hive.service.cli.CLIServiceTest#setUp()
+   */
+  @Override
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+  }
+
+  /* (non-Javadoc)
+   * @see org.apache.hive.service.cli.CLIServiceTest#tearDown()
+   */
+  @Override
+  @After
+  public void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hive/service/cli/thrift/TestThriftBinaryCLIService.java b/src/itests/hive-unit/src/test/java/org/apache/hive/service/cli/thrift/TestThriftBinaryCLIService.java
new file mode 100644
index 0000000..a632277
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hive/service/cli/thrift/TestThriftBinaryCLIService.java
@@ -0,0 +1,105 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.service.cli.thrift;
+
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.fail;
+
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hive.service.auth.HiveAuthFactory.AuthTypes;
+import org.apache.thrift.transport.TTransport;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+
+
+/**
+ *
+ * TestThriftBinaryCLIService.
+ * This tests ThriftCLIService started in binary mode.
+ *
+ */
+
+public class TestThriftBinaryCLIService extends ThriftCLIServiceTest {
+
+  private static String transportMode = "binary";
+  private static TTransport transport;
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    // Set up the base class
+    ThriftCLIServiceTest.setUpBeforeClass();
+
+    assertNotNull(port);
+    assertNotNull(hiveServer2);
+    assertNotNull(hiveConf);
+
+    hiveConf.setBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS, false);
+    hiveConf.setVar(ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST, host);
+    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_PORT, port);
+    hiveConf.setVar(ConfVars.HIVE_SERVER2_AUTHENTICATION, AuthTypes.NOSASL.toString());
+    hiveConf.setVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE, transportMode);
+
+    startHiveServer2WithConf(hiveConf);
+
+    // Open a binary transport
+    // Fail if the transport doesn't open
+    transport = createBinaryTransport();
+    try {
+      transport.open();
+    }
+    catch (Exception e) {
+      fail("Exception: " + e);
+    }
+  }
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    ThriftCLIServiceTest.tearDownAfterClass();
+  }
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @Override
+  @Before
+  public void setUp() throws Exception {
+    // Create and set the client
+    initClient(transport);
+    assertNotNull(client);
+  }
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @Override
+  @After
+  public void tearDown() throws Exception {
+
+  }
+
+
+}
\ No newline at end of file
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hive/service/cli/thrift/TestThriftHttpCLIService.java b/src/itests/hive-unit/src/test/java/org/apache/hive/service/cli/thrift/TestThriftHttpCLIService.java
new file mode 100644
index 0000000..65177dd
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hive/service/cli/thrift/TestThriftHttpCLIService.java
@@ -0,0 +1,216 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.service.cli.thrift;
+
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.fail;
+
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hive.jdbc.HttpBasicAuthInterceptor;
+import org.apache.hive.service.auth.HiveAuthFactory.AuthTypes;
+import org.apache.hive.service.server.HiveServer2;
+import org.apache.http.impl.client.DefaultHttpClient;
+import org.apache.thrift.transport.THttpClient;
+import org.apache.thrift.transport.TTransport;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/**
+*
+* TestThriftHttpCLIService.
+* This tests ThriftCLIService started in http mode.
+*
+*/
+
+public class TestThriftHttpCLIService extends ThriftCLIServiceTest {
+
+  private static String transportMode = "http";
+  private static String thriftHttpPath = "cliservice";
+  private static TTransport transport;
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    // Set up the base class
+    ThriftCLIServiceTest.setUpBeforeClass();
+
+    assertNotNull(port);
+    assertNotNull(hiveServer2);
+    assertNotNull(hiveConf);
+
+    hiveConf.setBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS, false);
+    hiveConf.setVar(ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST, host);
+    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_HTTP_PORT, port);
+    hiveConf.setVar(ConfVars.HIVE_SERVER2_AUTHENTICATION, AuthTypes.NOSASL.toString());
+    hiveConf.setVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE, transportMode);
+    hiveConf.setVar(ConfVars.HIVE_SERVER2_THRIFT_HTTP_PATH, thriftHttpPath);
+
+    startHiveServer2WithConf(hiveConf);
+
+    // Open an http transport
+    // Fail if the transport doesn't open
+    transport = createHttpTransport();
+    try {
+      transport.open();
+    }
+    catch (Exception e) {
+      fail("Exception: " + e);
+    }
+  }
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    ThriftCLIServiceTest.tearDownAfterClass();
+  }
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @Override
+  @Before
+  public void setUp() throws Exception {
+    // Create and set the client before every test from the transport
+    initClient(transport);
+    assertNotNull(client);
+  }
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @Override
+  @After
+  public void tearDown() throws Exception {
+
+  }
+
+  @Test
+  public void testIncompatibeClientServer() throws Exception {
+    // A binary client communicating with an http server should throw an exception
+    // Close the older http client transport
+    // The server is already running in Http mode
+    if (transport != null) {
+      transport.close();
+    }
+    // Create a binary transport and init the client
+    transport = createBinaryTransport();
+    // Create and set the client
+    initClient(transport);
+    assertNotNull(client);
+
+    // This will throw an expected exception since client-server modes are incompatible
+    testOpenSessionExpectedException();
+
+    // Close binary client transport
+    if (transport != null) {
+      transport.close();
+    }
+    // Create http transport (client is inited in setUp before every test from the transport)
+    transport = createHttpTransport();
+    try {
+      transport.open();
+    }
+    catch (Exception e) {
+      fail("Exception: " + e);
+    }
+  }
+
+  @Test
+  public void testIncorrectHttpPath() throws Exception {
+    // Close the older http client transport
+    if (transport != null) {
+      transport.close();
+    }
+    // Create an http transport with incorrect http path endpoint
+    thriftHttpPath = "wrong_path";
+    transport = createHttpTransport();
+    // Create and set the client
+    initClient(transport);
+    assertNotNull(client);
+
+    // This will throw an expected exception since
+    // client is communicating with the wrong http service endpoint
+    testOpenSessionExpectedException();
+
+    // Close incorrect client transport
+    // Reinit http client transport
+    thriftHttpPath = "cliservice";
+    if (transport != null) {
+      transport.close();
+    }
+    transport = createHttpTransport();
+    try {
+      transport.open();
+    }
+    catch (Exception e) {
+      fail("Exception: " + e);
+    }
+  }
+
+
+  private void testWithAuthMode(AuthTypes authType) throws Exception {
+    // Stop and restart HiveServer2 in given incorrect auth mode
+    stopHiveServer2();
+    hiveConf.setVar(ConfVars.HIVE_SERVER2_AUTHENTICATION, authType.toString());
+    hiveServer2 = new HiveServer2();
+    // HiveServer2 in Http mode will not start using KERBEROS/LDAP/CUSTOM auth types
+    startHiveServer2WithConf(hiveConf);
+
+    // This will throw an expected exception since Http server is not running
+    testOpenSessionExpectedException();
+
+    // Stop and restart back with the original config
+    stopHiveServer2();
+    hiveConf.setVar(ConfVars.HIVE_SERVER2_AUTHENTICATION, AuthTypes.NOSASL.toString());
+    hiveServer2 = new HiveServer2();
+    startHiveServer2WithConf(hiveConf);
+  }
+
+  @Test
+  public void testKerberosMode()  throws Exception {
+    testWithAuthMode(AuthTypes.KERBEROS);
+  }
+
+  @Test
+  public void testLDAPMode()  throws Exception {
+    testWithAuthMode(AuthTypes.LDAP);
+  }
+
+  @Test
+  public void testCustomMode()  throws Exception {
+    testWithAuthMode(AuthTypes.CUSTOM);
+  }
+
+  private static TTransport createHttpTransport() throws Exception {
+    DefaultHttpClient httpClient = new DefaultHttpClient();
+    String httpUrl = transportMode + "://" + host + ":" + port +
+        "/" + thriftHttpPath + "/";
+    httpClient.addRequestInterceptor(
+        new HttpBasicAuthInterceptor(anonymousUser, anonymousPasswd));
+    return new THttpClient(httpUrl, httpClient);
+  }
+
+}
\ No newline at end of file
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hive/service/server/TestHS2ThreadAllocation.java b/src/itests/hive-unit/src/test/java/org/apache/hive/service/server/TestHS2ThreadAllocation.java
new file mode 100644
index 0000000..225bf20
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hive/service/server/TestHS2ThreadAllocation.java
@@ -0,0 +1,256 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+//The tests here are heavily based on some timing, so there is some chance to fail.
+package org.apache.hive.service.server;
+
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Properties;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext;
+import org.apache.hadoop.hive.ql.hooks.HookContext;
+import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
+import org.apache.hive.jdbc.HiveConnection;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestHS2ThreadAllocation {
+  // Hook to verify ipaddress
+  public static class IpHookImpl implements ExecuteWithHookContext {
+    public static String userName = "";
+    public static String ipAddress = "";
+    public void run(HookContext hookContext) {
+      if (hookContext.getHookType().equals(HookType.POST_EXEC_HOOK)) {
+        Assert.assertNotNull(hookContext.getIpAddress(), "IP Address is null");
+        ipAddress = hookContext.getIpAddress();
+        Assert.assertNotNull(hookContext.getUserName(), "Username is null");
+        userName = hookContext.getUserName();
+      }
+    }
+  }
+
+  private static HiveServer2 hiveServer2;
+  private static final int MAX_THREADS = 10;
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    HiveConf hiveConf = new HiveConf();
+    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_MIN_WORKER_THREADS, 1);
+    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_MAX_WORKER_THREADS, MAX_THREADS);
+    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_LOGIN_TIMEOUT, 2);
+
+    hiveServer2 = new HiveServer2();
+    hiveServer2.init(hiveConf);
+    hiveServer2.start();
+    Thread.sleep(2000);
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    if (hiveServer2 != null) {
+      hiveServer2.stop();
+    }
+  }
+
+  class QueryRunner implements Runnable {
+
+    private int waitTime = 0;
+
+    QueryRunner(int delay) {
+      waitTime = delay;
+    }
+
+    QueryRunner() {
+    }
+
+
+    @Override
+    public void run() {
+
+      HiveConnection connection = null;
+      Statement statement = null;
+      try {
+        connection = new HiveConnection("jdbc:hive2://localhost:10000/default", new Properties());
+
+        statement = connection.createStatement();
+        if (statement.execute("show tables")) {
+          ResultSet results = statement.getResultSet();
+          while (results.next()) {
+            System.out.println(results.getString(1));
+          }
+          results.close();
+        }
+        statement.close();
+
+        if (waitTime > 0) {
+          Thread.sleep(waitTime);
+        }
+
+        statement = connection.createStatement();
+        if (statement.execute("show tables")) {
+          ResultSet results = statement.getResultSet();
+          while (results.next()) {
+            System.out.println(results.getString(1));
+          }
+          results.close();
+        }
+        statement.close();
+        connection.close();
+
+      } catch (SQLException e) {
+        e.printStackTrace();
+
+      } catch (InterruptedException e) {
+        e.printStackTrace();
+
+      } finally {
+        try {
+          if (statement != null) {
+            statement.close();
+          }
+
+          if (connection != null) {
+            connection.close();
+          }
+        } catch (Throwable e) {
+          e.printStackTrace();
+        }
+      }
+    }
+  }
+
+  @Test
+  public void testConnection() throws SQLException {
+    int i = MAX_THREADS;
+    for (; i > 0; i--) {
+
+      HiveConnection connection = new HiveConnection("jdbc:hive2://localhost:10000/default", new Properties());
+      Statement statement = connection.createStatement();
+
+      if (statement.execute("show tables")) {
+        ResultSet results = statement.getResultSet();
+        while (results.next()) {
+          System.out.println(results.getString(1));
+        }
+        results.close();
+      }
+      statement.close();
+      connection.close();
+    }
+    Assert.assertEquals(0, i);
+  }
+
+
+  @Test
+  public void testParallelConnections() throws InterruptedException {
+    List<Thread> threadList = new ArrayList<Thread>(10);
+    for (int i = 0; i < MAX_THREADS; i++) {
+      Thread thread = new Thread(new QueryRunner());
+      thread.setName("HiveConnectionTest-" + i);
+      thread.start();
+      threadList.add(thread);
+      Thread.sleep(100);
+    }
+
+    // Wait for all threads to complete/die
+    for (Thread thread : threadList) {
+      thread.join();
+      System.out.println(thread.getName() + " " + thread.isAlive());
+    }
+  }
+
+  @Test
+  public void testHS2StabilityOnLargeConnections() throws InterruptedException {
+    List<Thread> threadList = new ArrayList<Thread>(10);
+    HiveConnection connection = null;
+
+    for (int i = 0; i < MAX_THREADS; i++) {
+      Thread thread = new Thread(new QueryRunner(5000));
+      thread.setName("HiveConnectionTest-" + i);
+      threadList.add(thread);
+      thread.start();
+    }
+
+    // Above threads should have exploited the threads in HS2
+    Thread.sleep(100);
+    // Create another thread and see if the connection goes through.
+
+    boolean connectionFailed = false;
+    try {
+      connection = new HiveConnection("jdbc:hive2://localhost:10000/default", new Properties());
+    } catch (SQLException e) {
+      //e.printStackTrace();
+      System.out.println("Expected connection failure:" + e.getMessage());
+      connectionFailed = true;
+    }
+    Assert.assertEquals(connectionFailed, true);
+
+    // Hope all threads should be active at this time  -- based on timing, not a good test
+    for (Thread thread : threadList) {
+      Assert.assertEquals(thread.isAlive(), true);
+    }
+
+    // Wait for all threads to complete/die
+    for (Thread thread : threadList) {
+      System.out.println(thread.getName() + " " + thread.isAlive());
+      thread.join();
+    }
+
+    // Check if a new connection is possible
+    try {
+      connection = new HiveConnection("jdbc:hive2://localhost:10000/default", new Properties());
+    } catch (SQLException e) {
+      e.printStackTrace();
+      Assert.fail("Cannot create connection with free threads");
+    }
+    Assert.assertNotNull(connection);
+    try {
+      connection.close();
+    } catch (SQLException e) {
+      e.printStackTrace();
+      Assert.fail("Something went wrong with connection closure");
+    }
+  }
+
+  @Test
+  public void testExecuteStatementWithHook() throws Exception {
+    Properties connProp = new Properties();
+    connProp.setProperty("user", System.getProperty("user.name"));
+    connProp.setProperty("password", "");
+    HiveConnection connection = new HiveConnection("jdbc:hive2://localhost:10000/default", connProp);
+    connection.createStatement().execute("SET hive.exec.post.hooks =  " + IpHookImpl.class.getName());
+    connection.createStatement().execute("show tables");
+    Assert.assertEquals(System.getProperty("user.name"), IpHookImpl.userName);
+    Assert.assertTrue(IpHookImpl.ipAddress.contains("127.0.0.1"));
+    connection.close();
+  }
+
+}
+
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hive/service/server/TestHiveServer2Concurrency.java b/src/itests/hive-unit/src/test/java/org/apache/hive/service/server/TestHiveServer2Concurrency.java
new file mode 100644
index 0000000..2231f40
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hive/service/server/TestHiveServer2Concurrency.java
@@ -0,0 +1,103 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.service.server;
+
+import static org.junit.Assert.fail;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.QTestUtil;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/**
+ * TestHiveServer2Concurrency.
+ *
+ */
+public class TestHiveServer2Concurrency {
+
+  private static QTestUtil.QTestSetup miniZKCluster = null;
+  private static HiveServer2 hiveServer2;
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    HiveConf hiveConf = new HiveConf();
+
+    miniZKCluster = new QTestUtil.QTestSetup();
+    miniZKCluster.preTest(hiveConf);
+
+    hiveServer2 = new HiveServer2();
+    hiveServer2.init(hiveConf);
+    hiveServer2.start();
+    Thread.sleep(5000);
+  }
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    if (hiveServer2 != null) {
+      hiveServer2.stop();
+    }
+    if (miniZKCluster != null) {
+      try {
+        miniZKCluster.tearDown();
+      } catch (Exception e) {
+        e.printStackTrace();
+      }
+    }
+  }
+
+  class QueryRunner implements Runnable {
+
+    @Override
+    public void run() {
+      // TODO Auto-generated method stub
+
+    }
+
+  }
+
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @Before
+  public void setUp() throws Exception {
+  }
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @After
+  public void tearDown() throws Exception {
+  }
+
+  @Test
+  public void test() {
+    fail("Not yet implemented");
+  }
+
+}
diff --git a/src/itests/test-serde/src/main/java/org/apache/hadoop/hive/serde2/TestSerDe.java b/src/itests/test-serde/src/main/java/org/apache/hadoop/hive/serde2/TestSerDe.java
new file mode 100644
index 0000000..23e67e3
--- /dev/null
+++ b/src/itests/test-serde/src/main/java/org/apache/hadoop/hive/serde2/TestSerDe.java
@@ -0,0 +1,199 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.serde2;
+
+import java.nio.charset.CharacterCodingException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Properties;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.StructField;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+
+/**
+ * TestSerDe.
+ *
+ */
+public class TestSerDe extends AbstractSerDe {
+
+  public static final Log LOG = LogFactory.getLog(TestSerDe.class.getName());
+
+  public String getShortName() {
+    return shortName();
+  }
+
+  public static String shortName() {
+    return "test_meta";
+  }
+
+  public static final String DefaultSeparator = "\002";
+
+  private String separator;
+  // constant for now, will make it configurable later.
+  private final String nullString = "\\N";
+  private List<String> columnNames;
+  private ObjectInspector cachedObjectInspector;
+
+  @Override
+  public String toString() {
+    return "TestSerDe[" + separator + "," + columnNames + "]";
+  }
+
+  public TestSerDe() throws SerDeException {
+    separator = DefaultSeparator;
+  }
+
+  @Override
+  public void initialize(Configuration job, Properties tbl) throws SerDeException {
+    separator = DefaultSeparator;
+    String altSep = tbl.getProperty("testserde.default.serialization.format");
+    if (altSep != null && altSep.length() > 0) {
+      try {
+        byte[] b = new byte[1];
+        b[0] = Byte.valueOf(altSep).byteValue();
+        separator = new String(b);
+      } catch (NumberFormatException e) {
+        separator = altSep;
+      }
+    }
+
+    String columnProperty = tbl.getProperty("columns");
+    if (columnProperty == null || columnProperty.length() == 0) {
+      // Hack for tables with no columns
+      // Treat it as a table with a single column called "col"
+      cachedObjectInspector = ObjectInspectorFactory
+          .getReflectionObjectInspector(ColumnSet.class,
+          ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
+    } else {
+      columnNames = Arrays.asList(columnProperty.split(","));
+      cachedObjectInspector = MetadataListStructObjectInspector
+          .getInstance(columnNames);
+    }
+    LOG.info(getClass().getName() + ": initialized with columnNames: "
+        + columnNames);
+  }
+
+  public static Object deserialize(ColumnSet c, String row, String sep,
+      String nullString) throws Exception {
+    if (c.col == null) {
+      c.col = new ArrayList<String>();
+    } else {
+      c.col.clear();
+    }
+    String[] l1 = row.split(sep, -1);
+
+    for (String s : l1) {
+      if (s.equals(nullString)) {
+        c.col.add(null);
+      } else {
+        c.col.add(s);
+      }
+    }
+    return (c);
+  }
+
+  ColumnSet deserializeCache = new ColumnSet();
+
+  @Override
+  public Object deserialize(Writable field) throws SerDeException {
+    String row = null;
+    if (field instanceof BytesWritable) {
+      BytesWritable b = (BytesWritable) field;
+      try {
+        row = Text.decode(b.get(), 0, b.getSize());
+      } catch (CharacterCodingException e) {
+        throw new SerDeException(e);
+      }
+    } else if (field instanceof Text) {
+      row = field.toString();
+    }
+    try {
+      deserialize(deserializeCache, row, separator, nullString);
+      if (columnNames != null) {
+        assert (columnNames.size() == deserializeCache.col.size());
+      }
+      return deserializeCache;
+    } catch (ClassCastException e) {
+      throw new SerDeException(this.getClass().getName()
+          + " expects Text or BytesWritable", e);
+    } catch (Exception e) {
+      throw new SerDeException(e);
+    }
+  }
+
+  @Override
+  public ObjectInspector getObjectInspector() throws SerDeException {
+    return cachedObjectInspector;
+  }
+
+  @Override
+  public Class<? extends Writable> getSerializedClass() {
+    return Text.class;
+  }
+
+  Text serializeCache = new Text();
+
+  @Override
+  public Writable serialize(Object obj, ObjectInspector objInspector) throws SerDeException {
+
+    if (objInspector.getCategory() != Category.STRUCT) {
+      throw new SerDeException(getClass().toString()
+          + " can only serialize struct types, but we got: "
+          + objInspector.getTypeName());
+    }
+    StructObjectInspector soi = (StructObjectInspector) objInspector;
+    List<? extends StructField> fields = soi.getAllStructFieldRefs();
+
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < fields.size(); i++) {
+      if (i > 0) {
+        sb.append(separator);
+      }
+      Object column = soi.getStructFieldData(obj, fields.get(i));
+      if (fields.get(i).getFieldObjectInspector().getCategory() == Category.PRIMITIVE) {
+        // For primitive object, serialize to plain string
+        sb.append(column == null ? nullString : column.toString());
+      } else {
+        // For complex object, serialize to JSON format
+        sb.append(SerDeUtils.getJSONString(column, fields.get(i)
+            .getFieldObjectInspector()));
+      }
+    }
+    serializeCache.set(sb.toString());
+    return serializeCache;
+  }
+
+  @Override
+  public SerDeStats getSerDeStats() {
+    // no support for statistics
+    return null;
+  }
+
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseQTestUtil.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseQTestUtil.java
new file mode 100644
index 0000000..0558048
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseQTestUtil.java
@@ -0,0 +1,38 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.hbase;
+
+import org.apache.hadoop.hive.ql.QTestUtil;
+
+/**
+ * HBaseQTestUtil initializes HBase-specific test fixtures.
+ */
+public class HBaseQTestUtil extends QTestUtil {
+  public HBaseQTestUtil(
+    String outDir, String logDir, boolean miniMr, HBaseTestSetup setup)
+    throws Exception {
+
+    super(outDir, logDir, miniMr, null);
+    setup.preTest(conf);
+    super.init();
+  }
+
+  public void init() throws Exception {
+    // defer
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseTestSetup.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseTestSetup.java
new file mode 100644
index 0000000..cdc0a65
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseTestSetup.java
@@ -0,0 +1,161 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.hbase;
+
+import java.io.File;
+import java.io.IOException;
+import java.net.ServerSocket;
+import java.util.Arrays;
+
+import junit.extensions.TestSetup;
+import junit.framework.Test;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HConnectionManager;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.zookeeper.Watcher;
+
+/**
+ * HBaseTestSetup defines HBase-specific test fixtures which are
+ * reused across testcases.
+ */
+public class HBaseTestSetup extends TestSetup {
+
+  private MiniHBaseCluster hbaseCluster;
+  private int zooKeeperPort;
+  private String hbaseRoot;
+
+  private static final int NUM_REGIONSERVERS = 1;
+
+  public HBaseTestSetup(Test test) {
+    super(test);
+  }
+
+  void preTest(HiveConf conf) throws Exception {
+
+    setUpFixtures(conf);
+
+    conf.set("hbase.rootdir", hbaseRoot);
+    conf.set("hbase.master", hbaseCluster.getMaster().getServerName().getHostAndPort());
+    conf.set("hbase.zookeeper.property.clientPort", Integer.toString(zooKeeperPort));
+    String auxJars = conf.getAuxJars();
+    auxJars = ((auxJars == null) ? "" : (auxJars + ",")) + "file:///"
+      + new JobConf(conf, HBaseConfiguration.class).getJar();
+    auxJars += ",file:///" + new JobConf(conf, HBaseSerDe.class).getJar();
+    auxJars += ",file:///" + new JobConf(conf, Watcher.class).getJar();
+    conf.setAuxJars(auxJars);
+  }
+
+  private void setUpFixtures(HiveConf conf) throws Exception {
+    /* We are not starting zookeeper server here because
+     * QTestUtil already starts it.
+     */
+    int zkPort = conf.getInt("hive.zookeeper.client.port", -1);
+    if ((zkPort == zooKeeperPort) && (hbaseCluster != null)) {
+      return;
+    }
+    zooKeeperPort = zkPort;
+    String tmpdir =  System.getProperty("test.tmp.dir");
+    this.tearDown();
+    conf.set("hbase.master", "local");
+
+    hbaseRoot = "file:///" + tmpdir + "/hbase";
+    conf.set("hbase.rootdir", hbaseRoot);
+
+    conf.set("hbase.zookeeper.property.clientPort",
+      Integer.toString(zooKeeperPort));
+    Configuration hbaseConf = HBaseConfiguration.create(conf);
+    hbaseConf.setInt("hbase.master.port", findFreePort());
+    hbaseConf.setInt("hbase.master.info.port", -1);
+    hbaseConf.setInt("hbase.regionserver.port", findFreePort());
+    hbaseConf.setInt("hbase.regionserver.info.port", -1);
+    hbaseCluster = new MiniHBaseCluster(hbaseConf, NUM_REGIONSERVERS);
+    conf.set("hbase.master", hbaseCluster.getMaster().getServerName().getHostAndPort());
+    // opening the META table ensures that cluster is running
+    new HTable(hbaseConf, HConstants.META_TABLE_NAME);
+    createHBaseTable(hbaseConf);
+  }
+
+  private void createHBaseTable(Configuration hbaseConf) throws IOException {
+    final String HBASE_TABLE_NAME = "HiveExternalTable";
+    HTableDescriptor htableDesc = new HTableDescriptor(HBASE_TABLE_NAME.getBytes());
+    HColumnDescriptor hcolDesc = new HColumnDescriptor("cf".getBytes());
+    htableDesc.addFamily(hcolDesc);
+    HBaseAdmin hbaseAdmin = new HBaseAdmin(hbaseConf);
+    if(Arrays.asList(hbaseAdmin.listTables()).contains(htableDesc)){
+      // if table is already in there, don't recreate.
+      return;
+    }
+    hbaseAdmin.createTable(htableDesc);
+    HTable htable = new HTable(hbaseConf, HBASE_TABLE_NAME);
+
+    // data
+    Put [] puts = new Put [] {
+        new Put("key-1".getBytes()), new Put("key-2".getBytes()), new Put("key-3".getBytes()) };
+
+    boolean [] booleans = new boolean [] { true, false, true };
+    byte [] bytes = new byte [] { Byte.MIN_VALUE, -1, Byte.MAX_VALUE };
+    short [] shorts = new short [] { Short.MIN_VALUE, -1, Short.MAX_VALUE };
+    int [] ints = new int [] { Integer.MIN_VALUE, -1, Integer.MAX_VALUE };
+    long [] longs = new long [] { Long.MIN_VALUE, -1, Long.MAX_VALUE };
+    String [] strings = new String [] { "Hadoop, HBase,", "Hive", "Test Strings" };
+    float [] floats = new float [] { Float.MIN_VALUE, -1.0F, Float.MAX_VALUE };
+    double [] doubles = new double [] { Double.MIN_VALUE, -1.0, Double.MAX_VALUE };
+
+    // store data
+    for (int i = 0; i < puts.length; i++) {
+      puts[i].add("cf".getBytes(), "cq-boolean".getBytes(), Bytes.toBytes(booleans[i]));
+      puts[i].add("cf".getBytes(), "cq-byte".getBytes(), new byte [] { bytes[i] });
+      puts[i].add("cf".getBytes(), "cq-short".getBytes(), Bytes.toBytes(shorts[i]));
+      puts[i].add("cf".getBytes(), "cq-int".getBytes(), Bytes.toBytes(ints[i]));
+      puts[i].add("cf".getBytes(), "cq-long".getBytes(), Bytes.toBytes(longs[i]));
+      puts[i].add("cf".getBytes(), "cq-string".getBytes(), Bytes.toBytes(strings[i]));
+      puts[i].add("cf".getBytes(), "cq-float".getBytes(), Bytes.toBytes(floats[i]));
+      puts[i].add("cf".getBytes(), "cq-double".getBytes(), Bytes.toBytes(doubles[i]));
+
+      htable.put(puts[i]);
+    }
+  }
+
+  private static int findFreePort() throws IOException {
+    ServerSocket server = new ServerSocket(0);
+    int port = server.getLocalPort();
+    server.close();
+    return port;
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    if (hbaseCluster != null) {
+      HConnectionManager.deleteAllConnections(true);
+      hbaseCluster.shutdown();
+      hbaseCluster = null;
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
new file mode 100644
index 0000000..8ce1bbb
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
@@ -0,0 +1,1536 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql;
+
+import static org.apache.hadoop.hive.metastore.MetaStoreUtils.DEFAULT_DATABASE_NAME;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedReader;
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.FileReader;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.PrintStream;
+import java.io.Serializable;
+import java.io.StringWriter;
+import java.io.UnsupportedEncodingException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Deque;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.io.IOUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;
+import org.apache.hadoop.hive.cli.CliDriver;
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.common.io.CachingPrintStream;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.metastore.api.Index;
+import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.exec.Utilities.StreamPrinter;
+import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
+import org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager;
+import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.metadata.Table;
+import org.apache.hadoop.hive.ql.parse.ASTNode;
+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;
+import org.apache.hadoop.hive.ql.parse.ParseDriver;
+import org.apache.hadoop.hive.ql.parse.ParseException;
+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer;
+import org.apache.hadoop.hive.serde2.thrift.test.Complex;
+import org.apache.hadoop.hive.shims.HadoopShims;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.mapred.SequenceFileInputFormat;
+import org.apache.hadoop.mapred.SequenceFileOutputFormat;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.util.Shell;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.zookeeper.WatchedEvent;
+import org.apache.zookeeper.Watcher;
+import org.apache.zookeeper.ZooKeeper;
+
+/**
+ * QTestUtil.
+ *
+ */
+public class QTestUtil {
+
+  public static final String UTF_8 = "UTF-8";
+  private static final Log LOG = LogFactory.getLog("QTestUtil");
+
+  private String testWarehouse;
+  private final String testFiles;
+  protected final String outDir;
+  protected final String logDir;
+  private final TreeMap<String, String> qMap;
+  private final Set<String> qSkipSet;
+  private final Set<String> qSortSet;
+  private static final String SORT_SUFFIX = ".sorted";
+  public static final HashSet<String> srcTables = new HashSet<String>();
+  private ParseDriver pd;
+  private Hive db;
+  protected HiveConf conf;
+  private Driver drv;
+  private BaseSemanticAnalyzer sem;
+  private FileSystem fs;
+  protected final boolean overWrite;
+  private CliDriver cliDriver;
+  private HadoopShims.MiniMrShim mr = null;
+  private HadoopShims.MiniDFSShim dfs = null;
+  private boolean miniMr = false;
+  private String hadoopVer = null;
+  private QTestSetup setup = null;
+
+  static {
+    for (String srcTable : System.getProperty("test.src.tables", "").trim().split(",")) {
+      srcTable = srcTable.trim();
+      if (!srcTable.isEmpty()) {
+        srcTables.add(srcTable);
+      }
+    }
+    if (srcTables.isEmpty()) {
+      throw new AssertionError("Source tables cannot be empty");
+    }
+  }
+
+  public boolean deleteDirectory(File path) {
+    if (path.exists()) {
+      File[] files = path.listFiles();
+      for (File file : files) {
+        if (file.isDirectory()) {
+          deleteDirectory(file);
+        } else {
+          file.delete();
+        }
+      }
+    }
+    return (path.delete());
+  }
+
+  public void copyDirectoryToLocal(Path src, Path dest) throws Exception {
+
+    FileSystem srcFs = src.getFileSystem(conf);
+    FileSystem destFs = dest.getFileSystem(conf);
+    if (srcFs.exists(src)) {
+      FileStatus[] files = srcFs.listStatus(src);
+      for (FileStatus file : files) {
+        String name = file.getPath().getName();
+        Path dfs_path = file.getPath();
+        Path local_path = new Path(dest, name);
+
+        // If this is a source table we do not copy it out
+        if (srcTables.contains(name)) {
+          continue;
+        }
+
+        if (file.isDir()) {
+          if (!destFs.exists(local_path)) {
+            destFs.mkdirs(local_path);
+          }
+          copyDirectoryToLocal(dfs_path, local_path);
+        } else {
+          srcFs.copyToLocalFile(dfs_path, local_path);
+        }
+      }
+    }
+  }
+
+  static Pattern mapTok = Pattern.compile("(\\.?)(.*)_map_(.*)");
+  static Pattern reduceTok = Pattern.compile("(.*)(reduce_[^\\.]*)((\\..*)?)");
+
+  public void normalizeNames(File path) throws Exception {
+    if (path.isDirectory()) {
+      File[] files = path.listFiles();
+      for (File file : files) {
+        normalizeNames(file);
+      }
+    } else {
+      Matcher m = reduceTok.matcher(path.getName());
+      if (m.matches()) {
+        String name = m.group(1) + "reduce" + m.group(3);
+        path.renameTo(new File(path.getParent(), name));
+      } else {
+        m = mapTok.matcher(path.getName());
+        if (m.matches()) {
+          String name = m.group(1) + "map_" + m.group(3);
+          path.renameTo(new File(path.getParent(), name));
+        }
+      }
+    }
+  }
+
+  public QTestUtil(String outDir, String logDir) throws Exception {
+    this(outDir, logDir, false, "0.20");
+  }
+
+  public String getOutputDirectory() {
+    return outDir;
+  }
+
+  public String getLogDirectory() {
+    return logDir;
+  }
+
+  private String getHadoopMainVersion(String input) {
+    if (input == null) {
+      return null;
+    }
+    Pattern p = Pattern.compile("^(\\d+\\.\\d+).*");
+    Matcher m = p.matcher(input);
+    if (m.matches()) {
+      return m.group(1);
+    }
+    return null;
+  }
+
+  public void initConf() throws Exception {
+
+    if (Shell.WINDOWS) {
+      convertPathsFromWindowsToHdfs();
+    }
+
+    // Plug verifying metastore in for testing.
+    conf.setVar(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL,
+        "org.apache.hadoop.hive.metastore.VerifyingObjectStore");
+
+    if (miniMr) {
+      assert dfs != null;
+      assert mr != null;
+
+      mr.setupConfiguration(conf);
+
+      // set fs.default.name to the uri of mini-dfs
+      String dfsUriString = getHdfsUriString(dfs.getFileSystem().getUri().toString());
+      conf.setVar(HiveConf.ConfVars.HADOOPFS, dfsUriString);
+      // hive.metastore.warehouse.dir needs to be set relative to the mini-dfs
+      conf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE,
+                  (new Path(dfsUriString,
+                            "/build/ql/test/data/warehouse/")).toString());
+    }
+  }
+
+  private void convertPathsFromWindowsToHdfs() {
+    // Following local paths are used as HDFS paths in unit tests.
+    // It works well in Unix as the path notation in Unix and HDFS is more or less same.
+    // But when it comes to Windows, drive letter separator ':' & backslash '\" are invalid
+    // characters in HDFS so we need to converts these local paths to HDFS paths before using them
+    // in unit tests.
+
+    // hive.exec.scratchdir needs to be set relative to the mini-dfs
+    String orgWarehouseDir = conf.getVar(HiveConf.ConfVars.METASTOREWAREHOUSE);
+    conf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE, getHdfsUriString(orgWarehouseDir));
+
+    String orgTestTempDir = System.getProperty("test.tmp.dir");
+    System.setProperty("test.tmp.dir", getHdfsUriString(orgTestTempDir));
+
+    String orgTestDataDir = System.getProperty("test.src.data.dir");
+    System.setProperty("test.src.data.dir", getHdfsUriString(orgTestDataDir));
+
+    String orgScratchDir = conf.getVar(HiveConf.ConfVars.SCRATCHDIR);
+    conf.setVar(HiveConf.ConfVars.SCRATCHDIR, getHdfsUriString(orgScratchDir));
+
+    if (miniMr) {
+      String orgAuxJarFolder = conf.getAuxJars();
+      conf.setAuxJars(getHdfsUriString("file://" + orgAuxJarFolder));
+    }
+  }
+
+  private String getHdfsUriString(String uriStr) {
+    assert uriStr != null;
+    if(Shell.WINDOWS) {
+      // If the URI conversion is from Windows to HDFS then replace the '\' with '/'
+      // and remove the windows single drive letter & colon from absolute path.
+      return uriStr.replace('\\', '/')
+        .replaceFirst("/[c-zC-Z]:", "/")
+        .replaceFirst("^[c-zC-Z]:", "");
+    }
+
+    return uriStr;
+  }
+
+  public QTestUtil(String outDir, String logDir, boolean miniMr, String hadoopVer)
+    throws Exception {
+    this.outDir = outDir;
+    this.logDir = logDir;
+    conf = new HiveConf(Driver.class);
+    this.miniMr = miniMr;
+    this.hadoopVer = getHadoopMainVersion(hadoopVer);
+    qMap = new TreeMap<String, String>();
+    qSkipSet = new HashSet<String>();
+    qSortSet = new HashSet<String>();
+
+    if (miniMr) {
+      dfs = ShimLoader.getHadoopShims().getMiniDfs(conf, 4, true, null);
+      FileSystem fs = dfs.getFileSystem();
+      mr = ShimLoader.getHadoopShims().getMiniMrCluster(conf, 4, getHdfsUriString(fs.getUri().toString()), 1);
+    }
+
+    initConf();
+
+    // Use the current directory if it is not specified
+    String dataDir = conf.get("test.data.files");
+    if (dataDir == null) {
+      dataDir = new File(".").getAbsolutePath() + "/data/files";
+    }
+
+    testFiles = dataDir;
+
+    overWrite = "true".equalsIgnoreCase(System.getProperty("test.output.overwrite"));
+
+    setup = new QTestSetup();
+    setup.preTest(conf);
+    init();
+  }
+
+  public void shutdown() throws Exception {
+    cleanUp();
+    setup.tearDown();
+    if (mr != null) {
+      mr.shutdown();
+      mr = null;
+    }
+    FileSystem.closeAll();
+    if (dfs != null) {
+      dfs.shutdown();
+      dfs = null;
+    }
+  }
+
+  public String readEntireFileIntoString(File queryFile) throws IOException {
+    InputStreamReader isr = new InputStreamReader(
+        new BufferedInputStream(new FileInputStream(queryFile)), QTestUtil.UTF_8);
+    StringWriter sw = new StringWriter();
+    try {
+      IOUtils.copy(isr, sw);
+    } finally {
+      if (isr != null) {
+        isr.close();
+      }
+    }
+    return sw.toString();
+  }
+
+  public void addFile(String queryFile) throws IOException {
+    addFile(new File(queryFile));
+  }
+
+  public void addFile(File qf) throws IOException  {
+    String query = readEntireFileIntoString(qf);
+    qMap.put(qf.getName(), query);
+
+    if(checkHadoopVersionExclude(qf.getName(), query)
+      || checkOSExclude(qf.getName(), query)) {
+      qSkipSet.add(qf.getName());
+    }
+
+    if (checkNeedsSort(qf.getName(), query)) {
+      qSortSet.add(qf.getName());
+    }
+  }
+
+  private boolean checkNeedsSort(String fileName, String query) {
+    Pattern pattern = Pattern.compile("-- SORT_BEFORE_DIFF");
+    Matcher matcher = pattern.matcher(query);
+
+    if (matcher.find()) {
+      return true;
+    }
+    return false;
+  }
+
+  private boolean checkHadoopVersionExclude(String fileName, String query){
+
+    // Look for a hint to not run a test on some Hadoop versions
+    Pattern pattern = Pattern.compile("-- (EX|IN)CLUDE_HADOOP_MAJOR_VERSIONS\\((.*)\\)");
+
+    boolean excludeQuery = false;
+    boolean includeQuery = false;
+    Set<String> versionSet = new HashSet<String>();
+    String hadoopVer = ShimLoader.getMajorVersion();
+
+    Matcher matcher = pattern.matcher(query);
+
+    // Each qfile may include at most one INCLUDE or EXCLUDE directive.
+    //
+    // If a qfile contains an INCLUDE directive, and hadoopVer does
+    // not appear in the list of versions to include, then the qfile
+    // is skipped.
+    //
+    // If a qfile contains an EXCLUDE directive, and hadoopVer is
+    // listed in the list of versions to EXCLUDE, then the qfile is
+    // skipped.
+    //
+    // Otherwise, the qfile is included.
+
+    if (matcher.find()) {
+
+      String prefix = matcher.group(1);
+      if ("EX".equals(prefix)) {
+        excludeQuery = true;
+      } else {
+        includeQuery = true;
+      }
+
+      String versions = matcher.group(2);
+      for (String s : versions.split("\\,")) {
+        s = s.trim();
+        versionSet.add(s);
+      }
+    }
+
+    if (matcher.find()) {
+      //2nd match is not supposed to be there
+      String message = "QTestUtil: qfile " + fileName
+        + " contains more than one reference to (EX|IN)CLUDE_HADOOP_MAJOR_VERSIONS";
+      throw new UnsupportedOperationException(message);
+    }
+
+    if (excludeQuery && versionSet.contains(hadoopVer)) {
+      System.out.println("QTestUtil: " + fileName
+        + " EXCLUDE list contains Hadoop Version " + hadoopVer + ". Skipping...");
+      return true;
+    } else if (includeQuery && !versionSet.contains(hadoopVer)) {
+      System.out.println("QTestUtil: " + fileName
+        + " INCLUDE list does not contain Hadoop Version " + hadoopVer + ". Skipping...");
+      return true;
+    }
+    return false;
+  }
+
+  private boolean checkOSExclude(String fileName, String query){
+    // Look for a hint to not run a test on some Hadoop versions
+    Pattern pattern = Pattern.compile("-- (EX|IN)CLUDE_OS_WINDOWS");
+
+    // detect whether this query wants to be excluded or included
+    // on windows
+    Matcher matcher = pattern.matcher(query);
+    if (matcher.find()) {
+      String prefix = matcher.group(1);
+      if ("EX".equals(prefix)) {
+        //windows is to be exluded
+        if(Shell.WINDOWS){
+          System.out.println("Due to the OS being windows " +
+                             "adding the  query " + fileName +
+                             " to the set of tests to skip");
+          return true;
+        }
+      }
+      else  if(!Shell.WINDOWS){
+        //non windows to be exluded
+        System.out.println("Due to the OS not being windows " +
+                           "adding the  query " + fileName +
+                           " to the set of tests to skip");
+        return true;
+      }
+    }
+    return false;
+  }
+
+
+  /**
+   * Clear out any side effects of running tests
+   */
+  public void clearPostTestEffects() throws Exception {
+    setup.postTest(conf);
+  }
+
+  /**
+   * Clear out any side effects of running tests
+   */
+  public void clearTestSideEffects() throws Exception {
+    // Delete any tables other than the source tables
+    // and any databases other than the default database.
+    for (String dbName : db.getAllDatabases()) {
+      SessionState.get().setCurrentDatabase(dbName);
+      for (String tblName : db.getAllTables()) {
+        if (!DEFAULT_DATABASE_NAME.equals(dbName) || !srcTables.contains(tblName)) {
+          Table tblObj = db.getTable(tblName);
+          // dropping index table can not be dropped directly. Dropping the base
+          // table will automatically drop all its index table
+          if(tblObj.isIndexTable()) {
+            continue;
+          }
+          db.dropTable(dbName, tblName);
+        } else {
+          // this table is defined in srcTables, drop all indexes on it
+         List<Index> indexes = db.getIndexes(dbName, tblName, (short)-1);
+          if (indexes != null && indexes.size() > 0) {
+            for (Index index : indexes) {
+              db.dropIndex(dbName, tblName, index.getIndexName(), true);
+            }
+          }
+        }
+      }
+      if (!DEFAULT_DATABASE_NAME.equals(dbName)) {
+        db.dropDatabase(dbName);
+      }
+    }
+    SessionState.get().setCurrentDatabase(DEFAULT_DATABASE_NAME);
+
+    List<String> roleNames = db.getAllRoleNames();
+      for (String roleName : roleNames) {
+        db.dropRole(roleName);
+    }
+    // allocate and initialize a new conf since a test can
+    // modify conf by using 'set' commands
+    conf = new HiveConf (Driver.class);
+    initConf();
+    db = Hive.get(conf);  // propagate new conf to meta store
+    setup.preTest(conf);
+  }
+
+  public void cleanUp() throws Exception {
+    // Drop any tables that remain due to unsuccessful runs
+    for (String s : new String[] {"src", "src1", "src_json", "src_thrift",
+        "src_sequencefile", "srcpart", "srcbucket", "srcbucket2", "dest1",
+        "dest2", "dest3", "dest4", "dest4_sequencefile", "dest_j1", "dest_j2",
+        "dest_g1", "dest_g2", "fetchtask_ioexception"}) {
+      db.dropTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, s);
+    }
+
+    // delete any contents in the warehouse dir
+    Path p = new Path(testWarehouse);
+    FileSystem fs = p.getFileSystem(conf);
+
+    try {
+      FileStatus [] ls = fs.listStatus(p);
+      for (int i=0; (ls != null) && (i<ls.length); i++) {
+        fs.delete(ls[i].getPath(), true);
+      }
+    } catch (FileNotFoundException e) {
+      // Best effort
+    }
+
+    FunctionRegistry.unregisterTemporaryUDF("test_udaf");
+    FunctionRegistry.unregisterTemporaryUDF("test_error");
+  }
+
+  private void runLoadCmd(String loadCmd) throws Exception {
+    int ecode = 0;
+    ecode = drv.run(loadCmd).getResponseCode();
+    drv.close();
+    if (ecode != 0) {
+      throw new Exception("load command: " + loadCmd
+          + " failed with exit code= " + ecode);
+    }
+    return;
+  }
+
+  private void runCreateTableCmd(String createTableCmd) throws Exception {
+    int ecode = 0;
+    ecode = drv.run(createTableCmd).getResponseCode();
+    if (ecode != 0) {
+      throw new Exception("create table command: " + createTableCmd
+          + " failed with exit code= " + ecode);
+    }
+
+    return;
+  }
+
+  public void createSources() throws Exception {
+
+    startSessionState();
+    conf.setBoolean("hive.test.init.phase", true);
+
+    // Create a bunch of tables with columns key and value
+    LinkedList<String> cols = new LinkedList<String>();
+    cols.add("key");
+    cols.add("value");
+
+    LinkedList<String> part_cols = new LinkedList<String>();
+    part_cols.add("ds");
+    part_cols.add("hr");
+    db.createTable("srcpart", cols, part_cols, TextInputFormat.class,
+        IgnoreKeyTextOutputFormat.class);
+
+    Path fpath;
+    HashMap<String, String> part_spec = new HashMap<String, String>();
+    for (String ds : new String[] {"2008-04-08", "2008-04-09"}) {
+      for (String hr : new String[] {"11", "12"}) {
+        part_spec.clear();
+        part_spec.put("ds", ds);
+        part_spec.put("hr", hr);
+        // System.out.println("Loading partition with spec: " + part_spec);
+        // db.createPartition(srcpart, part_spec);
+        fpath = new Path(testFiles, "kv1.txt");
+        // db.loadPartition(fpath, srcpart.getName(), part_spec, true);
+        runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
+            + "' OVERWRITE INTO TABLE srcpart PARTITION (ds='" + ds + "',hr='"
+            + hr + "')");
+      }
+    }
+    ArrayList<String> bucketCols = new ArrayList<String>();
+    bucketCols.add("key");
+    runCreateTableCmd("CREATE TABLE srcbucket(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE");
+    // db.createTable("srcbucket", cols, null, TextInputFormat.class,
+    // IgnoreKeyTextOutputFormat.class, 2, bucketCols);
+    for (String fname : new String[] {"srcbucket0.txt", "srcbucket1.txt"}) {
+      fpath = new Path(testFiles, fname);
+      runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
+          + "' INTO TABLE srcbucket");
+    }
+
+    runCreateTableCmd("CREATE TABLE srcbucket2(key int, value string) "
+        + "CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE");
+    // db.createTable("srcbucket", cols, null, TextInputFormat.class,
+    // IgnoreKeyTextOutputFormat.class, 2, bucketCols);
+    for (String fname : new String[] {"srcbucket20.txt", "srcbucket21.txt",
+        "srcbucket22.txt", "srcbucket23.txt"}) {
+      fpath = new Path(testFiles, fname);
+      runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
+          + "' INTO TABLE srcbucket2");
+    }
+
+    for (String tname : new String[] {"src", "src1"}) {
+      db.createTable(tname, cols, null, TextInputFormat.class,
+          IgnoreKeyTextOutputFormat.class);
+    }
+    db.createTable("src_sequencefile", cols, null,
+        SequenceFileInputFormat.class, SequenceFileOutputFormat.class);
+
+    Table srcThrift =
+        new Table(SessionState.get().getCurrentDatabase(), "src_thrift");
+    srcThrift.setInputFormatClass(SequenceFileInputFormat.class.getName());
+    srcThrift.setOutputFormatClass(SequenceFileOutputFormat.class.getName());
+    srcThrift.setSerializationLib(ThriftDeserializer.class.getName());
+    srcThrift.setSerdeParam(serdeConstants.SERIALIZATION_CLASS, Complex.class
+        .getName());
+    srcThrift.setSerdeParam(serdeConstants.SERIALIZATION_FORMAT,
+        TBinaryProtocol.class.getName());
+    db.createTable(srcThrift);
+
+    LinkedList<String> json_cols = new LinkedList<String>();
+    json_cols.add("json");
+    db.createTable("src_json", json_cols, null, TextInputFormat.class,
+        IgnoreKeyTextOutputFormat.class);
+
+    // load the input data into the src table
+    fpath = new Path(testFiles, "kv1.txt");
+    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath() + "' INTO TABLE src");
+
+    // load the input data into the src table
+    fpath = new Path(testFiles, "kv3.txt");
+    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath() + "' INTO TABLE src1");
+
+    // load the input data into the src_sequencefile table
+    fpath = new Path(testFiles, "kv1.seq");
+    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
+        + "' INTO TABLE src_sequencefile");
+
+    // load the input data into the src_thrift table
+    fpath = new Path(testFiles, "complex.seq");
+    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
+        + "' INTO TABLE src_thrift");
+
+    // load the json data into the src_json table
+    fpath = new Path(testFiles, "json.txt");
+    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
+        + "' INTO TABLE src_json");
+    conf.setBoolean("hive.test.init.phase", false);
+  }
+
+  public void init() throws Exception {
+    // System.out.println(conf.toString());
+    testWarehouse = conf.getVar(HiveConf.ConfVars.METASTOREWAREHOUSE);
+    // conf.logVars(System.out);
+    // System.out.flush();
+
+    SessionState.start(conf);
+    db = Hive.get(conf);
+    fs = FileSystem.get(conf);
+    drv = new Driver(conf);
+    drv.init();
+    pd = new ParseDriver();
+    sem = new SemanticAnalyzer(conf);
+  }
+
+  public void init(String tname) throws Exception {
+    cleanUp();
+    createSources();
+
+    LinkedList<String> cols = new LinkedList<String>();
+    cols.add("key");
+    cols.add("value");
+
+    LinkedList<String> part_cols = new LinkedList<String>();
+    part_cols.add("ds");
+    part_cols.add("hr");
+
+    db.createTable("dest1", cols, null, TextInputFormat.class,
+        IgnoreKeyTextOutputFormat.class);
+    db.createTable("dest2", cols, null, TextInputFormat.class,
+        IgnoreKeyTextOutputFormat.class);
+
+    db.createTable("dest3", cols, part_cols, TextInputFormat.class,
+        IgnoreKeyTextOutputFormat.class);
+    Table dest3 = db.getTable("dest3");
+
+    HashMap<String, String> part_spec = new HashMap<String, String>();
+    part_spec.put("ds", "2008-04-08");
+    part_spec.put("hr", "12");
+    db.createPartition(dest3, part_spec);
+
+    db.createTable("dest4", cols, null, TextInputFormat.class,
+        IgnoreKeyTextOutputFormat.class);
+    db.createTable("dest4_sequencefile", cols, null,
+        SequenceFileInputFormat.class, SequenceFileOutputFormat.class);
+  }
+
+  public void cliInit(String tname) throws Exception {
+    cliInit(tname, true);
+  }
+
+  public void cliInit(String tname, boolean recreate) throws Exception {
+    if (recreate) {
+      cleanUp();
+      createSources();
+    }
+
+    HiveConf.setVar(conf, HiveConf.ConfVars.HIVE_AUTHENTICATOR_MANAGER,
+    "org.apache.hadoop.hive.ql.security.DummyAuthenticator");
+    CliSessionState ss = new CliSessionState(conf);
+    assert ss != null;
+    ss.in = System.in;
+
+    File qf = new File(outDir, tname);
+    File outf = null;
+    outf = new File(logDir);
+    outf = new File(outf, qf.getName().concat(".out"));
+    FileOutputStream fo = new FileOutputStream(outf);
+    ss.out = new PrintStream(fo, true, "UTF-8");
+    ss.err = new CachingPrintStream(fo, true, "UTF-8");
+    ss.setIsSilent(true);
+    SessionState oldSs = SessionState.get();
+    if (oldSs != null && oldSs.out != null && oldSs.out != System.out) {
+      oldSs.out.close();
+    }
+    SessionState.start(ss);
+
+    cliDriver = new CliDriver();
+    if (tname.equals("init_file.q")) {
+      ss.initFiles.add("../../data/scripts/test_init_file.sql");
+    }
+    cliDriver.processInitFiles(ss);
+  }
+
+  private CliSessionState startSessionState()
+      throws FileNotFoundException, UnsupportedEncodingException {
+
+    HiveConf.setVar(conf, HiveConf.ConfVars.HIVE_AUTHENTICATOR_MANAGER,
+        "org.apache.hadoop.hive.ql.security.DummyAuthenticator");
+
+    CliSessionState ss = new CliSessionState(conf);
+    assert ss != null;
+
+    SessionState.start(ss);
+    return ss;
+  }
+
+  public int executeOne(String tname) {
+    String q = qMap.get(tname);
+
+    if (q.indexOf(";") == -1) {
+      return -1;
+    }
+
+    String q1 = q.substring(0, q.indexOf(";") + 1);
+    String qrest = q.substring(q.indexOf(";") + 1);
+    qMap.put(tname, qrest);
+
+    System.out.println("Executing " + q1);
+    return cliDriver.processLine(q1);
+  }
+
+  public int execute(String tname) {
+    try {
+      return drv.run(qMap.get(tname)).getResponseCode();
+    } catch (CommandNeedRetryException e) {
+      // TODO Auto-generated catch block
+      e.printStackTrace();
+      return -1;
+    }
+  }
+
+  public int executeClient(String tname) {
+    String commands = qMap.get(tname);
+    StringBuilder newCommands = new StringBuilder(commands.length());
+    int lastMatchEnd = 0;
+    Matcher commentMatcher = Pattern.compile("^--.*$", Pattern.MULTILINE).matcher(commands);
+    while (commentMatcher.find()) {
+      newCommands.append(commands.substring(lastMatchEnd, commentMatcher.start()));
+      newCommands.append(commentMatcher.group().replaceAll("(?<!\\\\);", "\\\\;"));
+      lastMatchEnd = commentMatcher.end();
+    }
+    newCommands.append(commands.substring(lastMatchEnd, commands.length()));
+    commands = newCommands.toString();
+    return cliDriver.processLine(commands);
+  }
+
+  public boolean shouldBeSkipped(String tname) {
+    return qSkipSet.contains(tname);
+  }
+
+  public void convertSequenceFileToTextFile() throws Exception {
+    // Create an instance of hive in order to create the tables
+    testWarehouse = conf.getVar(HiveConf.ConfVars.METASTOREWAREHOUSE);
+    db = Hive.get(conf);
+    // Create dest4 to replace dest4_sequencefile
+    LinkedList<String> cols = new LinkedList<String>();
+    cols.add("key");
+    cols.add("value");
+
+    // Move all data from dest4_sequencefile to dest4
+    drv
+        .run("FROM dest4_sequencefile INSERT OVERWRITE TABLE dest4 SELECT dest4_sequencefile.*");
+
+    // Drop dest4_sequencefile
+    db.dropTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, "dest4_sequencefile",
+        true, true);
+  }
+
+  public int checkNegativeResults(String tname, Exception e) throws Exception {
+
+    File qf = new File(outDir, tname);
+    String expf = outPath(outDir.toString(), tname.concat(".out"));
+
+    File outf = null;
+    outf = new File(logDir);
+    outf = new File(outf, qf.getName().concat(".out"));
+
+    FileWriter outfd = new FileWriter(outf);
+    if (e instanceof ParseException) {
+      outfd.write("Parse Error: ");
+    } else if (e instanceof SemanticException) {
+      outfd.write("Semantic Exception: \n");
+    } else {
+      throw e;
+    }
+
+    outfd.write(e.getMessage());
+    outfd.close();
+
+    int exitVal = executeDiffCommand(outf.getPath(), expf, false,
+                                     qSortSet.contains(qf.getName()));
+    if (exitVal != 0 && overWrite) {
+      exitVal = overwriteResults(outf.getPath(), expf);
+    }
+
+    return exitVal;
+  }
+
+  public int checkParseResults(String tname, ASTNode tree) throws Exception {
+
+    if (tree != null) {
+      File parseDir = new File(outDir, "parse");
+      String expf = outPath(parseDir.toString(), tname.concat(".out"));
+
+      File outf = null;
+      outf = new File(logDir);
+      outf = new File(outf, tname.concat(".out"));
+
+      FileWriter outfd = new FileWriter(outf);
+      outfd.write(tree.toStringTree());
+      outfd.close();
+
+      int exitVal = executeDiffCommand(outf.getPath(), expf, false, false);
+
+      if (exitVal != 0 && overWrite) {
+        exitVal = overwriteResults(outf.getPath(), expf);
+      }
+
+      return exitVal;
+    } else {
+      throw new Exception("Parse tree is null");
+    }
+  }
+
+  private final Pattern[] xmlPlanMask = toPattern(new String[] {
+      "<java version=\".*\" class=\"java.beans.XMLDecoder\">",
+      "<string>.*/tmp/.*</string>",
+      "<string>file:.*</string>",
+      "<string>pfile:.*</string>",
+      "<string>[0-9]{10}</string>",
+      "<string>/.*/warehouse/.*</string>"
+  });
+
+  public int checkPlan(String tname, List<Task<? extends Serializable>> tasks) throws Exception {
+
+    if (tasks == null) {
+      throw new Exception("Plan is null");
+    }
+    File planDir = new File(outDir, "plan");
+    String planFile = outPath(planDir.toString(), tname + ".xml");
+
+    File outf = null;
+    outf = new File(logDir);
+    outf = new File(outf, tname.concat(".xml"));
+
+    FileOutputStream ofs = new FileOutputStream(outf);
+    try {
+      conf.set(HiveConf.ConfVars.PLAN_SERIALIZATION.varname, "javaXML");
+      for (Task<? extends Serializable> plan : tasks) {
+        Utilities.serializePlan(plan, ofs, conf);
+      }
+
+      fixXml4JDK7(outf.getPath());
+      maskPatterns(xmlPlanMask, outf.getPath());
+
+      int exitVal = executeDiffCommand(outf.getPath(), planFile, true, false);
+
+      if (exitVal != 0 && overWrite) {
+        exitVal = overwriteResults(outf.getPath(), planFile);
+      }
+      return exitVal;
+    } finally {
+      conf.set(HiveConf.ConfVars.PLAN_SERIALIZATION.varname, "kryo");
+    }
+  }
+
+  /**
+   * Given the current configurations (e.g., hadoop version and execution mode), return
+   * the correct file name to compare with the current test run output.
+   * @param outDir The directory where the reference log files are stored.
+   * @param testName The test file name (terminated by ".out").
+   * @return The file name appended with the configuration values if it exists.
+   */
+  public String outPath(String outDir, String testName) {
+    String ret = (new File(outDir, testName)).getPath();
+    // List of configurations. Currently the list consists of hadoop version and execution mode only
+    List<String> configs = new ArrayList<String>();
+    configs.add(this.hadoopVer);
+
+    Deque<String> stack = new LinkedList<String>();
+    StringBuilder sb = new StringBuilder();
+    sb.append(testName);
+    stack.push(sb.toString());
+
+    // example file names are input1.q.out_0.20.0_minimr or input2.q.out_0.17
+    for (String s: configs) {
+      sb.append('_');
+      sb.append(s);
+      stack.push(sb.toString());
+    }
+    while (stack.size() > 0) {
+      String fileName = stack.pop();
+      File f = new File(outDir, fileName);
+      if (f.exists()) {
+        ret = f.getPath();
+        break;
+      }
+    }
+   return ret;
+  }
+
+  /**
+   * Fix the XML generated by JDK7 which is slightly different from what's generated by JDK6,
+   * causing 40+ test failures. There are mainly two problems:
+   *
+   * 1. object element's properties, id and class, are in reverse order, i.e.
+   *    <object class="org.apache.hadoop.hive.ql.exec.MapRedTask" id="MapRedTask0">
+   *    which needs to be fixed to
+   *    <object id="MapRedTask0" class="org.apache.hadoop.hive.ql.exec.MapRedTask">
+   * 2. JDK introduces Enum as class, i.e.
+   *    <object id="GenericUDAFEvaluator$Mode0" class="java.lang.Enum">
+   *      <class>org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator$Mode</class>
+   *    which needs to be fixed to
+   *    <object id="GenericUDAFEvaluator$Mode0" class="org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator$Mode"
+   *     method="valueOf">
+   *
+   * Though not elegant, this allows these test cases to pass until we have a better serialization mechanism.
+   *
+   * Did I mention this is test code?
+   *
+   * @param fname the name of the file to fix
+   * @throws Exception in case of IO error
+   */
+  private static void fixXml4JDK7(String fname) throws Exception {
+    String version = System.getProperty("java.version");
+    if (!version.startsWith("1.7")) {
+      return;
+    }
+
+    BufferedReader in = new BufferedReader(new FileReader(fname));
+    BufferedWriter out = new BufferedWriter(new FileWriter(fname + ".orig"));
+    String line = null;
+    while (null != (line = in.readLine())) {
+      out.write(line);
+      out.write('\n');
+    }
+    in.close();
+    out.close();
+
+    in = new BufferedReader(new FileReader(fname + ".orig"));
+    out = new BufferedWriter(new FileWriter(fname));
+
+    while (null != (line = in.readLine())) {
+      if (line.indexOf("<object ") == -1 || line.indexOf("class=") == -1) {
+        out.write(line);
+      } else {
+        StringBuilder sb = new StringBuilder();
+        String prefix = line.substring(0, line.indexOf("<object") + 7);
+        sb.append( prefix );
+        String postfix = line.substring(line.lastIndexOf('"') + 1);
+        String id = getPropertyValue(line, "id");
+        if (id != null) {
+          sb.append(" id=" + id);
+        }
+        String cls = getPropertyValue(line, "class");
+        assert(cls != null);
+        if (cls.equals("\"java.lang.Enum\"")) {
+          line = in.readLine();
+          cls = "\"" + getElementValue(line, "class") + "\"";
+          sb.append(" class=" + cls + " method=\"valueOf\"" );
+        } else {
+          sb.append(" class=" + cls);
+        }
+
+        sb.append(postfix);
+        out.write(sb.toString());
+      }
+
+      out.write('\n');
+    }
+
+    in.close();
+    out.close();
+  }
+
+  /**
+   * Get the value of a property in line. The returned value has original quotes
+   */
+  private static String getPropertyValue(String line, String name) {
+    int start = line.indexOf( name + "=" );
+    if (start == -1) {
+      return null;
+    }
+    start += name.length() + 1;
+    int end = line.indexOf("\"", start + 1);
+    return line.substring( start, end + 1 );
+  }
+
+  /**
+   * Get the value of the element in input. (Note: the returned value has no quotes.)
+   */
+  private static String getElementValue(String line, String name) {
+    assert(line.contains("<" + name + ">"));
+    int start = line.indexOf("<" + name + ">") + name.length() + 2;
+    int end = line.indexOf("</" + name + ">");
+    return line.substring(start, end);
+  }
+
+  private Pattern[] toPattern(String[] patternStrs) {
+    Pattern[] patterns = new Pattern[patternStrs.length];
+    for (int i = 0; i < patternStrs.length; i++) {
+      patterns[i] = Pattern.compile(patternStrs[i]);
+    }
+    return patterns;
+  }
+
+  private void maskPatterns(Pattern[] patterns, String fname) throws Exception {
+    String maskPattern = "#### A masked pattern was here ####";
+
+    String line;
+    BufferedReader in;
+    BufferedWriter out;
+
+    in = new BufferedReader(new FileReader(fname));
+    out = new BufferedWriter(new FileWriter(fname + ".orig"));
+    while (null != (line = in.readLine())) {
+      // Ignore the empty lines on windows
+      if(line.isEmpty() && Shell.WINDOWS) {
+        continue;
+      }
+      out.write(line);
+      out.write('\n');
+    }
+    in.close();
+    out.close();
+
+    in = new BufferedReader(new FileReader(fname + ".orig"));
+    out = new BufferedWriter(new FileWriter(fname));
+
+    boolean lastWasMasked = false;
+    while (null != (line = in.readLine())) {
+      for (Pattern pattern : patterns) {
+        line = pattern.matcher(line).replaceAll(maskPattern);
+      }
+
+      if (line.equals(maskPattern)) {
+        // We're folding multiple masked lines into one.
+        if (!lastWasMasked) {
+          out.write(line);
+          out.write("\n");
+          lastWasMasked = true;
+        }
+      } else {
+        out.write(line);
+        out.write("\n");
+        lastWasMasked = false;
+      }
+    }
+
+    in.close();
+    out.close();
+  }
+
+  private final Pattern[] planMask = toPattern(new String[] {
+      ".*file:.*",
+      ".*pfile:.*",
+      ".*hdfs:.*",
+      ".*/tmp/.*",
+      ".*invalidscheme:.*",
+      ".*lastUpdateTime.*",
+      ".*lastAccessTime.*",
+      ".*lastModifiedTime.*",
+      ".*[Oo]wner.*",
+      ".*CreateTime.*",
+      ".*LastAccessTime.*",
+      ".*Location.*",
+      ".*LOCATION '.*",
+      ".*transient_lastDdlTime.*",
+      ".*last_modified_.*",
+      ".*at org.*",
+      ".*at sun.*",
+      ".*at java.*",
+      ".*at junit.*",
+      ".*Caused by:.*",
+      ".*LOCK_QUERYID:.*",
+      ".*LOCK_TIME:.*",
+      ".*grantTime.*",
+      ".*[.][.][.] [0-9]* more.*",
+      ".*job_[0-9_]*.*",
+      ".*job_local[0-9_]*.*",
+      ".*USING 'java -cp.*",
+      "^Deleted.*",
+  });
+
+  public int checkCliDriverResults(String tname) throws Exception {
+    assert(qMap.containsKey(tname));
+
+    String outFileName = outPath(outDir, tname + ".out");
+
+    File f = new File(logDir, tname + ".out");
+
+    maskPatterns(planMask, f.getPath());
+    int exitVal = executeDiffCommand(f.getPath(),
+                                     outFileName, false,
+                                     qSortSet.contains(tname));
+
+    if (exitVal != 0 && overWrite) {
+      exitVal = overwriteResults(f.getPath(), outFileName);
+    }
+
+    return exitVal;
+  }
+
+  private static int overwriteResults(String inFileName, String outFileName) throws Exception {
+    // This method can be replaced with Files.copy(source, target, REPLACE_EXISTING)
+    // once Hive uses JAVA 7.
+    System.out.println("Overwriting results " + inFileName + " to " + outFileName);
+    return executeCmd(new String[] {
+        "cp",
+        getQuotedString(inFileName),
+        getQuotedString(outFileName)
+      });
+  }
+
+  private static int executeDiffCommand(String inFileName,
+      String outFileName,
+      boolean ignoreWhiteSpace,
+      boolean sortResults
+      ) throws Exception {
+
+    int result = 0;
+
+    if (sortResults) {
+      // sort will try to open the output file in write mode on windows. We need to
+      // close it first.
+      SessionState ss = SessionState.get();
+      if (ss != null && ss.out != null && ss.out != System.out) {
+	ss.out.close();
+      }
+
+      String inSorted = inFileName + SORT_SUFFIX;
+      String outSorted = outFileName + SORT_SUFFIX;
+
+      result = sortFiles(inFileName, inSorted);
+      result |= sortFiles(outFileName, outSorted);
+      if (result != 0) {
+        System.err.println("ERROR: Could not sort files before comparing");
+        return result;
+      }
+      inFileName = inSorted;
+      outFileName = outSorted;
+    }
+
+    ArrayList<String> diffCommandArgs = new ArrayList<String>();
+    diffCommandArgs.add("diff");
+
+    // Text file comparison
+    diffCommandArgs.add("-a");
+
+    // Ignore changes in the amount of white space
+    if (ignoreWhiteSpace || Shell.WINDOWS) {
+      diffCommandArgs.add("-b");
+    }
+
+    // Files created on Windows machines have different line endings
+    // than files created on Unix/Linux. Windows uses carriage return and line feed
+    // ("\r\n") as a line ending, whereas Unix uses just line feed ("\n").
+    // Also StringBuilder.toString(), Stream to String conversions adds extra
+    // spaces at the end of the line.
+    if (Shell.WINDOWS) {
+      diffCommandArgs.add("--strip-trailing-cr"); // Strip trailing carriage return on input
+      diffCommandArgs.add("-B"); // Ignore changes whose lines are all blank
+    }
+    // Add files to compare to the arguments list
+    diffCommandArgs.add(getQuotedString(inFileName));
+    diffCommandArgs.add(getQuotedString(outFileName));
+
+    result = executeCmd(diffCommandArgs);
+
+    if (sortResults) {
+      new File(inFileName).delete();
+      new File(outFileName).delete();
+    }
+
+    return result;
+  }
+
+  private static int sortFiles(String in, String out) throws Exception {
+    return executeCmd(new String[] {
+        "sort",
+        getQuotedString(in),
+      }, out, null);
+  }
+
+  private static int executeCmd(Collection<String> args) throws Exception {
+    return executeCmd(args, null, null);
+  }
+
+  private static int executeCmd(String[] args) throws Exception {
+    return executeCmd(args, null, null);
+  }
+
+  private static int executeCmd(Collection<String> args, String outFile, String errFile) throws Exception {
+    String[] cmdArray = (String[]) args.toArray(new String[args.size()]);
+    return executeCmd(cmdArray, outFile, errFile);
+  }
+
+  private static int executeCmd(String[] args, String outFile, String errFile) throws Exception {
+    System.out.println("Running: " + org.apache.commons.lang.StringUtils.join(args, ' '));
+
+    PrintStream out = outFile == null ?
+      SessionState.getConsole().getChildOutStream() :
+      new PrintStream(new FileOutputStream(outFile), true);
+    PrintStream err = errFile == null ?
+      SessionState.getConsole().getChildErrStream() :
+      new PrintStream(new FileOutputStream(errFile), true);
+
+    Process executor = Runtime.getRuntime().exec(args);
+
+    StreamPrinter errPrinter = new StreamPrinter(executor.getErrorStream(), null, err);
+    StreamPrinter outPrinter = new StreamPrinter(executor.getInputStream(), null, out);
+
+    outPrinter.start();
+    errPrinter.start();
+
+    int result = executor.waitFor();
+
+    outPrinter.join();
+    errPrinter.join();
+
+    if (outFile != null) {
+      out.close();
+    }
+
+    if (errFile != null) {
+      err.close();
+    }
+
+    return result;
+  }
+
+  private static String getQuotedString(String str){
+    return Shell.WINDOWS ? String.format("\"%s\"", str) : str;
+  }
+
+  public ASTNode parseQuery(String tname) throws Exception {
+    return pd.parse(qMap.get(tname));
+  }
+
+  public void resetParser() throws SemanticException {
+    drv.init();
+    pd = new ParseDriver();
+    sem = new SemanticAnalyzer(conf);
+  }
+
+
+  public List<Task<? extends Serializable>> analyzeAST(ASTNode ast) throws Exception {
+
+    // Do semantic analysis and plan generation
+    Context ctx = new Context(conf);
+    while ((ast.getToken() == null) && (ast.getChildCount() > 0)) {
+      ast = (ASTNode) ast.getChild(0);
+    }
+    sem.getOutputs().clear();
+    sem.getInputs().clear();
+    sem.analyze(ast, ctx);
+    ctx.clear();
+    return sem.getRootTasks();
+  }
+
+  public TreeMap<String, String> getQMap() {
+    return qMap;
+  }
+
+  /**
+   * QTestSetup defines test fixtures which are reused across testcases,
+   * and are needed before any test can be run
+   */
+  public static class QTestSetup
+  {
+    private MiniZooKeeperCluster zooKeeperCluster = null;
+    private int zkPort;
+    private ZooKeeper zooKeeper;
+
+    public QTestSetup() {
+    }
+
+    public void preTest(HiveConf conf) throws Exception {
+
+      if (zooKeeperCluster == null) {
+        String tmpdir =  System.getProperty("test.tmp.dir");
+        zooKeeperCluster = new MiniZooKeeperCluster();
+        zkPort = zooKeeperCluster.startup(new File(tmpdir, "zookeeper"));
+      }
+
+      if (zooKeeper != null) {
+        zooKeeper.close();
+      }
+
+      int sessionTimeout = conf.getIntVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_SESSION_TIMEOUT);
+      zooKeeper = new ZooKeeper("localhost:" + zkPort, sessionTimeout, new Watcher() {
+        @Override
+        public void process(WatchedEvent arg0) {
+        }
+      });
+
+      String zkServer = "localhost";
+      conf.set("hive.zookeeper.quorum", zkServer);
+      conf.set("hive.zookeeper.client.port", "" + zkPort);
+    }
+
+    public void postTest(HiveConf conf) throws Exception {
+      if (zooKeeperCluster == null) {
+        return;
+      }
+
+      if (zooKeeper != null) {
+        zooKeeper.close();
+      }
+
+      ZooKeeperHiveLockManager.releaseAllLocks(conf);
+    }
+
+    public void tearDown() throws Exception {
+      if (zooKeeperCluster != null) {
+        zooKeeperCluster.shutdown();
+        zooKeeperCluster = null;
+      }
+    }
+  }
+
+  /**
+   * QTRunner: Runnable class for running a a single query file.
+   *
+   **/
+  public static class QTRunner implements Runnable {
+    private final QTestUtil qt;
+    private final String fname;
+
+    public QTRunner(QTestUtil qt, String fname) {
+      this.qt = qt;
+      this.fname = fname;
+    }
+
+    public void run() {
+      try {
+        // assumption is that environment has already been cleaned once globally
+        // hence each thread does not call cleanUp() and createSources() again
+        qt.cliInit(fname, false);
+        qt.executeClient(fname);
+      } catch (Throwable e) {
+        System.err.println("Query file " + fname + " failed with exception "
+            + e.getMessage());
+        e.printStackTrace();
+        outputTestFailureHelpMessage();
+      }
+    }
+  }
+
+  /**
+   * Setup to execute a set of query files. Uses QTestUtil to do so.
+   *
+   * @param qfiles
+   *          array of input query files containing arbitrary number of hive
+   *          queries
+   * @param resDir
+   *          output directory
+   * @param logDir
+   *          log directory
+   * @return one QTestUtil for each query file
+   */
+  public static QTestUtil[] queryListRunnerSetup(File[] qfiles, String resDir,
+      String logDir) throws Exception
+  {
+    QTestUtil[] qt = new QTestUtil[qfiles.length];
+    for (int i = 0; i < qfiles.length; i++) {
+      qt[i] = new QTestUtil(resDir, logDir, false, "0.20");
+      qt[i].addFile(qfiles[i]);
+      qt[i].clearTestSideEffects();
+    }
+
+    return qt;
+  }
+
+  /**
+   * Executes a set of query files in sequence.
+   *
+   * @param qfiles
+   *          array of input query files containing arbitrary number of hive
+   *          queries
+   * @param qt
+   *          array of QTestUtils, one per qfile
+   * @return true if all queries passed, false otw
+   */
+  public static boolean queryListRunnerSingleThreaded(File[] qfiles, QTestUtil[] qt)
+    throws Exception
+  {
+    boolean failed = false;
+    qt[0].cleanUp();
+    qt[0].createSources();
+    for (int i = 0; i < qfiles.length && !failed; i++) {
+      qt[i].clearTestSideEffects();
+      qt[i].cliInit(qfiles[i].getName(), false);
+      qt[i].executeClient(qfiles[i].getName());
+      int ecode = qt[i].checkCliDriverResults(qfiles[i].getName());
+      if (ecode != 0) {
+        failed = true;
+        System.err.println("Test " + qfiles[i].getName()
+            + " results check failed with error code " + ecode);
+        outputTestFailureHelpMessage();
+      }
+      qt[i].clearPostTestEffects();
+    }
+    return (!failed);
+  }
+
+  /**
+   * Executes a set of query files parallel.
+   *
+   * Each query file is run in a separate thread. The caller has to arrange
+   * that different query files do not collide (in terms of destination tables)
+   *
+   * @param qfiles
+   *          array of input query files containing arbitrary number of hive
+   *          queries
+   * @param qt
+   *          array of QTestUtils, one per qfile
+   * @return true if all queries passed, false otw
+   *
+   */
+  public static boolean queryListRunnerMultiThreaded(File[] qfiles, QTestUtil[] qt)
+    throws Exception
+  {
+    boolean failed = false;
+
+    // in multithreaded mode - do cleanup/initialization just once
+
+    qt[0].cleanUp();
+    qt[0].createSources();
+    qt[0].clearTestSideEffects();
+
+    QTRunner[] qtRunners = new QTestUtil.QTRunner[qfiles.length];
+    Thread[] qtThread = new Thread[qfiles.length];
+
+    for (int i = 0; i < qfiles.length; i++) {
+      qtRunners[i] = new QTestUtil.QTRunner(qt[i], qfiles[i].getName());
+      qtThread[i] = new Thread(qtRunners[i]);
+    }
+
+    for (int i = 0; i < qfiles.length; i++) {
+      qtThread[i].start();
+    }
+
+    for (int i = 0; i < qfiles.length; i++) {
+      qtThread[i].join();
+      int ecode = qt[i].checkCliDriverResults(qfiles[i].getName());
+      if (ecode != 0) {
+        failed = true;
+        System.err.println("Test " + qfiles[i].getName()
+            + " results check failed with error code " + ecode);
+        outputTestFailureHelpMessage();
+      }
+    }
+    return (!failed);
+  }
+
+  public static void outputTestFailureHelpMessage() {
+    System.err.println("See build/ql/tmp/hive.log, "
+        + "or try \"ant test ... -Dtest.silent=false\" to get more logs.");
+    System.err.flush();
+  }
+
+  public static String ensurePathEndsInSlash(String path) {
+    if(path == null) {
+      throw new NullPointerException("Path cannot be null");
+    }
+    if(path.endsWith(File.separator)) {
+      return path;
+    } else {
+      return path + File.separator;
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java
new file mode 100644
index 0000000..14fc430
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java
@@ -0,0 +1,84 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.Arrays;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Set;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.QueryPlan;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+
+import org.apache.hadoop.hive.ql.parse.ColumnAccessInfo;
+import org.mortbay.log.Log;
+
+/*
+ * This hook is used for verifying the column access information
+ * that is generated and maintained in the QueryPlan object by the
+ * ColumnAccessAnalyer. All the hook does is print out the columns
+ * accessed from each table as recorded in the ColumnAccessInfo
+ * in the QueryPlan.
+ */
+public class CheckColumnAccessHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    Log.info("Running CheckColumnAccessHook");
+    HiveConf conf = hookContext.getConf();
+    if (conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_COLLECT_SCANCOLS) == false) {
+      return;
+    }
+
+    QueryPlan plan = hookContext.getQueryPlan();
+    if (plan == null) {
+      return;
+    }
+
+    ColumnAccessInfo columnAccessInfo = hookContext.getQueryPlan().getColumnAccessInfo();
+    if (columnAccessInfo == null) {
+      return;
+    }
+
+    LogHelper console = SessionState.getConsole();
+    Map<String, Set<String>> tableToColumnAccessMap =
+      columnAccessInfo.getTableToColumnAccessMap();
+
+    // We need a new map to ensure output is always produced in the same order.
+    // This makes tests that use this hook deterministic.
+    Map<String, String> outputOrderedMap = new HashMap<String, String>();
+
+    for (Map.Entry<String, Set<String>> tableAccess : tableToColumnAccessMap.entrySet()) {
+      StringBuilder perTableInfo = new StringBuilder();
+      perTableInfo.append("Table:").append(tableAccess.getKey()).append("\n");
+      // Sort columns to make output deterministic
+      String[] columns = new String[tableAccess.getValue().size()];
+      tableAccess.getValue().toArray(columns);
+      Arrays.sort(columns);
+      perTableInfo.append("Columns:").append(StringUtils.join(columns, ','))
+        .append("\n");
+      outputOrderedMap.put(tableAccess.getKey(), perTableInfo.toString());
+    }
+
+    for (String perOperatorInfo : outputOrderedMap.values()) {
+      console.printError(perOperatorInfo);
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckQueryPropertiesHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckQueryPropertiesHook.java
new file mode 100644
index 0000000..faea9f1
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckQueryPropertiesHook.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import org.apache.hadoop.hive.ql.QueryProperties;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+
+/**
+ *
+ * CheckQueryPropertiesHook.
+ *
+ * This hook prints the values in the QueryProperties object contained in the QueryPlan
+ * in the HookContext passed to the hook.
+ */
+public class CheckQueryPropertiesHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    LogHelper console = SessionState.getConsole();
+
+    if (console == null) {
+      return;
+    }
+
+    QueryProperties queryProps = hookContext.getQueryPlan().getQueryProperties();
+
+    if (queryProps != null) {
+      console.printError("Has Join: " + queryProps.hasJoin());
+      console.printError("Has Group By: " + queryProps.hasGroupBy());
+      console.printError("Has Sort By: " + queryProps.hasSortBy());
+      console.printError("Has Order By: " + queryProps.hasOrderBy());
+      console.printError("Has Group By After Join: " + queryProps.hasJoinFollowedByGroupBy());
+      console.printError("Uses Script: " + queryProps.usesScript());
+      console.printError("Has Distribute By: " + queryProps.hasDistributeBy());
+      console.printError("Has Cluster By: " + queryProps.hasClusterBy());
+    }
+  }
+}
\ No newline at end of file
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java
new file mode 100644
index 0000000..8e19fad
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java
@@ -0,0 +1,85 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.List;
+import java.util.Map;
+import java.util.HashMap;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.QueryPlan;
+import org.apache.hadoop.hive.ql.parse.TableAccessInfo;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.plan.OperatorDesc;
+
+/*
+ * This hook is used for verifying the table access key information
+ * that is generated and maintained in the QueryPlan object by the
+ * TableAccessAnalyer. All the hook does is print out the table/keys
+ * per operator recorded in the TableAccessInfo in the QueryPlan.
+ */
+public class CheckTableAccessHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    HiveConf conf = hookContext.getConf();
+    if (conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_COLLECT_TABLEKEYS) == false) {
+      return;
+    }
+
+    QueryPlan plan = hookContext.getQueryPlan();
+    if (plan == null) {
+      return;
+    }
+
+    TableAccessInfo tableAccessInfo = hookContext.getQueryPlan().getTableAccessInfo();
+    if (tableAccessInfo == null ||
+        tableAccessInfo.getOperatorToTableAccessMap() == null ||
+        tableAccessInfo.getOperatorToTableAccessMap().isEmpty()) {
+      return;
+    }
+
+    LogHelper console = SessionState.getConsole();
+    Map<Operator<? extends OperatorDesc>, Map<String, List<String>>> operatorToTableAccessMap =
+      tableAccessInfo.getOperatorToTableAccessMap();
+
+    // We need a new map to ensure output is always produced in the same order.
+    // This makes tests that use this hook deterministic.
+    Map<String, String> outputOrderedMap = new HashMap<String, String>();
+
+    for (Map.Entry<Operator<? extends OperatorDesc>, Map<String, List<String>>> tableAccess:
+        operatorToTableAccessMap.entrySet()) {
+      StringBuilder perOperatorInfo = new StringBuilder();
+      perOperatorInfo.append("Operator:").append(tableAccess.getKey().getOperatorId())
+        .append("\n");
+      for (Map.Entry<String, List<String>> entry: tableAccess.getValue().entrySet()) {
+        perOperatorInfo.append("Table:").append(entry.getKey()).append("\n");
+        perOperatorInfo.append("Keys:").append(StringUtils.join(entry.getValue(), ','))
+          .append("\n");
+      }
+      outputOrderedMap.put(tableAccess.getKey().getOperatorId(), perOperatorInfo.toString());
+    }
+
+    for (String perOperatorInfo: outputOrderedMap.values()) {
+        console.printError(perOperatorInfo);
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/MapJoinCounterHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/MapJoinCounterHook.java
new file mode 100644
index 0000000..1b0d57e
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/MapJoinCounterHook.java
@@ -0,0 +1,75 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.List;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.QueryPlan;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.TaskRunner;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+
+public class MapJoinCounterHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    HiveConf conf = hookContext.getConf();
+    boolean enableConvert = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECONVERTJOIN);
+    if (!enableConvert) {
+      return;
+    }
+
+    QueryPlan plan = hookContext.getQueryPlan();
+    String queryID = plan.getQueryId();
+    // String query = SessionState.get().getCmd();
+
+    int convertedMapJoin = 0;
+    int commonJoin = 0;
+    int backupCommonJoin = 0;
+    int convertedLocalMapJoin = 0;
+    int localMapJoin = 0;
+
+    List<TaskRunner> list = hookContext.getCompleteTaskList();
+    for (TaskRunner tskRunner : list) {
+      Task tsk = tskRunner.getTask();
+      int tag = tsk.getTaskTag();
+      switch (tag) {
+      case Task.COMMON_JOIN:
+        commonJoin++;
+        break;
+      case Task.CONVERTED_LOCAL_MAPJOIN:
+        convertedLocalMapJoin++;
+        break;
+      case Task.CONVERTED_MAPJOIN:
+        convertedMapJoin++;
+        break;
+      case Task.BACKUP_COMMON_JOIN:
+        backupCommonJoin++;
+        break;
+      case Task.LOCAL_MAPJOIN:
+         localMapJoin++;
+         break;
+      }
+    }
+    LogHelper console = SessionState.getConsole();
+    console.printError("[MapJoinCounter PostHook] CONVERTED_LOCAL_MAPJOIN: " + convertedLocalMapJoin
+        + " CONVERTED_MAPJOIN: " + convertedMapJoin + " LOCAL_MAPJOIN: "+localMapJoin+ " COMMON_JOIN: "+commonJoin
+        + " BACKUP_COMMON_JOIN: " + backupCommonJoin);
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/OptrStatGroupByHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/OptrStatGroupByHook.java
new file mode 100644
index 0000000..828de5e
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/OptrStatGroupByHook.java
@@ -0,0 +1,109 @@
+/**
+ *  Licensed to the Apache Software Foundation (ASF) under one
+ *  or more contributor license agreements.  See the NOTICE file
+ *  distributed with this work for additional information
+ *  regarding copyright ownership.  The ASF licenses this file
+ *  to you under the Apache License, Version 2.0 (the
+ *  "License"); you may not use this file except in compliance
+ *  with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express  or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ **/
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Queue;
+import java.util.Set;
+import java.io.Serializable;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.TaskRunner;
+import org.apache.hadoop.hive.ql.plan.api.OperatorType;
+import org.apache.hadoop.hive.ql.plan.OperatorDesc;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+
+public class OptrStatGroupByHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    HiveConf conf = hookContext.getConf();
+
+    List<TaskRunner> completedTasks = hookContext.getCompleteTaskList();
+
+    boolean enableProgress = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEJOBPROGRESS);
+
+    /** For each task visit the opeartor tree and and if the operator is GROUPBY
+     *  then print the HASH_OUT Optr level stat value.
+     **/
+    if (completedTasks != null) {
+      for (TaskRunner taskRunner : completedTasks) {
+        Task<? extends Serializable> task = taskRunner.getTask();
+        if (task.isMapRedTask() && !task.isMapRedLocalTask()) {
+          Set<Operator<? extends OperatorDesc>> optrSet = getOptrsForTask(task);
+          for (Operator<? extends OperatorDesc> optr : optrSet) {
+            if (optr.getType() == OperatorType.GROUPBY) {
+               printCounterValue(optr.getCounters());
+            }
+          }
+        }
+      }
+    }
+  }
+
+  private void printCounterValue(HashMap<String, Long> ctrs) {
+    for (String ctrName : ctrs.keySet()) {
+      if (ctrName.contains("HASH_OUT")) {
+        SessionState.getConsole().printError(ctrName+"="+ctrs.get(ctrName));
+      }
+    }
+  }
+
+  private Set<Operator<? extends OperatorDesc>> getOptrsForTask(
+    Task<? extends Serializable> task) {
+
+    Collection<Operator<? extends OperatorDesc>> topOptrs = task.getTopOperators();
+    Set<Operator<? extends OperatorDesc>> allOptrs =
+      new HashSet<Operator<? extends OperatorDesc>>();
+    Queue<Operator<? extends OperatorDesc>> opsToVisit =
+      new LinkedList<Operator<? extends OperatorDesc>>();
+    if(topOptrs != null) {
+      opsToVisit.addAll(topOptrs);
+      addChildOptrs(opsToVisit, allOptrs);
+    }
+
+    return allOptrs;
+  }
+
+  private void addChildOptrs(
+    Queue<Operator<? extends OperatorDesc>> opsToVisit,
+    Set<Operator<? extends OperatorDesc>> opsVisited) {
+
+    if(opsToVisit == null || opsVisited == null) {
+      return;
+    }
+
+    while (opsToVisit.peek() != null) {
+      Operator<? extends OperatorDesc> op = opsToVisit.remove();
+      opsVisited.add(op);
+      if (op.getChildOperators() != null) {
+        for (Operator<? extends OperatorDesc> childOp : op.getChildOperators()) {
+          if (!opsVisited.contains(childOp)) {
+            opsToVisit.add(childOp);
+          }
+        }
+      }
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyCachingPrintStreamHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyCachingPrintStreamHook.java
new file mode 100644
index 0000000..6a93c21
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyCachingPrintStreamHook.java
@@ -0,0 +1,46 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import org.apache.hadoop.hive.common.io.CachingPrintStream;
+import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
+import org.apache.hadoop.hive.ql.session.SessionState;
+
+// If this is run as a pre or post execution hook, it writes a message to SessionState.err
+// (causing it to be cached if a CachingPrintStream is being used).  If it is run as a failure
+// hook, it will write what has been cached by the CachingPrintStream to SessionState.out for
+// verification.
+public class VerifyCachingPrintStreamHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    SessionState ss = SessionState.get();
+
+    assert(ss.err instanceof CachingPrintStream);
+
+    if (hookContext.getHookType() == HookType.ON_FAILURE_HOOK) {
+      assert(ss.err instanceof CachingPrintStream);
+      ss.out.println("Begin cached logs.");
+      for (String output : ((CachingPrintStream)ss.err).getOutput()) {
+        ss.out.println(output);
+      }
+      ss.out.println("End cached logs.");
+    } else {
+      ss.err.println("TEST, this should only appear once in the log.");
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyContentSummaryCacheHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyContentSummaryCacheHook.java
new file mode 100644
index 0000000..cdb9d03
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyContentSummaryCacheHook.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.Map;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.fs.ContentSummary;
+import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
+
+public class VerifyContentSummaryCacheHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    Map<String, ContentSummary> inputToCS = hookContext.getInputPathToContentSummary();
+    if (hookContext.getHookType().equals(HookType.PRE_EXEC_HOOK)) {
+      Assert.assertEquals(0, inputToCS.size());
+    } else {
+      Assert.assertEquals(1, inputToCS.size());
+      String tmp_dir = System.getProperty("test.tmp.dir");
+      for (String key : inputToCS.keySet()) {
+        if (!key.equals(tmp_dir + "/VerifyContentSummaryCacheHook") &&
+            !key.equals("pfile:" + tmp_dir + "/VerifyContentSummaryCacheHook")) {
+          Assert.fail("VerifyContentSummaryCacheHook fails the input path check");
+        }
+      }
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyHiveSortedInputFormatUsedHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyHiveSortedInputFormatUsedHook.java
new file mode 100644
index 0000000..a64086b
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyHiveSortedInputFormatUsedHook.java
@@ -0,0 +1,46 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.io.Serializable;
+import java.util.ArrayList;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
+import org.apache.hadoop.hive.ql.plan.MapredWork;
+
+public class VerifyHiveSortedInputFormatUsedHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    if (hookContext.getHookType().equals(HookType.POST_EXEC_HOOK)) {
+
+      // Go through the root tasks, and verify the input format of the map reduce task(s) is
+      // HiveSortedInputFormat
+      ArrayList<Task<? extends Serializable>> rootTasks =
+          hookContext.getQueryPlan().getRootTasks();
+      for (Task<? extends Serializable> rootTask : rootTasks) {
+        if (rootTask.getWork() instanceof MapredWork) {
+          Assert.assertTrue("The root map reduce task's input was not marked as sorted.",
+              ((MapredWork)rootTask.getWork()).getMapWork().isInputFormatSorted());
+        }
+      }
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyHooksRunInOrder.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyHooksRunInOrder.java
new file mode 100644
index 0000000..8a72eca
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyHooksRunInOrder.java
@@ -0,0 +1,228 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.io.Serializable;
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hive.ql.HiveDriverRunHook;
+import org.apache.hadoop.hive.ql.HiveDriverRunHookContext;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
+import org.apache.hadoop.hive.ql.parse.ASTNode;
+import org.apache.hadoop.hive.ql.parse.AbstractSemanticAnalyzerHook;
+import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+
+/**
+ * VerifyHooksRunInOrder.
+ *
+ * Has to subclasses RunFirst and RunSecond which can be run as either pre or post hooks.
+ * Verifies that RunFirst is executed before RunSecond as the same type of hook.  I.e. if they are
+ * run as both Pre and Post hooks, RunSecond checks that RunFirst was run as a Pre or Post hook
+ * respectively.
+ *
+ * When running this, be sure to specify RunFirst before RunSecond in the configuration variable.
+ */
+public class VerifyHooksRunInOrder {
+
+  private static boolean preHookRunFirstRan = false;
+  private static boolean postHookRunFirstRan = false;
+  private static boolean staticAnalysisPreHookFirstRan = false;
+  private static boolean staticAnalysisPostHookFirstRan = false;
+  private static boolean driverRunPreHookFirstRan = false;
+  private static boolean driverRunPostHookFirstRan = false;
+
+  public static class RunFirst implements ExecuteWithHookContext {
+    public void run(HookContext hookContext) {
+      LogHelper console = SessionState.getConsole();
+
+      if (console == null) {
+        return;
+      }
+
+      // This is simply to verify that the hooks were in fact run
+      console.printError("Running RunFirst for " + hookContext.getHookType());
+
+      if (hookContext.getHookType() == HookType.PRE_EXEC_HOOK) {
+        preHookRunFirstRan = true;
+      } else {
+        postHookRunFirstRan = true;
+      }
+    }
+  }
+
+  public static class RunSecond implements ExecuteWithHookContext {
+    public void run(HookContext hookContext) throws Exception {
+      LogHelper console = SessionState.getConsole();
+
+      if (console == null) {
+        return;
+      }
+
+      // This is simply to verify that the hooks were in fact run
+      console.printError("Running RunSecond for " + hookContext.getHookType());
+
+      if (hookContext.getHookType() == HookType.PRE_EXEC_HOOK) {
+        Assert.assertTrue("Pre hooks did not run in the order specified.", preHookRunFirstRan);
+      } else {
+        Assert.assertTrue("Post hooks did not run in the order specified.", postHookRunFirstRan);
+      }
+    }
+  }
+
+  public static class RunFirstSemanticAnalysisHook extends AbstractSemanticAnalyzerHook {
+    @Override
+    public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,ASTNode ast)
+        throws SemanticException {
+      LogHelper console = SessionState.getConsole();
+
+      if (console == null) {
+        return ast;
+      }
+
+      // This is simply to verify that the hooks were in fact run
+      console.printError("Running RunFirst for Pre Analysis Hook");
+
+      staticAnalysisPreHookFirstRan = true;
+
+      return ast;
+    }
+
+    @Override
+    public void postAnalyze(HiveSemanticAnalyzerHookContext context,
+        List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+      LogHelper console = SessionState.getConsole();
+
+      if (console == null) {
+        return;
+      }
+
+      // This is simply to verify that the hooks were in fact run
+      console.printError("Running RunFirst for Post Analysis Hook");
+
+      staticAnalysisPostHookFirstRan = true;
+    }
+  }
+
+  public static class RunSecondSemanticAnalysisHook extends AbstractSemanticAnalyzerHook {
+    @Override
+    public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,ASTNode ast)
+        throws SemanticException {
+      LogHelper console = SessionState.getConsole();
+
+      if (console == null) {
+        return ast;
+      }
+
+      // This is simply to verify that the hooks were in fact run
+      console.printError("Running RunSecond for Pre Analysis Hook");
+
+      Assert.assertTrue("Pre Analysis Hooks did not run in the order specified.",
+                        staticAnalysisPreHookFirstRan);
+
+      return ast;
+    }
+
+    @Override
+    public void postAnalyze(HiveSemanticAnalyzerHookContext context,
+        List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+      LogHelper console = SessionState.getConsole();
+
+      if (console == null) {
+        return;
+      }
+
+      // This is simply to verify that the hooks were in fact run
+      console.printError("Running RunSecond for Post Analysis Hook");
+
+      Assert.assertTrue("Post Analysis Hooks did not run in the order specified.",
+                        staticAnalysisPostHookFirstRan);
+    }
+  }
+
+  public static class RunFirstDriverRunHook implements HiveDriverRunHook {
+
+    @Override
+    public void preDriverRun(HiveDriverRunHookContext hookContext) throws Exception {
+      LogHelper console = SessionState.getConsole();
+
+      if (console == null) {
+        return;
+      }
+
+      // This is simply to verify that the hooks were in fact run
+      console.printError("Running RunFirst for Pre Driver Run Hook");
+
+      driverRunPreHookFirstRan = true;
+    }
+
+    @Override
+    public void postDriverRun(HiveDriverRunHookContext hookContext) throws Exception {
+      LogHelper console = SessionState.getConsole();
+
+      if (console == null) {
+        return;
+      }
+
+      // This is simply to verify that the hooks were in fact run
+      console.printError("Running RunFirst for Post Driver Run Hook");
+
+      driverRunPostHookFirstRan = true;
+    }
+
+  }
+
+  public static class RunSecondDriverRunHook implements HiveDriverRunHook {
+
+    @Override
+    public void preDriverRun(HiveDriverRunHookContext hookContext) throws Exception {
+      LogHelper console = SessionState.getConsole();
+
+      if (console == null) {
+        return;
+      }
+
+      // This is simply to verify that the hooks were in fact run
+      console.printError("Running RunSecond for Pre Driver Run Hook");
+
+      Assert.assertTrue("Driver Run Hooks did not run in the order specified.",
+          driverRunPreHookFirstRan);
+    }
+
+    @Override
+    public void postDriverRun(HiveDriverRunHookContext hookContext) throws Exception {
+      LogHelper console = SessionState.getConsole();
+
+      if (console == null) {
+        return;
+      }
+
+      // This is simply to verify that the hooks were in fact run
+      console.printError("Running RunSecond for Post Driver Run Hook");
+
+      Assert.assertTrue("Driver Run Hooks did not run in the order specified.",
+          driverRunPostHookFirstRan);
+    }
+
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyIsLocalModeHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyIsLocalModeHook.java
new file mode 100644
index 0000000..166bd5b
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyIsLocalModeHook.java
@@ -0,0 +1,42 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.TaskRunner;
+import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
+
+public class VerifyIsLocalModeHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    if (hookContext.getHookType().equals(HookType.POST_EXEC_HOOK)) {
+      List<TaskRunner> taskRunners = hookContext.getCompleteTaskList();
+      for (TaskRunner taskRunner : taskRunners) {
+        Task task = taskRunner.getTask();
+        if (task.isMapRedTask()) {
+          Assert.assertTrue("VerifyIsLocalModeHook fails because a isLocalMode was not set for a task.",
+              task.isLocalMode());
+        }
+      }
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyNumReducersHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyNumReducersHook.java
new file mode 100644
index 0000000..4b2c184
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyNumReducersHook.java
@@ -0,0 +1,50 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hive.ql.MapRedStats;
+import org.apache.hadoop.hive.ql.session.SessionState;
+
+/**
+ *
+ * VerifyNumReducersHook.
+ *
+ * Provided a query involves exactly 1 map reduce job, this hook can be used to verify that the
+ * number of reducers matches what is expected.
+ *
+ * Use the config VerifyNumReducersHook.num.reducers to specify the expected number of reducers.
+ */
+public class VerifyNumReducersHook implements ExecuteWithHookContext {
+
+  public static final String BUCKET_CONFIG = "VerifyNumReducersHook.num.reducers";
+
+  public void run(HookContext hookContext) {
+    SessionState ss = SessionState.get();
+    Assert.assertNotNull("SessionState returned null");
+
+    int expectedReducers = hookContext.getConf().getInt(BUCKET_CONFIG, 0);
+    List<MapRedStats> stats = ss.getLastMapRedStatsList();
+    Assert.assertEquals("Number of MapReduce jobs is incorrect", 1, stats.size());
+
+    Assert.assertEquals("NumReducers is incorrect", expectedReducers, stats.get(0).getNumReduce());
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java
new file mode 100644
index 0000000..5cc4079
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java
@@ -0,0 +1,34 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import org.junit.Assert;
+
+
+public class VerifyOutputTableLocationSchemeIsFileHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    for (WriteEntity output : hookContext.getOutputs()) {
+      if (output.getType() == WriteEntity.Type.TABLE) {
+        String scheme = output.getTable().getDataLocation().getScheme();
+        Assert.assertTrue(output.getTable().getTableName() + " has a location which has a " +
+              "scheme other than file: " + scheme, scheme.equals("file"));
+      }
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java
new file mode 100644
index 0000000..41c178a
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java
@@ -0,0 +1,59 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map.Entry;
+
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+
+/**
+ *
+ * VerifyOverriddenConfigsHook.
+ *
+ * This hook is meant to be used for testing.  It prints the keys and values of config variables
+ * which have been noted by the session state as changed by the user.  It only prints specific
+ * variables as others may change and that should not affect this test.
+ */
+public class VerifyOverriddenConfigsHook implements ExecuteWithHookContext {
+
+  // A config variable set via a System Propery, a config variable set in the CLI,
+  // a config variable not in the default List of config variables, and a config variable in the
+  // default list of conifg variables, but which has not been overridden
+  private static String[] keysArray =
+    {"mapred.job.tracker", "hive.exec.post.hooks", "hive.config.doesnt.exit",
+     "hive.exec.mode.local.auto"};
+  private static List<String> keysList = Arrays.asList(keysArray);
+
+  public void run(HookContext hookContext) {
+    LogHelper console = SessionState.getConsole();
+    SessionState ss = SessionState.get();
+
+    if (console == null || ss == null) {
+      return;
+    }
+
+    for (Entry<String, String> entry : ss.getOverriddenConfigurations().entrySet()) {
+      if (keysList.contains(entry.getKey())) {
+        console.printError("Key: " + entry.getKey() + ", Value: " + entry.getValue());
+      }
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java
new file mode 100644
index 0000000..ce377be
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java
@@ -0,0 +1,49 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hive.ql.metadata.Partition;
+import org.apache.hadoop.hive.ql.metadata.Table;
+
+// This hook verifies that the location of every partition in the inputs and outputs does not
+// start with the location of the table.  It is a very simple check to make sure the location is
+// not a subdirectory.
+public class VerifyPartitionIsNotSubdirectoryOfTableHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    for (WriteEntity output : hookContext.getOutputs()) {
+      if (output.getType() == WriteEntity.Type.PARTITION) {
+        verify (output.getPartition(), output.getTable());
+      }
+    }
+
+    for (ReadEntity input : hookContext.getInputs()) {
+      if (input.getType() == ReadEntity.Type.PARTITION) {
+        verify (input.getPartition(), input.getTable());
+      }
+    }
+  }
+
+  private void verify(Partition partition, Table table) {
+    Assert.assertFalse("The location of the partition: " + partition.getName() + " was a " +
+        "subdirectory of the location of the table: " + table.getTableName(),
+        partition.getPartitionPath().toString().startsWith(table.getPath().toString()));
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java
new file mode 100644
index 0000000..4406036
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java
@@ -0,0 +1,48 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hive.ql.metadata.Partition;
+import org.apache.hadoop.hive.ql.metadata.Table;
+
+// This hook verifies that the location of every partition in the inputs and outputs starts with
+// the location of the table.  It is a very simple check to make sure it is a subdirectory.
+public class VerifyPartitionIsSubdirectoryOfTableHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    for (WriteEntity output : hookContext.getOutputs()) {
+      if (output.getType() == WriteEntity.Type.PARTITION) {
+        verify (output.getPartition(), output.getTable());
+      }
+    }
+
+    for (ReadEntity input : hookContext.getInputs()) {
+      if (input.getType() == ReadEntity.Type.PARTITION) {
+        verify (input.getPartition(), input.getTable());
+      }
+    }
+  }
+
+  private void verify(Partition partition, Table table) {
+    Assert.assertTrue("The location of the partition: " + partition.getName() + " was not a " +
+        "subdirectory of the location of the table: " + table.getTableName(),
+        partition.getPartitionPath().toString().startsWith(table.getPath().toString()));
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifySessionStateLocalErrorsHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifySessionStateLocalErrorsHook.java
new file mode 100644
index 0000000..b6ec8ba
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifySessionStateLocalErrorsHook.java
@@ -0,0 +1,47 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.List;
+import java.util.Map.Entry;
+
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+
+/**
+ *
+ * VerifySessionStateLocalErrorsHook.
+ *
+ * This hook is intended for testing that the localMapRedErrors variable in SessionState is
+ * populated when a local map reduce job fails.  It prints the ID of the stage that failed, and
+ * the lines recorded in the variable.
+ */
+public class VerifySessionStateLocalErrorsHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    LogHelper console = SessionState.getConsole();
+
+    for (Entry<String, List<String>> entry : SessionState.get().getLocalMapRedErrors().entrySet()) {
+      console.printError("ID: " + entry.getKey());
+
+      for (String line : entry.getValue()) {
+        console.printError(line);
+      }
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifySessionStateStackTracesHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifySessionStateStackTracesHook.java
new file mode 100644
index 0000000..129d57a
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifySessionStateStackTracesHook.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.List;
+import java.util.Map.Entry;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+
+/**
+ *
+ * VerifySessionStateStackTracesHook.
+ *
+ * Writes the first line of each stack trace collected to the console, to either verify stack
+ * traces were or were not collected.
+ */
+public class VerifySessionStateStackTracesHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) {
+    LogHelper console = SessionState.getConsole();
+
+    for (Entry<String, List<List<String>>> entry :
+        SessionState.get().getStackTraces().entrySet()) {
+
+      for (List<String> stackTrace : entry.getValue()) {
+        // Only print the first line of the stack trace as it contains the error message, and other
+        // lines may contain line numbers which are volatile
+        // Also only take the string after the first two spaces, because the prefix is a date and
+        // and time stamp
+        console.printError(StringUtils.substringAfter(
+            StringUtils.substringAfter(stackTrace.get(0), " "), " "));
+      }
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyTableDirectoryIsEmptyHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyTableDirectoryIsEmptyHook.java
new file mode 100644
index 0000000..c2005e4
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyTableDirectoryIsEmptyHook.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.io.IOException;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.session.SessionState;
+
+// This hook verifies that the location of every output table is empty
+public class VerifyTableDirectoryIsEmptyHook implements ExecuteWithHookContext {
+
+  public void run(HookContext hookContext) throws IOException {
+    for (WriteEntity output : hookContext.getOutputs()) {
+      Path tableLocation = new Path(output.getTable().getDataLocation().toString());
+      FileSystem fs = tableLocation.getFileSystem(SessionState.get().getConf());
+      assert(fs.listStatus(tableLocation).length == 0);
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/io/udf/Rot13InputFormat.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/io/udf/Rot13InputFormat.java
new file mode 100644
index 0000000..ddf62c3
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/io/udf/Rot13InputFormat.java
@@ -0,0 +1,71 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.io.udf;
+
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.LineRecordReader;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+
+import java.io.IOException;
+
+/**
+A simple input format that does a rot13 on the inputs
+ */
+class Rot13InputFormat extends TextInputFormat {
+
+  public static void rot13(byte[] bytes, int offset, int length) {
+    for(int i=offset; i < offset+length; i++) {
+      if (bytes[i] >= 'A' && bytes[i] <= 'Z') {
+        bytes[i] = (byte) ('A' + (bytes[i] - 'A' + 13) % 26);
+      } else if (bytes[i] >= 'a' && bytes[i] <= 'z') {
+        bytes[i] = (byte) ('a' + (bytes[i] - 'a' + 13) % 26);
+      }
+    }
+  }
+
+  private static class Rot13LineRecordReader extends LineRecordReader {
+    Rot13LineRecordReader(JobConf job, FileSplit split) throws IOException {
+      super(job, split);
+    }
+
+    public synchronized boolean next(LongWritable key,
+                                     Text value) throws IOException {
+      boolean result = super.next(key, value);
+      if (result) {
+        System.out.println("Read " + value);
+        rot13(value.getBytes(), 0, value.getLength());
+        System.out.println("Returned " + value);
+      }
+      return result;
+    }
+  }
+
+  public RecordReader<LongWritable, Text>
+    getRecordReader(InputSplit genericSplit, JobConf job,
+                    Reporter reporter) throws IOException {
+    reporter.setStatus(genericSplit.toString());
+    return new Rot13LineRecordReader(job, (FileSplit) genericSplit);
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/io/udf/Rot13OutputFormat.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/io/udf/Rot13OutputFormat.java
new file mode 100644
index 0000000..e2d92a5
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/io/udf/Rot13OutputFormat.java
@@ -0,0 +1,75 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.io.udf;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;
+import org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.util.Progressable;
+
+import java.io.IOException;
+import java.util.Properties;
+
+public class Rot13OutputFormat
+  extends HiveIgnoreKeyTextOutputFormat<LongWritable,Text> {
+
+  @Override
+  public RecordWriter
+    getHiveRecordWriter(JobConf jc,
+                        Path outPath,
+                        Class<? extends Writable> valueClass,
+                        boolean isCompressed,
+                        Properties tableProperties,
+                        Progressable progress) throws IOException {
+    final RecordWriter result =
+      super.getHiveRecordWriter(jc,outPath,valueClass,isCompressed,
+        tableProperties,progress);
+    final Reporter reporter = (Reporter) progress;
+    reporter.setStatus("got here");
+    System.out.println("Got a reporter " + reporter);
+    return new RecordWriter() {
+      @Override
+      public void write(Writable w) throws IOException {
+        if (w instanceof Text) {
+          Text value = (Text) w;
+          Rot13InputFormat.rot13(value.getBytes(), 0, value.getLength());
+          result.write(w);
+        } else if (w instanceof BytesWritable) {
+          BytesWritable value = (BytesWritable) w;
+          Rot13InputFormat.rot13(value.getBytes(), 0, value.getLength());
+          result.write(w);
+        } else {
+          throw new IllegalArgumentException("need text or bytes writable " +
+            " instead of " + w.getClass().getName());
+        }
+      }
+
+      @Override
+      public void close(boolean abort) throws IOException {
+        result.close(abort);
+      }
+    };
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook.java
new file mode 100644
index 0000000..8d9cad5
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook.java
@@ -0,0 +1,104 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.metadata;
+
+import java.io.Serializable;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.hive.ql.exec.DDLTask;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.parse.ASTNode;
+import org.apache.hadoop.hive.ql.parse.AbstractSemanticAnalyzerHook;
+import org.apache.hadoop.hive.ql.parse.HiveParser;
+import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.plan.CreateTableDesc;
+
+public class DummySemanticAnalyzerHook extends AbstractSemanticAnalyzerHook{
+
+  private AbstractSemanticAnalyzerHook hook;
+
+  @Override
+  public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)
+  throws SemanticException {
+
+    switch (ast.getToken().getType()) {
+
+    case HiveParser.TOK_CREATETABLE:
+      hook = new DummyCreateTableHook();
+      return hook.preAnalyze(context, ast);
+
+    case HiveParser.TOK_DROPTABLE:
+    case HiveParser.TOK_DESCTABLE:
+      return ast;
+
+    default:
+      throw new SemanticException("Operation not supported.");
+    }
+  }
+
+  public DummySemanticAnalyzerHook() {
+
+  }
+
+  @Override
+  public void postAnalyze(HiveSemanticAnalyzerHookContext context,
+      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+
+    if(hook != null) {
+      hook.postAnalyze(context, rootTasks);
+    }
+  }
+}
+
+class DummyCreateTableHook extends AbstractSemanticAnalyzerHook{
+
+  @Override
+  public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)
+  throws SemanticException {
+
+    int numCh = ast.getChildCount();
+
+    for (int num = 1; num < numCh; num++) {
+      ASTNode child = (ASTNode) ast.getChild(num);
+
+      switch (child.getToken().getType()) {
+
+      case HiveParser.TOK_QUERY:
+        throw new SemanticException("CTAS not supported.");
+      }
+    }
+    return ast;
+  }
+
+  @Override
+  public void postAnalyze(HiveSemanticAnalyzerHookContext context,
+      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+    CreateTableDesc desc = ((DDLTask)rootTasks.get(rootTasks.size()-1)).getWork().getCreateTblDesc();
+    Map<String,String> tblProps = desc.getTblProps();
+    if(tblProps == null) {
+      tblProps = new HashMap<String, String>();
+    }
+    tblProps.put("createdBy", DummyCreateTableHook.class.getName());
+    tblProps.put("Message", "Open Source rocks!!");
+    desc.setTblProps(tblProps);
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook1.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook1.java
new file mode 100644
index 0000000..9f86637
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook1.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.metadata;
+
+import java.io.Serializable;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.hive.ql.exec.DDLTask;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.parse.ASTNode;
+import org.apache.hadoop.hive.ql.parse.AbstractSemanticAnalyzerHook;
+import org.apache.hadoop.hive.ql.parse.HiveParser;
+import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.plan.CreateTableDesc;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+
+public class DummySemanticAnalyzerHook1 extends AbstractSemanticAnalyzerHook {
+  static int count = 0;
+  int myCount = -1;
+  boolean isCreateTable;
+
+  @Override
+  public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)
+      throws SemanticException {
+    LogHelper console = SessionState.getConsole();
+    isCreateTable = (ast.getToken().getType() == HiveParser.TOK_CREATETABLE);
+    myCount = count++;
+    if (isCreateTable) {
+      console.printError("DummySemanticAnalyzerHook1 Pre: Count " + myCount);
+    }
+    return ast;
+  }
+
+  public DummySemanticAnalyzerHook1() {
+  }
+
+  @Override
+  public void postAnalyze(HiveSemanticAnalyzerHookContext context,
+      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+    count = 0;
+    if (!isCreateTable) {
+      return;
+    }
+
+    CreateTableDesc desc = ((DDLTask) rootTasks.get(rootTasks.size() - 1)).getWork()
+        .getCreateTblDesc();
+    Map<String, String> tblProps = desc.getTblProps();
+    if (tblProps == null) {
+      tblProps = new HashMap<String, String>();
+    }
+    tblProps.put("createdBy", DummyCreateTableHook.class.getName());
+    tblProps.put("Message", "Hive rocks!! Count: " + myCount);
+
+    LogHelper console = SessionState.getConsole();
+    console.printError("DummySemanticAnalyzerHook1 Post: Hive rocks!! Count: " + myCount);
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/DummyAuthenticator.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/DummyAuthenticator.java
new file mode 100644
index 0000000..578a177
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/DummyAuthenticator.java
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.security;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+
+public class DummyAuthenticator implements HiveAuthenticationProvider {
+
+  private List<String> groupNames;
+  private String userName;
+  private Configuration conf;
+
+  public DummyAuthenticator() {
+    this.groupNames = new ArrayList<String>();
+    groupNames.add("hive_test_group1");
+    groupNames.add("hive_test_group2");
+    userName = "hive_test_user";
+  }
+
+  @Override
+  public void destroy() throws HiveException{
+    return;
+  }
+
+  @Override
+  public List<String> getGroupNames() {
+    return groupNames;
+  }
+
+  @Override
+  public String getUserName() {
+    return userName;
+  }
+
+  @Override
+  public void setConf(Configuration conf) {
+    this.conf = conf;
+  }
+
+  public Configuration getConf() {
+    return this.conf;
+  }
+
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/DummyHiveMetastoreAuthorizationProvider.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/DummyHiveMetastoreAuthorizationProvider.java
new file mode 100644
index 0000000..195a5a4
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/DummyHiveMetastoreAuthorizationProvider.java
@@ -0,0 +1,204 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.security;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.ql.metadata.AuthorizationException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.metadata.Partition;
+import org.apache.hadoop.hive.ql.metadata.Table;
+import org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider;
+import org.apache.hadoop.hive.ql.security.authorization.Privilege;
+
+public class DummyHiveMetastoreAuthorizationProvider implements HiveMetastoreAuthorizationProvider {
+
+
+  protected HiveAuthenticationProvider authenticator;
+
+  public enum AuthCallContextType {
+    USER,
+    DB,
+    TABLE,
+    PARTITION,
+    TABLE_AND_PARTITION
+  };
+
+  class AuthCallContext {
+
+    public AuthCallContextType type;
+    public List<Object> authObjects;
+    public Privilege[] readRequiredPriv;
+    public Privilege[] writeRequiredPriv;
+
+    AuthCallContext(AuthCallContextType typeOfCall,
+        Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv) {
+      this.type = typeOfCall;
+      this.authObjects = new ArrayList<Object>();
+      this.readRequiredPriv = readRequiredPriv;
+      this.writeRequiredPriv = writeRequiredPriv;
+    }
+    AuthCallContext(AuthCallContextType typeOfCall, Object authObject,
+        Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv) {
+      this(typeOfCall,readRequiredPriv,writeRequiredPriv);
+      this.authObjects.add(authObject);
+    }
+    AuthCallContext(AuthCallContextType typeOfCall, List<? extends Object> authObjects,
+        Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv) {
+      this(typeOfCall,readRequiredPriv,writeRequiredPriv);
+      this.authObjects.addAll(authObjects);
+    }
+  }
+
+  public static final List<AuthCallContext> authCalls = new ArrayList<AuthCallContext>();
+
+  private Configuration conf;
+  public static final Log LOG = LogFactory.getLog(
+      DummyHiveMetastoreAuthorizationProvider.class);;
+
+  @Override
+  public Configuration getConf() {
+    return this.conf;
+  }
+
+  @Override
+  public void setConf(Configuration conf) {
+    this.conf = conf;
+    try {
+      init(conf);
+    } catch (HiveException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  @Override
+  public HiveAuthenticationProvider getAuthenticator() {
+    return authenticator;
+  }
+
+  @Override
+  public void setAuthenticator(HiveAuthenticationProvider authenticator) {
+    this.authenticator = authenticator;
+  }
+
+  @Override
+  public void init(Configuration conf) throws HiveException {
+    debugLog("DHMAP.init");
+  }
+
+  @Override
+  public void authorize(Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
+      throws HiveException, AuthorizationException {
+    debugLog("DHMAP.authorize " +
+      "read:" + debugPrivPrint(readRequiredPriv) +
+      " , write:" + debugPrivPrint(writeRequiredPriv)
+      );
+    authCalls.add(new AuthCallContext(AuthCallContextType.USER,
+        readRequiredPriv, writeRequiredPriv));
+  }
+
+  @Override
+  public void authorize(Database db, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
+      throws HiveException, AuthorizationException {
+    debugLog("DHMAP.authorizedb " +
+        "db:" + db.getName() +
+        " , read:" + debugPrivPrint(readRequiredPriv) +
+        " , write:" + debugPrivPrint(writeRequiredPriv)
+        );
+    authCalls.add(new AuthCallContext(AuthCallContextType.DB,
+        db, readRequiredPriv, writeRequiredPriv));
+  }
+
+  @Override
+  public void authorize(Table table, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
+      throws HiveException, AuthorizationException {
+    debugLog("DHMAP.authorizetbl " +
+        "tbl:" + table.getCompleteName() +
+        " , read:" + debugPrivPrint(readRequiredPriv) +
+        " , write:" + debugPrivPrint(writeRequiredPriv)
+        );
+    authCalls.add(new AuthCallContext(AuthCallContextType.TABLE,
+        table, readRequiredPriv, writeRequiredPriv));
+
+  }
+
+  @Override
+  public void authorize(Partition part, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
+      throws HiveException, AuthorizationException {
+    debugLog("DHMAP.authorizepart " +
+        "tbl:" + part.getTable().getCompleteName() +
+        " , part: " + part.getName() +
+        " , read:" + debugPrivPrint(readRequiredPriv) +
+        " , write:" + debugPrivPrint(writeRequiredPriv)
+        );
+    authCalls.add(new AuthCallContext(AuthCallContextType.PARTITION,
+        part, readRequiredPriv, writeRequiredPriv));
+
+  }
+
+  @Override
+  public void authorize(Table table, Partition part, List<String> columns,
+      Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv) throws HiveException,
+      AuthorizationException {
+    debugLog("DHMAP.authorizecols " +
+        "tbl:" + table.getCompleteName() +
+        " , part: " + part.getName() +
+        " . cols: " + columns.toString() +
+        " , read:" + debugPrivPrint(readRequiredPriv) +
+        " , write:" + debugPrivPrint(writeRequiredPriv)
+        );
+    List<Object> authObjects = new ArrayList<Object>();
+    authObjects.add(table);
+    authObjects.add(part);
+    authCalls.add(new AuthCallContext(AuthCallContextType.TABLE_AND_PARTITION,
+        authObjects, readRequiredPriv, writeRequiredPriv));
+
+  }
+
+  private void debugLog(String s) {
+    LOG.debug(s);
+  }
+
+  private String debugPrivPrint(Privilege[] privileges) {
+    StringBuffer sb = new StringBuffer();
+    sb.append("Privileges{");
+    if (privileges != null){
+    for (Privilege p : privileges){
+      sb.append(p.toString());
+    }
+    }else{
+      sb.append("null");
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+
+  @Override
+  public void setMetaStoreHandler(HMSHandler handler) {
+    debugLog("DHMAP.setMetaStoreHandler");
+  }
+
+
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java
new file mode 100644
index 0000000..2dd225e
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java
@@ -0,0 +1,105 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.security;
+
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+
+/**
+ *
+ * InjectableDummyAuthenticator - An implementation of HiveMetastoreAuthenticationProvider
+ * that wraps another Authenticator, but when asked to inject a user provided username
+ * and groupnames, does so. This can be toggled back and forth to use in testing
+ */
+public class InjectableDummyAuthenticator implements HiveMetastoreAuthenticationProvider {
+
+  private static String userName;
+  private static List<String> groupNames;
+  private static boolean injectMode;
+  private static Class<? extends HiveMetastoreAuthenticationProvider> hmapClass =
+      HadoopDefaultMetastoreAuthenticator.class;
+  private HiveMetastoreAuthenticationProvider hmap;
+
+  public static void injectHmapClass(Class<? extends HiveMetastoreAuthenticationProvider> clazz){
+    hmapClass = clazz;
+  }
+
+  public static void injectUserName(String user){
+    userName = user;
+  }
+
+  public static void injectGroupNames(List<String> groups){
+    groupNames = groups;
+  }
+
+  public static void injectMode(boolean mode){
+    injectMode = mode;
+  }
+
+  @Override
+  public String getUserName() {
+    if (injectMode){
+      return userName;
+    } else {
+      return hmap.getUserName();
+    }
+  }
+
+  @Override
+  public List<String> getGroupNames() {
+    if (injectMode) {
+      return groupNames;
+    } else {
+      return hmap.getGroupNames();
+    }
+  }
+
+  @Override
+  public Configuration getConf() {
+    return hmap.getConf();
+  }
+
+  @Override
+  public void setConf(Configuration config) {
+    try {
+      hmap = (HiveMetastoreAuthenticationProvider) hmapClass.newInstance();
+    } catch (InstantiationException e) {
+      throw new RuntimeException("Whoops, could not create an Authenticator of class " +
+          hmapClass.getName());
+    } catch (IllegalAccessException e) {
+      throw new RuntimeException("Whoops, could not create an Authenticator of class " +
+          hmapClass.getName());
+    }
+
+    hmap.setConf(config);
+  }
+
+  @Override
+  public void setMetaStoreHandler(HMSHandler handler) {
+    hmap.setMetaStoreHandler(handler);
+  }
+
+  @Override
+  public void destroy() throws HiveException {
+    hmap.destroy();
+  }
+
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java
new file mode 100644
index 0000000..969e5b9
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java
@@ -0,0 +1,61 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.stats;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ * An test implementation for StatsAggregator.
+ * The method corresponding to the configuration parameter
+ * hive.test.dummystats.aggregator fail, whereas all
+ * other methods succeed.
+ */
+
+public class DummyStatsAggregator implements StatsAggregator {
+  String errorMethod = null;
+
+  // This is a test. The parameter hive.test.dummystats.aggregator's value
+  // denotes the method which needs to throw an error.
+  public boolean connect(Configuration hconf) {
+    errorMethod = hconf.get("hive.test.dummystats.aggregator", "");
+    if (errorMethod.equalsIgnoreCase("connect")) {
+      return false;
+    }
+
+    return true;
+  }
+
+  public String aggregateStats(String keyPrefix, String statType) {
+    return null;
+  }
+
+  public boolean closeConnection() {
+    if (errorMethod.equalsIgnoreCase("closeConnection")) {
+      return false;
+    }
+    return true;
+  }
+
+  public boolean cleanUp(String keyPrefix) {
+    if (errorMethod.equalsIgnoreCase("cleanUp")) {
+      return false;
+    }
+    return true;
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java
new file mode 100644
index 0000000..4dd632d
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java
@@ -0,0 +1,69 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.stats;
+
+import java.util.Map;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ * An test implementation for StatsPublisher.
+ * The method corresponding to the configuration parameter
+ * hive.test.dummystats.publisher fail, whereas all
+ * other methods succeed
+ */
+
+public class DummyStatsPublisher implements StatsPublisher {
+
+  String errorMethod = null;
+
+  // This is a test. The parameter hive.test.dummystats.publisher's value
+  // denotes the method which needs to throw an error.
+  public boolean init(Configuration hconf) {
+    errorMethod = hconf.get("hive.test.dummystats.publisher", "");
+    if (errorMethod.equalsIgnoreCase("init")) {
+      return false;
+    }
+
+    return true;
+  }
+
+  public boolean connect(Configuration hconf) {
+    errorMethod = hconf.get("hive.test.dummystats.publisher", "");
+    if (errorMethod.equalsIgnoreCase("connect")) {
+      return false;
+    }
+
+    return true;
+  }
+
+  public boolean publishStat(String fileID, Map<String, String> stats) {
+    if (errorMethod.equalsIgnoreCase("publishStat")) {
+      return false;
+    }
+    return true;
+  }
+
+  public boolean closeConnection() {
+    if (errorMethod.equalsIgnoreCase("closeConnection")) {
+      return false;
+    }
+    return true;
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java
new file mode 100644
index 0000000..fafd68b
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.stats;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.session.SessionState;
+
+/**
+ * An test implementation for StatsAggregator.
+ * aggregateStats prints the length of the keyPrefix to SessionState's out stream
+ * All other methods are no-ops.
+ */
+
+public class KeyVerifyingStatsAggregator implements StatsAggregator {
+
+  public boolean connect(Configuration hconf) {
+    return true;
+  }
+
+  public String aggregateStats(String keyPrefix, String statType) {
+    SessionState ss = SessionState.get();
+    // Have to use the length instead of the actual prefix because the prefix is location dependent
+    // 17 is 16 (16 byte MD5 hash) + 1 for the path separator
+    // Can be less than 17 due to unicode characters
+    ss.out.println("Stats prefix is hashed: " + new Boolean(keyPrefix.length() <= 17));
+    return null;
+  }
+
+  public boolean closeConnection() {
+    return true;
+  }
+
+  public boolean cleanUp(String keyPrefix) {
+    return true;
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDAFTestMax.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDAFTestMax.java
new file mode 100644
index 0000000..eda2aa4
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDAFTestMax.java
@@ -0,0 +1,295 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf;
+
+import org.apache.hadoop.hive.ql.exec.UDAF;
+import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+
+/**
+ * UDAFTestMax.
+ *
+ */
+public class UDAFTestMax extends UDAF {
+
+  /**
+   * MaxShortEvaluator.
+   *
+   */
+  public static class MaxShortEvaluator implements UDAFEvaluator {
+    private short mMax;
+    private boolean mEmpty;
+
+    public MaxShortEvaluator() {
+      super();
+      init();
+    }
+
+    public void init() {
+      mMax = 0;
+      mEmpty = true;
+    }
+
+    public boolean iterate(ShortWritable o) {
+      if (o != null) {
+        if (mEmpty) {
+          mMax = o.get();
+          mEmpty = false;
+        } else {
+          mMax = (short) Math.max(mMax, o.get());
+        }
+      }
+      return true;
+    }
+
+    public ShortWritable terminatePartial() {
+      return mEmpty ? null : new ShortWritable(mMax);
+    }
+
+    public boolean merge(ShortWritable o) {
+      return iterate(o);
+    }
+
+    public ShortWritable terminate() {
+      return mEmpty ? null : new ShortWritable(mMax);
+    }
+  }
+
+  /**
+   * MaxIntEvaluator.
+   *
+   */
+  public static class MaxIntEvaluator implements UDAFEvaluator {
+    private int mMax;
+    private boolean mEmpty;
+
+    public MaxIntEvaluator() {
+      super();
+      init();
+    }
+
+    public void init() {
+      mMax = 0;
+      mEmpty = true;
+    }
+
+    public boolean iterate(IntWritable o) {
+      if (o != null) {
+        if (mEmpty) {
+          mMax = o.get();
+          mEmpty = false;
+        } else {
+          mMax = Math.max(mMax, o.get());
+        }
+      }
+      return true;
+    }
+
+    public IntWritable terminatePartial() {
+      return mEmpty ? null : new IntWritable(mMax);
+    }
+
+    public boolean merge(IntWritable o) {
+      return iterate(o);
+    }
+
+    public IntWritable terminate() {
+      return mEmpty ? null : new IntWritable(mMax);
+    }
+  }
+
+  /**
+   * MaxLongEvaluator.
+   *
+   */
+  public static class MaxLongEvaluator implements UDAFEvaluator {
+    private long mMax;
+    private boolean mEmpty;
+
+    public MaxLongEvaluator() {
+      super();
+      init();
+    }
+
+    public void init() {
+      mMax = 0;
+      mEmpty = true;
+    }
+
+    public boolean iterate(LongWritable o) {
+      if (o != null) {
+        if (mEmpty) {
+          mMax = o.get();
+          mEmpty = false;
+        } else {
+          mMax = Math.max(mMax, o.get());
+        }
+      }
+      return true;
+    }
+
+    public LongWritable terminatePartial() {
+      return mEmpty ? null : new LongWritable(mMax);
+    }
+
+    public boolean merge(LongWritable o) {
+      return iterate(o);
+    }
+
+    public LongWritable terminate() {
+      return mEmpty ? null : new LongWritable(mMax);
+    }
+  }
+
+  /**
+   * MaxFloatEvaluator.
+   *
+   */
+  public static class MaxFloatEvaluator implements UDAFEvaluator {
+    private float mMax;
+    private boolean mEmpty;
+
+    public MaxFloatEvaluator() {
+      super();
+      init();
+    }
+
+    public void init() {
+      mMax = 0;
+      mEmpty = true;
+    }
+
+    public boolean iterate(FloatWritable o) {
+      if (o != null) {
+        if (mEmpty) {
+          mMax = o.get();
+          mEmpty = false;
+        } else {
+          mMax = Math.max(mMax, o.get());
+        }
+      }
+      return true;
+    }
+
+    public FloatWritable terminatePartial() {
+      return mEmpty ? null : new FloatWritable(mMax);
+    }
+
+    public boolean merge(FloatWritable o) {
+      return iterate(o);
+    }
+
+    public FloatWritable terminate() {
+      return mEmpty ? null : new FloatWritable(mMax);
+    }
+  }
+
+  /**
+   * MaxDoubleEvaluator.
+   *
+   */
+  public static class MaxDoubleEvaluator implements UDAFEvaluator {
+    private double mMax;
+    private boolean mEmpty;
+
+    public MaxDoubleEvaluator() {
+      super();
+      init();
+    }
+
+    public void init() {
+      mMax = 0;
+      mEmpty = true;
+    }
+
+    public boolean iterate(DoubleWritable o) {
+      if (o != null) {
+        if (mEmpty) {
+          mMax = o.get();
+          mEmpty = false;
+        } else {
+          mMax = Math.max(mMax, o.get());
+        }
+      }
+      return true;
+    }
+
+    public DoubleWritable terminatePartial() {
+      return mEmpty ? null : new DoubleWritable(mMax);
+    }
+
+    public boolean merge(DoubleWritable o) {
+      return iterate(o);
+    }
+
+    public DoubleWritable terminate() {
+      return mEmpty ? null : new DoubleWritable(mMax);
+    }
+  }
+
+  /**
+   * MaxStringEvaluator.
+   *
+   */
+  public static class MaxStringEvaluator implements UDAFEvaluator {
+    private Text mMax;
+    private boolean mEmpty;
+
+    public MaxStringEvaluator() {
+      super();
+      init();
+    }
+
+    public void init() {
+      mMax = null;
+      mEmpty = true;
+    }
+
+    public boolean iterate(Text o) {
+      if (o != null) {
+        if (mEmpty) {
+          mMax = new Text(o);
+          mEmpty = false;
+        } else if (ShimLoader.getHadoopShims().compareText(mMax, o) < 0) {
+          mMax.set(o);
+        }
+      }
+      return true;
+    }
+
+    public Text terminatePartial() {
+      return mEmpty ? null : mMax;
+    }
+
+    public boolean merge(Text o) {
+      return iterate(o);
+    }
+
+    public Text terminate() {
+      return mEmpty ? null : mMax;
+    }
+  }
+
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestErrorOnFalse.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestErrorOnFalse.java
new file mode 100644
index 0000000..66a30ab
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestErrorOnFalse.java
@@ -0,0 +1,35 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf;
+
+import org.apache.hadoop.hive.ql.exec.UDF;
+
+/**
+ * A UDF for testing, which throws RuntimeException if  the length of a string.
+ */
+public class UDFTestErrorOnFalse extends UDF {
+
+  public int evaluate(Boolean b) {
+    if (b) {
+      return 1;
+    } else {
+      throw new RuntimeException("UDFTestErrorOnFalse got b=false");
+    }
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestLength.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestLength.java
new file mode 100644
index 0000000..9e75c51
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestLength.java
@@ -0,0 +1,39 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf;
+
+import org.apache.hadoop.hive.ql.exec.UDF;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Text;
+
+/**
+ * A UDF for testing, which evaluates the length of a string.
+ */
+public class UDFTestLength extends UDF {
+
+  IntWritable result = new IntWritable();
+
+  public IntWritable evaluate(Text s) {
+    if (s == null) {
+      return null;
+    }
+    result.set(s.toString().length());
+    return result;
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestLength2.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestLength2.java
new file mode 100644
index 0000000..b1aab45
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/UDFTestLength2.java
@@ -0,0 +1,35 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf;
+
+import org.apache.hadoop.hive.ql.exec.UDF;
+
+/**
+ * A UDF for testing, which evaluates the length of a string. This UDF uses Java
+ * Primitive classes for parameters.
+ */
+public class UDFTestLength2 extends UDF {
+
+  public Integer evaluate(String s) {
+    if (s == null) {
+      return null;
+    }
+    return Integer.valueOf(s.length());
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/DummyContextUDF.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/DummyContextUDF.java
new file mode 100644
index 0000000..d3b525e
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/DummyContextUDF.java
@@ -0,0 +1,54 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import org.apache.hadoop.hive.ql.exec.MapredContext;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.Counters;
+import org.apache.hadoop.mapred.Reporter;
+
+public class DummyContextUDF extends GenericUDF {
+
+  private MapredContext context;
+  private LongWritable result = new LongWritable();
+
+  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
+    return PrimitiveObjectInspectorFactory.writableLongObjectInspector;
+  }
+
+  public Object evaluate(DeferredObject[] arguments) throws HiveException {
+    Reporter reporter = context.getReporter();
+    Counters.Counter counter = reporter.getCounter("org.apache.hadoop.mapred.Task$Counter", "MAP_INPUT_RECORDS");
+    result.set(counter.getValue());
+    return result;
+  }
+
+  public String getDisplayString(String[] children) {
+    return "dummy-func()";
+  }
+
+  @Override
+    public void configure(MapredContext context) {
+    this.context = context;
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSumList.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSumList.java
new file mode 100644
index 0000000..55d7912
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSumList.java
@@ -0,0 +1,160 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.ql.exec.Description;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.util.JavaDataModel;
+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
+import org.apache.hadoop.io.LongWritable;
+
+/**
+ * GenericUDAFSum.
+ *
+ */
+@Description(name = "sum_list", value = "_FUNC_(x) - Returns the sum of a set of numbers")
+public class GenericUDAFSumList extends AbstractGenericUDAFResolver {
+
+  static final Log LOG = LogFactory.getLog(GenericUDAFSumList.class.getName());
+
+  @Override
+  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info)
+    throws SemanticException {
+    ObjectInspector[] inspectors = info.getParameterObjectInspectors();
+    if (inspectors.length != 1) {
+      throw new UDFArgumentTypeException(inspectors.length - 1,
+          "Exactly one argument is expected.");
+    }
+
+    if (inspectors[0].getCategory() != ObjectInspector.Category.LIST) {
+      throw new UDFArgumentTypeException(0, "Argument should be a list type");
+    }
+
+    ListObjectInspector listOI = (ListObjectInspector) inspectors[0];
+    ObjectInspector elementOI = listOI.getListElementObjectInspector();
+
+    if (elementOI.getCategory() != ObjectInspector.Category.PRIMITIVE) {
+      throw new UDFArgumentTypeException(0,
+          "Only primitive type arguments are accepted but "
+          + elementOI.getTypeName() + " is passed.");
+    }
+    PrimitiveObjectInspector.PrimitiveCategory pcat =
+        ((PrimitiveObjectInspector)elementOI).getPrimitiveCategory();
+    return new GenericUDAFSumLong();
+  }
+
+  /**
+   * GenericUDAFSumLong.
+   *
+   */
+  public static class GenericUDAFSumLong extends GenericUDAFEvaluator {
+    private ListObjectInspector listOI;
+    private PrimitiveObjectInspector elementOI;
+    private ObjectInspectorConverters.Converter toLong;
+    private PrimitiveObjectInspector inputOI;
+    private LongWritable result;
+
+    @Override
+    public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException {
+      assert (parameters.length == 1);
+      super.init(m, parameters);
+      result = new LongWritable(0);
+      if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) {
+        listOI = (ListObjectInspector) parameters[0];
+        elementOI = (PrimitiveObjectInspector) listOI.getListElementObjectInspector();
+        toLong = ObjectInspectorConverters.getConverter(elementOI,
+            PrimitiveObjectInspectorFactory.javaLongObjectInspector);
+      } else {
+        inputOI = (PrimitiveObjectInspector) parameters[0];
+      }
+      return PrimitiveObjectInspectorFactory.writableLongObjectInspector;
+    }
+
+    /** class for storing double sum value. */
+    @AggregationType(estimable = true)
+    static class SumLongAgg extends AbstractAggregationBuffer {
+      boolean empty;
+      long sum;
+      @Override
+      public int estimate() { return JavaDataModel.PRIMITIVES1 + JavaDataModel.PRIMITIVES2; }
+    }
+
+    @Override
+    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
+      SumLongAgg result = new SumLongAgg();
+      reset(result);
+      return result;
+    }
+
+    @Override
+    public void reset(AggregationBuffer agg) throws HiveException {
+      SumLongAgg myagg = (SumLongAgg) agg;
+      myagg.empty = true;
+      myagg.sum = 0;
+    }
+
+    @Override
+    public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {
+      assert (parameters.length == 1);
+      SumLongAgg myagg = (SumLongAgg) agg;
+      int length = listOI.getListLength(parameters[0]);
+      for (int i = 0; i < length; i++) {
+        Object element = listOI.getListElement(parameters[0], i);
+        if (element != null) {
+          myagg.sum += (Long)toLong.convert(element);
+          myagg.empty = false;
+        }
+      }
+    }
+
+    @Override
+    public Object terminatePartial(AggregationBuffer agg) throws HiveException {
+      return terminate(agg);
+    }
+
+    @Override
+    public void merge(AggregationBuffer agg, Object partial) throws HiveException {
+      if (partial != null) {
+        SumLongAgg myagg = (SumLongAgg) agg;
+        myagg.sum += PrimitiveObjectInspectorUtils.getLong(partial, inputOI);
+        myagg.empty = false;
+      }
+    }
+
+    @Override
+    public Object terminate(AggregationBuffer agg) throws HiveException {
+      SumLongAgg myagg = (SumLongAgg) agg;
+      if (myagg.empty) {
+        return null;
+      }
+      result.set(myagg.sum);
+      return result;
+    }
+
+  }
+
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFEvaluateNPE.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFEvaluateNPE.java
new file mode 100644
index 0000000..4080c9f
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFEvaluateNPE.java
@@ -0,0 +1,81 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import java.lang.NullPointerException;
+
+import org.apache.hadoop.hive.ql.exec.Description;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Text;
+
+/**
+ * GenericUDFEvaluateNPE
+ * This UDF is to throw an Null Pointer Exception
+ * It is used to test hive failure handling
+ *
+ */
+@Description(name = "evaluate_npe", value = "_FUNC_(string) - "
+  + "Throws an NPE in the GenericUDF.evaluate() method. "
+  + "Used for testing GenericUDF error handling.")
+public class GenericUDFEvaluateNPE extends GenericUDF {
+  private ObjectInspector[] argumentOIs;
+  private final Text result= new Text();
+
+  @Override
+  public ObjectInspector initialize(ObjectInspector[] arguments)
+      throws UDFArgumentException {
+    if (arguments.length != 1) {
+      throw new UDFArgumentLengthException(
+          "The function evaluate_npe(string)"
+            + "needs only one argument.");
+    }
+
+    if (!arguments[0].getTypeName().equals(serdeConstants.STRING_TYPE_NAME)) {
+      throw new UDFArgumentTypeException(0,
+        "Argument 1 of function evaluate_npe must be \""
+        + serdeConstants.STRING_TYPE_NAME + "but \""
+        + arguments[0].getTypeName() + "\" was found.");
+    }
+
+    argumentOIs = arguments;
+    return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
+  }
+
+  @Override
+  public Object evaluate(DeferredObject[] arguments) throws HiveException {
+    if (true) {
+        throw new NullPointerException("evaluate null pointer exception");
+    }
+    return result;
+  }
+
+  @Override
+  public String getDisplayString(String[] children) {
+    assert (children.length == 1);
+    return "evaluate_npe(" + children[0] + ")";
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java
new file mode 100644
index 0000000..4ec7431
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
+
+/**
+ * A test GenericUDF to return native Java's boolean type
+ */
+public class GenericUDFTestGetJavaBoolean extends GenericUDF {
+  ObjectInspector[] argumentOIs;
+
+  @Override
+  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
+    argumentOIs = arguments;
+    return PrimitiveObjectInspectorFactory.javaBooleanObjectInspector;
+  }
+
+  @Override
+  public Object evaluate(DeferredObject[] arguments) throws HiveException {
+    String input = ((StringObjectInspector) argumentOIs[0]).getPrimitiveJavaObject(arguments[0].get());
+    if (input.equalsIgnoreCase("true")) {
+      return Boolean.TRUE;
+    } else if (input.equalsIgnoreCase("false")) {
+      return false;
+    } else {
+      return null;
+    }
+  }
+
+  @Override
+  public String getDisplayString(String[] children) {
+    assert (children.length == 1);
+    return "TestGetJavaBoolean(" + children[0] + ")";
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java
new file mode 100644
index 0000000..ead45ae
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
+
+/**
+ * A test GenericUDF to return native Java's string type
+ */
+public class GenericUDFTestGetJavaString extends GenericUDF {
+  ObjectInspector[] argumentOIs;
+
+  @Override
+  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
+    argumentOIs = arguments;
+    return PrimitiveObjectInspectorFactory.javaStringObjectInspector;
+  }
+
+  @Override
+  public Object evaluate(DeferredObject[] arguments) throws HiveException {
+    if (arguments[0].get() == null) {
+      return null;
+    }
+    return ((StringObjectInspector) argumentOIs[0]).getPrimitiveJavaObject(arguments[0].get());
+  }
+
+  @Override
+  public String getDisplayString(String[] children) {
+    assert (children.length == 1);
+    return "GenericUDFTestGetJavaString(" + children[0] + ")";
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java
new file mode 100644
index 0000000..dedf91d
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java
@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
+import org.apache.hadoop.io.Text;
+
+/**
+ * Mimics oracle's function translate(str1, str2, str3).
+ */
+public class GenericUDFTestTranslate extends GenericUDF {
+  private transient ObjectInspector[] argumentOIs;
+
+  /**
+   * Return a corresponding ordinal from an integer.
+   */
+  static String getOrdinal(int i) {
+    int unit = i % 10;
+    return (i <= 0) ? "" : (i != 11 && unit == 1) ? i + "st"
+        : (i != 12 && unit == 2) ? i + "nd" : (i != 13 && unit == 3) ? i + "rd"
+        : i + "th";
+  }
+
+  @Override
+  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
+    if (arguments.length != 3) {
+      throw new UDFArgumentLengthException(
+          "The function TRANSLATE(expr,from_string,to_string) accepts exactly 3 arguments, but "
+          + arguments.length + " arguments is found.");
+    }
+
+    for (int i = 0; i < 3; i++) {
+      if (arguments[i].getTypeName() != serdeConstants.STRING_TYPE_NAME
+          && arguments[i].getTypeName() != serdeConstants.VOID_TYPE_NAME) {
+        throw new UDFArgumentTypeException(i, "The " + getOrdinal(i + 1)
+            + " argument of function TRANSLATE is expected to \""
+            + serdeConstants.STRING_TYPE_NAME + "\", but \""
+            + arguments[i].getTypeName() + "\" is found");
+      }
+    }
+
+    argumentOIs = arguments;
+    return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
+  }
+
+  private final Text resultText = new Text();
+
+  @Override
+  public Object evaluate(DeferredObject[] arguments) throws HiveException {
+    if (arguments[0].get() == null || arguments[1].get() == null
+        || arguments[2].get() == null) {
+      return null;
+    }
+    String exprString = ((StringObjectInspector) argumentOIs[0])
+        .getPrimitiveJavaObject(arguments[0].get());
+    String fromString = ((StringObjectInspector) argumentOIs[1])
+        .getPrimitiveJavaObject(arguments[1].get());
+    String toString = ((StringObjectInspector) argumentOIs[2])
+        .getPrimitiveJavaObject(arguments[2].get());
+
+    char[] expr = exprString.toCharArray();
+    char[] from = fromString.toCharArray();
+    char[] to = toString.toCharArray();
+    char[] result = new char[expr.length];
+    System.arraycopy(expr, 0, result, 0, expr.length);
+    Set<Character> seen = new HashSet<Character>();
+
+    for (int i = 0; i < from.length; i++) {
+      if (seen.contains(from[i])) {
+        continue;
+      }
+      seen.add(from[i]);
+      for (int j = 0; j < expr.length; j++) {
+        if (expr[j] == from[i]) {
+          result[j] = (i < to.length) ? to[i] : 0;
+        }
+      }
+    }
+
+    int pos = 0;
+    for (int i = 0; i < result.length; i++) {
+      if (result[i] != 0) {
+        result[pos++] = result[i];
+      }
+    }
+    resultText.set(new String(result, 0, pos));
+    return resultText;
+  }
+
+  @Override
+  public String getDisplayString(String[] children) {
+    assert (children.length == 3);
+    return "translate(" + children[0] + "," + children[1] + "," + children[2]
+        + ")";
+  }
+}
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/scripts/extracturl.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/scripts/extracturl.java
new file mode 100644
index 0000000..2d9b037
--- /dev/null
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/scripts/extracturl.java
@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.scripts;
+
+import java.io.BufferedReader;
+import java.io.InputStreamReader;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * extracturl.
+ *
+ */
+public final class extracturl {
+
+  protected static final Pattern pattern = Pattern.compile(
+      "<a href=\"http://([\\w\\d]+\\.html)\">link</a>",
+      Pattern.CASE_INSENSITIVE);
+  static InputStreamReader converter = new InputStreamReader(System.in);
+  static BufferedReader in = new BufferedReader(converter);
+
+  public static void main(String[] args) {
+    String input;
+    try {
+      while ((input = in.readLine()) != null) {
+        Matcher m = pattern.matcher(input);
+
+        while (m.find()) {
+          String url = input.substring(m.start(1), m.end(1));
+          System.out.println(url + "\t" + "1");
+        }
+      }
+    } catch (Exception e) {
+      e.printStackTrace();
+      System.exit(1);
+    }
+  }
+
+  private extracturl() {
+    // prevent instantiation
+  }
+}
diff --git a/src/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java b/src/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
deleted file mode 100644
index e1107dd..0000000
--- a/src/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
+++ /dev/null
@@ -1,1143 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.jdbc;
-
-import static org.apache.hadoop.hive.ql.exec.ExplainTask.EXPL_COLUMN_NAME;
-import static org.apache.hadoop.hive.ql.processors.SetProcessor.SET_COLUMN_NAME;
-
-import java.sql.Connection;
-import java.sql.DatabaseMetaData;
-import java.sql.DriverManager;
-import java.sql.DriverPropertyInfo;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.ResultSetMetaData;
-import java.sql.SQLException;
-import java.sql.Statement;
-import java.sql.Types;
-import java.util.Arrays;
-import java.util.Date;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Set;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.conf.HiveConf;
-
-/**
- * TestJdbcDriver.
- *
- */
-public class TestJdbcDriver extends TestCase {
-  private static final String driverName = "org.apache.hadoop.hive.jdbc.HiveDriver";
-  private static final String tableName = "testHiveJdbcDriver_Table";
-  private static final String tableComment = "Simple table";
-  private static final String viewName = "testHiveJdbcDriverView";
-  private static final String viewComment = "Simple view";
-  private static final String partitionedTableName = "testHiveJdbcDriverPartitionedTable";
-  private static final String partitionedColumnName = "partcolabc";
-  private static final String partitionedColumnValue = "20090619";
-  private static final String partitionedTableComment = "Partitioned table";
-  private static final String dataTypeTableName = "testDataTypeTable";
-  private static final String dataTypeTableComment = "Table with many column data types";
-  private final HiveConf conf;
-  private final Path dataFilePath;
-  private final Path dataTypeDataFilePath;
-  private Connection con;
-  private boolean standAloneServer = false;
-
-  public TestJdbcDriver(String name) {
-    super(name);
-    conf = new HiveConf(TestJdbcDriver.class);
-    String dataFileDir = conf.get("test.data.files").replace('\\', '/')
-        .replace("c:", "");
-    dataFilePath = new Path(dataFileDir, "kv1.txt");
-    dataTypeDataFilePath = new Path(dataFileDir, "datatypes.txt");
-    standAloneServer = "true".equals(System
-        .getProperty("test.service.standalone.server"));
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    Class.forName(driverName);
-    if (standAloneServer) {
-      // get connection
-      con = DriverManager.getConnection("jdbc:hive://localhost:10000/default",
-          "", "");
-    } else {
-      con = DriverManager.getConnection("jdbc:hive://", "", "");
-    }
-    assertNotNull("Connection is null", con);
-    assertFalse("Connection should not be closed", con.isClosed());
-    Statement stmt = con.createStatement();
-    assertNotNull("Statement is null", stmt);
-
-    stmt.executeQuery("set hive.support.concurrency = false");
-
-    // drop table. ignore error.
-    try {
-      stmt.executeQuery("drop table " + tableName);
-    } catch (Exception ex) {
-      fail(ex.toString());
-    }
-
-    // create table
-    ResultSet res = stmt.executeQuery("create table " + tableName
-        + " (under_col int comment 'the under column', value string) comment '"
-        + tableComment + "'");
-    assertFalse(res.next());
-
-    // load data
-    res = stmt.executeQuery("load data local inpath '"
-        + dataFilePath.toString() + "' into table " + tableName);
-    assertFalse(res.next());
-
-    // also initialize a paritioned table to test against.
-
-    // drop table. ignore error.
-    try {
-      stmt.executeQuery("drop table " + partitionedTableName);
-    } catch (Exception ex) {
-      fail(ex.toString());
-    }
-
-    res = stmt.executeQuery("create table " + partitionedTableName
-        + " (under_col int, value string) comment '"+partitionedTableComment
-            +"' partitioned by (" + partitionedColumnName + " STRING)");
-    assertFalse(res.next());
-
-    // load data
-    res = stmt.executeQuery("load data local inpath '"
-        + dataFilePath.toString() + "' into table " + partitionedTableName
-        + " PARTITION (" + partitionedColumnName + "="
-        + partitionedColumnValue + ")");
-    assertFalse(res.next());
-
-    // drop table. ignore error.
-    try {
-      stmt.executeQuery("drop table " + dataTypeTableName);
-    } catch (Exception ex) {
-      fail(ex.toString());
-    }
-
-    res = stmt.executeQuery("create table " + dataTypeTableName
-        + " (c1 int, c2 boolean, c3 double, c4 string,"
-        + " c5 array<int>, c6 map<int,string>, c7 map<string,string>,"
-        + " c8 struct<r:string,s:int,t:double>,"
-        + " c9 tinyint, c10 smallint, c11 float, c12 bigint,"
-        + " c13 array<array<string>>,"
-        + " c14 map<int, map<int,int>>,"
-        + " c15 struct<r:int,s:struct<a:int,b:string>>,"
-        + " c16 array<struct<m:map<string,string>,n:int>>,"
-        + " c17 timestamp, "
-        + " c18 decimal,"
-        + " c19 binary,"
-        + " c20 date) comment'" + dataTypeTableComment
-            +"' partitioned by (dt STRING)");
-    assertFalse(res.next());
-
-    // load data
-    res = stmt.executeQuery("load data local inpath '"
-        + dataTypeDataFilePath.toString() + "' into table " + dataTypeTableName
-        + " PARTITION (dt='20090619')");
-    assertFalse(res.next());
-
-    // drop view. ignore error.
-    try {
-      stmt.executeQuery("drop view " + viewName);
-    } catch (Exception ex) {
-      fail(ex.toString());
-    }
-
-    // create view
-    res = stmt.executeQuery("create view " + viewName + " comment '"+viewComment
-            +"' as select * from "+ tableName);
-    assertFalse(res.next());
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-
-    // drop table
-    Statement stmt = con.createStatement();
-    assertNotNull("Statement is null", stmt);
-    ResultSet res = stmt.executeQuery("drop table " + tableName);
-    assertFalse(res.next());
-    res = stmt.executeQuery("drop table " + partitionedTableName);
-    assertFalse(res.next());
-    res = stmt.executeQuery("drop table " + dataTypeTableName);
-    assertFalse(res.next());
-
-    con.close();
-    assertTrue("Connection should be closed", con.isClosed());
-
-    Exception expectedException = null;
-    try {
-      con.createStatement();
-    } catch (Exception e) {
-      expectedException = e;
-    }
-
-    assertNotNull(
-        "createStatement() on closed connection should throw exception",
-        expectedException);
-  }
-
-  /**
-   * verify 'explain ...' resultset
-   * @throws SQLException
-   */
-  public void testExplainStmt() throws SQLException {
-    Statement stmt = con.createStatement();
-
-    ResultSet res = stmt.executeQuery(
-        "explain select c1, c2, c3, c4, c5 as a, c6, c7, c8, c9, c10, c11, c12, " +
-        "c1*2, sentences(null, null, null) as b from " + dataTypeTableName + " limit 1");
-
-    ResultSetMetaData md = res.getMetaData();
-    assertEquals(md.getColumnCount(), 1); // only one result column
-    assertEquals(md.getColumnLabel(1), EXPL_COLUMN_NAME); // verify the column name
-    //verify that there is data in the resultset
-    assertTrue("Nothing returned explain", res.next());
-  }
-
-  public void testPrepareStatement() {
-
-    String sql = "from (select count(1) from "
-        + tableName
-        + " where   'not?param?not?param' <> 'not_param??not_param' and ?=? "
-        + " and 1=? and 2=? and 3.0=? and 4.0=? and 'test\\'string\"'=? and 5=? and ?=? "
-        + " and date '2012-01-01' = date ?"
-        + " ) t  select '2011-03-25' ddate,'China',true bv, 10 num limit 10";
-
-     ///////////////////////////////////////////////
-    //////////////////// correct testcase
-    //////////////////// executed twice: once with the typed ps setters, once with the generic setObject
-    //////////////////////////////////////////////
-    try {
-      PreparedStatement ps = createPreapredStatementUsingSetXXX(sql);
-      ResultSet res = ps.executeQuery();
-      assertPreparedStatementResultAsExpected(res);
-      ps.close();
-
-      ps = createPreapredStatementUsingSetObject(sql);
-      res = ps.executeQuery();
-      assertPreparedStatementResultAsExpected(res);
-      ps.close();
-
-    } catch (Exception e) {
-      e.printStackTrace();
-      fail(e.toString());
-    }
-
-     ///////////////////////////////////////////////
-    //////////////////// other failure testcases
-    //////////////////////////////////////////////
-    // set nothing for prepared sql
-    Exception expectedException = null;
-    try {
-      PreparedStatement ps = con.prepareStatement(sql);
-      ps.executeQuery();
-    } catch (Exception e) {
-      expectedException = e;
-    }
-    assertNotNull(
-        "Execute the un-setted sql statement should throw exception",
-        expectedException);
-
-    // set some of parameters for prepared sql, not all of them.
-    expectedException = null;
-    try {
-      PreparedStatement ps = con.prepareStatement(sql);
-      ps.setBoolean(1, true);
-      ps.setBoolean(2, true);
-      ps.executeQuery();
-    } catch (Exception e) {
-      expectedException = e;
-    }
-    assertNotNull(
-        "Execute the invalid setted sql statement should throw exception",
-        expectedException);
-
-    // set the wrong type parameters for prepared sql.
-    expectedException = null;
-    try {
-      PreparedStatement ps = con.prepareStatement(sql);
-
-      // wrong type here
-      ps.setString(1, "wrong");
-
-      assertTrue(true);
-      ResultSet res = ps.executeQuery();
-      if (!res.next()) {
-        throw new Exception("there must be a empty result set");
-      }
-    } catch (Exception e) {
-      expectedException = e;
-    }
-    assertNotNull(
-        "Execute the invalid setted sql statement should throw exception",
-        expectedException);
-
-    // setObject to the yet unknown type java.util.Date
-    expectedException = null;
-    try {
-      PreparedStatement ps = con.prepareStatement(sql);
-      ps.setObject(1, new Date());
-      ps.executeQuery();
-    } catch (Exception e) {
-      expectedException = e;
-    }
-    assertNotNull(
-        "Setting to an unknown type should throw an exception",
-        expectedException);
-
-  }
-
-  private PreparedStatement createPreapredStatementUsingSetObject(String sql) throws SQLException {
-    PreparedStatement ps = con.prepareStatement(sql);
-
-    ps.setObject(1, true); //setBoolean
-    ps.setObject(2, true); //setBoolean
-
-    ps.setObject(3, Short.valueOf("1")); //setShort
-    ps.setObject(4, 2); //setInt
-    ps.setObject(5, 3f); //setFloat
-    ps.setObject(6, Double.valueOf(4)); //setDouble
-    ps.setObject(7, "test'string\""); //setString
-    ps.setObject(8, 5L); //setLong
-    ps.setObject(9, (byte) 1); //setByte
-    ps.setObject(10, (byte) 1); //setByte
-    ps.setString(11, "2012-01-01"); //setString
-
-    ps.setMaxRows(2);
-    return ps;
-  }
-
-  private PreparedStatement createPreapredStatementUsingSetXXX(String sql) throws SQLException {
-    PreparedStatement ps = con.prepareStatement(sql);
-
-    ps.setBoolean(1, true); //setBoolean
-    ps.setBoolean(2, true); //setBoolean
-
-    ps.setShort(3, Short.valueOf("1")); //setShort
-    ps.setInt(4, 2); //setInt
-    ps.setFloat(5, 3f); //setFloat
-    ps.setDouble(6, Double.valueOf(4)); //setDouble
-    ps.setString(7, "test'string\""); //setString
-    ps.setLong(8, 5L); //setLong
-    ps.setByte(9, (byte) 1); //setByte
-    ps.setByte(10, (byte) 1); //setByte
-    ps.setString(11, "2012-01-01"); //setString
-
-    ps.setMaxRows(2);
-    return ps;
-  }
-
-  private void assertPreparedStatementResultAsExpected(ResultSet res ) throws SQLException {
-    assertNotNull(res);
-
-    while (res.next()) {
-      assertEquals("2011-03-25", res.getString("ddate"));
-      assertEquals("10", res.getString("num"));
-      assertEquals((byte) 10, res.getByte("num"));
-      assertEquals("2011-03-25", res.getDate("ddate").toString());
-      assertEquals(Double.valueOf(10).doubleValue(), res.getDouble("num"), 0.1);
-      assertEquals(10, res.getInt("num"));
-      assertEquals(Short.valueOf("10").shortValue(), res.getShort("num"));
-      assertEquals(10L, res.getLong("num"));
-      assertEquals(true, res.getBoolean("bv"));
-      Object o = res.getObject("ddate");
-      assertNotNull(o);
-      o = res.getObject("num");
-      assertNotNull(o);
-    }
-    res.close();
-    assertTrue(true);
-  }
-
-  public final void testSelectAll() throws Exception {
-    doTestSelectAll(tableName, -1, -1); // tests not setting maxRows (return all)
-    doTestSelectAll(tableName, 0, -1); // tests setting maxRows to 0 (return all)
-  }
-
-  public final void testSelectAllPartioned() throws Exception {
-    doTestSelectAll(partitionedTableName, -1, -1); // tests not setting maxRows
-    // (return all)
-    doTestSelectAll(partitionedTableName, 0, -1); // tests setting maxRows to 0
-    // (return all)
-  }
-
-  public final void testSelectAllMaxRows() throws Exception {
-    doTestSelectAll(tableName, 100, -1);
-  }
-
-  public final void testSelectAllFetchSize() throws Exception {
-    doTestSelectAll(tableName, 100, 20);
-  }
-
-  public void testNullType() throws Exception {
-    Statement stmt = con.createStatement();
-    try {
-      ResultSet res = stmt.executeQuery("select null from " + dataTypeTableName);
-      assertTrue(res.next());
-      assertNull(res.getObject(1));
-    } finally {
-      stmt.close();
-    }
-  }
-
-  public void testDataTypes() throws Exception {
-    Statement stmt = con.createStatement();
-
-    ResultSet res = stmt.executeQuery(
-        "select * from " + dataTypeTableName + " order by c1");
-    ResultSetMetaData meta = res.getMetaData();
-
-    // row 1
-    assertTrue(res.next());
-    // skip the last (partitioning) column since it is always non-null
-    for (int i = 1; i < meta.getColumnCount(); i++) {
-      assertNull(res.getObject(i));
-    }
-
-    // row 2
-    assertTrue(res.next());
-    assertEquals(-1, res.getInt(1));
-    assertEquals(false, res.getBoolean(2));
-    assertEquals(-1.1d, res.getDouble(3));
-    assertEquals("", res.getString(4));
-    assertEquals("[]", res.getString(5));
-    assertEquals("{}", res.getString(6));
-    assertEquals("{}", res.getString(7));
-    assertEquals("[null, null, null]", res.getString(8));
-    assertEquals(-1, res.getByte(9));
-    assertEquals(-1, res.getShort(10));
-    assertEquals(-1.0f, res.getFloat(11));
-    assertEquals(-1, res.getLong(12));
-    assertEquals("[]", res.getString(13));
-    assertEquals("{}", res.getString(14));
-    assertEquals("[null, null]", res.getString(15));
-    assertEquals("[]", res.getString(16));
-    assertEquals(null, res.getString(17));
-    assertEquals(null, res.getTimestamp(17));
-    assertEquals(null, res.getBigDecimal(18));
-    assertEquals(null, res.getString(20));
-    assertEquals(null, res.getDate(20));
-
-    // row 3
-    assertTrue(res.next());
-    assertEquals(1, res.getInt(1));
-    assertEquals(true, res.getBoolean(2));
-    assertEquals(1.1d, res.getDouble(3));
-    assertEquals("1", res.getString(4));
-    assertEquals("[1, 2]", res.getString(5));
-    assertEquals("{1=x, 2=y}", res.getString(6));
-    assertEquals("{k=v}", res.getString(7));
-    assertEquals("[a, 9, 2.2]", res.getString(8));
-    assertEquals(1, res.getByte(9));
-    assertEquals(1, res.getShort(10));
-    assertEquals(1.0f, res.getFloat(11));
-    assertEquals(1, res.getLong(12));
-    assertEquals("[[a, b], [c, d]]", res.getString(13));
-    assertEquals("{1={11=12, 13=14}, 2={21=22}}", res.getString(14));
-    assertEquals("[1, [2, x]]", res.getString(15));
-    assertEquals("[[{}, 1], [{c=d, a=b}, 2]]", res.getString(16));
-    assertEquals("2012-04-22 09:00:00.123456789", res.getString(17));
-    assertEquals("2012-04-22 09:00:00.123456789", res.getTimestamp(17).toString());
-    assertEquals("123456789.0123456", res.getBigDecimal(18).toString());
-    assertEquals("2013-01-01", res.getString(20));
-    assertEquals("2013-01-01", res.getDate(20).toString());
-
-    // test getBoolean rules on non-boolean columns
-    assertEquals(true, res.getBoolean(1));
-    assertEquals(true, res.getBoolean(4));
-
-    // no more rows
-    assertFalse(res.next());
-  }
-
-  private void doTestSelectAll(String tableName, int maxRows, int fetchSize) throws Exception {
-    boolean isPartitionTable = tableName.equals(partitionedTableName);
-
-    Statement stmt = con.createStatement();
-    if (maxRows >= 0) {
-      stmt.setMaxRows(maxRows);
-    }
-    if (fetchSize > 0) {
-      stmt.setFetchSize(fetchSize);
-      assertEquals(fetchSize, stmt.getFetchSize());
-    }
-
-    // JDBC says that 0 means return all, which is the default
-    int expectedMaxRows = maxRows < 1 ? 0 : maxRows;
-
-    assertNotNull("Statement is null", stmt);
-    assertEquals("Statement max rows not as expected", expectedMaxRows, stmt
-        .getMaxRows());
-    assertFalse("Statement should not be closed", stmt.isClosed());
-
-    ResultSet res;
-
-    // run some queries
-    res = stmt.executeQuery("select * from " + tableName);
-    assertNotNull("ResultSet is null", res);
-    assertTrue("getResultSet() not returning expected ResultSet", res == stmt
-        .getResultSet());
-    assertEquals("get update count not as expected", 0, stmt.getUpdateCount());
-    int i = 0;
-
-    ResultSetMetaData meta = res.getMetaData();
-    int expectedColCount = isPartitionTable ? 3 : 2;
-    assertEquals(
-      "Unexpected column count", expectedColCount, meta.getColumnCount());
-
-    boolean moreRow = res.next();
-    while (moreRow) {
-      try {
-        i++;
-        assertEquals(res.getInt(1), res.getInt("under_col"));
-        assertEquals(res.getString(1), res.getString("under_col"));
-        assertEquals(res.getString(2), res.getString("value"));
-        if (isPartitionTable) {
-          assertEquals(res.getString(3), partitionedColumnValue);
-          assertEquals(res.getString(3), res.getString(partitionedColumnName));
-        }
-        assertFalse("Last result value was not null", res.wasNull());
-        assertNull("No warnings should be found on ResultSet", res
-            .getWarnings());
-        res.clearWarnings(); // verifying that method is supported
-
-        // System.out.println(res.getString(1) + " " + res.getString(2));
-        assertEquals(
-            "getInt and getString don't align for the same result value",
-            String.valueOf(res.getInt(1)), res.getString(1));
-        assertEquals("Unexpected result found", "val_" + res.getString(1), res
-            .getString(2));
-        moreRow = res.next();
-      } catch (SQLException e) {
-        System.out.println(e.toString());
-        e.printStackTrace();
-        throw new Exception(e.toString());
-      }
-    }
-
-    // supposed to get 500 rows if maxRows isn't set
-    int expectedRowCount = maxRows > 0 ? maxRows : 500;
-    assertEquals("Incorrect number of rows returned", expectedRowCount, i);
-
-    // should have no more rows
-    assertEquals(false, moreRow);
-
-    assertNull("No warnings should be found on statement", stmt.getWarnings());
-    stmt.clearWarnings(); // verifying that method is supported
-
-    assertNull("No warnings should be found on connection", con.getWarnings());
-    con.clearWarnings(); // verifying that method is supported
-
-    stmt.close();
-    assertTrue("Statement should be closed", stmt.isClosed());
-  }
-
-  public void testErrorMessages() throws SQLException {
-    String invalidSyntaxSQLState = "42000";
-
-    // These tests inherently cause exceptions to be written to the test output
-    // logs. This is undesirable, since you it might appear to someone looking
-    // at the test output logs as if something is failing when it isn't. Not
-    // sure
-    // how to get around that.
-    doTestErrorCase("SELECTT * FROM " + tableName,
-        "cannot recognize input near 'SELECTT' '*' 'FROM'",
-        invalidSyntaxSQLState, 40000);
-    doTestErrorCase("SELECT * FROM some_table_that_does_not_exist",
-        "Table not found", "42S02", 10001);
-    doTestErrorCase("drop table some_table_that_does_not_exist",
-        "Table not found", "42S02", 10001);
-    doTestErrorCase("SELECT invalid_column FROM " + tableName,
-        "Invalid table alias or column reference", invalidSyntaxSQLState, 10004);
-    doTestErrorCase("SELECT invalid_function(under_col) FROM " + tableName,
-    "Invalid function", invalidSyntaxSQLState, 10011);
-
-    // TODO: execute errors like this currently don't return good error
-    // codes and messages. This should be fixed.
-    doTestErrorCase(
-        "create table " + tableName + " (key int, value string)",
-        "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask",
-        "08S01", 1);
-  }
-
-  private void doTestErrorCase(String sql, String expectedMessage,
-      String expectedSQLState, int expectedErrorCode) throws SQLException {
-    Statement stmt = con.createStatement();
-    boolean exceptionFound = false;
-    try {
-      stmt.executeQuery(sql);
-    } catch (SQLException e) {
-      assertTrue("Adequate error messaging not found for '" + sql + "': "
-          + e.getMessage(), e.getMessage().contains(expectedMessage));
-      assertEquals("Expected SQLState not found for '" + sql + "'",
-          expectedSQLState, e.getSQLState());
-      assertEquals("Expected error code not found for '" + sql + "'",
-          expectedErrorCode, e.getErrorCode());
-      exceptionFound = true;
-    }
-
-    assertNotNull("Exception should have been thrown for query: " + sql,
-        exceptionFound);
-  }
-
-  public void testShowTables() throws SQLException {
-    Statement stmt = con.createStatement();
-    assertNotNull("Statement is null", stmt);
-
-    ResultSet res = stmt.executeQuery("show tables");
-
-    boolean testTableExists = false;
-    while (res.next()) {
-      assertNotNull("table name is null in result set", res.getString(1));
-      if (tableName.equalsIgnoreCase(res.getString(1))) {
-        testTableExists = true;
-      }
-    }
-
-    assertTrue("table name " + tableName
-        + " not found in SHOW TABLES result set", testTableExists);
-  }
-
-  public void testMetaDataGetTables() throws SQLException {
-    Map<String, Object[]> tests = new HashMap<String, Object[]>();
-    tests.put("test%jdbc%", new Object[]{"testhivejdbcdriver_table"
-            , "testhivejdbcdriverpartitionedtable"
-            , "testhivejdbcdriverview"});
-    tests.put("%jdbcdriver\\_table", new Object[]{"testhivejdbcdriver_table"});
-    tests.put("testhivejdbcdriver\\_table", new Object[]{"testhivejdbcdriver_table"});
-    tests.put("test_ivejdbcdri_er\\_table", new Object[]{"testhivejdbcdriver_table"});
-    tests.put("test_ivejdbcdri_er_table", new Object[]{"testhivejdbcdriver_table"});
-    tests.put("test_ivejdbcdri_er%table", new Object[]{
-        "testhivejdbcdriver_table", "testhivejdbcdriverpartitionedtable" });
-    tests.put("%jdbc%", new Object[]{ "testhivejdbcdriver_table"
-            , "testhivejdbcdriverpartitionedtable"
-            , "testhivejdbcdriverview"});
-    tests.put("", new Object[]{});
-
-    for (String checkPattern: tests.keySet()) {
-      ResultSet rs = (ResultSet)con.getMetaData().getTables("default", null, checkPattern, null);
-      int cnt = 0;
-      while (rs.next()) {
-        String resultTableName = rs.getString("TABLE_NAME");
-        assertEquals("Get by index different from get by name.", rs.getString(3), resultTableName);
-        assertEquals("Excpected a different table.", tests.get(checkPattern)[cnt], resultTableName);
-        String resultTableComment = rs.getString("REMARKS");
-        assertTrue("Missing comment on the table.", resultTableComment.length()>0);
-        String tableType = rs.getString("TABLE_TYPE");
-        if (resultTableName.endsWith("view")) {
-          assertEquals("Expected a tabletype view but got something else.", "VIEW", tableType);
-        }
-        cnt++;
-      }
-      rs.close();
-      assertEquals("Received an incorrect number of tables.", tests.get(checkPattern).length, cnt);
-    }
-
-    // only ask for the views.
-    ResultSet rs = (ResultSet)con.getMetaData().getTables("default", null, null
-            , new String[]{"VIEW"});
-    int cnt=0;
-    while (rs.next()) {
-      cnt++;
-    }
-    rs.close();
-    assertEquals("Incorrect number of views found.", 1, cnt);
-  }
-
-  public void testMetaDataGetCatalogs() throws SQLException {
-    ResultSet rs = (ResultSet)con.getMetaData().getCatalogs();
-    int cnt = 0;
-    while (rs.next()) {
-      String catalogname = rs.getString("TABLE_CAT");
-      assertEquals("Get by index different from get by name", rs.getString(1), catalogname);
-      switch(cnt) {
-        case 0:
-          assertEquals("default", catalogname);
-          break;
-        default:
-          fail("More then one catalog found.");
-          break;
-      }
-      cnt++;
-    }
-    rs.close();
-    assertEquals("Incorrect catalog count", 1, cnt);
-  }
-
-  public void testMetaDataGetSchemas() throws SQLException {
-    ResultSet rs = (ResultSet)con.getMetaData().getSchemas();
-    int cnt = 0;
-    while (rs.next()) {
-      cnt++;
-    }
-    rs.close();
-    assertEquals("Incorrect schema count", 0, cnt);
-  }
-
-  public void testMetaDataGetTableTypes() throws SQLException {
-    ResultSet rs = (ResultSet)con.getMetaData().getTableTypes();
-    Set<String> tabletypes = new HashSet();
-    tabletypes.add("TABLE");
-    tabletypes.add("EXTERNAL TABLE");
-    tabletypes.add("VIEW");
-
-    int cnt = 0;
-    while (rs.next()) {
-      String tabletype = rs.getString("TABLE_TYPE");
-      assertEquals("Get by index different from get by name", rs.getString(1), tabletype);
-      tabletypes.remove(tabletype);
-      cnt++;
-    }
-    rs.close();
-    assertEquals("Incorrect tabletype count.", 0, tabletypes.size());
-    assertTrue("Found less tabletypes then we test for.", cnt >= tabletypes.size());
-  }
-
-  public void testMetaDataGetColumns() throws SQLException {
-    Map<String[], Integer> tests = new HashMap<String[], Integer>();
-    tests.put(new String[]{"testhivejdbcdriver\\_table", null}, 2);
-    tests.put(new String[]{"testhivejdbc%", null}, 7);
-    tests.put(new String[]{"testhiveJDBC%", null}, 7);
-    tests.put(new String[]{"testhiveJDB\\C%", null}, 0);
-    tests.put(new String[]{"%jdbcdriver\\_table", null}, 2);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_col"}, 1);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_COL"}, 1);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_co_"}, 1);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "under_col"}, 1);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "und%"}, 1);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "%"}, 2);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "_%"}, 2);
-
-    for (String[] checkPattern: tests.keySet()) {
-      ResultSet rs = con.getMetaData().getColumns(null, null, checkPattern[0],
-          checkPattern[1]);
-
-      // validate the metadata for the getColumns result set
-      ResultSetMetaData rsmd = rs.getMetaData();
-      assertEquals("TABLE_CAT", rsmd.getColumnName(1));
-
-      int cnt = 0;
-      while (rs.next()) {
-        String columnname = rs.getString("COLUMN_NAME");
-        int ordinalPos = rs.getInt("ORDINAL_POSITION");
-        switch(cnt) {
-          case 0:
-            assertEquals("Wrong column name found", "under_col", columnname);
-            assertEquals("Wrong ordinal position found", ordinalPos, 1);
-            break;
-          case 1:
-            assertEquals("Wrong column name found", "value", columnname);
-            assertEquals("Wrong ordinal position found", ordinalPos, 2);
-            break;
-          default:
-            break;
-        }
-        cnt++;
-      }
-      rs.close();
-      assertEquals("Found less columns then we test for.", tests.get(checkPattern).intValue(), cnt);
-    }
-  }
-
-  /**
-   * Validate the Metadata for the result set of a metadata getColumns call.
-   */
-  public void testMetaDataGetColumnsMetaData() throws SQLException {
-    ResultSet rs = (ResultSet)con.getMetaData().getColumns(null, null
-            , "testhivejdbcdriver\\_table", null);
-
-    ResultSetMetaData rsmd = rs.getMetaData();
-
-    assertEquals("TABLE_CAT", rsmd.getColumnName(1));
-    assertEquals(Types.VARCHAR, rsmd.getColumnType(1));
-    assertEquals(Integer.MAX_VALUE, rsmd.getColumnDisplaySize(1));
-
-    assertEquals("ORDINAL_POSITION", rsmd.getColumnName(17));
-    assertEquals(Types.INTEGER, rsmd.getColumnType(17));
-    assertEquals(11, rsmd.getColumnDisplaySize(17));
-  }
-
-  public void testConversionsBaseResultSet() throws SQLException {
-    ResultSet rs = new HiveMetaDataResultSet(Arrays.asList("key")
-            , Arrays.asList("long")
-            , Arrays.asList(1234, "1234", "abc")) {
-      private int cnt=1;
-      public boolean next() throws SQLException {
-        if (cnt<data.size()) {
-          row = Arrays.asList(data.get(cnt));
-          cnt++;
-          return true;
-        } else {
-          return false;
-        }
-      }
-    };
-
-    while (rs.next()) {
-      String key = rs.getString("key");
-      if ("1234".equals(key)) {
-        assertEquals("Converting a string column into a long failed.", rs.getLong("key"), 1234L);
-        assertEquals("Converting a string column into a int failed.", rs.getInt("key"), 1234);
-      } else if ("abc".equals(key)) {
-        Object result = null;
-        Exception expectedException = null;
-        try {
-          result = rs.getLong("key");
-        } catch (SQLException e) {
-          expectedException = e;
-        }
-        assertTrue("Trying to convert 'abc' into a long should not work.", expectedException!=null);
-        try {
-          result = rs.getInt("key");
-        } catch (SQLException e) {
-          expectedException = e;
-        }
-        assertTrue("Trying to convert 'abc' into a int should not work.", expectedException!=null);
-      }
-    }
-  }
-
-  public void testDescribeTable() throws SQLException {
-    Statement stmt = con.createStatement();
-    assertNotNull("Statement is null", stmt);
-
-    ResultSet res = stmt.executeQuery("describe " + tableName);
-    res.next();
-    assertEquals(true, res.getString(1).contains("under_col"));
-    assertEquals(true, res.getString(2).contains("int"));
-    res.next();
-    assertEquals(true, res.getString(1).contains("value"));
-    assertEquals(true, res.getString(2).contains("string"));
-    assertFalse("More results found than expected", res.next());
-  }
-
-  public void testDatabaseMetaData() throws SQLException {
-    DatabaseMetaData meta = con.getMetaData();
-
-    assertEquals("Hive", meta.getDatabaseProductName());
-    assertEquals("1", meta.getDatabaseProductVersion());
-    assertEquals(DatabaseMetaData.sqlStateSQL99, meta.getSQLStateType());
-    assertNull(meta.getProcedures(null, null, null));
-    assertFalse(meta.supportsCatalogsInTableDefinitions());
-    assertFalse(meta.supportsSchemasInTableDefinitions());
-    assertFalse(meta.supportsSchemasInDataManipulation());
-    assertFalse(meta.supportsMultipleResultSets());
-    assertFalse(meta.supportsStoredProcedures());
-    assertTrue(meta.supportsAlterTableWithAddColumn());
-  }
-
-  public void testResultSetMetaData() throws SQLException {
-    Statement stmt = con.createStatement();
-
-    ResultSet res = stmt.executeQuery(
-        "select c1, c2, c3, c4, c5 as a, c6, c7, c8, c9, c10, c11, c12, " +
-        "c1*2, sentences(null, null, null) as b, c17, c18, c20 from " + dataTypeTableName +
-        " limit 1");
-    ResultSetMetaData meta = res.getMetaData();
-
-    ResultSet colRS = con.getMetaData().getColumns(null, null,
-        dataTypeTableName.toLowerCase(), null);
-
-    assertEquals(17, meta.getColumnCount());
-
-    assertTrue(colRS.next());
-
-    assertEquals("c1", meta.getColumnName(1));
-    assertEquals(Types.INTEGER, meta.getColumnType(1));
-    assertEquals("int", meta.getColumnTypeName(1));
-    assertEquals(11, meta.getColumnDisplaySize(1));
-    assertEquals(10, meta.getPrecision(1));
-    assertEquals(0, meta.getScale(1));
-
-    assertEquals("c1", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.INTEGER, colRS.getInt("DATA_TYPE"));
-    assertEquals("int", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(1), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(1), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c2", meta.getColumnName(2));
-    assertEquals("boolean", meta.getColumnTypeName(2));
-    assertEquals(Types.BOOLEAN, meta.getColumnType(2));
-    assertEquals(1, meta.getColumnDisplaySize(2));
-    assertEquals(1, meta.getPrecision(2));
-    assertEquals(0, meta.getScale(2));
-
-    assertEquals("c2", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.BOOLEAN, colRS.getInt("DATA_TYPE"));
-    assertEquals("boolean", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(2), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(2), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c3", meta.getColumnName(3));
-    assertEquals(Types.DOUBLE, meta.getColumnType(3));
-    assertEquals("double", meta.getColumnTypeName(3));
-    assertEquals(25, meta.getColumnDisplaySize(3));
-    assertEquals(15, meta.getPrecision(3));
-    assertEquals(15, meta.getScale(3));
-
-    assertEquals("c3", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.DOUBLE, colRS.getInt("DATA_TYPE"));
-    assertEquals("double", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(3), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(3), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c4", meta.getColumnName(4));
-    assertEquals(Types.VARCHAR, meta.getColumnType(4));
-    assertEquals("string", meta.getColumnTypeName(4));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(4));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(4));
-    assertEquals(0, meta.getScale(4));
-
-    assertEquals("c4", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
-    assertEquals("string", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(4), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(4), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("a", meta.getColumnName(5));
-    assertEquals(Types.VARCHAR, meta.getColumnType(5));
-    assertEquals("string", meta.getColumnTypeName(5));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(5));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(5));
-    assertEquals(0, meta.getScale(5));
-
-    assertEquals("c5", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
-    assertEquals("array<int>", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(5), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(5), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c6", meta.getColumnName(6));
-    assertEquals(Types.VARCHAR, meta.getColumnType(6));
-    assertEquals("string", meta.getColumnTypeName(6));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(6));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(6));
-    assertEquals(0, meta.getScale(6));
-
-    assertEquals("c6", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
-    assertEquals("map<int,string>", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(6), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(6), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c7", meta.getColumnName(7));
-    assertEquals(Types.VARCHAR, meta.getColumnType(7));
-    assertEquals("string", meta.getColumnTypeName(7));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(7));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(7));
-    assertEquals(0, meta.getScale(7));
-
-    assertEquals("c7", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
-    assertEquals("map<string,string>", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(7), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(7), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c8", meta.getColumnName(8));
-    assertEquals(Types.VARCHAR, meta.getColumnType(8));
-    assertEquals("string", meta.getColumnTypeName(8));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(8));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(8));
-    assertEquals(0, meta.getScale(8));
-
-    assertEquals("c8", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
-    assertEquals("struct<r:string,s:int,t:double>", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(8), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(8), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c9", meta.getColumnName(9));
-    assertEquals(Types.TINYINT, meta.getColumnType(9));
-    assertEquals("tinyint", meta.getColumnTypeName(9));
-    assertEquals(4, meta.getColumnDisplaySize(9));
-    assertEquals(3, meta.getPrecision(9));
-    assertEquals(0, meta.getScale(9));
-
-    assertEquals("c9", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.TINYINT, colRS.getInt("DATA_TYPE"));
-    assertEquals("tinyint", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(9), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(9), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c10", meta.getColumnName(10));
-    assertEquals(Types.SMALLINT, meta.getColumnType(10));
-    assertEquals("smallint", meta.getColumnTypeName(10));
-    assertEquals(6, meta.getColumnDisplaySize(10));
-    assertEquals(5, meta.getPrecision(10));
-    assertEquals(0, meta.getScale(10));
-
-    assertEquals("c10", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.SMALLINT, colRS.getInt("DATA_TYPE"));
-    assertEquals("smallint", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(10), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(10), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c11", meta.getColumnName(11));
-    assertEquals(Types.FLOAT, meta.getColumnType(11));
-    assertEquals("float", meta.getColumnTypeName(11));
-    assertEquals(24, meta.getColumnDisplaySize(11));
-    assertEquals(7, meta.getPrecision(11));
-    assertEquals(7, meta.getScale(11));
-
-    assertEquals("c11", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.FLOAT, colRS.getInt("DATA_TYPE"));
-    assertEquals("float", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(11), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(11), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c12", meta.getColumnName(12));
-    assertEquals(Types.BIGINT, meta.getColumnType(12));
-    assertEquals("bigint", meta.getColumnTypeName(12));
-    assertEquals(20, meta.getColumnDisplaySize(12));
-    assertEquals(19, meta.getPrecision(12));
-    assertEquals(0, meta.getScale(12));
-
-    assertEquals("c12", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.BIGINT, colRS.getInt("DATA_TYPE"));
-    assertEquals("bigint", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(12), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(12), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertEquals("_c12", meta.getColumnName(13));
-    assertEquals(Types.INTEGER, meta.getColumnType(13));
-    assertEquals("int", meta.getColumnTypeName(13));
-    assertEquals(11, meta.getColumnDisplaySize(13));
-    assertEquals(10, meta.getPrecision(13));
-    assertEquals(0, meta.getScale(13));
-
-    assertEquals("b", meta.getColumnName(14));
-    assertEquals(Types.VARCHAR, meta.getColumnType(14));
-    assertEquals("string", meta.getColumnTypeName(14));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(14));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(14));
-    assertEquals(0, meta.getScale(14));
-
-    assertEquals("c17", meta.getColumnName(15));
-    assertEquals(Types.TIMESTAMP, meta.getColumnType(15));
-    assertEquals("timestamp", meta.getColumnTypeName(15));
-    assertEquals(29, meta.getColumnDisplaySize(15));
-    assertEquals(29, meta.getPrecision(15));
-    assertEquals(9, meta.getScale(15));
-
-    assertEquals("c18", meta.getColumnName(16));
-    assertEquals(Types.DECIMAL, meta.getColumnType(16));
-    assertEquals("decimal", meta.getColumnTypeName(16));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(16));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(16));
-    assertEquals(Integer.MAX_VALUE, meta.getScale(16));
-
-    assertEquals("c20", meta.getColumnName(17));
-    assertEquals(Types.DATE, meta.getColumnType(17));
-    assertEquals("date", meta.getColumnTypeName(17));
-    assertEquals(10, meta.getColumnDisplaySize(17));
-    assertEquals(10, meta.getPrecision(17));
-    assertEquals(0, meta.getScale(17));
-
-    for (int i = 1; i <= meta.getColumnCount(); i++) {
-      assertFalse(meta.isAutoIncrement(i));
-      assertFalse(meta.isCurrency(i));
-      assertEquals(ResultSetMetaData.columnNullable, meta.isNullable(i));
-    }
-  }
-
-  // [url] [host] [port] [db]
-  private static final String[][] URL_PROPERTIES = new String[][] {
-      {"jdbc:hive://", "", "", "default"},
-      {"jdbc:hive://localhost:10001/default", "localhost", "10001", "default"},
-      {"jdbc:hive://localhost/notdefault", "localhost", "10000", "notdefault"},
-      {"jdbc:hive://foo:1243", "foo", "1243", "default"}};
-
-  public void testDriverProperties() throws SQLException {
-    HiveDriver driver = new HiveDriver();
-
-    for (String[] testValues : URL_PROPERTIES) {
-      DriverPropertyInfo[] dpi = driver.getPropertyInfo(testValues[0], null);
-      assertEquals("unexpected DriverPropertyInfo array size", 3, dpi.length);
-      assertDpi(dpi[0], "HOST", testValues[1]);
-      assertDpi(dpi[1], "PORT", testValues[2]);
-      assertDpi(dpi[2], "DBNAME", testValues[3]);
-    }
-
-  }
-
-  private static void assertDpi(DriverPropertyInfo dpi, String name,
-      String value) {
-    assertEquals("Invalid DriverPropertyInfo name", name, dpi.name);
-    assertEquals("Invalid DriverPropertyInfo value", value, dpi.value);
-    assertEquals("Invalid DriverPropertyInfo required", false, dpi.required);
-  }
-
-
-  /**
-   * validate schema generated by "set" command
-   * @throws SQLException
-   */
-  public void testSetCommand() throws SQLException {
-    // execute set command
-    String sql = "set -v";
-    Statement stmt = con.createStatement();
-    ResultSet res = stmt.executeQuery(sql);
-
-    // Validate resultset columns
-    ResultSetMetaData md = res.getMetaData() ;
-    assertEquals(1, md.getColumnCount());
-    assertEquals(SET_COLUMN_NAME, md.getColumnLabel(1));
-
-    //check if there is data in the resultset
-    assertTrue("Nothing returned by set -v", res.next());
-
-    res.close();
-    stmt.close();
-  }
-
-}
diff --git a/src/jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java b/src/jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java
deleted file mode 100644
index b05d9af..0000000
--- a/src/jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java
+++ /dev/null
@@ -1,1745 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hive.jdbc;
-
-import static org.apache.hadoop.hive.ql.exec.ExplainTask.EXPL_COLUMN_NAME;
-import static org.apache.hadoop.hive.ql.processors.SetProcessor.SET_COLUMN_NAME;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertNull;
-import static org.junit.Assert.assertTrue;
-import static org.junit.Assert.fail;
-
-import java.sql.Connection;
-import java.sql.DatabaseMetaData;
-import java.sql.DriverManager;
-import java.sql.DriverPropertyInfo;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.ResultSetMetaData;
-import java.sql.SQLException;
-import java.sql.Statement;
-import java.sql.Types;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import java.util.Set;
-import java.util.regex.Pattern;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.TableType;
-import org.apache.hive.common.util.HiveVersionInfo;
-import org.apache.hive.jdbc.Utils.JdbcConnectionParams;
-import org.apache.hive.service.cli.operation.ClassicTableTypeMapping;
-import org.apache.hive.service.cli.operation.ClassicTableTypeMapping.ClassicTableTypes;
-import org.apache.hive.service.cli.operation.HiveTableTypeMapping;
-import org.apache.hive.service.cli.operation.TableTypeMappingFactory.TableTypeMappings;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-/**
- * TestJdbcDriver2
- *
- */
-public class TestJdbcDriver2 {
-  private static final String driverName = "org.apache.hive.jdbc.HiveDriver";
-  private static final String tableName = "testHiveJdbcDriver_Table";
-  private static final String tableComment = "Simple table";
-  private static final String viewName = "testHiveJdbcDriverView";
-  private static final String viewComment = "Simple view";
-  private static final String partitionedTableName = "testHiveJdbcDriverPartitionedTable";
-  private static final String partitionedColumnName = "partcolabc";
-  private static final String partitionedColumnValue = "20090619";
-  private static final String partitionedTableComment = "Partitioned table";
-  private static final String dataTypeTableName = "testDataTypeTable";
-  private static final String dataTypeTableComment = "Table with many column data types";
-  private final HiveConf conf;
-  private final Path dataFilePath;
-  private final Path dataTypeDataFilePath;
-  private Connection con;
-  private static boolean standAloneServer = false;
-  private static final float floatCompareDelta = 0.0001f;
-
-  public TestJdbcDriver2() {
-    conf = new HiveConf(TestJdbcDriver2.class);
-    String dataFileDir = conf.get("test.data.files").replace('\\', '/')
-        .replace("c:", "");
-    dataFilePath = new Path(dataFileDir, "kv1.txt");
-    dataTypeDataFilePath = new Path(dataFileDir, "datatypes.txt");
-    standAloneServer = "true".equals(System
-        .getProperty("test.service.standalone.server"));
-  }
-
-  @BeforeClass
-  public static void setUpBeforeClass() throws SQLException, ClassNotFoundException{
-    Class.forName(driverName);
-    Connection con1 = getConnection();
-
-    Statement stmt1 = con1.createStatement();
-    assertNotNull("Statement is null", stmt1);
-
-    stmt1.execute("set hive.support.concurrency = false");
-
-    DatabaseMetaData metadata = con1.getMetaData();
-
-    // Drop databases created by other test cases
-    ResultSet databaseRes = metadata.getSchemas();
-
-    while(databaseRes.next()){
-      String db = databaseRes.getString(1);
-      if(!db.equals("default")){
-        System.err.println("Dropping database " + db);
-        stmt1.execute("DROP DATABASE " + db + " CASCADE");
-      }
-    }
-    stmt1.close();
-    con1.close();
-  }
-
-  @Before
-  public void setUp() throws Exception {
-    con = getConnection();
-
-    Statement stmt = con.createStatement();
-    assertNotNull("Statement is null", stmt);
-
-    stmt.execute("set hive.support.concurrency = false");
-
-    // drop table. ignore error.
-    try {
-      stmt.execute("drop table " + tableName);
-    } catch (Exception ex) {
-      fail(ex.toString());
-    }
-
-    // create table
-    stmt.execute("create table " + tableName
-        + " (under_col int comment 'the under column', value string) comment '"
-        + tableComment + "'");
-
-    // load data
-    stmt.execute("load data local inpath '"
-        + dataFilePath.toString() + "' into table " + tableName);
-
-    // also initialize a paritioned table to test against.
-
-    // drop table. ignore error.
-    try {
-      stmt.execute("drop table " + partitionedTableName);
-    } catch (Exception ex) {
-      fail(ex.toString());
-    }
-
-    stmt.execute("create table " + partitionedTableName
-        + " (under_col int, value string) comment '"+partitionedTableComment
-        +"' partitioned by (" + partitionedColumnName + " STRING)");
-
-    // load data
-    stmt.execute("load data local inpath '"
-        + dataFilePath.toString() + "' into table " + partitionedTableName
-        + " PARTITION (" + partitionedColumnName + "="
-        + partitionedColumnValue + ")");
-
-    // drop table. ignore error.
-    try {
-      stmt.execute("drop table " + dataTypeTableName);
-    } catch (Exception ex) {
-      fail(ex.toString());
-    }
-
-    stmt.execute("create table " + dataTypeTableName
-        + " (c1 int, c2 boolean, c3 double, c4 string,"
-        + " c5 array<int>, c6 map<int,string>, c7 map<string,string>,"
-        + " c8 struct<r:string,s:int,t:double>,"
-        + " c9 tinyint, c10 smallint, c11 float, c12 bigint,"
-        + " c13 array<array<string>>,"
-        + " c14 map<int, map<int,int>>,"
-        + " c15 struct<r:int,s:struct<a:int,b:string>>,"
-        + " c16 array<struct<m:map<string,string>,n:int>>,"
-        + " c17 timestamp, "
-        + " c18 decimal, "
-        + " c19 binary, "
-        + " c20 date,"
-        + " c21 varchar(20)"
-        + ") comment'" + dataTypeTableComment
-        +"' partitioned by (dt STRING)");
-
-    stmt.execute("load data local inpath '"
-        + dataTypeDataFilePath.toString() + "' into table " + dataTypeTableName
-        + " PARTITION (dt='20090619')");
-
-    // drop view. ignore error.
-    try {
-      stmt.execute("drop view " + viewName);
-    } catch (Exception ex) {
-      fail(ex.toString());
-    }
-
-    // create view
-    stmt.execute("create view " + viewName + " comment '"+viewComment
-        +"' as select * from "+ tableName);
-  }
-
-  private static Connection getConnection() throws SQLException {
-    Connection con1;
-    if (standAloneServer) {
-      // get connection
-      con1 = DriverManager.getConnection("jdbc:hive2://localhost:10000/default",
-          "", "");
-    } else {
-      con1 = DriverManager.getConnection("jdbc:hive2://", "", "");
-    }
-    assertNotNull("Connection is null", con1);
-    assertFalse("Connection should not be closed", con1.isClosed());
-
-    return con1;
-  }
-
-  @After
-  public void tearDown() throws Exception {
-    // drop table
-    Statement stmt = con.createStatement();
-    assertNotNull("Statement is null", stmt);
-    stmt.execute("drop table " + tableName);
-    stmt.execute("drop table " + partitionedTableName);
-    stmt.execute("drop table " + dataTypeTableName);
-
-    con.close();
-    assertTrue("Connection should be closed", con.isClosed());
-
-    Exception expectedException = null;
-    try {
-      con.createStatement();
-    } catch (Exception e) {
-      expectedException = e;
-    }
-
-    assertNotNull(
-        "createStatement() on closed connection should throw exception",
-        expectedException);
-  }
-
-  @Test
-  public void testBadURL() throws Exception {
-    checkBadUrl("jdbc:hive2://localhost:10000;principal=test");
-    checkBadUrl("jdbc:hive2://localhost:10000;" +
-        "principal=hive/HiveServer2Host@YOUR-REALM.COM");
-    checkBadUrl("jdbc:hive2://localhost:10000test");
-  }
-
-
-  private void checkBadUrl(String url) throws SQLException {
-    try{
-      DriverManager.getConnection(url, "", "");
-      fail("should have thrown IllegalArgumentException but did not ");
-    }catch(IllegalArgumentException i){
-      assertTrue(i.getMessage().contains("Bad URL format. Hostname not found "
-          + " in authority part of the url"));
-    }
-  }
-
-  @Test
-  public void testParentReferences() throws Exception {
-    /* Test parent references from Statement */
-    Statement s = this.con.createStatement();
-    ResultSet rs = s.executeQuery("SELECT * FROM " + dataTypeTableName);
-
-    rs.close();
-    s.close();
-
-    assertTrue(s.getConnection() == this.con);
-    assertTrue(rs.getStatement() == s);
-
-    /* Test parent references from PreparedStatement */
-    PreparedStatement ps = this.con.prepareStatement("SELECT * FROM " + dataTypeTableName);
-    rs = ps.executeQuery();
-
-    rs.close();
-    ps.close();
-
-    assertTrue(ps.getConnection() == this.con);
-    assertTrue(rs.getStatement() == ps);
-
-    /* Test DatabaseMetaData queries which do not have a parent Statement */
-    DatabaseMetaData md = this.con.getMetaData();
-
-    assertTrue(md.getConnection() == this.con);
-
-    rs = md.getCatalogs();
-    assertNull(rs.getStatement());
-    rs.close();
-
-    rs = md.getColumns(null, null, null, null);
-    assertNull(rs.getStatement());
-    rs.close();
-
-    rs = md.getFunctions(null, null, null);
-    assertNull(rs.getStatement());
-    rs.close();
-
-    rs = md.getImportedKeys(null, null, null);
-    assertNull(rs.getStatement());
-    rs.close();
-
-    rs = md.getPrimaryKeys(null, null, null);
-    assertNull(rs.getStatement());
-    rs.close();
-
-    rs = md.getProcedureColumns(null, null, null, null);
-    assertNull(rs.getStatement());
-    rs.close();
-
-    rs = md.getProcedures(null, null, null);
-    assertNull(rs.getStatement());
-    rs.close();
-
-    rs = md.getSchemas();
-    assertNull(rs.getStatement());
-    rs.close();
-
-    rs = md.getTableTypes();
-    assertNull(rs.getStatement());
-    rs.close();
-
-    rs = md.getTables(null, null, null, null);
-    assertNull(rs.getStatement());
-    rs.close();
-
-    rs = md.getTypeInfo();
-    assertNull(rs.getStatement());
-    rs.close();
-  }
-
-  @Test
-  public void testDataTypes2() throws Exception {
-    Statement stmt = con.createStatement();
-
-    ResultSet res = stmt.executeQuery(
-        "select c5, c1 from " + dataTypeTableName + " order by c1");
-    ResultSetMetaData meta = res.getMetaData();
-
-    // row 1
-    assertTrue(res.next());
-    // skip the last (partitioning) column since it is always non-null
-    for (int i = 1; i < meta.getColumnCount(); i++) {
-      assertNull(res.getObject(i));
-    }
-
-  }
-
-  @Test
-  public void testErrorDiag() throws SQLException {
-    Statement stmt = con.createStatement();
-
-    // verify syntax error
-    try {
-      ResultSet res = stmt.executeQuery("select from " + dataTypeTableName);
-    } catch (SQLException e) {
-      assertEquals("42000", e.getSQLState());
-    }
-
-    // verify table not fuond error
-    try {
-      ResultSet res = stmt.executeQuery("select * from nonTable");
-    } catch (SQLException e) {
-      assertEquals("42S02", e.getSQLState());
-    }
-
-    // verify invalid column error
-    try {
-      ResultSet res = stmt.executeQuery("select zzzz from " + dataTypeTableName);
-    } catch (SQLException e) {
-      assertEquals("42000", e.getSQLState());
-    }
-
-  }
-
-  /**
-   * verify 'explain ...' resultset
-   * @throws SQLException
-   */
-  @Test
-  public void testExplainStmt() throws SQLException {
-    Statement stmt = con.createStatement();
-
-    ResultSet res = stmt.executeQuery(
-        "explain select c1, c2, c3, c4, c5 as a, c6, c7, c8, c9, c10, c11, c12, " +
-            "c1*2, sentences(null, null, null) as b from " + dataTypeTableName + " limit 1");
-
-    ResultSetMetaData md = res.getMetaData();
-    // only one result column
-    assertEquals(md.getColumnCount(), 1);
-    // verify the column name
-    assertEquals(md.getColumnLabel(1), EXPL_COLUMN_NAME);
-    //verify that there is data in the resultset
-    assertTrue("Nothing returned explain", res.next());
-  }
-
-  @Test
-  public void testPrepareStatement() {
-
-    String sql = "from (select count(1) from "
-        + tableName
-        + " where   'not?param?not?param' <> 'not_param??not_param' and ?=? "
-        + " and 1=? and 2=? and 3.0=? and 4.0=? and 'test\\'string\"'=? and 5=? and ?=? "
-        + " and date '2012-01-01' = date ?"
-        + " ) t  select '2011-03-25' ddate,'China',true bv, 10 num limit 10";
-
-    ///////////////////////////////////////////////
-    //////////////////// correct testcase
-    //////////////////////////////////////////////
-    try {
-      PreparedStatement ps = con.prepareStatement(sql);
-
-      ps.setBoolean(1, true);
-      ps.setBoolean(2, true);
-
-      ps.setShort(3, Short.valueOf("1"));
-      ps.setInt(4, 2);
-      ps.setFloat(5, 3f);
-      ps.setDouble(6, Double.valueOf(4));
-      ps.setString(7, "test'string\"");
-      ps.setLong(8, 5L);
-      ps.setByte(9, (byte) 1);
-      ps.setByte(10, (byte) 1);
-      ps.setString(11, "2012-01-01");
-
-      ps.setMaxRows(2);
-
-      assertTrue(true);
-
-      ResultSet res = ps.executeQuery();
-      assertNotNull(res);
-
-      while (res.next()) {
-        assertEquals("2011-03-25", res.getString("ddate"));
-        assertEquals("10", res.getString("num"));
-        assertEquals((byte) 10, res.getByte("num"));
-        assertEquals("2011-03-25", res.getDate("ddate").toString());
-        assertEquals(Double.valueOf(10).doubleValue(), res.getDouble("num"), 0.1);
-        assertEquals(10, res.getInt("num"));
-        assertEquals(Short.valueOf("10").shortValue(), res.getShort("num"));
-        assertEquals(10L, res.getLong("num"));
-        assertEquals(true, res.getBoolean("bv"));
-        Object o = res.getObject("ddate");
-        assertNotNull(o);
-        o = res.getObject("num");
-        assertNotNull(o);
-      }
-      res.close();
-      assertTrue(true);
-
-      ps.close();
-      assertTrue(true);
-
-    } catch (Exception e) {
-      e.printStackTrace();
-      fail(e.toString());
-    }
-
-    ///////////////////////////////////////////////
-    //////////////////// other failure testcases
-    //////////////////////////////////////////////
-    // set nothing for prepared sql
-    Exception expectedException = null;
-    try {
-      PreparedStatement ps = con.prepareStatement(sql);
-      ps.executeQuery();
-    } catch (Exception e) {
-      expectedException = e;
-    }
-    assertNotNull(
-        "Execute the un-setted sql statement should throw exception",
-        expectedException);
-
-    // set some of parameters for prepared sql, not all of them.
-    expectedException = null;
-    try {
-      PreparedStatement ps = con.prepareStatement(sql);
-      ps.setBoolean(1, true);
-      ps.setBoolean(2, true);
-      ps.executeQuery();
-    } catch (Exception e) {
-      expectedException = e;
-    }
-    assertNotNull(
-        "Execute the invalid setted sql statement should throw exception",
-        expectedException);
-
-    // set the wrong type parameters for prepared sql.
-    expectedException = null;
-    try {
-      PreparedStatement ps = con.prepareStatement(sql);
-
-      // wrong type here
-      ps.setString(1, "wrong");
-
-      assertTrue(true);
-      ResultSet res = ps.executeQuery();
-      if (!res.next()) {
-        throw new Exception("there must be a empty result set");
-      }
-    } catch (Exception e) {
-      expectedException = e;
-    }
-    assertNotNull(
-        "Execute the invalid setted sql statement should throw exception",
-        expectedException);
-  }
-
-  /**
-   * Execute non-select statements using execute() and executeUpdated() APIs
-   * of PreparedStatement interface
-   * @throws Exception
-   */
-  @Test
-  public void testExecutePreparedStatement() throws Exception {
-    String key = "testKey";
-    String val1 = "val1";
-    String val2 = "val2";
-    PreparedStatement ps = con.prepareStatement("set " + key + " = ?");
-
-    // execute() of Prepared statement
-    ps.setString(1, val1);
-    ps.execute();
-    verifyConfValue(key, val1);
-
-    // executeUpdate() of Prepared statement
-    ps.clearParameters();
-    ps.setString(1, val2);
-    ps.executeUpdate();
-    verifyConfValue(key, val2);
-  }
-
-  /**
-   * Execute "set x" and extract value from key=val format result
-   * Verify the extracted value
-   * @param stmt
-   * @return
-   * @throws Exception
-   */
-  private void verifyConfValue(String key, String expectedVal) throws Exception {
-    Statement stmt = con.createStatement();
-    ResultSet res = stmt.executeQuery("set " + key);
-    assertTrue(res.next());
-    String resultValues[] = res.getString(1).split("="); // "key = 'val'"
-    assertEquals("Result not in key = val format", 2, resultValues.length);
-    String result = resultValues[1].substring(1, resultValues[1].length() -1); // remove '
-    assertEquals("Conf value should be set by execute()", expectedVal, result);
-  }
-
-  @Test
-  public final void testSelectAll() throws Exception {
-    doTestSelectAll(tableName, -1, -1); // tests not setting maxRows (return all)
-    doTestSelectAll(tableName, 0, -1); // tests setting maxRows to 0 (return all)
-  }
-
-  @Test
-  public final void testSelectAllPartioned() throws Exception {
-    doTestSelectAll(partitionedTableName, -1, -1); // tests not setting maxRows
-    // (return all)
-    doTestSelectAll(partitionedTableName, 0, -1); // tests setting maxRows to 0
-    // (return all)
-  }
-
-  @Test
-  public final void testSelectAllMaxRows() throws Exception {
-    doTestSelectAll(tableName, 100, -1);
-  }
-
-  @Test
-  public final void testSelectAllFetchSize() throws Exception {
-    doTestSelectAll(tableName, 100, 20);
-  }
-
-  @Test
-  public void testNullType() throws Exception {
-    Statement stmt = con.createStatement();
-    try {
-      ResultSet res = stmt.executeQuery("select null from " + dataTypeTableName);
-      assertTrue(res.next());
-      assertNull(res.getObject(1));
-    } finally {
-      stmt.close();
-    }
-  }
-
-  // executeQuery should always throw a SQLException,
-  // when it executes a non-ResultSet query (like create)
-  @Test
-  public void testExecuteQueryException() throws Exception {
-    Statement stmt = con.createStatement();
-    try {
-      stmt.executeQuery("create table test_t2 (under_col int, value string)");
-      fail("Expecting SQLException");
-    }
-    catch (SQLException e) {
-      System.out.println("Caught an expected SQLException: " + e.getMessage());
-    }
-    finally {
-      stmt.close();
-    }
-  }
-
-  private void checkResultSetExpected(Statement stmt, List<String> setupQueries, String testQuery,
-      boolean isExpectedResultSet) throws Exception {
-    boolean hasResultSet;
-    // execute the setup queries
-    for(String setupQuery: setupQueries) {
-      try {
-        stmt.execute(setupQuery);
-      } catch (Exception e) {
-        failWithExceptionMsg(e);
-      }
-    }
-    // execute the test query
-    try {
-      hasResultSet = stmt.execute(testQuery);
-      assertEquals(hasResultSet, isExpectedResultSet);
-    }
-    catch(Exception e) {
-      failWithExceptionMsg(e);
-    }
-  }
-
-  private void failWithExceptionMsg(Exception e) {
-    e.printStackTrace();
-    fail(e.toString());
-  }
-
-  @Test
-  public void testNullResultSet() throws Exception {
-    List<String> setupQueries = new ArrayList<String>();
-    String testQuery;
-    Statement stmt = con.createStatement();
-
-    // -select- should return a ResultSet
-    try {
-      stmt.executeQuery("select * from " + tableName);
-      System.out.println("select: success");
-    } catch(SQLException e) {
-      failWithExceptionMsg(e);
-    }
-
-    // -create- should not return a ResultSet
-    setupQueries.add("drop table test_t1");
-    testQuery = "create table test_t1 (under_col int, value string)";
-    checkResultSetExpected(stmt, setupQueries, testQuery, false);
-    setupQueries.clear();
-
-    // -create table as select- should not return a ResultSet
-    setupQueries.add("drop table test_t1");
-    testQuery = "create table test_t1 as select * from " + tableName;
-    checkResultSetExpected(stmt, setupQueries, testQuery, false);
-    setupQueries.clear();
-
-    // -insert table as select- should not return a ResultSet
-    setupQueries.add("drop table test_t1");
-    setupQueries.add("create table test_t1 (under_col int, value string)");
-    testQuery = "insert into table test_t1 select under_col, value from "  + tableName;
-    checkResultSetExpected(stmt, setupQueries, testQuery, false);
-    setupQueries.clear();
-
-    stmt.close();
-  }
-
-  @Test
-  public void testCloseResultSet() throws Exception {
-    Statement stmt = con.createStatement();
-
-    // execute query, ignore exception if any
-    ResultSet res = stmt.executeQuery("select * from " + tableName);
-    // close ResultSet, ignore exception if any
-    res.close();
-    // A statement should be open even after ResultSet#close
-    assertFalse(stmt.isClosed());
-    // A Statement#cancel after ResultSet#close should be a no-op
-    try {
-      stmt.cancel();
-    } catch(SQLException e) {
-      failWithExceptionMsg(e);
-    }
-    stmt.close();
-
-    stmt = con.createStatement();
-    // execute query, ignore exception if any
-    res = stmt.executeQuery("select * from " + tableName);
-    // close ResultSet, ignore exception if any
-    res.close();
-    // A Statement#execute after ResultSet#close should be fine too
-    try {
-      stmt.executeQuery("select * from " + tableName);
-    } catch(SQLException e) {
-      failWithExceptionMsg(e);
-    }
-    // A Statement#close after ResultSet#close should close the statement
-    stmt.close();
-    assertTrue(stmt.isClosed());
-  }
-
-  @Test
-  public void testDataTypes() throws Exception {
-    Statement stmt = con.createStatement();
-
-    ResultSet res = stmt.executeQuery(
-        "select * from " + dataTypeTableName + " order by c1");
-    ResultSetMetaData meta = res.getMetaData();
-
-    // row 1
-    assertTrue(res.next());
-    // skip the last (partitioning) column since it is always non-null
-    for (int i = 1; i < meta.getColumnCount(); i++) {
-      assertNull(res.getObject(i));
-    }
-    // getXXX returns 0 for numeric types, false for boolean and null for other
-    assertEquals(0, res.getInt(1));
-    assertEquals(false, res.getBoolean(2));
-    assertEquals(0d, res.getDouble(3), floatCompareDelta);
-    assertEquals(null, res.getString(4));
-    assertEquals(null, res.getString(5));
-    assertEquals(null, res.getString(6));
-    assertEquals(null, res.getString(7));
-    assertEquals(null, res.getString(8));
-    assertEquals(0, res.getByte(9));
-    assertEquals(0, res.getShort(10));
-    assertEquals(0f, res.getFloat(11), floatCompareDelta);
-    assertEquals(0L, res.getLong(12));
-    assertEquals(null, res.getString(13));
-    assertEquals(null, res.getString(14));
-    assertEquals(null, res.getString(15));
-    assertEquals(null, res.getString(16));
-    assertEquals(null, res.getString(17));
-    assertEquals(null, res.getString(18));
-    assertEquals(null, res.getString(19));
-    assertEquals(null, res.getString(20));
-    assertEquals(null, res.getDate(20));
-    assertEquals(null, res.getString(21));
-
-    // row 2
-    assertTrue(res.next());
-    assertEquals(-1, res.getInt(1));
-    assertEquals(false, res.getBoolean(2));
-    assertEquals(-1.1d, res.getDouble(3), floatCompareDelta);
-    assertEquals("", res.getString(4));
-    assertEquals("[]", res.getString(5));
-    assertEquals("{}", res.getString(6));
-    assertEquals("{}", res.getString(7));
-    assertEquals("{\"r\":null,\"s\":null,\"t\":null}", res.getString(8));
-    assertEquals(-1, res.getByte(9));
-    assertEquals(-1, res.getShort(10));
-    assertEquals(-1.0f, res.getFloat(11), floatCompareDelta);
-    assertEquals(-1, res.getLong(12));
-    assertEquals("[]", res.getString(13));
-    assertEquals("{}", res.getString(14));
-    assertEquals("{\"r\":null,\"s\":null}", res.getString(15));
-    assertEquals("[]", res.getString(16));
-    assertEquals(null, res.getString(17));
-    assertEquals(null, res.getTimestamp(17));
-    assertEquals(null, res.getBigDecimal(18));
-    assertEquals(null, res.getString(19));
-    assertEquals(null, res.getString(20));
-    assertEquals(null, res.getDate(20));
-    assertEquals(null, res.getString(21));
-
-    // row 3
-    assertTrue(res.next());
-    assertEquals(1, res.getInt(1));
-    assertEquals(true, res.getBoolean(2));
-    assertEquals(1.1d, res.getDouble(3), floatCompareDelta);
-    assertEquals("1", res.getString(4));
-    assertEquals("[1,2]", res.getString(5));
-    assertEquals("{1:\"x\",2:\"y\"}", res.getString(6));
-    assertEquals("{\"k\":\"v\"}", res.getString(7));
-    assertEquals("{\"r\":\"a\",\"s\":9,\"t\":2.2}", res.getString(8));
-    assertEquals(1, res.getByte(9));
-    assertEquals(1, res.getShort(10));
-    assertEquals(1.0f, res.getFloat(11), floatCompareDelta);
-    assertEquals(1, res.getLong(12));
-    assertEquals("[[\"a\",\"b\"],[\"c\",\"d\"]]", res.getString(13));
-    assertEquals("{1:{11:12,13:14},2:{21:22}}", res.getString(14));
-    assertEquals("{\"r\":1,\"s\":{\"a\":2,\"b\":\"x\"}}", res.getString(15));
-    assertEquals("[{\"m\":{},\"n\":1},{\"m\":{\"a\":\"b\",\"c\":\"d\"},\"n\":2}]", res.getString(16));
-    assertEquals("2012-04-22 09:00:00.123456789", res.getString(17));
-    assertEquals("2012-04-22 09:00:00.123456789", res.getTimestamp(17).toString());
-    assertEquals("123456789.0123456", res.getBigDecimal(18).toString());
-    assertEquals("abcd", res.getString(19));
-    assertEquals("2013-01-01", res.getString(20));
-    assertEquals("2013-01-01", res.getDate(20).toString());
-    assertEquals("abc123", res.getString(21));
-
-    // test getBoolean rules on non-boolean columns
-    assertEquals(true, res.getBoolean(1));
-    assertEquals(true, res.getBoolean(4));
-
-    // test case sensitivity
-    assertFalse(meta.isCaseSensitive(1));
-    assertFalse(meta.isCaseSensitive(2));
-    assertFalse(meta.isCaseSensitive(3));
-    assertTrue(meta.isCaseSensitive(4));
-
-    // no more rows
-    assertFalse(res.next());
-  }
-
-  private void doTestSelectAll(String tableName, int maxRows, int fetchSize) throws Exception {
-    boolean isPartitionTable = tableName.equals(partitionedTableName);
-
-    Statement stmt = con.createStatement();
-    if (maxRows >= 0) {
-      stmt.setMaxRows(maxRows);
-    }
-    if (fetchSize > 0) {
-      stmt.setFetchSize(fetchSize);
-      assertEquals(fetchSize, stmt.getFetchSize());
-    }
-
-    // JDBC says that 0 means return all, which is the default
-    int expectedMaxRows = maxRows < 1 ? 0 : maxRows;
-
-    assertNotNull("Statement is null", stmt);
-    assertEquals("Statement max rows not as expected", expectedMaxRows, stmt
-        .getMaxRows());
-    assertFalse("Statement should not be closed", stmt.isClosed());
-
-    ResultSet res;
-
-    // run some queries
-    res = stmt.executeQuery("select * from " + tableName);
-    assertNotNull("ResultSet is null", res);
-    assertTrue("getResultSet() not returning expected ResultSet", res == stmt
-        .getResultSet());
-    assertEquals("get update count not as expected", 0, stmt.getUpdateCount());
-    int i = 0;
-
-    ResultSetMetaData meta = res.getMetaData();
-    int expectedColCount = isPartitionTable ? 3 : 2;
-    assertEquals(
-        "Unexpected column count", expectedColCount, meta.getColumnCount());
-
-    boolean moreRow = res.next();
-    while (moreRow) {
-      try {
-        i++;
-        assertEquals(res.getInt(1), res.getInt("under_col"));
-        assertEquals(res.getString(1), res.getString("under_col"));
-        assertEquals(res.getString(2), res.getString("value"));
-        if (isPartitionTable) {
-          assertEquals(res.getString(3), partitionedColumnValue);
-          assertEquals(res.getString(3), res.getString(partitionedColumnName));
-        }
-        assertFalse("Last result value was not null", res.wasNull());
-        assertNull("No warnings should be found on ResultSet", res
-            .getWarnings());
-        res.clearWarnings(); // verifying that method is supported
-
-        // System.out.println(res.getString(1) + " " + res.getString(2));
-        assertEquals(
-            "getInt and getString don't align for the same result value",
-            String.valueOf(res.getInt(1)), res.getString(1));
-        assertEquals("Unexpected result found", "val_" + res.getString(1), res
-            .getString(2));
-        moreRow = res.next();
-      } catch (SQLException e) {
-        System.out.println(e.toString());
-        e.printStackTrace();
-        throw new Exception(e.toString());
-      }
-    }
-
-    // supposed to get 500 rows if maxRows isn't set
-    int expectedRowCount = maxRows > 0 ? maxRows : 500;
-    assertEquals("Incorrect number of rows returned", expectedRowCount, i);
-
-    // should have no more rows
-    assertEquals(false, moreRow);
-
-    assertNull("No warnings should be found on statement", stmt.getWarnings());
-    stmt.clearWarnings(); // verifying that method is supported
-
-    assertNull("No warnings should be found on connection", con.getWarnings());
-    con.clearWarnings(); // verifying that method is supported
-
-    stmt.close();
-    assertTrue("Statement should be closed", stmt.isClosed());
-  }
-
-  @Test
-  public void testErrorMessages() throws SQLException {
-    String invalidSyntaxSQLState = "42000";
-
-    // These tests inherently cause exceptions to be written to the test output
-    // logs. This is undesirable, since you it might appear to someone looking
-    // at the test output logs as if something is failing when it isn't. Not
-    // sure
-    // how to get around that.
-    doTestErrorCase("SELECTT * FROM " + tableName,
-        "cannot recognize input near 'SELECTT' '*' 'FROM'",
-        invalidSyntaxSQLState, 40000);
-    doTestErrorCase("SELECT * FROM some_table_that_does_not_exist",
-        "Table not found", "42S02", 10001);
-    doTestErrorCase("drop table some_table_that_does_not_exist",
-        "Table not found", "42S02", 10001);
-    doTestErrorCase("SELECT invalid_column FROM " + tableName,
-        "Invalid table alias or column reference", invalidSyntaxSQLState, 10004);
-    doTestErrorCase("SELECT invalid_function(under_col) FROM " + tableName,
-        "Invalid function", invalidSyntaxSQLState, 10011);
-
-    // TODO: execute errors like this currently don't return good error
-    // codes and messages. This should be fixed.
-    doTestErrorCase(
-        "create table " + tableName + " (key int, value string)",
-        "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask",
-        "08S01", 1);
-  }
-
-  private void doTestErrorCase(String sql, String expectedMessage,
-      String expectedSQLState, int expectedErrorCode) throws SQLException {
-    Statement stmt = con.createStatement();
-    boolean exceptionFound = false;
-    try {
-      stmt.execute(sql);
-    } catch (SQLException e) {
-      assertTrue("Adequate error messaging not found for '" + sql + "': "
-          + e.getMessage(), e.getMessage().contains(expectedMessage));
-      assertEquals("Expected SQLState not found for '" + sql + "'",
-          expectedSQLState, e.getSQLState());
-      assertEquals("Expected error code not found for '" + sql + "'",
-          expectedErrorCode, e.getErrorCode());
-      exceptionFound = true;
-    }
-
-    assertNotNull("Exception should have been thrown for query: " + sql,
-        exceptionFound);
-  }
-
-  @Test
-  public void testShowTables() throws SQLException {
-    Statement stmt = con.createStatement();
-    assertNotNull("Statement is null", stmt);
-
-    ResultSet res = stmt.executeQuery("show tables");
-
-    boolean testTableExists = false;
-    while (res.next()) {
-      assertNotNull("table name is null in result set", res.getString(1));
-      if (tableName.equalsIgnoreCase(res.getString(1))) {
-        testTableExists = true;
-      }
-    }
-
-    assertTrue("table name " + tableName
-        + " not found in SHOW TABLES result set", testTableExists);
-  }
-
-  @Test
-  public void testMetaDataGetTables() throws SQLException {
-    getTablesTest(ClassicTableTypes.TABLE.toString(), ClassicTableTypes.VIEW.toString());
-  }
-
-  @Test
-  public  void testMetaDataGetTablesHive() throws SQLException {
-    Statement stmt = con.createStatement();
-    stmt.execute("set " + HiveConf.ConfVars.HIVE_SERVER2_TABLE_TYPE_MAPPING.varname +
-        " = " + TableTypeMappings.HIVE.toString());
-    getTablesTest(TableType.MANAGED_TABLE.toString(), TableType.VIRTUAL_VIEW.toString());
-  }
-
-  @Test
-  public  void testMetaDataGetTablesClassic() throws SQLException {
-    Statement stmt = con.createStatement();
-    stmt.execute("set " + HiveConf.ConfVars.HIVE_SERVER2_TABLE_TYPE_MAPPING.varname +
-        " = " + TableTypeMappings.CLASSIC.toString());
-    stmt.close();
-    getTablesTest(ClassicTableTypes.TABLE.toString(), ClassicTableTypes.VIEW.toString());
-  }
-
-  /**
-   * Test the type returned for pre-created table type table and view type
-   * table
-   * @param tableTypeName expected table type
-   * @param viewTypeName expected view type
-   * @throws SQLException
-   */
-  private void getTablesTest(String tableTypeName, String viewTypeName) throws SQLException {
-    Map<String, Object[]> tests = new HashMap<String, Object[]>();
-    tests.put("test%jdbc%", new Object[]{"testhivejdbcdriver_table"
-        , "testhivejdbcdriverpartitionedtable"
-        , "testhivejdbcdriverview"});
-    tests.put("%jdbcdriver\\_table", new Object[]{"testhivejdbcdriver_table"});
-    tests.put("testhivejdbcdriver\\_table", new Object[]{"testhivejdbcdriver_table"});
-    tests.put("test_ivejdbcdri_er\\_table", new Object[]{"testhivejdbcdriver_table"});
-    tests.put("test_ivejdbcdri_er_table", new Object[]{"testhivejdbcdriver_table"});
-    tests.put("test_ivejdbcdri_er%table", new Object[]{
-        "testhivejdbcdriver_table", "testhivejdbcdriverpartitionedtable" });
-    tests.put("%jdbc%", new Object[]{ "testhivejdbcdriver_table"
-        , "testhivejdbcdriverpartitionedtable"
-        , "testhivejdbcdriverview"});
-    tests.put("", new Object[]{});
-
-    for (String checkPattern: tests.keySet()) {
-      ResultSet rs = (ResultSet)con.getMetaData().getTables("default", null, checkPattern, null);
-      ResultSetMetaData resMeta = rs.getMetaData();
-      assertEquals(5, resMeta.getColumnCount());
-      assertEquals("TABLE_CAT", resMeta.getColumnName(1));
-      assertEquals("TABLE_SCHEM", resMeta.getColumnName(2));
-      assertEquals("TABLE_NAME", resMeta.getColumnName(3));
-      assertEquals("TABLE_TYPE", resMeta.getColumnName(4));
-      assertEquals("REMARKS", resMeta.getColumnName(5));
-
-      int cnt = 0;
-      while (rs.next()) {
-        String resultTableName = rs.getString("TABLE_NAME");
-        assertEquals("Get by index different from get by name.", rs.getString(3), resultTableName);
-        assertEquals("Excpected a different table.", tests.get(checkPattern)[cnt], resultTableName);
-        String resultTableComment = rs.getString("REMARKS");
-        assertTrue("Missing comment on the table.", resultTableComment.length()>0);
-        String tableType = rs.getString("TABLE_TYPE");
-        if (resultTableName.endsWith("view")) {
-          assertEquals("Expected a tabletype view but got something else.", viewTypeName, tableType);
-        } else {
-          assertEquals("Expected a tabletype table but got something else.", tableTypeName, tableType);
-        }
-        cnt++;
-      }
-      rs.close();
-      assertEquals("Received an incorrect number of tables.", tests.get(checkPattern).length, cnt);
-    }
-
-    // only ask for the views.
-    ResultSet rs = (ResultSet)con.getMetaData().getTables("default", null, null
-        , new String[]{viewTypeName});
-    int cnt=0;
-    while (rs.next()) {
-      cnt++;
-    }
-    rs.close();
-    assertEquals("Incorrect number of views found.", 1, cnt);
-  }
-
-  @Test
-  public void testMetaDataGetCatalogs() throws SQLException {
-    ResultSet rs = (ResultSet)con.getMetaData().getCatalogs();
-    ResultSetMetaData resMeta = rs.getMetaData();
-    assertEquals(1, resMeta.getColumnCount());
-    assertEquals("TABLE_CAT", resMeta.getColumnName(1));
-
-    assertFalse(rs.next());
-  }
-
-  @Test
-  public void testMetaDataGetSchemas() throws SQLException {
-    ResultSet rs = (ResultSet)con.getMetaData().getSchemas();
-    ResultSetMetaData resMeta = rs.getMetaData();
-    assertEquals(2, resMeta.getColumnCount());
-    assertEquals("TABLE_SCHEM", resMeta.getColumnName(1));
-    assertEquals("TABLE_CATALOG", resMeta.getColumnName(2));
-
-    assertTrue(rs.next());
-    assertEquals("default", rs.getString(1));
-
-    assertFalse(rs.next());
-    rs.close();
-  }
-
-  // test default table types returned in
-  // Connection.getMetaData().getTableTypes()
-  @Test
-  public void testMetaDataGetTableTypes() throws SQLException {
-    metaDataGetTableTypeTest(new ClassicTableTypeMapping().getTableTypeNames());
-  }
-
-  // test default table types returned in
-  // Connection.getMetaData().getTableTypes() when type config is set to "HIVE"
-  @Test
-  public void testMetaDataGetHiveTableTypes() throws SQLException {
-    Statement stmt = con.createStatement();
-    stmt.execute("set " + HiveConf.ConfVars.HIVE_SERVER2_TABLE_TYPE_MAPPING.varname +
-        " = " + TableTypeMappings.HIVE.toString());
-    stmt.close();
-    metaDataGetTableTypeTest(new HiveTableTypeMapping().getTableTypeNames());
-  }
-
-  // test default table types returned in
-  // Connection.getMetaData().getTableTypes() when type config is set to "CLASSIC"
-  @Test
-  public void testMetaDataGetClassicTableTypes() throws SQLException {
-    Statement stmt = con.createStatement();
-    stmt.execute("set " + HiveConf.ConfVars.HIVE_SERVER2_TABLE_TYPE_MAPPING.varname +
-        " = " + TableTypeMappings.CLASSIC.toString());
-    stmt.close();
-    metaDataGetTableTypeTest(new ClassicTableTypeMapping().getTableTypeNames());
-  }
-
-  /**
-   * Test if Connection.getMetaData().getTableTypes() returns expected
-   *  tabletypes
-   * @param tabletypes expected table types
-   * @throws SQLException
-   */
-  private void metaDataGetTableTypeTest(Set<String> tabletypes)
-      throws SQLException {
-    ResultSet rs = (ResultSet)con.getMetaData().getTableTypes();
-
-    int cnt = 0;
-    while (rs.next()) {
-      String tabletype = rs.getString("TABLE_TYPE");
-      assertEquals("Get by index different from get by name", rs.getString(1), tabletype);
-      tabletypes.remove(tabletype);
-      cnt++;
-    }
-    rs.close();
-    assertEquals("Incorrect tabletype count.", 0, tabletypes.size());
-    assertTrue("Found less tabletypes then we test for.", cnt >= tabletypes.size());
-  }
-
-  @Test
-  public void testMetaDataGetColumns() throws SQLException {
-    Map<String[], Integer> tests = new HashMap<String[], Integer>();
-    tests.put(new String[]{"testhivejdbcdriver\\_table", null}, 2);
-    tests.put(new String[]{"testhivejdbc%", null}, 7);
-    tests.put(new String[]{"testhiveJDBC%", null}, 7);
-    tests.put(new String[]{"%jdbcdriver\\_table", null}, 2);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_col"}, 1);
-    //    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_COL"}, 1);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "under\\_co_"}, 1);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "under_col"}, 1);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "und%"}, 1);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "%"}, 2);
-    tests.put(new String[]{"%jdbcdriver\\_table%", "_%"}, 2);
-
-    for (String[] checkPattern: tests.keySet()) {
-      ResultSet rs = con.getMetaData().getColumns(null, null, checkPattern[0],
-          checkPattern[1]);
-
-      // validate the metadata for the getColumns result set
-      ResultSetMetaData rsmd = rs.getMetaData();
-      assertEquals("TABLE_CAT", rsmd.getColumnName(1));
-
-      int cnt = 0;
-      while (rs.next()) {
-        String columnname = rs.getString("COLUMN_NAME");
-        int ordinalPos = rs.getInt("ORDINAL_POSITION");
-        switch(cnt) {
-        case 0:
-          assertEquals("Wrong column name found", "under_col", columnname);
-          assertEquals("Wrong ordinal position found", ordinalPos, 1);
-          break;
-        case 1:
-          assertEquals("Wrong column name found", "value", columnname);
-          assertEquals("Wrong ordinal position found", ordinalPos, 2);
-          break;
-        default:
-          break;
-        }
-        cnt++;
-      }
-      rs.close();
-      assertEquals("Found less columns then we test for.", tests.get(checkPattern).intValue(), cnt);
-    }
-  }
-
-  /**
-   * Validate the Metadata for the result set of a metadata getColumns call.
-   */
-  @Test
-  public void testMetaDataGetColumnsMetaData() throws SQLException {
-    ResultSet rs = (ResultSet)con.getMetaData().getColumns(null, null
-        , "testhivejdbcdriver\\_table", null);
-
-    ResultSetMetaData rsmd = rs.getMetaData();
-
-    assertEquals("TABLE_CAT", rsmd.getColumnName(1));
-    assertEquals(Types.VARCHAR, rsmd.getColumnType(1));
-    assertEquals(Integer.MAX_VALUE, rsmd.getColumnDisplaySize(1));
-
-    assertEquals("ORDINAL_POSITION", rsmd.getColumnName(17));
-    assertEquals(Types.INTEGER, rsmd.getColumnType(17));
-    assertEquals(11, rsmd.getColumnDisplaySize(17));
-  }
-
-  /*
-  public void testConversionsBaseResultSet() throws SQLException {
-    ResultSet rs = new HiveMetaDataResultSet(Arrays.asList("key")
-            , Arrays.asList("long")
-            , Arrays.asList(1234, "1234", "abc")) {
-      private int cnt=1;
-      public boolean next() throws SQLException {
-        if (cnt<data.size()) {
-          row = Arrays.asList(data.get(cnt));
-          cnt++;
-          return true;
-        } else {
-          return false;
-        }
-      }
-    };
-
-    while (rs.next()) {
-      String key = rs.getString("key");
-      if ("1234".equals(key)) {
-        assertEquals("Converting a string column into a long failed.", rs.getLong("key"), 1234L);
-        assertEquals("Converting a string column into a int failed.", rs.getInt("key"), 1234);
-      } else if ("abc".equals(key)) {
-        Object result = null;
-        Exception expectedException = null;
-        try {
-          result = rs.getLong("key");
-        } catch (SQLException e) {
-          expectedException = e;
-        }
-        assertTrue("Trying to convert 'abc' into a long should not work.", expectedException!=null);
-        try {
-          result = rs.getInt("key");
-        } catch (SQLException e) {
-          expectedException = e;
-        }
-        assertTrue("Trying to convert 'abc' into a int should not work.", expectedException!=null);
-      }
-    }
-  }
-   */
-
-  @Test
-  public void testDescribeTable() throws SQLException {
-    Statement stmt = con.createStatement();
-    assertNotNull("Statement is null", stmt);
-
-    ResultSet res = stmt.executeQuery("describe " + tableName);
-
-    res.next();
-    assertEquals("Column name 'under_col' not found", "under_col", res.getString(1).trim());
-    assertEquals("Column type 'under_col' for column under_col not found", "int", res
-        .getString(2).trim());
-    res.next();
-    assertEquals("Column name 'value' not found", "value", res.getString(1).trim());
-    assertEquals("Column type 'string' for column key not found", "string", res
-        .getString(2).trim());
-
-    assertFalse("More results found than expected", res.next());
-  }
-
-  @Test
-  public void testDatabaseMetaData() throws SQLException {
-    DatabaseMetaData meta = con.getMetaData();
-
-    assertEquals("Hive", meta.getDatabaseProductName());
-    assertEquals(HiveVersionInfo.getVersion(), meta.getDatabaseProductVersion());
-    assertEquals(System.getProperty("hive.version"), meta.getDatabaseProductVersion());
-    assertTrue("verifying hive version pattern. got " + meta.getDatabaseProductVersion(),
-        Pattern.matches("\\d+\\.\\d+\\.\\d+.*", meta.getDatabaseProductVersion()) );
-
-    assertEquals(DatabaseMetaData.sqlStateSQL99, meta.getSQLStateType());
-    assertFalse(meta.supportsCatalogsInTableDefinitions());
-    assertFalse(meta.supportsSchemasInTableDefinitions());
-    assertFalse(meta.supportsSchemasInDataManipulation());
-    assertFalse(meta.supportsMultipleResultSets());
-    assertFalse(meta.supportsStoredProcedures());
-    assertTrue(meta.supportsAlterTableWithAddColumn());
-  }
-
-  @Test
-  public void testResultSetMetaData() throws SQLException {
-    Statement stmt = con.createStatement();
-
-    ResultSet res = stmt.executeQuery(
-        "select c1, c2, c3, c4, c5 as a, c6, c7, c8, c9, c10, c11, c12, " +
-            "c1*2, sentences(null, null, null) as b, c17, c18, c20, c21 from " + dataTypeTableName +
-        " limit 1");
-    ResultSetMetaData meta = res.getMetaData();
-
-    ResultSet colRS = con.getMetaData().getColumns(null, null,
-        dataTypeTableName.toLowerCase(), null);
-
-    assertEquals(18, meta.getColumnCount());
-
-    assertTrue(colRS.next());
-
-    assertEquals("c1", meta.getColumnName(1));
-    assertEquals(Types.INTEGER, meta.getColumnType(1));
-    assertEquals("int", meta.getColumnTypeName(1));
-    assertEquals(11, meta.getColumnDisplaySize(1));
-    assertEquals(10, meta.getPrecision(1));
-    assertEquals(0, meta.getScale(1));
-
-    assertEquals("c1", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.INTEGER, colRS.getInt("DATA_TYPE"));
-    assertEquals("int", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(1), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(1), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c2", meta.getColumnName(2));
-    assertEquals("boolean", meta.getColumnTypeName(2));
-    assertEquals(Types.BOOLEAN, meta.getColumnType(2));
-    assertEquals(1, meta.getColumnDisplaySize(2));
-    assertEquals(1, meta.getPrecision(2));
-    assertEquals(0, meta.getScale(2));
-
-    assertEquals("c2", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.BOOLEAN, colRS.getInt("DATA_TYPE"));
-    assertEquals("boolean", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getScale(2), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c3", meta.getColumnName(3));
-    assertEquals(Types.DOUBLE, meta.getColumnType(3));
-    assertEquals("double", meta.getColumnTypeName(3));
-    assertEquals(25, meta.getColumnDisplaySize(3));
-    assertEquals(15, meta.getPrecision(3));
-    assertEquals(15, meta.getScale(3));
-
-    assertEquals("c3", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.DOUBLE, colRS.getInt("DATA_TYPE"));
-    assertEquals("double", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(3), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(3), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c4", meta.getColumnName(4));
-    assertEquals(Types.VARCHAR, meta.getColumnType(4));
-    assertEquals("string", meta.getColumnTypeName(4));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(4));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(4));
-    assertEquals(0, meta.getScale(4));
-
-    assertEquals("c4", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
-    assertEquals("string", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(4), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(4), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("a", meta.getColumnName(5));
-    assertEquals(Types.VARCHAR, meta.getColumnType(5));
-    assertEquals("string", meta.getColumnTypeName(5));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(5));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(5));
-    assertEquals(0, meta.getScale(5));
-
-    assertEquals("c5", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
-    assertEquals("array<int>", colRS.getString("TYPE_NAME").toLowerCase());
-
-    assertTrue(colRS.next());
-
-    assertEquals("c6", meta.getColumnName(6));
-    assertEquals(Types.VARCHAR, meta.getColumnType(6));
-    assertEquals("string", meta.getColumnTypeName(6));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(6));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(6));
-    assertEquals(0, meta.getScale(6));
-
-    assertEquals("c6", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
-    assertEquals("map<int,string>", colRS.getString("TYPE_NAME").toLowerCase());
-
-    assertTrue(colRS.next());
-
-    assertEquals("c7", meta.getColumnName(7));
-    assertEquals(Types.VARCHAR, meta.getColumnType(7));
-    assertEquals("string", meta.getColumnTypeName(7));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(7));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(7));
-    assertEquals(0, meta.getScale(7));
-
-    assertEquals("c7", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
-    assertEquals("map<string,string>", colRS.getString("TYPE_NAME").toLowerCase());
-
-    assertTrue(colRS.next());
-
-    assertEquals("c8", meta.getColumnName(8));
-    assertEquals(Types.VARCHAR, meta.getColumnType(8));
-    assertEquals("string", meta.getColumnTypeName(8));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(8));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(8));
-    assertEquals(0, meta.getScale(8));
-
-    assertEquals("c8", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.VARCHAR, colRS.getInt("DATA_TYPE"));
-    assertEquals("struct<r:string,s:int,t:double>", colRS.getString("TYPE_NAME").toLowerCase());
-
-    assertTrue(colRS.next());
-
-    assertEquals("c9", meta.getColumnName(9));
-    assertEquals(Types.TINYINT, meta.getColumnType(9));
-    assertEquals("tinyint", meta.getColumnTypeName(9));
-    assertEquals(4, meta.getColumnDisplaySize(9));
-    assertEquals(3, meta.getPrecision(9));
-    assertEquals(0, meta.getScale(9));
-
-    assertEquals("c9", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.TINYINT, colRS.getInt("DATA_TYPE"));
-    assertEquals("tinyint", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(9), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(9), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c10", meta.getColumnName(10));
-    assertEquals(Types.SMALLINT, meta.getColumnType(10));
-    assertEquals("smallint", meta.getColumnTypeName(10));
-    assertEquals(6, meta.getColumnDisplaySize(10));
-    assertEquals(5, meta.getPrecision(10));
-    assertEquals(0, meta.getScale(10));
-
-    assertEquals("c10", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.SMALLINT, colRS.getInt("DATA_TYPE"));
-    assertEquals("smallint", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(10), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(10), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c11", meta.getColumnName(11));
-    assertEquals(Types.FLOAT, meta.getColumnType(11));
-    assertEquals("float", meta.getColumnTypeName(11));
-    assertEquals(24, meta.getColumnDisplaySize(11));
-    assertEquals(7, meta.getPrecision(11));
-    assertEquals(7, meta.getScale(11));
-
-    assertEquals("c11", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.FLOAT, colRS.getInt("DATA_TYPE"));
-    assertEquals("float", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(11), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(11), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertTrue(colRS.next());
-
-    assertEquals("c12", meta.getColumnName(12));
-    assertEquals(Types.BIGINT, meta.getColumnType(12));
-    assertEquals("bigint", meta.getColumnTypeName(12));
-    assertEquals(20, meta.getColumnDisplaySize(12));
-    assertEquals(19, meta.getPrecision(12));
-    assertEquals(0, meta.getScale(12));
-
-    assertEquals("c12", colRS.getString("COLUMN_NAME"));
-    assertEquals(Types.BIGINT, colRS.getInt("DATA_TYPE"));
-    assertEquals("bigint", colRS.getString("TYPE_NAME").toLowerCase());
-    assertEquals(meta.getPrecision(12), colRS.getInt("COLUMN_SIZE"));
-    assertEquals(meta.getScale(12), colRS.getInt("DECIMAL_DIGITS"));
-
-    assertEquals("_c12", meta.getColumnName(13));
-    assertEquals(Types.INTEGER, meta.getColumnType(13));
-    assertEquals("int", meta.getColumnTypeName(13));
-    assertEquals(11, meta.getColumnDisplaySize(13));
-    assertEquals(10, meta.getPrecision(13));
-    assertEquals(0, meta.getScale(13));
-
-    assertEquals("b", meta.getColumnName(14));
-    assertEquals(Types.VARCHAR, meta.getColumnType(14));
-    assertEquals("string", meta.getColumnTypeName(14));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(14));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(14));
-    assertEquals(0, meta.getScale(14));
-
-    assertEquals("c17", meta.getColumnName(15));
-    assertEquals(Types.TIMESTAMP, meta.getColumnType(15));
-    assertEquals("timestamp", meta.getColumnTypeName(15));
-    assertEquals(29, meta.getColumnDisplaySize(15));
-    assertEquals(29, meta.getPrecision(15));
-    assertEquals(9, meta.getScale(15));
-
-    assertEquals("c18", meta.getColumnName(16));
-    assertEquals(Types.DECIMAL, meta.getColumnType(16));
-    assertEquals("decimal", meta.getColumnTypeName(16));
-    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(16));
-    assertEquals(Integer.MAX_VALUE, meta.getPrecision(16));
-    assertEquals(Integer.MAX_VALUE, meta.getScale(16));
-
-    assertEquals("c20", meta.getColumnName(17));
-    assertEquals(Types.DATE, meta.getColumnType(17));
-    assertEquals("date", meta.getColumnTypeName(17));
-    assertEquals(10, meta.getColumnDisplaySize(17));
-    assertEquals(10, meta.getPrecision(17));
-    assertEquals(0, meta.getScale(17));
-
-    assertEquals("c21", meta.getColumnName(18));
-    assertEquals(Types.VARCHAR, meta.getColumnType(18));
-    assertEquals("varchar", meta.getColumnTypeName(18));
-    // varchar columns should have correct display size/precision
-    assertEquals(20, meta.getColumnDisplaySize(18));
-    assertEquals(20, meta.getPrecision(18));
-    assertEquals(0, meta.getScale(18));
-
-    for (int i = 1; i <= meta.getColumnCount(); i++) {
-      assertFalse(meta.isAutoIncrement(i));
-      assertFalse(meta.isCurrency(i));
-      assertEquals(ResultSetMetaData.columnNullable, meta.isNullable(i));
-    }
-  }
-
-  // [url] [host] [port] [db]
-  private static final String[][] URL_PROPERTIES = new String[][] {
-    // binary mode
-    {"jdbc:hive2://", "", "", "default"},
-    {"jdbc:hive2://localhost:10001/default", "localhost", "10001", "default"},
-    {"jdbc:hive2://localhost/notdefault", "localhost", "10000", "notdefault"},
-    {"jdbc:hive2://foo:1243", "foo", "1243", "default"},
-
-    // http mode
-    {"jdbc:hive2://server:10002/db;user=foo;password=bar?" +
-        "hive.server2.transport.mode=http;" +
-        "hive.server2.thrift.http.path=hs2",
-        "server", "10002", "db"},
-  };
-
-  @Test
-  public void testDriverProperties() throws SQLException {
-    HiveDriver driver = new HiveDriver();
-    for (String[] testValues : URL_PROPERTIES) {
-      DriverPropertyInfo[] dpi = driver.getPropertyInfo(testValues[0], null);
-      assertEquals("unexpected DriverPropertyInfo array size", 3, dpi.length);
-      assertDpi(dpi[0], "HOST", testValues[1]);
-      assertDpi(dpi[1], "PORT", testValues[2]);
-      assertDpi(dpi[2], "DBNAME", testValues[3]);
-    }
-  }
-
-  private static final String[][] HTTP_URL_PROPERTIES = new String[][] {
-    {"jdbc:hive2://server:10002/db;" +
-        "user=foo;password=bar?" +
-        "hive.server2.transport.mode=http;" +
-        "hive.server2.thrift.http.path=hs2", "server", "10002", "db", "http", "hs2"},
-        {"jdbc:hive2://server:10000/testdb;" +
-            "user=foo;password=bar?" +
-            "hive.server2.transport.mode=binary;" +
-            "hive.server2.thrift.http.path=", "server", "10000", "testdb", "binary", ""},
-  };
-
-  @Test
-  public void testParseUrlHttpMode() throws SQLException {
-    HiveDriver driver = new HiveDriver();
-    for (String[] testValues : HTTP_URL_PROPERTIES) {
-      JdbcConnectionParams params = Utils.parseURL(testValues[0]);
-      assertEquals(params.getHost(), testValues[1]);
-      assertEquals(params.getPort(), Integer.parseInt(testValues[2]));
-      assertEquals(params.getDbName(), testValues[3]);
-      assertEquals(params.getHiveConfs().get("hive.server2.transport.mode"), testValues[4]);
-      assertEquals(params.getHiveConfs().get("hive.server2.thrift.http.path"), testValues[5]);
-    }
-  }
-
-  private static void assertDpi(DriverPropertyInfo dpi, String name,
-      String value) {
-    assertEquals("Invalid DriverPropertyInfo name", name, dpi.name);
-    assertEquals("Invalid DriverPropertyInfo value", value, dpi.value);
-    assertEquals("Invalid DriverPropertyInfo required", false, dpi.required);
-  }
-
-
-  /**
-   * validate schema generated by "set" command
-   * @throws SQLException
-   */
-  @Test
-  public void testSetCommand() throws SQLException {
-    // execute set command
-    String sql = "set -v";
-    Statement stmt = con.createStatement();
-    ResultSet res = stmt.executeQuery(sql);
-
-    // Validate resultset columns
-    ResultSetMetaData md = res.getMetaData() ;
-    assertEquals(1, md.getColumnCount());
-    assertEquals(SET_COLUMN_NAME, md.getColumnLabel(1));
-
-    //check if there is data in the resultset
-    assertTrue("Nothing returned by set -v", res.next());
-
-    res.close();
-    stmt.close();
-  }
-
-  /**
-   * Validate error on closed resultset
-   * @throws SQLException
-   */
-  @Test
-  public void testPostClose() throws SQLException {
-    Statement stmt = con.createStatement();
-    ResultSet res = stmt.executeQuery("select * from " + tableName);
-    assertNotNull("ResultSet is null", res);
-    res.close();
-    try { res.getInt(1); fail("Expected SQLException"); }
-    catch (SQLException e) { }
-    try { res.getMetaData(); fail("Expected SQLException"); }
-    catch (SQLException e) { }
-    try { res.setFetchSize(10); fail("Expected SQLException"); }
-    catch (SQLException e) { }
-  }
-
-  /*
-   * The JDBC spec says when you have duplicate column names,
-   * the first one should be returned.
-   */
-  @Test
-  public void testDuplicateColumnNameOrder() throws SQLException {
-    Statement stmt = con.createStatement();
-    ResultSet rs = stmt.executeQuery("SELECT 1 AS a, 2 AS a from " + tableName);
-    assertTrue(rs.next());
-    assertEquals(1, rs.getInt("a"));
-  }
-
-
-  /**
-   * Test bad args to getXXX()
-   * @throws SQLException
-   */
-  @Test
-  public void testOutOfBoundCols() throws SQLException {
-    Statement stmt = con.createStatement();
-
-    ResultSet res = stmt.executeQuery(
-        "select * from " + tableName);
-
-    // row 1
-    assertTrue(res.next());
-
-    try {
-      res.getInt(200);
-    } catch (SQLException e) {
-    }
-
-    try {
-      res.getInt("zzzz");
-    } catch (SQLException e) {
-    }
-
-  }
-
-  /**
-   * Verify selecting using builtin UDFs
-   * @throws SQLException
-   */
-  @Test
-  public void testBuiltInUDFCol() throws SQLException {
-    Statement stmt = con.createStatement();
-    ResultSet res = stmt.executeQuery("select c12, bin(c12) from " + dataTypeTableName
-        + " where c1=1");
-    ResultSetMetaData md = res.getMetaData();
-    assertEquals(md.getColumnCount(), 2); // only one result column
-    assertEquals(md.getColumnLabel(2), "_c1" ); // verify the system generated column name
-    assertTrue(res.next());
-    assertEquals(res.getLong(1), 1);
-    assertEquals(res.getString(2), "1");
-    res.close();
-  }
-
-  /**
-   * Verify selecting named expression columns
-   * @throws SQLException
-   */
-  @Test
-  public void testExprCol() throws SQLException {
-    Statement stmt = con.createStatement();
-    ResultSet res = stmt.executeQuery("select c1+1 as col1, length(c4) as len from " + dataTypeTableName
-        + " where c1=1");
-    ResultSetMetaData md = res.getMetaData();
-    assertEquals(md.getColumnCount(), 2); // only one result column
-    assertEquals(md.getColumnLabel(1), "col1" ); // verify the column name
-    assertEquals(md.getColumnLabel(2), "len" ); // verify the column name
-    assertTrue(res.next());
-    assertEquals(res.getInt(1), 2);
-    assertEquals(res.getInt(2), 1);
-    res.close();
-  }
-
-  /**
-   * test getProcedureColumns()
-   * @throws SQLException
-   */
-  @Test
-  public void testProcCols() throws SQLException {
-    DatabaseMetaData dbmd = con.getMetaData();
-    assertNotNull(dbmd);
-    // currently getProcedureColumns always returns an empty resultset for Hive
-    ResultSet res = dbmd.getProcedureColumns(null, null, null, null);
-    ResultSetMetaData md = res.getMetaData();
-    assertEquals(md.getColumnCount(), 20);
-    assertFalse(res.next());
-  }
-
-  /**
-   * test testProccedures()
-   * @throws SQLException
-   */
-  @Test
-  public void testProccedures() throws SQLException {
-    DatabaseMetaData dbmd = con.getMetaData();
-    assertNotNull(dbmd);
-    // currently testProccedures always returns an empty resultset for Hive
-    ResultSet res = dbmd.getProcedures(null, null, null);
-    ResultSetMetaData md = res.getMetaData();
-    assertEquals(md.getColumnCount(), 9);
-    assertFalse(res.next());
-  }
-
-  /**
-   * test getPrimaryKeys()
-   * @throws SQLException
-   */
-  @Test
-  public void testPrimaryKeys() throws SQLException {
-    DatabaseMetaData dbmd = con.getMetaData();
-    assertNotNull(dbmd);
-    // currently getPrimaryKeys always returns an empty resultset for Hive
-    ResultSet res = dbmd.getPrimaryKeys(null, null, null);
-    ResultSetMetaData md = res.getMetaData();
-    assertEquals(md.getColumnCount(), 6);
-    assertFalse(res.next());
-  }
-
-  /**
-   * test getImportedKeys()
-   * @throws SQLException
-   */
-  @Test
-  public void testImportedKeys() throws SQLException {
-    DatabaseMetaData dbmd = con.getMetaData();
-    assertNotNull(dbmd);
-    // currently getImportedKeys always returns an empty resultset for Hive
-    ResultSet res = dbmd.getImportedKeys(null, null, null);
-    ResultSetMetaData md = res.getMetaData();
-    assertEquals(md.getColumnCount(), 14);
-    assertFalse(res.next());
-  }
-
-  /**
-   * If the Driver implementation understands the URL, it will return a Connection object;
-   * otherwise it returns null
-   */
-  @Test
-  public void testInvalidURL() throws Exception {
-    HiveDriver driver = new HiveDriver();
-    Connection conn = driver.connect("jdbc:derby://localhost:10000/default", new Properties());
-    assertNull(conn);
-  }
-}
diff --git a/src/maven-rollforward.sh b/src/maven-rollforward.sh
index b183f7a..77e1457 100644
--- a/src/maven-rollforward.sh
+++ b/src/maven-rollforward.sh
@@ -7,7 +7,7 @@ move_source() {
     echo $source
   fi
   mkdir -p $(dirname $target)
-  mv $source $target
+  git mv $source $target
 }
 move_source serde/src/test/org/apache/hadoop/hive/serde2/TestSerdeWithFieldComments.java itests/hive-unit/src/test/java/org/apache/hadoop/hive/serde2/TestSerdeWithFieldComments.java
 move_source serde/src/test/org/apache/hadoop/hive/serde2/dynamic_type/TestDynamicSerDe.java itests/hive-unit/src/test/java/org/apache/hadoop/hive/serde2/dynamic_type/TestDynamicSerDe.java
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestEmbeddedHiveMetaStore.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestEmbeddedHiveMetaStore.java
deleted file mode 100644
index b6b0e6e..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestEmbeddedHiveMetaStore.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.util.StringUtils;
-
-public class TestEmbeddedHiveMetaStore extends TestHiveMetaStore {
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    hiveConf.setBoolean(
-        HiveConf.ConfVars.HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS.varname, true);
-    warehouse = new Warehouse(hiveConf);
-    try {
-      client = new HiveMetaStoreClient(hiveConf, null);
-    } catch (Throwable e) {
-      System.err.println("Unable to open the metastore");
-      System.err.println(StringUtils.stringifyException(e));
-      throw new Exception(e);
-    }
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    try {
-      super.tearDown();
-      client.close();
-    } catch (Throwable e) {
-      System.err.println("Unable to close metastore");
-      System.err.println(StringUtils.stringifyException(e));
-      throw new Exception(e);
-    }
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
deleted file mode 100644
index 663abd6..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
+++ /dev/null
@@ -1,2679 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import java.sql.Connection;
-import java.sql.PreparedStatement;
-import java.sql.SQLException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;
-import org.apache.hadoop.hive.metastore.api.ColumnStatistics;
-import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;
-import org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc;
-import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;
-import org.apache.hadoop.hive.metastore.api.ConfigValSecurityException;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.InvalidObjectException;
-import org.apache.hadoop.hive.metastore.api.InvalidOperationException;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
-import org.apache.hadoop.hive.metastore.api.Order;
-import org.apache.hadoop.hive.metastore.api.Partition;
-import org.apache.hadoop.hive.metastore.api.SerDeInfo;
-import org.apache.hadoop.hive.metastore.api.SkewedInfo;
-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
-import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;
-import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.metastore.api.Type;
-import org.apache.hadoop.hive.metastore.api.UnknownDBException;
-import org.apache.hadoop.hive.ql.exec.Utilities;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.util.StringUtils;
-import org.apache.thrift.TException;
-
-import com.google.common.collect.Lists;
-
-public abstract class TestHiveMetaStore extends TestCase {
-  private static final Log LOG = LogFactory.getLog(TestHiveMetaStore.class);
-  protected static HiveMetaStoreClient client;
-  protected static HiveConf hiveConf;
-  protected static Warehouse warehouse;
-  protected static boolean isThriftClient = false;
-
-  private static final String TEST_DB1_NAME = "testdb1";
-  private static final String TEST_DB2_NAME = "testdb2";
-
-  @Override
-  protected void setUp() throws Exception {
-    hiveConf = new HiveConf(this.getClass());
-    warehouse = new Warehouse(hiveConf);
-
-    // set some values to use for getting conf. vars
-    hiveConf.set("hive.metastore.metrics.enabled","true");
-    hiveConf.set("hive.key1", "value1");
-    hiveConf.set("hive.key2", "http://www.example.com");
-    hiveConf.set("hive.key3", "");
-    hiveConf.set("hive.key4", "0");
-  }
-
-  public void testNameMethods() {
-    Map<String, String> spec = new LinkedHashMap<String, String>();
-    spec.put("ds", "2008-07-01 14:13:12");
-    spec.put("hr", "14");
-    List<String> vals = new ArrayList<String>();
-    for(String v : spec.values()) {
-      vals.add(v);
-    }
-    String partName = "ds=2008-07-01 14%3A13%3A12/hr=14";
-
-    try {
-      List<String> testVals = client.partitionNameToVals(partName);
-      assertTrue("Values from name are incorrect", vals.equals(testVals));
-
-      Map<String, String> testSpec = client.partitionNameToSpec(partName);
-      assertTrue("Spec from name is incorrect", spec.equals(testSpec));
-
-      List<String> emptyVals = client.partitionNameToVals("");
-      assertTrue("Values should be empty", emptyVals.size() == 0);
-
-      Map<String, String> emptySpec =  client.partitionNameToSpec("");
-      assertTrue("Spec should be empty", emptySpec.size() == 0);
-    } catch (Exception e) {
-      assert(false);
-    }
-  }
-
-  /**
-   * tests create table and partition and tries to drop the table without
-   * droppping the partition
-   *
-   * @throws Exception
-   */
-  public void testPartition() throws Exception {
-    partitionTester(client, hiveConf);
-  }
-
-  public static void partitionTester(HiveMetaStoreClient client, HiveConf hiveConf)
-    throws Exception {
-    try {
-      String dbName = "compdb";
-      String tblName = "comptbl";
-      String typeName = "Person";
-      List<String> vals = makeVals("2008-07-01 14:13:12", "14");
-      List<String> vals2 = makeVals("2008-07-01 14:13:12", "15");
-      List<String> vals3 = makeVals("2008-07-02 14:13:12", "15");
-      List<String> vals4 = makeVals("2008-07-03 14:13:12", "151");
-
-      client.dropTable(dbName, tblName);
-      silentDropDatabase(dbName);
-      Database db = new Database();
-      db.setName(dbName);
-      client.createDatabase(db);
-      db = client.getDatabase(dbName);
-      Path dbPath = new Path(db.getLocationUri());
-      FileSystem fs = FileSystem.get(dbPath.toUri(), hiveConf);
-      boolean inheritPerms = hiveConf.getBoolVar(
-          HiveConf.ConfVars.HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS);
-      FsPermission dbPermission = fs.getFileStatus(dbPath).getPermission();
-      if (inheritPerms) {
-         //Set different perms for the database dir for further tests
-         dbPermission = new FsPermission((short)488);
-         fs.setPermission(dbPath, dbPermission);
-      }
-
-      client.dropType(typeName);
-      Type typ1 = new Type();
-      typ1.setName(typeName);
-      typ1.setFields(new ArrayList<FieldSchema>(2));
-      typ1.getFields().add(
-          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-      typ1.getFields().add(
-          new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-      client.createType(typ1);
-
-      Table tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(tblName);
-      StorageDescriptor sd = new StorageDescriptor();
-      tbl.setSd(sd);
-      sd.setCols(typ1.getFields());
-      sd.setCompressed(false);
-      sd.setNumBuckets(1);
-      sd.setParameters(new HashMap<String, String>());
-      sd.getParameters().put("test_param_1", "Use this for comments etc");
-      sd.setBucketCols(new ArrayList<String>(2));
-      sd.getBucketCols().add("name");
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters()
-          .put(serdeConstants.SERIALIZATION_FORMAT, "1");
-      sd.setSortCols(new ArrayList<Order>());
-      sd.setStoredAsSubDirectories(false);
-
-      //skewed information
-      SkewedInfo skewInfor = new SkewedInfo();
-      skewInfor.setSkewedColNames(Arrays.asList("name"));
-      List<String> skv = Arrays.asList("1");
-      skewInfor.setSkewedColValues(Arrays.asList(skv));
-      Map<List<String>, String> scvlm = new HashMap<List<String>, String>();
-      scvlm.put(skv, "location1");
-      skewInfor.setSkewedColValueLocationMaps(scvlm);
-      sd.setSkewedInfo(skewInfor);
-
-      tbl.setPartitionKeys(new ArrayList<FieldSchema>(2));
-      tbl.getPartitionKeys().add(
-          new FieldSchema("ds", serdeConstants.STRING_TYPE_NAME, ""));
-      tbl.getPartitionKeys().add(
-          new FieldSchema("hr", serdeConstants.STRING_TYPE_NAME, ""));
-
-      client.createTable(tbl);
-
-      if (isThriftClient) {
-        // the createTable() above does not update the location in the 'tbl'
-        // object when the client is a thrift client and the code below relies
-        // on the location being present in the 'tbl' object - so get the table
-        // from the metastore
-        tbl = client.getTable(dbName, tblName);
-      }
-
-      assertEquals(dbPermission, fs.getFileStatus(new Path(tbl.getSd().getLocation()))
-          .getPermission());
-
-      Partition part = makePartitionObject(dbName, tblName, vals, tbl, "/part1");
-      Partition part2 = makePartitionObject(dbName, tblName, vals2, tbl, "/part2");
-      Partition part3 = makePartitionObject(dbName, tblName, vals3, tbl, "/part3");
-      Partition part4 = makePartitionObject(dbName, tblName, vals4, tbl, "/part4");
-
-      // check if the partition exists (it shouldn't)
-      boolean exceptionThrown = false;
-      try {
-        Partition p = client.getPartition(dbName, tblName, vals);
-      } catch(Exception e) {
-        assertEquals("partition should not have existed",
-            NoSuchObjectException.class, e.getClass());
-        exceptionThrown = true;
-      }
-      assertTrue("getPartition() should have thrown NoSuchObjectException", exceptionThrown);
-      Partition retp = client.add_partition(part);
-      assertNotNull("Unable to create partition " + part, retp);
-      assertEquals(dbPermission, fs.getFileStatus(new Path(retp.getSd().getLocation()))
-          .getPermission());
-      Partition retp2 = client.add_partition(part2);
-      assertNotNull("Unable to create partition " + part2, retp2);
-      assertEquals(dbPermission, fs.getFileStatus(new Path(retp2.getSd().getLocation()))
-          .getPermission());
-      Partition retp3 = client.add_partition(part3);
-      assertNotNull("Unable to create partition " + part3, retp3);
-      assertEquals(dbPermission, fs.getFileStatus(new Path(retp3.getSd().getLocation()))
-          .getPermission());
-      Partition retp4 = client.add_partition(part4);
-      assertNotNull("Unable to create partition " + part4, retp4);
-      assertEquals(dbPermission, fs.getFileStatus(new Path(retp4.getSd().getLocation()))
-          .getPermission());
-
-      Partition part_get = client.getPartition(dbName, tblName, part.getValues());
-      if(isThriftClient) {
-        // since we are using thrift, 'part' will not have the create time and
-        // last DDL time set since it does not get updated in the add_partition()
-        // call - likewise part2 and part3 - set it correctly so that equals check
-        // doesn't fail
-        adjust(client, part, dbName, tblName);
-        adjust(client, part2, dbName, tblName);
-        adjust(client, part3, dbName, tblName);
-      }
-      assertTrue("Partitions are not same", part.equals(part_get));
-
-      String partName = "ds=2008-07-01 14%3A13%3A12/hr=14";
-      String part2Name = "ds=2008-07-01 14%3A13%3A12/hr=15";
-      String part3Name ="ds=2008-07-02 14%3A13%3A12/hr=15";
-      String part4Name ="ds=2008-07-03 14%3A13%3A12/hr=151";
-
-      part_get = client.getPartition(dbName, tblName, partName);
-      assertTrue("Partitions are not the same", part.equals(part_get));
-
-      // Test partition listing with a partial spec - ds is specified but hr is not
-      List<String> partialVals = new ArrayList<String>();
-      partialVals.add(vals.get(0));
-      Set<Partition> parts = new HashSet<Partition>();
-      parts.add(part);
-      parts.add(part2);
-
-      List<Partition> partial = client.listPartitions(dbName, tblName, partialVals,
-          (short) -1);
-      assertTrue("Should have returned 2 partitions", partial.size() == 2);
-      assertTrue("Not all parts returned", partial.containsAll(parts));
-
-      Set<String> partNames = new HashSet<String>();
-      partNames.add(partName);
-      partNames.add(part2Name);
-      List<String> partialNames = client.listPartitionNames(dbName, tblName, partialVals,
-          (short) -1);
-      assertTrue("Should have returned 2 partition names", partialNames.size() == 2);
-      assertTrue("Not all part names returned", partialNames.containsAll(partNames));
-
-      partNames.add(part3Name);
-      partNames.add(part4Name);
-      partialVals.clear();
-      partialVals.add("");
-      partialNames = client.listPartitionNames(dbName, tblName, partialVals, (short) -1);
-      assertTrue("Should have returned 4 partition names", partialNames.size() == 4);
-      assertTrue("Not all part names returned", partialNames.containsAll(partNames));
-
-      // Test partition listing with a partial spec - hr is specified but ds is not
-      parts.clear();
-      parts.add(part2);
-      parts.add(part3);
-
-      partialVals.clear();
-      partialVals.add("");
-      partialVals.add(vals2.get(1));
-
-      partial = client.listPartitions(dbName, tblName, partialVals, (short) -1);
-      assertEquals("Should have returned 2 partitions", 2, partial.size());
-      assertTrue("Not all parts returned", partial.containsAll(parts));
-
-      partNames.clear();
-      partNames.add(part2Name);
-      partNames.add(part3Name);
-      partialNames = client.listPartitionNames(dbName, tblName, partialVals,
-          (short) -1);
-      assertEquals("Should have returned 2 partition names", 2, partialNames.size());
-      assertTrue("Not all part names returned", partialNames.containsAll(partNames));
-
-      // Verify escaped partition names don't return partitions
-      exceptionThrown = false;
-      try {
-        String badPartName = "ds=2008-07-01 14%3A13%3A12/hrs=14";
-        client.getPartition(dbName, tblName, badPartName);
-      } catch(NoSuchObjectException e) {
-        exceptionThrown = true;
-      }
-      assertTrue("Bad partition spec should have thrown an exception", exceptionThrown);
-
-      Path partPath = new Path(part.getSd().getLocation());
-
-
-      assertTrue(fs.exists(partPath));
-      client.dropPartition(dbName, tblName, part.getValues(), true);
-      assertFalse(fs.exists(partPath));
-
-      // Test append_partition_by_name
-      client.appendPartition(dbName, tblName, partName);
-      Partition part5 = client.getPartition(dbName, tblName, part.getValues());
-      assertTrue("Append partition by name failed", part5.getValues().equals(vals));;
-      Path part5Path = new Path(part5.getSd().getLocation());
-      assertTrue(fs.exists(part5Path));
-
-      // Test drop_partition_by_name
-      assertTrue("Drop partition by name failed",
-          client.dropPartition(dbName, tblName, partName, true));
-      assertFalse(fs.exists(part5Path));
-
-      // add the partition again so that drop table with a partition can be
-      // tested
-      retp = client.add_partition(part);
-      assertNotNull("Unable to create partition " + part, retp);
-      assertEquals(dbPermission, fs.getFileStatus(new Path(retp.getSd().getLocation()))
-          .getPermission());
-
-      // test add_partitions
-
-      List<String> mvals1 = makeVals("2008-07-04 14:13:12", "14641");
-      List<String> mvals2 = makeVals("2008-07-04 14:13:12", "14642");
-      List<String> mvals3 = makeVals("2008-07-04 14:13:12", "14643");
-      List<String> mvals4 = makeVals("2008-07-04 14:13:12", "14643"); // equal to 3
-      List<String> mvals5 = makeVals("2008-07-04 14:13:12", "14645");
-
-      Exception savedException;
-
-      // add_partitions(empty list) : ok, normal operation
-      client.add_partitions(new ArrayList<Partition>());
-
-      // add_partitions(1,2,3) : ok, normal operation
-      Partition mpart1 = makePartitionObject(dbName, tblName, mvals1, tbl, "/mpart1");
-      Partition mpart2 = makePartitionObject(dbName, tblName, mvals2, tbl, "/mpart2");
-      Partition mpart3 = makePartitionObject(dbName, tblName, mvals3, tbl, "/mpart3");
-      client.add_partitions(Arrays.asList(mpart1,mpart2,mpart3));
-
-      if(isThriftClient) {
-        // do DDL time munging if thrift mode
-        adjust(client, mpart1, dbName, tblName);
-        adjust(client, mpart2, dbName, tblName);
-        adjust(client, mpart3, dbName, tblName);
-      }
-      verifyPartitionsPublished(client, dbName, tblName,
-          Arrays.asList(mvals1.get(0)),
-          Arrays.asList(mpart1,mpart2,mpart3));
-
-      Partition mpart4 = makePartitionObject(dbName, tblName, mvals4, tbl, "/mpart4");
-      Partition mpart5 = makePartitionObject(dbName, tblName, mvals5, tbl, "/mpart5");
-
-      // create dir for /mpart5
-      Path mp5Path = new Path(mpart5.getSd().getLocation());
-      warehouse.mkdirs(mp5Path);
-      assertTrue(fs.exists(mp5Path));
-      assertEquals(dbPermission, fs.getFileStatus(mp5Path).getPermission());
-
-      // add_partitions(5,4) : err = duplicate keyvals on mpart4
-      savedException = null;
-      try {
-        client.add_partitions(Arrays.asList(mpart5,mpart4));
-      } catch (Exception e) {
-        savedException = e;
-      } finally {
-        assertNotNull(savedException);
-      }
-
-      // check that /mpart4 does not exist, but /mpart5 still does.
-      assertTrue(fs.exists(mp5Path));
-      assertFalse(fs.exists(new Path(mpart4.getSd().getLocation())));
-
-      // add_partitions(5) : ok
-      client.add_partitions(Arrays.asList(mpart5));
-
-      if(isThriftClient) {
-        // do DDL time munging if thrift mode
-        adjust(client, mpart5, dbName, tblName);
-      }
-
-      verifyPartitionsPublished(client, dbName, tblName,
-          Arrays.asList(mvals1.get(0)),
-          Arrays.asList(mpart1,mpart2,mpart3,mpart5));
-
-      //// end add_partitions tests
-
-      client.dropTable(dbName, tblName);
-
-      client.dropType(typeName);
-
-      // recreate table as external, drop partition and it should
-      // still exist
-      tbl.setParameters(new HashMap<String, String>());
-      tbl.getParameters().put("EXTERNAL", "TRUE");
-      client.createTable(tbl);
-      retp = client.add_partition(part);
-      assertTrue(fs.exists(partPath));
-      client.dropPartition(dbName, tblName, part.getValues(), true);
-      assertTrue(fs.exists(partPath));
-
-      for (String tableName : client.getTables(dbName, "*")) {
-        client.dropTable(dbName, tableName);
-      }
-
-      client.dropDatabase(dbName);
-
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testPartition() failed.");
-      throw e;
-    }
-  }
-
-  private static void verifyPartitionsPublished(HiveMetaStoreClient client,
-      String dbName, String tblName, List<String> partialSpec,
-      List<Partition> expectedPartitions)
-          throws NoSuchObjectException, MetaException, TException {
-    // Test partition listing with a partial spec
-
-    List<Partition> mpartial = client.listPartitions(dbName, tblName, partialSpec,
-        (short) -1);
-    assertEquals("Should have returned "+expectedPartitions.size()+
-        " partitions, returned " + mpartial.size(),
-        expectedPartitions.size(), mpartial.size());
-    assertTrue("Not all parts returned", mpartial.containsAll(expectedPartitions));
-  }
-
-  private static List<String> makeVals(String ds, String id) {
-    List <String> vals4 = new ArrayList<String>(2);
-    vals4 = new ArrayList<String>(2);
-    vals4.add(ds);
-    vals4.add(id);
-    return vals4;
-  }
-
-  private static Partition makePartitionObject(String dbName, String tblName,
-      List<String> ptnVals, Table tbl, String ptnLocationSuffix) {
-    Partition part4 = new Partition();
-    part4.setDbName(dbName);
-    part4.setTableName(tblName);
-    part4.setValues(ptnVals);
-    part4.setParameters(new HashMap<String, String>());
-    part4.setSd(tbl.getSd().deepCopy());
-    part4.getSd().setSerdeInfo(tbl.getSd().getSerdeInfo().deepCopy());
-    part4.getSd().setLocation(tbl.getSd().getLocation() + ptnLocationSuffix);
-    return part4;
-  }
-
-  public void testListPartitions() throws Throwable {
-    // create a table with multiple partitions
-    String dbName = "compdb";
-    String tblName = "comptbl";
-    String typeName = "Person";
-
-    cleanUp(dbName, tblName, typeName);
-
-    List<List<String>> values = new ArrayList<List<String>>();
-    values.add(makeVals("2008-07-01 14:13:12", "14"));
-    values.add(makeVals("2008-07-01 14:13:12", "15"));
-    values.add(makeVals("2008-07-02 14:13:12", "15"));
-    values.add(makeVals("2008-07-03 14:13:12", "151"));
-
-    createMultiPartitionTableSchema(dbName, tblName, typeName, values);
-
-    List<Partition> partitions = client.listPartitions(dbName, tblName, (short)-1);
-    assertNotNull("should have returned partitions", partitions);
-    assertEquals(" should have returned " + values.size() +
-      " partitions", values.size(), partitions.size());
-
-    partitions = client.listPartitions(dbName, tblName, (short)(values.size()/2));
-
-    assertNotNull("should have returned partitions", partitions);
-    assertEquals(" should have returned " + values.size() / 2 +
-      " partitions",values.size() / 2, partitions.size());
-
-
-    partitions = client.listPartitions(dbName, tblName, (short) (values.size() * 2));
-
-    assertNotNull("should have returned partitions", partitions);
-    assertEquals(" should have returned " + values.size() +
-      " partitions",values.size(), partitions.size());
-
-    cleanUp(dbName, tblName, typeName);
-
-  }
-
-
-
-  public void testListPartitionNames() throws Throwable {
-    // create a table with multiple partitions
-    String dbName = "compdb";
-    String tblName = "comptbl";
-    String typeName = "Person";
-
-    cleanUp(dbName, tblName, typeName);
-
-    List<List<String>> values = new ArrayList<List<String>>();
-    values.add(makeVals("2008-07-01 14:13:12", "14"));
-    values.add(makeVals("2008-07-01 14:13:12", "15"));
-    values.add(makeVals("2008-07-02 14:13:12", "15"));
-    values.add(makeVals("2008-07-03 14:13:12", "151"));
-
-
-
-    createMultiPartitionTableSchema(dbName, tblName, typeName, values);
-
-    List<String> partitions = client.listPartitionNames(dbName, tblName, (short)-1);
-    assertNotNull("should have returned partitions", partitions);
-    assertEquals(" should have returned " + values.size() +
-      " partitions", values.size(), partitions.size());
-
-    partitions = client.listPartitionNames(dbName, tblName, (short)(values.size()/2));
-
-    assertNotNull("should have returned partitions", partitions);
-    assertEquals(" should have returned " + values.size() / 2 +
-      " partitions",values.size() / 2, partitions.size());
-
-
-    partitions = client.listPartitionNames(dbName, tblName, (short) (values.size() * 2));
-
-    assertNotNull("should have returned partitions", partitions);
-    assertEquals(" should have returned " + values.size() +
-      " partitions",values.size(), partitions.size());
-
-    cleanUp(dbName, tblName, typeName);
-
-  }
-
-
-  public void testDropTable() throws Throwable {
-    // create a table with multiple partitions
-    String dbName = "compdb";
-    String tblName = "comptbl";
-    String typeName = "Person";
-
-    cleanUp(dbName, tblName, typeName);
-
-    List<List<String>> values = new ArrayList<List<String>>();
-    values.add(makeVals("2008-07-01 14:13:12", "14"));
-    values.add(makeVals("2008-07-01 14:13:12", "15"));
-    values.add(makeVals("2008-07-02 14:13:12", "15"));
-    values.add(makeVals("2008-07-03 14:13:12", "151"));
-
-    createMultiPartitionTableSchema(dbName, tblName, typeName, values);
-
-    client.dropTable(dbName, tblName);
-    client.dropType(typeName);
-
-    boolean exceptionThrown = false;
-    try {
-      client.getTable(dbName, tblName);
-    } catch(Exception e) {
-      assertEquals("table should not have existed",
-          NoSuchObjectException.class, e.getClass());
-      exceptionThrown = true;
-    }
-    assertTrue("Table " + tblName + " should have been dropped ", exceptionThrown);
-
-  }
-
-  public void testAlterViewParititon() throws Throwable {
-    String dbName = "compdb";
-    String tblName = "comptbl";
-    String viewName = "compView";
-
-    client.dropTable(dbName, tblName);
-    silentDropDatabase(dbName);
-    Database db = new Database();
-    db.setName(dbName);
-    db.setDescription("Alter Partition Test database");
-    client.createDatabase(db);
-
-    ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
-    cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-    cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-
-    Table tbl = new Table();
-    tbl.setDbName(dbName);
-    tbl.setTableName(tblName);
-    StorageDescriptor sd = new StorageDescriptor();
-    tbl.setSd(sd);
-    sd.setCols(cols);
-    sd.setCompressed(false);
-    sd.setParameters(new HashMap<String, String>());
-    sd.setSerdeInfo(new SerDeInfo());
-    sd.getSerdeInfo().setName(tbl.getTableName());
-    sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-    sd.getSerdeInfo().getParameters()
-        .put(serdeConstants.SERIALIZATION_FORMAT, "1");
-    sd.setSortCols(new ArrayList<Order>());
-
-    client.createTable(tbl);
-
-    if (isThriftClient) {
-      // the createTable() above does not update the location in the 'tbl'
-      // object when the client is a thrift client and the code below relies
-      // on the location being present in the 'tbl' object - so get the table
-      // from the metastore
-      tbl = client.getTable(dbName, tblName);
-    }
-
-    ArrayList<FieldSchema> viewCols = new ArrayList<FieldSchema>(1);
-    viewCols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-
-    ArrayList<FieldSchema> viewPartitionCols = new ArrayList<FieldSchema>(1);
-    viewPartitionCols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-
-    Table view = new Table();
-    view.setDbName(dbName);
-    view.setTableName(viewName);
-    view.setTableType(TableType.VIRTUAL_VIEW.name());
-    view.setPartitionKeys(viewPartitionCols);
-    view.setViewOriginalText("SELECT income, name FROM " + tblName);
-    view.setViewExpandedText("SELECT `" + tblName + "`.`income`, `" + tblName +
-        "`.`name` FROM `" + dbName + "`.`" + tblName + "`");
-    StorageDescriptor viewSd = new StorageDescriptor();
-    view.setSd(viewSd);
-    viewSd.setCols(viewCols);
-    viewSd.setCompressed(false);
-    viewSd.setParameters(new HashMap<String, String>());
-    viewSd.setSerdeInfo(new SerDeInfo());
-    viewSd.getSerdeInfo().setParameters(new HashMap<String, String>());
-
-    client.createTable(view);
-
-    if (isThriftClient) {
-      // the createTable() above does not update the location in the 'tbl'
-      // object when the client is a thrift client and the code below relies
-      // on the location being present in the 'tbl' object - so get the table
-      // from the metastore
-      view = client.getTable(dbName, viewName);
-    }
-
-    List<String> vals = new ArrayList<String>(1);
-    vals.add("abc");
-
-    Partition part = new Partition();
-    part.setDbName(dbName);
-    part.setTableName(viewName);
-    part.setValues(vals);
-    part.setParameters(new HashMap<String, String>());
-
-    client.add_partition(part);
-
-    Partition part2 = client.getPartition(dbName, viewName, part.getValues());
-
-    part2.getParameters().put("a", "b");
-
-    client.alter_partition(dbName, viewName, part2);
-
-    Partition part3 = client.getPartition(dbName, viewName, part.getValues());
-    assertEquals("couldn't view alter partition", part3.getParameters().get(
-        "a"), "b");
-
-    client.dropTable(dbName, viewName);
-
-    client.dropTable(dbName, tblName);
-
-    client.dropDatabase(dbName);
-  }
-
-  public void testAlterPartition() throws Throwable {
-
-    try {
-      String dbName = "compdb";
-      String tblName = "comptbl";
-      List<String> vals = new ArrayList<String>(2);
-      vals.add("2008-07-01");
-      vals.add("14");
-
-      client.dropTable(dbName, tblName);
-      silentDropDatabase(dbName);
-      Database db = new Database();
-      db.setName(dbName);
-      db.setDescription("Alter Partition Test database");
-      client.createDatabase(db);
-
-      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
-      cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-      cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-
-      Table tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(tblName);
-      StorageDescriptor sd = new StorageDescriptor();
-      tbl.setSd(sd);
-      sd.setCols(cols);
-      sd.setCompressed(false);
-      sd.setNumBuckets(1);
-      sd.setParameters(new HashMap<String, String>());
-      sd.getParameters().put("test_param_1", "Use this for comments etc");
-      sd.setBucketCols(new ArrayList<String>(2));
-      sd.getBucketCols().add("name");
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters()
-          .put(serdeConstants.SERIALIZATION_FORMAT, "1");
-      sd.setSortCols(new ArrayList<Order>());
-
-      tbl.setPartitionKeys(new ArrayList<FieldSchema>(2));
-      tbl.getPartitionKeys().add(
-          new FieldSchema("ds", serdeConstants.STRING_TYPE_NAME, ""));
-      tbl.getPartitionKeys().add(
-          new FieldSchema("hr", serdeConstants.INT_TYPE_NAME, ""));
-
-      client.createTable(tbl);
-
-      if (isThriftClient) {
-        // the createTable() above does not update the location in the 'tbl'
-        // object when the client is a thrift client and the code below relies
-        // on the location being present in the 'tbl' object - so get the table
-        // from the metastore
-        tbl = client.getTable(dbName, tblName);
-      }
-
-      Partition part = new Partition();
-      part.setDbName(dbName);
-      part.setTableName(tblName);
-      part.setValues(vals);
-      part.setParameters(new HashMap<String, String>());
-      part.setSd(tbl.getSd());
-      part.getSd().setSerdeInfo(tbl.getSd().getSerdeInfo());
-      part.getSd().setLocation(tbl.getSd().getLocation() + "/part1");
-
-      client.add_partition(part);
-
-      Partition part2 = client.getPartition(dbName, tblName, part.getValues());
-
-      part2.getParameters().put("retention", "10");
-      part2.getSd().setNumBuckets(12);
-      part2.getSd().getSerdeInfo().getParameters().put("abc", "1");
-      client.alter_partition(dbName, tblName, part2);
-
-      Partition part3 = client.getPartition(dbName, tblName, part.getValues());
-      assertEquals("couldn't alter partition", part3.getParameters().get(
-          "retention"), "10");
-      assertEquals("couldn't alter partition", part3.getSd().getSerdeInfo()
-          .getParameters().get("abc"), "1");
-      assertEquals("couldn't alter partition", part3.getSd().getNumBuckets(),
-          12);
-
-      client.dropTable(dbName, tblName);
-
-      client.dropDatabase(dbName);
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testPartition() failed.");
-      throw e;
-    }
-  }
-
-  public void testRenamePartition() throws Throwable {
-
-    try {
-      String dbName = "compdb1";
-      String tblName = "comptbl1";
-      List<String> vals = new ArrayList<String>(2);
-      vals.add("2011-07-11");
-      vals.add("8");
-      String part_path = "/ds=2011-07-11/hr=8";
-      List<String> tmp_vals = new ArrayList<String>(2);
-      tmp_vals.add("tmp_2011-07-11");
-      tmp_vals.add("-8");
-      String part2_path = "/ds=tmp_2011-07-11/hr=-8";
-
-      client.dropTable(dbName, tblName);
-      silentDropDatabase(dbName);
-      Database db = new Database();
-      db.setName(dbName);
-      db.setDescription("Rename Partition Test database");
-      client.createDatabase(db);
-
-      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
-      cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-      cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-
-      Table tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(tblName);
-      StorageDescriptor sd = new StorageDescriptor();
-      tbl.setSd(sd);
-      sd.setCols(cols);
-      sd.setCompressed(false);
-      sd.setNumBuckets(1);
-      sd.setParameters(new HashMap<String, String>());
-      sd.getParameters().put("test_param_1", "Use this for comments etc");
-      sd.setBucketCols(new ArrayList<String>(2));
-      sd.getBucketCols().add("name");
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters()
-          .put(serdeConstants.SERIALIZATION_FORMAT, "1");
-      sd.setSortCols(new ArrayList<Order>());
-
-      tbl.setPartitionKeys(new ArrayList<FieldSchema>(2));
-      tbl.getPartitionKeys().add(
-          new FieldSchema("ds", serdeConstants.STRING_TYPE_NAME, ""));
-      tbl.getPartitionKeys().add(
-          new FieldSchema("hr", serdeConstants.INT_TYPE_NAME, ""));
-
-      client.createTable(tbl);
-
-      if (isThriftClient) {
-        // the createTable() above does not update the location in the 'tbl'
-        // object when the client is a thrift client and the code below relies
-        // on the location being present in the 'tbl' object - so get the table
-        // from the metastore
-        tbl = client.getTable(dbName, tblName);
-      }
-
-      Partition part = new Partition();
-      part.setDbName(dbName);
-      part.setTableName(tblName);
-      part.setValues(vals);
-      part.setParameters(new HashMap<String, String>());
-      part.setSd(tbl.getSd().deepCopy());
-      part.getSd().setSerdeInfo(tbl.getSd().getSerdeInfo());
-      part.getSd().setLocation(tbl.getSd().getLocation() + "/part1");
-      part.getParameters().put("retention", "10");
-      part.getSd().setNumBuckets(12);
-      part.getSd().getSerdeInfo().getParameters().put("abc", "1");
-
-      client.add_partition(part);
-
-      part.setValues(tmp_vals);
-      client.renamePartition(dbName, tblName, vals, part);
-
-      boolean exceptionThrown = false;
-      try {
-        Partition p = client.getPartition(dbName, tblName, vals);
-      } catch(Exception e) {
-        assertEquals("partition should not have existed",
-            NoSuchObjectException.class, e.getClass());
-        exceptionThrown = true;
-      }
-      assertTrue("Expected NoSuchObjectException", exceptionThrown);
-
-      Partition part3 = client.getPartition(dbName, tblName, tmp_vals);
-      assertEquals("couldn't rename partition", part3.getParameters().get(
-          "retention"), "10");
-      assertEquals("couldn't rename partition", part3.getSd().getSerdeInfo()
-          .getParameters().get("abc"), "1");
-      assertEquals("couldn't rename partition", part3.getSd().getNumBuckets(),
-          12);
-      assertEquals("new partition sd matches", part3.getSd().getLocation(),
-          tbl.getSd().getLocation() + part2_path);
-
-      part.setValues(vals);
-      client.renamePartition(dbName, tblName, tmp_vals, part);
-
-      exceptionThrown = false;
-      try {
-        Partition p = client.getPartition(dbName, tblName, tmp_vals);
-      } catch(Exception e) {
-        assertEquals("partition should not have existed",
-            NoSuchObjectException.class, e.getClass());
-        exceptionThrown = true;
-      }
-      assertTrue("Expected NoSuchObjectException", exceptionThrown);
-
-      part3 = client.getPartition(dbName, tblName, vals);
-      assertEquals("couldn't rename partition", part3.getParameters().get(
-          "retention"), "10");
-      assertEquals("couldn't rename partition", part3.getSd().getSerdeInfo()
-          .getParameters().get("abc"), "1");
-      assertEquals("couldn't rename partition", part3.getSd().getNumBuckets(),
-          12);
-      assertEquals("new partition sd matches", part3.getSd().getLocation(),
-          tbl.getSd().getLocation() + part_path);
-
-      client.dropTable(dbName, tblName);
-
-      client.dropDatabase(dbName);
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testRenamePartition() failed.");
-      throw e;
-    }
-  }
-
-  public void testDatabase() throws Throwable {
-    try {
-      // clear up any existing databases
-      silentDropDatabase(TEST_DB1_NAME);
-      silentDropDatabase(TEST_DB2_NAME);
-
-      Database db = new Database();
-      db.setName(TEST_DB1_NAME);
-      client.createDatabase(db);
-
-      db = client.getDatabase(TEST_DB1_NAME);
-
-      assertEquals("name of returned db is different from that of inserted db",
-          TEST_DB1_NAME, db.getName());
-      assertEquals("location of the returned db is different from that of inserted db",
-          warehouse.getDatabasePath(db).toString(), db.getLocationUri());
-
-      Database db2 = new Database();
-      db2.setName(TEST_DB2_NAME);
-      client.createDatabase(db2);
-
-      db2 = client.getDatabase(TEST_DB2_NAME);
-
-      assertEquals("name of returned db is different from that of inserted db",
-          TEST_DB2_NAME, db2.getName());
-      assertEquals("location of the returned db is different from that of inserted db",
-          warehouse.getDatabasePath(db2).toString(), db2.getLocationUri());
-
-      List<String> dbs = client.getDatabases(".*");
-
-      assertTrue("first database is not " + TEST_DB1_NAME, dbs.contains(TEST_DB1_NAME));
-      assertTrue("second database is not " + TEST_DB2_NAME, dbs.contains(TEST_DB2_NAME));
-
-      client.dropDatabase(TEST_DB1_NAME);
-      client.dropDatabase(TEST_DB2_NAME);
-      silentDropDatabase(TEST_DB1_NAME);
-      silentDropDatabase(TEST_DB2_NAME);
-    } catch (Throwable e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testDatabase() failed.");
-      throw e;
-    }
-  }
-
-  public void testDatabaseLocationWithPermissionProblems() throws Exception {
-
-    // Note: The following test will fail if you are running this test as root. Setting
-    // permission to '0' on the database folder will not preclude root from being able
-    // to create the necessary files.
-
-    if (System.getProperty("user.name").equals("root")) {
-      System.err.println("Skipping test because you are running as root!");
-      return;
-    }
-
-    silentDropDatabase(TEST_DB1_NAME);
-
-    Database db = new Database();
-    db.setName(TEST_DB1_NAME);
-    String dbLocation =
-      HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/test/_testDB_create_";
-    FileSystem fs = FileSystem.get(new Path(dbLocation).toUri(), hiveConf);
-    fs.mkdirs(
-              new Path(HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/test"),
-              new FsPermission((short) 0));
-    db.setLocationUri(dbLocation);
-
-
-    boolean createFailed = false;
-    try {
-      client.createDatabase(db);
-    } catch (MetaException cantCreateDB) {
-      createFailed = true;
-    } finally {
-      // Cleanup
-      if (!createFailed) {
-        try {
-          client.dropDatabase(TEST_DB1_NAME);
-        } catch(Exception e) {
-          System.err.println("Failed to remove database in cleanup: " + e.getMessage());
-        }
-      }
-
-      fs.setPermission(new Path(HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/test"),
-                       new FsPermission((short) 755));
-      fs.delete(new Path(HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/test"), true);
-    }
-
-    assertTrue("Database creation succeeded even with permission problem", createFailed);
-  }
-
-  public void testDatabaseLocation() throws Throwable {
-    try {
-      // clear up any existing databases
-      silentDropDatabase(TEST_DB1_NAME);
-
-      Database db = new Database();
-      db.setName(TEST_DB1_NAME);
-      String dbLocation =
-          HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/_testDB_create_";
-      db.setLocationUri(dbLocation);
-      client.createDatabase(db);
-
-      db = client.getDatabase(TEST_DB1_NAME);
-
-      assertEquals("name of returned db is different from that of inserted db",
-          TEST_DB1_NAME, db.getName());
-      assertEquals("location of the returned db is different from that of inserted db",
-          warehouse.getDnsPath(new Path(dbLocation)).toString(), db.getLocationUri());
-
-      client.dropDatabase(TEST_DB1_NAME);
-      silentDropDatabase(TEST_DB1_NAME);
-
-      boolean objectNotExist = false;
-      try {
-        client.getDatabase(TEST_DB1_NAME);
-      } catch (NoSuchObjectException e) {
-        objectNotExist = true;
-      }
-      assertTrue("Database " + TEST_DB1_NAME + " exists ", objectNotExist);
-
-      db = new Database();
-      db.setName(TEST_DB1_NAME);
-      dbLocation =
-          HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "/_testDB_file_";
-      FileSystem fs = FileSystem.get(new Path(dbLocation).toUri(), hiveConf);
-      fs.createNewFile(new Path(dbLocation));
-      fs.deleteOnExit(new Path(dbLocation));
-      db.setLocationUri(dbLocation);
-
-      boolean createFailed = false;
-      try {
-        client.createDatabase(db);
-      } catch (MetaException cantCreateDB) {
-        System.err.println(cantCreateDB.getMessage());
-        createFailed = true;
-      }
-      assertTrue("Database creation succeeded even location exists and is a file", createFailed);
-
-      objectNotExist = false;
-      try {
-        client.getDatabase(TEST_DB1_NAME);
-      } catch (NoSuchObjectException e) {
-        objectNotExist = true;
-      }
-      assertTrue("Database " + TEST_DB1_NAME + " exists when location is specified and is a file",
-          objectNotExist);
-
-    } catch (Throwable e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testDatabaseLocation() failed.");
-      throw e;
-    }
-  }
-
-
-  public void testSimpleTypeApi() throws Exception {
-    try {
-      client.dropType(serdeConstants.INT_TYPE_NAME);
-
-      Type typ1 = new Type();
-      typ1.setName(serdeConstants.INT_TYPE_NAME);
-      boolean ret = client.createType(typ1);
-      assertTrue("Unable to create type", ret);
-
-      Type typ1_2 = client.getType(serdeConstants.INT_TYPE_NAME);
-      assertNotNull(typ1_2);
-      assertEquals(typ1.getName(), typ1_2.getName());
-
-      ret = client.dropType(serdeConstants.INT_TYPE_NAME);
-      assertTrue("unable to drop type integer", ret);
-
-      boolean exceptionThrown = false;
-      try {
-        client.getType(serdeConstants.INT_TYPE_NAME);
-      } catch (NoSuchObjectException e) {
-        exceptionThrown = true;
-      }
-      assertTrue("Expected NoSuchObjectException", exceptionThrown);
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testSimpleTypeApi() failed.");
-      throw e;
-    }
-  }
-
-  // TODO:pc need to enhance this with complex fields and getType_all function
-  public void testComplexTypeApi() throws Exception {
-    try {
-      client.dropType("Person");
-
-      Type typ1 = new Type();
-      typ1.setName("Person");
-      typ1.setFields(new ArrayList<FieldSchema>(2));
-      typ1.getFields().add(
-          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-      typ1.getFields().add(
-          new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-      boolean ret = client.createType(typ1);
-      assertTrue("Unable to create type", ret);
-
-      Type typ1_2 = client.getType("Person");
-      assertNotNull("type Person not found", typ1_2);
-      assertEquals(typ1.getName(), typ1_2.getName());
-      assertEquals(typ1.getFields().size(), typ1_2.getFields().size());
-      assertEquals(typ1.getFields().get(0), typ1_2.getFields().get(0));
-      assertEquals(typ1.getFields().get(1), typ1_2.getFields().get(1));
-
-      client.dropType("Family");
-
-      Type fam = new Type();
-      fam.setName("Family");
-      fam.setFields(new ArrayList<FieldSchema>(2));
-      fam.getFields().add(
-          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-      fam.getFields().add(
-          new FieldSchema("members",
-              MetaStoreUtils.getListType(typ1.getName()), ""));
-
-      ret = client.createType(fam);
-      assertTrue("Unable to create type " + fam.getName(), ret);
-
-      Type fam2 = client.getType("Family");
-      assertNotNull("type Person not found", fam2);
-      assertEquals(fam.getName(), fam2.getName());
-      assertEquals(fam.getFields().size(), fam2.getFields().size());
-      assertEquals(fam.getFields().get(0), fam2.getFields().get(0));
-      assertEquals(fam.getFields().get(1), fam2.getFields().get(1));
-
-      ret = client.dropType("Family");
-      assertTrue("unable to drop type Family", ret);
-
-      ret = client.dropType("Person");
-      assertTrue("unable to drop type Person", ret);
-
-      boolean exceptionThrown = false;
-      try {
-        client.getType("Person");
-      } catch (NoSuchObjectException e) {
-        exceptionThrown = true;
-      }
-      assertTrue("Expected NoSuchObjectException", exceptionThrown);
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testComplexTypeApi() failed.");
-      throw e;
-    }
-  }
-
-  public void testSimpleTable() throws Exception {
-    try {
-      String dbName = "simpdb";
-      String tblName = "simptbl";
-      String tblName2 = "simptbl2";
-      String typeName = "Person";
-
-      client.dropTable(dbName, tblName);
-      silentDropDatabase(dbName);
-
-      Database db = new Database();
-      db.setName(dbName);
-      client.createDatabase(db);
-
-      client.dropType(typeName);
-      Type typ1 = new Type();
-      typ1.setName(typeName);
-      typ1.setFields(new ArrayList<FieldSchema>(2));
-      typ1.getFields().add(
-          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-      typ1.getFields().add(
-          new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-      client.createType(typ1);
-
-      Table tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(tblName);
-      StorageDescriptor sd = new StorageDescriptor();
-      tbl.setSd(sd);
-      sd.setCols(typ1.getFields());
-      sd.setCompressed(false);
-      sd.setNumBuckets(1);
-      sd.setParameters(new HashMap<String, String>());
-      sd.getParameters().put("test_param_1", "Use this for comments etc");
-      sd.setBucketCols(new ArrayList<String>(2));
-      sd.getBucketCols().add("name");
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters().put(
-          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
-      sd.getSerdeInfo().setSerializationLib(
-          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
-      tbl.setPartitionKeys(new ArrayList<FieldSchema>());
-
-      client.createTable(tbl);
-
-      if (isThriftClient) {
-        // the createTable() above does not update the location in the 'tbl'
-        // object when the client is a thrift client and the code below relies
-        // on the location being present in the 'tbl' object - so get the table
-        // from the metastore
-        tbl = client.getTable(dbName, tblName);
-      }
-
-      Table tbl2 = client.getTable(dbName, tblName);
-      assertNotNull(tbl2);
-      assertEquals(tbl2.getDbName(), dbName);
-      assertEquals(tbl2.getTableName(), tblName);
-      assertEquals(tbl2.getSd().getCols().size(), typ1.getFields().size());
-      assertEquals(tbl2.getSd().isCompressed(), false);
-      assertEquals(tbl2.getSd().getNumBuckets(), 1);
-      assertEquals(tbl2.getSd().getLocation(), tbl.getSd().getLocation());
-      assertNotNull(tbl2.getSd().getSerdeInfo());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters().put(
-          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
-
-      tbl2.setTableName(tblName2);
-      tbl2.setParameters(new HashMap<String, String>());
-      tbl2.getParameters().put("EXTERNAL", "TRUE");
-      tbl2.getSd().setLocation(tbl.getSd().getLocation() + "-2");
-
-      List<FieldSchema> fieldSchemas = client.getFields(dbName, tblName);
-      assertNotNull(fieldSchemas);
-      assertEquals(fieldSchemas.size(), tbl.getSd().getCols().size());
-      for (FieldSchema fs : tbl.getSd().getCols()) {
-        assertTrue(fieldSchemas.contains(fs));
-      }
-
-      List<FieldSchema> fieldSchemasFull = client.getSchema(dbName, tblName);
-      assertNotNull(fieldSchemasFull);
-      assertEquals(fieldSchemasFull.size(), tbl.getSd().getCols().size()
-          + tbl.getPartitionKeys().size());
-      for (FieldSchema fs : tbl.getSd().getCols()) {
-        assertTrue(fieldSchemasFull.contains(fs));
-      }
-      for (FieldSchema fs : tbl.getPartitionKeys()) {
-        assertTrue(fieldSchemasFull.contains(fs));
-      }
-
-      client.createTable(tbl2);
-      if (isThriftClient) {
-        tbl2 = client.getTable(tbl2.getDbName(), tbl2.getTableName());
-      }
-
-      Table tbl3 = client.getTable(dbName, tblName2);
-      assertNotNull(tbl3);
-      assertEquals(tbl3.getDbName(), dbName);
-      assertEquals(tbl3.getTableName(), tblName2);
-      assertEquals(tbl3.getSd().getCols().size(), typ1.getFields().size());
-      assertEquals(tbl3.getSd().isCompressed(), false);
-      assertEquals(tbl3.getSd().getNumBuckets(), 1);
-      assertEquals(tbl3.getSd().getLocation(), tbl2.getSd().getLocation());
-      assertEquals(tbl3.getParameters(), tbl2.getParameters());
-
-      fieldSchemas = client.getFields(dbName, tblName2);
-      assertNotNull(fieldSchemas);
-      assertEquals(fieldSchemas.size(), tbl2.getSd().getCols().size());
-      for (FieldSchema fs : tbl2.getSd().getCols()) {
-        assertTrue(fieldSchemas.contains(fs));
-      }
-
-      fieldSchemasFull = client.getSchema(dbName, tblName2);
-      assertNotNull(fieldSchemasFull);
-      assertEquals(fieldSchemasFull.size(), tbl2.getSd().getCols().size()
-          + tbl2.getPartitionKeys().size());
-      for (FieldSchema fs : tbl2.getSd().getCols()) {
-        assertTrue(fieldSchemasFull.contains(fs));
-      }
-      for (FieldSchema fs : tbl2.getPartitionKeys()) {
-        assertTrue(fieldSchemasFull.contains(fs));
-      }
-
-      assertEquals("Use this for comments etc", tbl2.getSd().getParameters()
-          .get("test_param_1"));
-      assertEquals("name", tbl2.getSd().getBucketCols().get(0));
-      assertTrue("Partition key list is not empty",
-          (tbl2.getPartitionKeys() == null)
-              || (tbl2.getPartitionKeys().size() == 0));
-
-      //test get_table_objects_by_name functionality
-      ArrayList<String> tableNames = new ArrayList<String>();
-      tableNames.add(tblName2);
-      tableNames.add(tblName);
-      tableNames.add(tblName2);
-      List<Table> foundTables = client.getTableObjectsByName(dbName, tableNames);
-
-      assertEquals(foundTables.size(), 2);
-      for (Table t: foundTables) {
-        if (t.getTableName().equals(tblName2)) {
-          assertEquals(t.getSd().getLocation(), tbl2.getSd().getLocation());
-        } else {
-          assertEquals(t.getTableName(), tblName);
-          assertEquals(t.getSd().getLocation(), tbl.getSd().getLocation());
-        }
-        assertEquals(t.getSd().getCols().size(), typ1.getFields().size());
-        assertEquals(t.getSd().isCompressed(), false);
-        assertEquals(foundTables.get(0).getSd().getNumBuckets(), 1);
-        assertNotNull(t.getSd().getSerdeInfo());
-        assertEquals(t.getDbName(), dbName);
-      }
-
-      tableNames.add(1, "table_that_doesnt_exist");
-      foundTables = client.getTableObjectsByName(dbName, tableNames);
-      assertEquals(foundTables.size(), 2);
-
-      InvalidOperationException ioe = null;
-      try {
-        foundTables = client.getTableObjectsByName(dbName, null);
-      } catch (InvalidOperationException e) {
-        ioe = e;
-      }
-      assertNotNull(ioe);
-      assertTrue("Table not found", ioe.getMessage().contains("null tables"));
-
-      UnknownDBException udbe = null;
-      try {
-        foundTables = client.getTableObjectsByName("db_that_doesnt_exist", tableNames);
-      } catch (UnknownDBException e) {
-        udbe = e;
-      }
-      assertNotNull(udbe);
-      assertTrue("DB not found", udbe.getMessage().contains("not find database db_that_doesnt_exist"));
-
-      udbe = null;
-      try {
-        foundTables = client.getTableObjectsByName("", tableNames);
-      } catch (UnknownDBException e) {
-        udbe = e;
-      }
-      assertNotNull(udbe);
-      assertTrue("DB not found", udbe.getMessage().contains("is null or empty"));
-
-      FileSystem fs = FileSystem.get((new Path(tbl.getSd().getLocation())).toUri(), hiveConf);
-      client.dropTable(dbName, tblName);
-      assertFalse(fs.exists(new Path(tbl.getSd().getLocation())));
-
-      client.dropTable(dbName, tblName2);
-      assertTrue(fs.exists(new Path(tbl2.getSd().getLocation())));
-
-      client.dropType(typeName);
-      client.dropDatabase(dbName);
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testSimpleTable() failed.");
-      throw e;
-    }
-  }
-
-  public void testColumnStatistics() throws Throwable {
-
-    String dbName = "columnstatstestdb";
-    String tblName = "tbl";
-    String typeName = "Person";
-    String tblOwner = "testowner";
-    int lastAccessed = 6796;
-
-    try {
-      cleanUp(dbName, tblName, typeName);
-      Database db = new Database();
-      db.setName(dbName);
-      client.createDatabase(db);
-      createTableForTestFilter(dbName,tblName, tblOwner, lastAccessed, true);
-
-      // Create a ColumnStatistics Obj
-      String[] colName = new String[]{"income", "name"};
-      double lowValue = 50000.21;
-      double highValue = 1200000.4525;
-      long numNulls = 3;
-      long numDVs = 22;
-      double avgColLen = 50.30;
-      long maxColLen = 102;
-      String[] colType = new String[] {"double", "string"};
-      boolean isTblLevel = true;
-      String partName = null;
-      List<ColumnStatisticsObj> statsObjs = new ArrayList<ColumnStatisticsObj>();
-
-      ColumnStatisticsDesc statsDesc = new ColumnStatisticsDesc();
-      statsDesc.setDbName(dbName);
-      statsDesc.setTableName(tblName);
-      statsDesc.setIsTblLevel(isTblLevel);
-      statsDesc.setPartName(partName);
-
-      ColumnStatisticsObj statsObj = new ColumnStatisticsObj();
-      statsObj.setColName(colName[0]);
-      statsObj.setColType(colType[0]);
-
-      ColumnStatisticsData statsData = new ColumnStatisticsData();
-      DoubleColumnStatsData numericStats = new DoubleColumnStatsData();
-      statsData.setDoubleStats(numericStats);
-
-      statsData.getDoubleStats().setHighValue(highValue);
-      statsData.getDoubleStats().setLowValue(lowValue);
-      statsData.getDoubleStats().setNumDVs(numDVs);
-      statsData.getDoubleStats().setNumNulls(numNulls);
-
-      statsObj.setStatsData(statsData);
-      statsObjs.add(statsObj);
-
-      statsObj = new ColumnStatisticsObj();
-      statsObj.setColName(colName[1]);
-      statsObj.setColType(colType[1]);
-
-      statsData = new ColumnStatisticsData();
-      StringColumnStatsData stringStats = new StringColumnStatsData();
-      statsData.setStringStats(stringStats);
-      statsData.getStringStats().setAvgColLen(avgColLen);
-      statsData.getStringStats().setMaxColLen(maxColLen);
-      statsData.getStringStats().setNumDVs(numDVs);
-      statsData.getStringStats().setNumNulls(numNulls);
-
-      statsObj.setStatsData(statsData);
-      statsObjs.add(statsObj);
-
-      ColumnStatistics colStats = new ColumnStatistics();
-      colStats.setStatsDesc(statsDesc);
-      colStats.setStatsObj(statsObjs);
-
-      // write stats objs persistently
-      client.updateTableColumnStatistics(colStats);
-
-      // retrieve the stats obj that was just written
-      ColumnStatistics colStats2 = client.getTableColumnStatistics(dbName, tblName, colName[0]);
-
-     // compare stats obj to ensure what we get is what we wrote
-      assertNotNull(colStats2);
-      assertEquals(colStats2.getStatsDesc().getDbName(), dbName);
-      assertEquals(colStats2.getStatsDesc().getTableName(), tblName);
-      assertEquals(colStats2.getStatsObj().get(0).getColName(), colName[0]);
-      assertEquals(colStats2.getStatsObj().get(0).getStatsData().getDoubleStats().getLowValue(),
-        lowValue);
-      assertEquals(colStats2.getStatsObj().get(0).getStatsData().getDoubleStats().getHighValue(),
-        highValue);
-      assertEquals(colStats2.getStatsObj().get(0).getStatsData().getDoubleStats().getNumNulls(),
-        numNulls);
-      assertEquals(colStats2.getStatsObj().get(0).getStatsData().getDoubleStats().getNumDVs(),
-        numDVs);
-      assertEquals(colStats2.getStatsDesc().isIsTblLevel(), isTblLevel);
-
-      // test delete column stats; if no col name is passed all column stats associated with the
-      // table is deleted
-      boolean status = client.deleteTableColumnStatistics(dbName, tblName, null);
-      assertTrue(status);
-      // try to query stats for a column for which stats doesn't exist
-      try {
-        colStats2 = client.getTableColumnStatistics(dbName, tblName, colName[1]);
-        assertTrue(true);
-      } catch (NoSuchObjectException e) {
-        System.out.println("Statistics for column=" + colName[1] + " not found");
-      }
-
-      colStats.setStatsDesc(statsDesc);
-      colStats.setStatsObj(statsObjs);
-
-      // update table level column stats
-      client.updateTableColumnStatistics(colStats);
-
-      // query column stats for column whose stats were updated in the previous call
-      colStats2 = client.getTableColumnStatistics(dbName, tblName, colName[0]);
-
-      // partition level column statistics test
-      // create a table with multiple partitions
-      cleanUp(dbName, tblName, typeName);
-
-      List<List<String>> values = new ArrayList<List<String>>();
-      values.add(makeVals("2008-07-01 14:13:12", "14"));
-      values.add(makeVals("2008-07-01 14:13:12", "15"));
-      values.add(makeVals("2008-07-02 14:13:12", "15"));
-      values.add(makeVals("2008-07-03 14:13:12", "151"));
-
-      createMultiPartitionTableSchema(dbName, tblName, typeName, values);
-
-      List<String> partitions = client.listPartitionNames(dbName, tblName, (short)-1);
-
-      partName = partitions.get(0);
-      isTblLevel = false;
-
-      // create a new columnstatistics desc to represent partition level column stats
-      statsDesc = new ColumnStatisticsDesc();
-      statsDesc.setDbName(dbName);
-      statsDesc.setTableName(tblName);
-      statsDesc.setPartName(partName);
-      statsDesc.setIsTblLevel(isTblLevel);
-
-      colStats = new ColumnStatistics();
-      colStats.setStatsDesc(statsDesc);
-      colStats.setStatsObj(statsObjs);
-
-     client.updatePartitionColumnStatistics(colStats);
-
-     colStats2 = client.getPartitionColumnStatistics(dbName, tblName, partName, colName[1]);
-
-     // compare stats obj to ensure what we get is what we wrote
-     assertNotNull(colStats2);
-     assertEquals(colStats2.getStatsDesc().getDbName(), dbName);
-     assertEquals(colStats2.getStatsDesc().getTableName(), tblName);
-     assertEquals(colStats.getStatsDesc().getPartName(), partName);
-     assertEquals(colStats2.getStatsObj().get(0).getColName(), colName[1]);
-     assertEquals(colStats2.getStatsObj().get(0).getStatsData().getStringStats().getMaxColLen(),
-       maxColLen);
-     assertEquals(colStats2.getStatsObj().get(0).getStatsData().getStringStats().getAvgColLen(),
-       avgColLen);
-     assertEquals(colStats2.getStatsObj().get(0).getStatsData().getStringStats().getNumNulls(),
-       numNulls);
-     assertEquals(colStats2.getStatsObj().get(0).getStatsData().getStringStats().getNumDVs(),
-       numDVs);
-     assertEquals(colStats2.getStatsDesc().isIsTblLevel(), isTblLevel);
-
-     // test stats deletion at partition level
-     client.deletePartitionColumnStatistics(dbName, tblName, partName, colName[1]);
-
-     colStats2 = client.getPartitionColumnStatistics(dbName, tblName, partName, colName[0]);
-
-     // test get stats on a column for which stats doesn't exist
-     try {
-       colStats2 = client.getPartitionColumnStatistics(dbName, tblName, partName, colName[1]);
-       assertTrue(true);
-     } catch (NoSuchObjectException e) {
-       System.out.println("Statistics for column=" + colName[1] + " not found");
-     }
-
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testColumnStatistics() failed.");
-      throw e;
-    } finally {
-      cleanUp(dbName, tblName, typeName);
-    }
-  }
-
-  public void testAlterTable() throws Exception {
-    String dbName = "alterdb";
-    String invTblName = "alter-tbl";
-    String tblName = "altertbl";
-
-    try {
-      client.dropTable(dbName, tblName);
-      silentDropDatabase(dbName);
-
-      Database db = new Database();
-      db.setName(dbName);
-      client.createDatabase(db);
-
-      ArrayList<FieldSchema> invCols = new ArrayList<FieldSchema>(2);
-      invCols.add(new FieldSchema("n-ame", serdeConstants.STRING_TYPE_NAME, ""));
-      invCols.add(new FieldSchema("in.come", serdeConstants.INT_TYPE_NAME, ""));
-
-      Table tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(invTblName);
-      StorageDescriptor sd = new StorageDescriptor();
-      tbl.setSd(sd);
-      sd.setCols(invCols);
-      sd.setCompressed(false);
-      sd.setNumBuckets(1);
-      sd.setParameters(new HashMap<String, String>());
-      sd.getParameters().put("test_param_1", "Use this for comments etc");
-      sd.setBucketCols(new ArrayList<String>(2));
-      sd.getBucketCols().add("name");
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters().put(
-          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
-      boolean failed = false;
-      try {
-        client.createTable(tbl);
-      } catch (InvalidObjectException ex) {
-        failed = true;
-      }
-      if (!failed) {
-        assertTrue("Able to create table with invalid name: " + invTblName,
-            false);
-      }
-
-      // create an invalid table which has wrong column type
-      ArrayList<FieldSchema> invColsInvType = new ArrayList<FieldSchema>(2);
-      invColsInvType.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-      invColsInvType.add(new FieldSchema("income", "xyz", ""));
-      tbl.setTableName(tblName);
-      tbl.getSd().setCols(invColsInvType);
-      boolean failChecker = false;
-      try {
-        client.createTable(tbl);
-      } catch (InvalidObjectException ex) {
-        failChecker = true;
-      }
-      if (!failChecker) {
-        assertTrue("Able to create table with invalid column type: " + invTblName,
-            false);
-      }
-
-      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
-      cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-      cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-
-      // create a valid table
-      tbl.setTableName(tblName);
-      tbl.getSd().setCols(cols);
-      client.createTable(tbl);
-
-      if (isThriftClient) {
-        tbl = client.getTable(tbl.getDbName(), tbl.getTableName());
-      }
-
-      // now try to invalid alter table
-      Table tbl2 = client.getTable(dbName, tblName);
-      failed = false;
-      try {
-        tbl2.setTableName(invTblName);
-        tbl2.getSd().setCols(invCols);
-        client.alter_table(dbName, tblName, tbl2);
-      } catch (InvalidOperationException ex) {
-        failed = true;
-      }
-      if (!failed) {
-        assertTrue("Able to rename table with invalid name: " + invTblName,
-            false);
-      }
-
-      //try an invalid alter table with partition key name
-      Table tbl_pk = client.getTable(tbl.getDbName(), tbl.getTableName());
-      List<FieldSchema> partitionKeys = tbl_pk.getPartitionKeys();
-      for (FieldSchema fs : partitionKeys) {
-        fs.setName("invalid_to_change_name");
-        fs.setComment("can_change_comment");
-      }
-      tbl_pk.setPartitionKeys(partitionKeys);
-      try {
-        client.alter_table(dbName, tblName, tbl_pk);
-      } catch (InvalidOperationException ex) {
-        failed = true;
-      }
-      assertTrue("Should not have succeeded in altering partition key name", failed);
-
-      //try a valid alter table partition key comment
-      failed = false;
-      tbl_pk = client.getTable(tbl.getDbName(), tbl.getTableName());
-      partitionKeys = tbl_pk.getPartitionKeys();
-      for (FieldSchema fs : partitionKeys) {
-        fs.setComment("can_change_comment");
-      }
-      tbl_pk.setPartitionKeys(partitionKeys);
-      try {
-        client.alter_table(dbName, tblName, tbl_pk);
-      } catch (InvalidOperationException ex) {
-        failed = true;
-      }
-      assertFalse("Should not have failed alter table partition comment", failed);
-      Table newT = client.getTable(tbl.getDbName(), tbl.getTableName());
-      assertEquals(partitionKeys, newT.getPartitionKeys());
-
-      // try a valid alter table
-      tbl2.setTableName(tblName + "_renamed");
-      tbl2.getSd().setCols(cols);
-      tbl2.getSd().setNumBuckets(32);
-      client.alter_table(dbName, tblName, tbl2);
-      Table tbl3 = client.getTable(dbName, tbl2.getTableName());
-      assertEquals("Alter table didn't succeed. Num buckets is different ",
-          tbl2.getSd().getNumBuckets(), tbl3.getSd().getNumBuckets());
-      // check that data has moved
-      FileSystem fs = FileSystem.get((new Path(tbl.getSd().getLocation())).toUri(), hiveConf);
-      assertFalse("old table location still exists", fs.exists(new Path(tbl
-          .getSd().getLocation())));
-      assertTrue("data did not move to new location", fs.exists(new Path(tbl3
-          .getSd().getLocation())));
-
-      if (!isThriftClient) {
-        assertEquals("alter table didn't move data correct location", tbl3
-            .getSd().getLocation(), tbl2.getSd().getLocation());
-      }
-
-      // alter table with invalid column type
-      tbl_pk.getSd().setCols(invColsInvType);
-      failed = false;
-      try {
-        client.alter_table(dbName, tbl2.getTableName(), tbl_pk);
-      } catch (InvalidOperationException ex) {
-        failed = true;
-      }
-      assertTrue("Should not have succeeded in altering column", failed);
-
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testSimpleTable() failed.");
-      throw e;
-    } finally {
-      silentDropDatabase(dbName);
-    }
-  }
-
-  public void testComplexTable() throws Exception {
-
-    String dbName = "compdb";
-    String tblName = "comptbl";
-    String typeName = "Person";
-
-    try {
-      client.dropTable(dbName, tblName);
-      silentDropDatabase(dbName);
-      Database db = new Database();
-      db.setName(dbName);
-      client.createDatabase(db);
-
-      client.dropType(typeName);
-      Type typ1 = new Type();
-      typ1.setName(typeName);
-      typ1.setFields(new ArrayList<FieldSchema>(2));
-      typ1.getFields().add(
-          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-      typ1.getFields().add(
-          new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-      client.createType(typ1);
-
-      Table tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(tblName);
-      StorageDescriptor sd = new StorageDescriptor();
-      tbl.setSd(sd);
-      sd.setCols(typ1.getFields());
-      sd.setCompressed(false);
-      sd.setNumBuckets(1);
-      sd.setParameters(new HashMap<String, String>());
-      sd.getParameters().put("test_param_1", "Use this for comments etc");
-      sd.setBucketCols(new ArrayList<String>(2));
-      sd.getBucketCols().add("name");
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters().put(
-          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "9");
-      sd.getSerdeInfo().setSerializationLib(
-          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
-
-      tbl.setPartitionKeys(new ArrayList<FieldSchema>(2));
-      tbl.getPartitionKeys().add(
-          new FieldSchema("ds",
-              org.apache.hadoop.hive.serde.serdeConstants.DATE_TYPE_NAME, ""));
-      tbl.getPartitionKeys().add(
-          new FieldSchema("hr",
-              org.apache.hadoop.hive.serde.serdeConstants.INT_TYPE_NAME, ""));
-
-      client.createTable(tbl);
-
-      Table tbl2 = client.getTable(dbName, tblName);
-      assertEquals(tbl2.getDbName(), dbName);
-      assertEquals(tbl2.getTableName(), tblName);
-      assertEquals(tbl2.getSd().getCols().size(), typ1.getFields().size());
-      assertFalse(tbl2.getSd().isCompressed());
-      assertFalse(tbl2.getSd().isStoredAsSubDirectories());
-      assertEquals(tbl2.getSd().getNumBuckets(), 1);
-
-      assertEquals("Use this for comments etc", tbl2.getSd().getParameters()
-          .get("test_param_1"));
-      assertEquals("name", tbl2.getSd().getBucketCols().get(0));
-
-      assertNotNull(tbl2.getPartitionKeys());
-      assertEquals(2, tbl2.getPartitionKeys().size());
-      assertEquals(serdeConstants.DATE_TYPE_NAME, tbl2.getPartitionKeys().get(0)
-          .getType());
-      assertEquals(serdeConstants.INT_TYPE_NAME, tbl2.getPartitionKeys().get(1)
-          .getType());
-      assertEquals("ds", tbl2.getPartitionKeys().get(0).getName());
-      assertEquals("hr", tbl2.getPartitionKeys().get(1).getName());
-
-      List<FieldSchema> fieldSchemas = client.getFields(dbName, tblName);
-      assertNotNull(fieldSchemas);
-      assertEquals(fieldSchemas.size(), tbl.getSd().getCols().size());
-      for (FieldSchema fs : tbl.getSd().getCols()) {
-        assertTrue(fieldSchemas.contains(fs));
-      }
-
-      List<FieldSchema> fieldSchemasFull = client.getSchema(dbName, tblName);
-      assertNotNull(fieldSchemasFull);
-      assertEquals(fieldSchemasFull.size(), tbl.getSd().getCols().size()
-          + tbl.getPartitionKeys().size());
-      for (FieldSchema fs : tbl.getSd().getCols()) {
-        assertTrue(fieldSchemasFull.contains(fs));
-      }
-      for (FieldSchema fs : tbl.getPartitionKeys()) {
-        assertTrue(fieldSchemasFull.contains(fs));
-      }
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testComplexTable() failed.");
-      throw e;
-    } finally {
-      client.dropTable(dbName, tblName);
-      boolean ret = client.dropType(typeName);
-      assertTrue("Unable to drop type " + typeName, ret);
-      client.dropDatabase(dbName);
-    }
-  }
-
-  public void testTableDatabase() throws Exception {
-    String dbName = "testDb";
-    String tblName_1 = "testTbl_1";
-    String tblName_2 = "testTbl_2";
-
-    try {
-      silentDropDatabase(dbName);
-
-      Database db = new Database();
-      db.setName(dbName);
-      String dbLocation =
-          HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREWAREHOUSE) + "_testDB_table_create_";
-      db.setLocationUri(dbLocation);
-      client.createDatabase(db);
-      db = client.getDatabase(dbName);
-
-      Table tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(tblName_1);
-
-      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
-      cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-      cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-
-      StorageDescriptor sd = new StorageDescriptor();
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters().put(
-          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "9");
-      sd.getSerdeInfo().setSerializationLib(
-          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
-
-      tbl.setSd(sd);
-      tbl.getSd().setCols(cols);
-      client.createTable(tbl);
-      tbl = client.getTable(dbName, tblName_1);
-
-      Path path = new Path(tbl.getSd().getLocation());
-      System.err.println("Table's location " + path + ", Database's location " + db.getLocationUri());
-      assertEquals("Table location is not a subset of the database location",
-          path.getParent().toString(), db.getLocationUri());
-
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testTableDatabase() failed.");
-      throw e;
-    } finally {
-      silentDropDatabase(dbName);
-    }
-  }
-
-
-  public void testGetConfigValue() {
-
-    String val = "value";
-
-    if (!isThriftClient) {
-      try {
-        assertEquals(client.getConfigValue("hive.key1", val), "value1");
-        assertEquals(client.getConfigValue("hive.key2", val), "http://www.example.com");
-        assertEquals(client.getConfigValue("hive.key3", val), "");
-        assertEquals(client.getConfigValue("hive.key4", val), "0");
-        assertEquals(client.getConfigValue("hive.key5", val), val);
-        assertEquals(client.getConfigValue(null, val), val);
-      } catch (ConfigValSecurityException e) {
-        e.printStackTrace();
-        assert (false);
-      } catch (TException e) {
-        e.printStackTrace();
-        assert (false);
-      }
-    }
-
-    boolean threwException = false;
-    try {
-      // Attempting to get the password should throw an exception
-      client.getConfigValue("javax.jdo.option.ConnectionPassword", "password");
-    } catch (ConfigValSecurityException e) {
-      threwException = true;
-    } catch (TException e) {
-      e.printStackTrace();
-      assert (false);
-    }
-    assert (threwException);
-  }
-
-  private static void adjust(HiveMetaStoreClient client, Partition part,
-      String dbName, String tblName)
-  throws NoSuchObjectException, MetaException, TException {
-    Partition part_get = client.getPartition(dbName, tblName, part.getValues());
-    part.setCreateTime(part_get.getCreateTime());
-    part.putToParameters(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.DDL_TIME, Long.toString(part_get.getCreateTime()));
-  }
-
-  private static void silentDropDatabase(String dbName) throws MetaException, TException {
-    try {
-      for (String tableName : client.getTables(dbName, "*")) {
-        client.dropTable(dbName, tableName);
-      }
-      client.dropDatabase(dbName);
-    } catch (NoSuchObjectException e) {
-    } catch (InvalidOperationException e) {
-    }
-  }
-
-  /**
-   * Tests for list partition by filter functionality.
-   * @throws Exception
-   */
-
-  public void testPartitionFilter() throws Exception {
-    String dbName = "filterdb";
-    String tblName = "filtertbl";
-
-    silentDropDatabase(dbName);
-
-    Database db = new Database();
-    db.setName(dbName);
-    client.createDatabase(db);
-
-    ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
-    cols.add(new FieldSchema("c1", serdeConstants.STRING_TYPE_NAME, ""));
-    cols.add(new FieldSchema("c2", serdeConstants.INT_TYPE_NAME, ""));
-
-    ArrayList<FieldSchema> partCols = new ArrayList<FieldSchema>(3);
-    partCols.add(new FieldSchema("p1", serdeConstants.STRING_TYPE_NAME, ""));
-    partCols.add(new FieldSchema("p2", serdeConstants.STRING_TYPE_NAME, ""));
-    partCols.add(new FieldSchema("p3", serdeConstants.INT_TYPE_NAME, ""));
-
-    Table tbl = new Table();
-    tbl.setDbName(dbName);
-    tbl.setTableName(tblName);
-    StorageDescriptor sd = new StorageDescriptor();
-    tbl.setSd(sd);
-    sd.setCols(cols);
-    sd.setCompressed(false);
-    sd.setNumBuckets(1);
-    sd.setParameters(new HashMap<String, String>());
-    sd.setBucketCols(new ArrayList<String>());
-    sd.setSerdeInfo(new SerDeInfo());
-    sd.getSerdeInfo().setName(tbl.getTableName());
-    sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-    sd.getSerdeInfo().getParameters()
-        .put(serdeConstants.SERIALIZATION_FORMAT, "1");
-    sd.setSortCols(new ArrayList<Order>());
-
-    tbl.setPartitionKeys(partCols);
-    client.createTable(tbl);
-
-    tbl = client.getTable(dbName, tblName);
-
-    add_partition(client, tbl, Lists.newArrayList("p11", "p21", "31"), "part1");
-    add_partition(client, tbl, Lists.newArrayList("p11", "p22", "32"), "part2");
-    add_partition(client, tbl, Lists.newArrayList("p12", "p21", "31"), "part3");
-    add_partition(client, tbl, Lists.newArrayList("p12", "p23", "32"), "part4");
-    add_partition(client, tbl, Lists.newArrayList("p13", "p24", "31"), "part5");
-    add_partition(client, tbl, Lists.newArrayList("p13", "p25", "-33"), "part6");
-
-    // Test equals operator for strings and integers.
-    checkFilter(client, dbName, tblName, "p1 = \"p11\"", 2);
-    checkFilter(client, dbName, tblName, "p1 = \"p12\"", 2);
-    checkFilter(client, dbName, tblName, "p2 = \"p21\"", 2);
-    checkFilter(client, dbName, tblName, "p2 = \"p23\"", 1);
-    checkFilter(client, dbName, tblName, "p3 = 31", 3);
-    checkFilter(client, dbName, tblName, "p3 = 33", 0);
-    checkFilter(client, dbName, tblName, "p3 = -33", 1);
-    checkFilter(client, dbName, tblName, "p1 = \"p11\" and p2=\"p22\"", 1);
-    checkFilter(client, dbName, tblName, "p1 = \"p11\" or p2=\"p23\"", 3);
-    checkFilter(client, dbName, tblName, "p1 = \"p11\" or p1=\"p12\"", 4);
-    checkFilter(client, dbName, tblName, "p1 = \"p11\" or p1=\"p12\"", 4);
-    checkFilter(client, dbName, tblName, "p1 = \"p11\" or p1=\"p12\"", 4);
-    checkFilter(client, dbName, tblName, "p1 = \"p11\" and p3 = 31", 1);
-    checkFilter(client, dbName, tblName, "p3 = -33 or p1 = \"p12\"", 3);
-
-    // Test not-equals operator for strings and integers.
-    checkFilter(client, dbName, tblName, "p1 != \"p11\"", 4);
-    checkFilter(client, dbName, tblName, "p2 != \"p23\"", 5);
-    checkFilter(client, dbName, tblName, "p2 != \"p33\"", 6);
-    checkFilter(client, dbName, tblName, "p3 != 32", 4);
-    checkFilter(client, dbName, tblName, "p3 != 8589934592", 6);
-    checkFilter(client, dbName, tblName, "p1 != \"p11\" and p1 != \"p12\"", 2);
-    checkFilter(client, dbName, tblName, "p1 != \"p11\" and p2 != \"p22\"", 4);
-    checkFilter(client, dbName, tblName, "p1 != \"p11\" or p2 != \"p22\"", 5);
-    checkFilter(client, dbName, tblName, "p1 != \"p12\" and p2 != \"p25\"", 3);
-    checkFilter(client, dbName, tblName, "p1 != \"p12\" or p2 != \"p25\"", 6);
-    checkFilter(client, dbName, tblName, "p3 != -33 or p1 != \"p13\"", 5);
-    checkFilter(client, dbName, tblName, "p1 != \"p11\" and p3 = 31", 2);
-    checkFilter(client, dbName, tblName, "p3 != 31 and p1 = \"p12\"", 1);
-
-    // Test reverse order.
-    checkFilter(client, dbName, tblName, "31 != p3 and p1 = \"p12\"", 1);
-    checkFilter(client, dbName, tblName, "\"p23\" = p2", 1);
-
-    // Test and/or more...
-    checkFilter(client, dbName, tblName,
-        "p1 = \"p11\" or (p1=\"p12\" and p2=\"p21\")", 3);
-    checkFilter(client, dbName, tblName,
-       "p1 = \"p11\" or (p1=\"p12\" and p2=\"p21\") Or " +
-       "(p1=\"p13\" aNd p2=\"p24\")", 4);
-    //test for and or precedence
-    checkFilter(client, dbName, tblName,
-       "p1=\"p12\" and (p2=\"p27\" Or p2=\"p21\")", 1);
-    checkFilter(client, dbName, tblName,
-       "p1=\"p12\" and p2=\"p27\" Or p2=\"p21\"", 2);
-
-    // Test gt/lt/lte/gte/like for strings.
-    checkFilter(client, dbName, tblName, "p1 > \"p12\"", 2);
-    checkFilter(client, dbName, tblName, "p1 >= \"p12\"", 4);
-    checkFilter(client, dbName, tblName, "p1 < \"p12\"", 2);
-    checkFilter(client, dbName, tblName, "p1 <= \"p12\"", 4);
-    checkFilter(client, dbName, tblName, "p1 like \"p1.*\"", 6);
-    checkFilter(client, dbName, tblName, "p2 like \"p.*3\"", 1);
-
-    // Test gt/lt/lte/gte for numbers.
-    checkFilter(client, dbName, tblName, "p3 < 0", 1);
-    checkFilter(client, dbName, tblName, "p3 >= -33", 6);
-    checkFilter(client, dbName, tblName, "p3 > -33", 5);
-    checkFilter(client, dbName, tblName, "p3 > 31 and p3 < 32", 0);
-    checkFilter(client, dbName, tblName, "p3 > 31 or p3 < 31", 3);
-    checkFilter(client, dbName, tblName, "p3 > 30 or p3 < 30", 6);
-    checkFilter(client, dbName, tblName, "p3 >= 31 or p3 < -32", 6);
-    checkFilter(client, dbName, tblName, "p3 >= 32", 2);
-    checkFilter(client, dbName, tblName, "p3 > 32", 0);
-
-    // Test between
-    checkFilter(client, dbName, tblName, "p1 between \"p11\" and \"p12\"", 4);
-    checkFilter(client, dbName, tblName, "p1 not between \"p11\" and \"p12\"", 2);
-    checkFilter(client, dbName, tblName, "p3 not between 0 and 2", 6);
-    checkFilter(client, dbName, tblName, "p3 between 31 and 32", 5);
-    checkFilter(client, dbName, tblName, "p3 between 32 and 31", 0);
-    checkFilter(client, dbName, tblName, "p3 between -32 and 34 and p3 not between 31 and 32", 0);
-    checkFilter(client, dbName, tblName, "p3 between 1 and 3 or p3 not between 1 and 3", 6);
-    checkFilter(client, dbName, tblName,
-        "p3 between 31 and 32 and p1 between \"p12\" and \"p14\"", 3);
-
-    //Test for setting the maximum partition count
-    List<Partition> partitions = client.listPartitionsByFilter(dbName,
-        tblName, "p1 >= \"p12\"", (short) 2);
-    assertEquals("User specified row limit for partitions",
-        2, partitions.size());
-
-    //Negative tests
-    Exception me = null;
-    try {
-      client.listPartitionsByFilter(dbName,
-          tblName, "p3 >= \"p12\"", (short) -1);
-    } catch(MetaException e) {
-      me = e;
-    }
-    assertNotNull(me);
-    assertTrue("Filter on int partition key", me.getMessage().contains(
-          "Filtering is supported only on partition keys of type string"));
-
-    me = null;
-    try {
-      client.listPartitionsByFilter(dbName,
-          tblName, "c1 >= \"p12\"", (short) -1);
-    } catch(MetaException e) {
-      me = e;
-    }
-    assertNotNull(me);
-    assertTrue("Filter on invalid key", me.getMessage().contains(
-          "<c1> is not a partitioning key for the table"));
-
-    me = null;
-    try {
-      client.listPartitionsByFilter(dbName,
-          tblName, "c1 >= ", (short) -1);
-    } catch(MetaException e) {
-      me = e;
-    }
-    assertNotNull(me);
-    assertTrue("Invalid filter string", me.getMessage().contains(
-          "Error parsing partition filter"));
-
-    me = null;
-    try {
-      client.listPartitionsByFilter("invDBName",
-          "invTableName", "p1 = \"p11\"", (short) -1);
-    } catch(NoSuchObjectException e) {
-      me = e;
-    }
-    assertNotNull(me);
-    assertTrue("NoSuchObject exception", me.getMessage().contains(
-          "database/table does not exist"));
-
-    client.dropTable(dbName, tblName);
-    client.dropDatabase(dbName);
-  }
-
-
-  /**
-   * Test filtering on table with single partition
-   * @throws Exception
-   */
-  public void testFilterSinglePartition() throws Exception {
-      String dbName = "filterdb";
-      String tblName = "filtertbl";
-
-      List<String> vals = new ArrayList<String>(1);
-      vals.add("p11");
-      List <String> vals2 = new ArrayList<String>(1);
-      vals2.add("p12");
-      List <String> vals3 = new ArrayList<String>(1);
-      vals3.add("p13");
-
-      silentDropDatabase(dbName);
-
-      Database db = new Database();
-      db.setName(dbName);
-      client.createDatabase(db);
-
-      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
-      cols.add(new FieldSchema("c1", serdeConstants.STRING_TYPE_NAME, ""));
-      cols.add(new FieldSchema("c2", serdeConstants.INT_TYPE_NAME, ""));
-
-      ArrayList<FieldSchema> partCols = new ArrayList<FieldSchema>(1);
-      partCols.add(new FieldSchema("p1", serdeConstants.STRING_TYPE_NAME, ""));
-
-      Table tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(tblName);
-      StorageDescriptor sd = new StorageDescriptor();
-      tbl.setSd(sd);
-      sd.setCols(cols);
-      sd.setCompressed(false);
-      sd.setNumBuckets(1);
-      sd.setParameters(new HashMap<String, String>());
-      sd.setBucketCols(new ArrayList<String>());
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters()
-          .put(serdeConstants.SERIALIZATION_FORMAT, "1");
-      sd.setSortCols(new ArrayList<Order>());
-
-      tbl.setPartitionKeys(partCols);
-      client.createTable(tbl);
-
-      tbl = client.getTable(dbName, tblName);
-
-      add_partition(client, tbl, vals, "part1");
-      add_partition(client, tbl, vals2, "part2");
-      add_partition(client, tbl, vals3, "part3");
-
-      checkFilter(client, dbName, tblName, "p1 = \"p12\"", 1);
-      checkFilter(client, dbName, tblName, "p1 < \"p12\"", 1);
-      checkFilter(client, dbName, tblName, "p1 > \"p12\"", 1);
-      checkFilter(client, dbName, tblName, "p1 >= \"p12\"", 2);
-      checkFilter(client, dbName, tblName, "p1 <= \"p12\"", 2);
-      checkFilter(client, dbName, tblName, "p1 <> \"p12\"", 2);
-      checkFilter(client, dbName, tblName, "p1 like \"p1.*\"", 3);
-      checkFilter(client, dbName, tblName, "p1 like \"p.*2\"", 1);
-
-      client.dropTable(dbName, tblName);
-      client.dropDatabase(dbName);
-  }
-
-  /**
-   * Test filtering based on the value of the last partition
-   * @throws Exception
-   */
-  public void testFilterLastPartition() throws Exception {
-      String dbName = "filterdb";
-      String tblName = "filtertbl";
-
-      List<String> vals = new ArrayList<String>(2);
-      vals.add("p11");
-      vals.add("p21");
-      List <String> vals2 = new ArrayList<String>(2);
-      vals2.add("p11");
-      vals2.add("p22");
-      List <String> vals3 = new ArrayList<String>(2);
-      vals3.add("p12");
-      vals3.add("p21");
-
-      cleanUp(dbName, tblName, null);
-
-      createDb(dbName);
-
-      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
-      cols.add(new FieldSchema("c1", serdeConstants.STRING_TYPE_NAME, ""));
-      cols.add(new FieldSchema("c2", serdeConstants.INT_TYPE_NAME, ""));
-
-      ArrayList<FieldSchema> partCols = new ArrayList<FieldSchema>(2);
-      partCols.add(new FieldSchema("p1", serdeConstants.STRING_TYPE_NAME, ""));
-      partCols.add(new FieldSchema("p2", serdeConstants.STRING_TYPE_NAME, ""));
-
-      Map<String, String> serdParams = new HashMap<String, String>();
-      serdParams.put(serdeConstants.SERIALIZATION_FORMAT, "1");
-      StorageDescriptor sd = createStorageDescriptor(tblName, partCols, null, serdParams);
-
-      Table tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(tblName);
-      tbl.setSd(sd);
-      tbl.setPartitionKeys(partCols);
-      client.createTable(tbl);
-      tbl = client.getTable(dbName, tblName);
-
-      add_partition(client, tbl, vals, "part1");
-      add_partition(client, tbl, vals2, "part2");
-      add_partition(client, tbl, vals3, "part3");
-
-      checkFilter(client, dbName, tblName, "p2 = \"p21\"", 2);
-      checkFilter(client, dbName, tblName, "p2 < \"p23\"", 3);
-      checkFilter(client, dbName, tblName, "p2 > \"p21\"", 1);
-      checkFilter(client, dbName, tblName, "p2 >= \"p21\"", 3);
-      checkFilter(client, dbName, tblName, "p2 <= \"p21\"", 2);
-      checkFilter(client, dbName, tblName, "p2 <> \"p12\"", 3);
-      checkFilter(client, dbName, tblName, "p2 != \"p12\"", 3);
-      checkFilter(client, dbName, tblName, "p2 like \"p2.*\"", 3);
-      checkFilter(client, dbName, tblName, "p2 like \"p.*2\"", 1);
-
-      try {
-        checkFilter(client, dbName, tblName, "p2 !< 'dd'", 0);
-        fail("Invalid operator not detected");
-      } catch (MetaException e) {
-        // expected exception due to lexer error
-      }
-
-      cleanUp(dbName, tblName, null);
-  }
-
-  private void checkFilter(HiveMetaStoreClient client, String dbName,
-        String tblName, String filter, int expectedCount)
-        throws MetaException, NoSuchObjectException, TException {
-    LOG.debug("Testing filter: " + filter);
-    List<Partition> partitions = client.listPartitionsByFilter(dbName,
-            tblName, filter, (short) -1);
-
-    assertEquals("Partition count expected for filter " + filter,
-            expectedCount, partitions.size());
-  }
-
-  private void add_partition(HiveMetaStoreClient client, Table table,
-      List<String> vals, String location) throws InvalidObjectException,
-        AlreadyExistsException, MetaException, TException {
-
-    Partition part = new Partition();
-    part.setDbName(table.getDbName());
-    part.setTableName(table.getTableName());
-    part.setValues(vals);
-    part.setParameters(new HashMap<String, String>());
-    part.setSd(table.getSd().deepCopy());
-    part.getSd().setSerdeInfo(table.getSd().getSerdeInfo());
-    part.getSd().setLocation(table.getSd().getLocation() + location);
-
-    client.add_partition(part);
-  }
-
-  /**
-   * Tests {@link HiveMetaStoreClient#newSynchronizedClient}.  Does not
-   * actually test multithreading, but does verify that the proxy
-   * at least works correctly.
-   */
-  public void testSynchronized() throws Exception {
-    int currentNumberOfDbs = client.getAllDatabases().size();
-
-    IMetaStoreClient synchronizedClient =
-      HiveMetaStoreClient.newSynchronizedClient(client);
-    List<String> databases = synchronizedClient.getAllDatabases();
-    assertEquals(currentNumberOfDbs, databases.size());
-  }
-
-  public void testTableFilter() throws Exception {
-    try {
-      String dbName = "testTableFilter";
-      String owner1 = "testOwner1";
-      String owner2 = "testOwner2";
-      int lastAccessTime1 = 90;
-      int lastAccessTime2 = 30;
-      String tableName1 = "table1";
-      String tableName2 = "table2";
-      String tableName3 = "table3";
-
-      client.dropTable(dbName, tableName1);
-      client.dropTable(dbName, tableName2);
-      client.dropTable(dbName, tableName3);
-      silentDropDatabase(dbName);
-      Database db = new Database();
-      db.setName(dbName);
-      db.setDescription("Alter Partition Test database");
-      client.createDatabase(db);
-
-      Table table1 = createTableForTestFilter(dbName,tableName1, owner1, lastAccessTime1, true);
-      Table table2 = createTableForTestFilter(dbName,tableName2, owner2, lastAccessTime2, true);
-      Table table3 = createTableForTestFilter(dbName,tableName3, owner1, lastAccessTime2, false);
-
-      List<String> tableNames;
-      String filter;
-      //test owner
-      //owner like ".*Owner.*" and owner like "test.*"
-      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_OWNER +
-          " like \".*Owner.*\" and " +
-          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_OWNER +
-          " like  \"test.*\"";
-      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
-      assertEquals(tableNames.size(), 3);
-      assert(tableNames.contains(table1.getTableName()));
-      assert(tableNames.contains(table2.getTableName()));
-      assert(tableNames.contains(table3.getTableName()));
-
-      //owner = "testOwner1"
-      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_OWNER +
-          " = \"testOwner1\"";
-
-      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
-      assertEquals(2, tableNames.size());
-      assert(tableNames.contains(table1.getTableName()));
-      assert(tableNames.contains(table3.getTableName()));
-
-      //lastAccessTime < 90
-      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_LAST_ACCESS +
-          " < 90";
-
-      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
-      assertEquals(2, tableNames.size());
-      assert(tableNames.contains(table2.getTableName()));
-      assert(tableNames.contains(table3.getTableName()));
-
-      //lastAccessTime > 90
-      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_LAST_ACCESS +
-      " > 90";
-
-      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
-      assertEquals(0, tableNames.size());
-
-      //test params
-      //test_param_2 = "50"
-      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
-          "test_param_2 = \"50\"";
-
-      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
-      assertEquals(2, tableNames.size());
-      assert(tableNames.contains(table1.getTableName()));
-      assert(tableNames.contains(table2.getTableName()));
-
-      //test_param_2 = "75"
-      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
-          "test_param_2 = \"75\"";
-
-      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
-      assertEquals(0, tableNames.size());
-
-      //key_dne = "50"
-      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
-          "key_dne = \"50\"";
-
-      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
-      assertEquals(0, tableNames.size());
-
-      //test_param_1 != "yellow"
-      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
-          "test_param_1 <> \"yellow\"";
-
-      tableNames = client.listTableNamesByFilter(dbName, filter, (short) 2);
-      assertEquals(2, tableNames.size());
-
-      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
-          "test_param_1 != \"yellow\"";
-
-      tableNames = client.listTableNamesByFilter(dbName, filter, (short) 2);
-      assertEquals(2, tableNames.size());
-
-      //owner = "testOwner1" and (lastAccessTime = 30 or test_param_1 = "hi")
-      filter = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_OWNER +
-        " = \"testOwner1\" and (" +
-        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_LAST_ACCESS +
-        " = 30 or " +
-        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.HIVE_FILTER_FIELD_PARAMS +
-        "test_param_1 = \"hi\")";
-      tableNames = client.listTableNamesByFilter(dbName, filter, (short)-1);
-
-      assertEquals(2, tableNames.size());
-      assert(tableNames.contains(table1.getTableName()));
-      assert(tableNames.contains(table3.getTableName()));
-
-      //Negative tests
-      Exception me = null;
-      try {
-        filter = "badKey = \"testOwner1\"";
-        tableNames = client.listTableNamesByFilter(dbName, filter, (short) -1);
-      } catch(MetaException e) {
-        me = e;
-      }
-      assertNotNull(me);
-      assertTrue("Bad filter key test", me.getMessage().contains(
-            "Invalid key name in filter"));
-
-      client.dropTable(dbName, tableName1);
-      client.dropTable(dbName, tableName2);
-      client.dropTable(dbName, tableName3);
-      client.dropDatabase(dbName);
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testTableFilter() failed.");
-      throw e;
-    }
-  }
-
-  private Table createTableForTestFilter(String dbName, String tableName, String owner,
-    int lastAccessTime, boolean hasSecondParam) throws Exception {
-
-    ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
-    cols.add(new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-    cols.add(new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-
-    Map<String, String> params = new HashMap<String, String>();
-    params.put("sd_param_1", "Use this for comments etc");
-
-    Map<String, String> serdParams = new HashMap<String, String>();
-    serdParams.put(serdeConstants.SERIALIZATION_FORMAT, "1");
-
-    StorageDescriptor sd = createStorageDescriptor(tableName, cols, params, serdParams);
-
-    Map<String, String> partitionKeys = new HashMap<String, String>();
-    partitionKeys.put("ds", serdeConstants.STRING_TYPE_NAME);
-    partitionKeys.put("hr", serdeConstants.INT_TYPE_NAME);
-
-    Map<String, String> tableParams =  new HashMap<String, String>();
-    tableParams.put("test_param_1", "hi");
-    if(hasSecondParam) {
-      tableParams.put("test_param_2", "50");
-    }
-
-    Table tbl = createTable(dbName, tableName, owner, tableParams,
-        partitionKeys, sd, lastAccessTime);
-
-    if (isThriftClient) {
-      // the createTable() above does not update the location in the 'tbl'
-      // object when the client is a thrift client and the code below relies
-      // on the location being present in the 'tbl' object - so get the table
-      // from the metastore
-      tbl = client.getTable(dbName, tableName);
-    }
-    return tbl;
-  }
-  /**
-   * Verify that if another  client, either a metastore Thrift server or  a Hive CLI instance
-   * renames a table recently created by this instance, and hence potentially in its cache, the
-   * current instance still sees the change.
-   * @throws Exception
-   */
-  public void testConcurrentMetastores() throws Exception {
-    String dbName = "concurrentdb";
-    String tblName = "concurrenttbl";
-    String renameTblName = "rename_concurrenttbl";
-
-    try {
-      cleanUp(dbName, tblName, null);
-
-      createDb(dbName);
-
-      ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
-      cols.add(new FieldSchema("c1", serdeConstants.STRING_TYPE_NAME, ""));
-      cols.add(new FieldSchema("c2", serdeConstants.INT_TYPE_NAME, ""));
-
-      Map<String, String> params = new HashMap<String, String>();
-      params.put("test_param_1", "Use this for comments etc");
-
-      Map<String, String> serdParams = new HashMap<String, String>();
-      serdParams.put(serdeConstants.SERIALIZATION_FORMAT, "1");
-
-      StorageDescriptor sd =  createStorageDescriptor(tblName, cols, params, serdParams);
-
-      createTable(dbName, tblName, null, null, null, sd, 0);
-
-      // get the table from the client, verify the name is correct
-      Table tbl2 = client.getTable(dbName, tblName);
-
-      assertEquals("Client returned table with different name.", tbl2.getTableName(), tblName);
-
-      // Simulate renaming via another metastore Thrift server or another Hive CLI instance
-      updateTableNameInDB(tblName, renameTblName);
-
-      // get the table from the client again, verify the name has been updated
-      Table tbl3 = client.getTable(dbName, renameTblName);
-
-      assertEquals("Client returned table with different name after rename.",
-          tbl3.getTableName(), renameTblName);
-
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testConcurrentMetastores() failed.");
-      throw e;
-    } finally {
-      silentDropDatabase(dbName);
-    }
-  }
-
-
-
-  /**
-   * This method simulates another Hive metastore renaming a table, by accessing the db and
-   * updating the name.
-   *
-   * Unfortunately, derby cannot be run in two different JVMs simultaneously, but the only way
-   * to rename without having it put in this client's cache is to run a metastore in a separate JVM,
-   * so this simulation is required.
-   * @param oldTableName
-   * @param newTableName
-   * @throws SQLException
-   */
-  private void updateTableNameInDB(String oldTableName, String newTableName) throws SQLException {
-    String connectionStr = HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTORECONNECTURLKEY);
-    int interval= HiveConf.getIntVar(hiveConf, HiveConf.ConfVars.METASTOREINTERVAL);
-    int attempts = HiveConf.getIntVar(hiveConf, HiveConf.ConfVars.METASTOREATTEMPTS);
-
-
-    Utilities.SQLCommand<Void> execUpdate = new Utilities.SQLCommand<Void>() {
-      @Override
-      public Void run(PreparedStatement stmt) throws SQLException {
-        stmt.executeUpdate();
-        return null;
-      }
-    };
-
-    Connection conn = Utilities.connectWithRetry(connectionStr, interval, attempts);
-
-    PreparedStatement updateStmt = Utilities.prepareWithRetry(conn,
-        "UPDATE TBLS SET tbl_name = '" + newTableName + "' WHERE tbl_name = '" + oldTableName + "'",
-        interval, attempts);
-
-    Utilities.executeWithRetry(execUpdate, updateStmt, interval, attempts);
-  }
-
-  private void cleanUp(String dbName, String tableName, String typeName) throws Exception {
-    if(dbName != null && tableName != null) {
-      client.dropTable(dbName, tableName);
-    }
-    if(dbName != null) {
-      silentDropDatabase(dbName);
-    }
-    if(typeName != null) {
-      client.dropType(typeName);
-    }
-  }
-
-  private Database createDb(String dbName) throws Exception {
-    if(null == dbName) { return null; }
-    Database db = new Database();
-    db.setName(dbName);
-    client.createDatabase(db);
-    return db;
-  }
-
-  private Type createType(String typeName, Map<String, String> fields) throws Throwable {
-    Type typ1 = new Type();
-    typ1.setName(typeName);
-    typ1.setFields(new ArrayList<FieldSchema>(fields.size()));
-    for(String fieldName : fields.keySet()) {
-      typ1.getFields().add(
-          new FieldSchema(fieldName, fields.get(fieldName), ""));
-    }
-    client.createType(typ1);
-    return typ1;
-  }
-
-  private Table createTable(String dbName, String tblName, String owner,
-      Map<String,String> tableParams, Map<String, String> partitionKeys,
-      StorageDescriptor sd, int lastAccessTime) throws Exception {
-    Table tbl = new Table();
-    tbl.setDbName(dbName);
-    tbl.setTableName(tblName);
-    if(tableParams != null) {
-      tbl.setParameters(tableParams);
-    }
-
-    if(owner != null) {
-      tbl.setOwner(owner);
-    }
-
-    if(partitionKeys != null) {
-      tbl.setPartitionKeys(new ArrayList<FieldSchema>(partitionKeys.size()));
-      for(String key : partitionKeys.keySet()) {
-        tbl.getPartitionKeys().add(
-            new FieldSchema(key, partitionKeys.get(key), ""));
-      }
-    }
-
-    tbl.setSd(sd);
-    tbl.setLastAccessTime(lastAccessTime);
-
-    client.createTable(tbl);
-    return tbl;
-  }
-
-  private StorageDescriptor createStorageDescriptor(String tableName,
-    List<FieldSchema> cols, Map<String, String> params, Map<String, String> serdParams)  {
-    StorageDescriptor sd = new StorageDescriptor();
-
-    sd.setCols(cols);
-    sd.setCompressed(false);
-    sd.setNumBuckets(1);
-    sd.setParameters(params);
-    sd.setBucketCols(new ArrayList<String>(2));
-    sd.getBucketCols().add("name");
-    sd.setSerdeInfo(new SerDeInfo());
-    sd.getSerdeInfo().setName(tableName);
-    sd.getSerdeInfo().setParameters(serdParams);
-    sd.getSerdeInfo().getParameters()
-        .put(serdeConstants.SERIALIZATION_FORMAT, "1");
-    sd.setSortCols(new ArrayList<Order>());
-
-    return sd;
-  }
-
-  private List<Partition> createPartitions(String dbName, Table tbl,
-      List<List<String>> values)  throws Throwable {
-    int i = 1;
-    List<Partition> partitions = new ArrayList<Partition>();
-    for(List<String> vals : values) {
-      Partition part = makePartitionObject(dbName, tbl.getTableName(), vals, tbl, "/part"+i);
-      i++;
-      // check if the partition exists (it shouldn't)
-      boolean exceptionThrown = false;
-      try {
-        Partition p = client.getPartition(dbName, tbl.getTableName(), vals);
-      } catch(Exception e) {
-        assertEquals("partition should not have existed",
-            NoSuchObjectException.class, e.getClass());
-        exceptionThrown = true;
-      }
-      assertTrue("getPartition() should have thrown NoSuchObjectException", exceptionThrown);
-      Partition retp = client.add_partition(part);
-      assertNotNull("Unable to create partition " + part, retp);
-      partitions.add(retp);
-    }
-    return partitions;
-  }
-
-  private void createMultiPartitionTableSchema(String dbName, String tblName,
-      String typeName, List<List<String>> values)
-      throws Throwable, MetaException, TException, NoSuchObjectException {
-    createDb(dbName);
-
-    Map<String, String> fields = new HashMap<String, String>();
-    fields.put("name", serdeConstants.STRING_TYPE_NAME);
-    fields.put("income", serdeConstants.INT_TYPE_NAME);
-
-    Type typ1 = createType(typeName, fields);
-
-    Map<String , String> partitionKeys = new HashMap<String, String>();
-    partitionKeys.put("ds", serdeConstants.STRING_TYPE_NAME);
-    partitionKeys.put("hr", serdeConstants.STRING_TYPE_NAME);
-
-    Map<String, String> params = new HashMap<String, String>();
-    params.put("test_param_1", "Use this for comments etc");
-
-    Map<String, String> serdParams = new HashMap<String, String>();
-    serdParams.put(serdeConstants.SERIALIZATION_FORMAT, "1");
-
-    StorageDescriptor sd =  createStorageDescriptor(tblName, typ1.getFields(), params, serdParams);
-
-    Table tbl = createTable(dbName, tblName, null, null, partitionKeys, sd, 0);
-
-    if (isThriftClient) {
-      // the createTable() above does not update the location in the 'tbl'
-      // object when the client is a thrift client and the code below relies
-      // on the location being present in the 'tbl' object - so get the table
-      // from the metastore
-      tbl = client.getTable(dbName, tblName);
-    }
-
-    createPartitions(dbName, tbl, values);
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStoreWithEnvironmentContext.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStoreWithEnvironmentContext.java
deleted file mode 100644
index 7e2c27d..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStoreWithEnvironmentContext.java
+++ /dev/null
@@ -1,222 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.EnvironmentContext;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.Partition;
-import org.apache.hadoop.hive.metastore.api.SerDeInfo;
-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
-import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.metastore.events.AddPartitionEvent;
-import org.apache.hadoop.hive.metastore.events.AlterTableEvent;
-import org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent;
-import org.apache.hadoop.hive.metastore.events.CreateTableEvent;
-import org.apache.hadoop.hive.metastore.events.DropDatabaseEvent;
-import org.apache.hadoop.hive.metastore.events.DropPartitionEvent;
-import org.apache.hadoop.hive.metastore.events.DropTableEvent;
-import org.apache.hadoop.hive.metastore.events.ListenerEvent;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.mortbay.log.Log;
-
-/**
- * TestHiveMetaStoreWithEnvironmentContext. Test case for _with_environment_context
- * calls in {@link org.apache.hadoop.hive.metastore.HiveMetaStore}
- */
-public class TestHiveMetaStoreWithEnvironmentContext extends TestCase {
-
-  private HiveConf hiveConf;
-  private HiveMetaStoreClient msc;
-  private EnvironmentContext envContext;
-  private final Database db = new Database();
-  private Table table = new Table();
-  private final Partition partition = new Partition();
-
-  private static final String dbName = "hive3252";
-  private static final String tblName = "tmptbl";
-  private static final String renamed = "tmptbl2";
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-
-    System.setProperty("hive.metastore.event.listeners",
-        DummyListener.class.getName());
-
-    int port = MetaStoreUtils.findFreePort();
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-
-    hiveConf = new HiveConf(this.getClass());
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    SessionState.start(new CliSessionState(hiveConf));
-    msc = new HiveMetaStoreClient(hiveConf, null);
-
-    msc.dropDatabase(dbName, true, true);
-
-    Map<String, String> envProperties = new HashMap<String, String>();
-    envProperties.put("hadoop.job.ugi", "test_user");
-    envContext = new EnvironmentContext(envProperties);
-
-    db.setName(dbName);
-
-    Map<String, String> tableParams = new HashMap<String, String>();
-    tableParams.put("a", "string");
-    List<FieldSchema> partitionKeys = new ArrayList<FieldSchema>();
-    partitionKeys.add(new FieldSchema("b", "string", ""));
-
-    List<FieldSchema> cols = new ArrayList<FieldSchema>();
-    cols.add(new FieldSchema("a", "string", ""));
-    cols.add(new FieldSchema("b", "string", ""));
-    StorageDescriptor sd = new StorageDescriptor();
-    sd.setCols(cols);
-    sd.setCompressed(false);
-    sd.setParameters(tableParams);
-    sd.setSerdeInfo(new SerDeInfo());
-    sd.getSerdeInfo().setName(tblName);
-    sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-    sd.getSerdeInfo().getParameters().put(serdeConstants.SERIALIZATION_FORMAT, "1");
-
-    table.setDbName(dbName);
-    table.setTableName(tblName);
-    table.setParameters(tableParams);
-    table.setPartitionKeys(partitionKeys);
-    table.setSd(sd);
-
-    List<String> partValues = new ArrayList<String>();
-    partValues.add("2011");
-    partition.setDbName(dbName);
-    partition.setTableName(tblName);
-    partition.setValues(partValues);
-    partition.setSd(table.getSd().deepCopy());
-    partition.getSd().setSerdeInfo(table.getSd().getSerdeInfo().deepCopy());
-
-    DummyListener.notifyList.clear();
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-  public void testEnvironmentContext() throws Exception {
-    int listSize = 0;
-
-    List<ListenerEvent> notifyList = DummyListener.notifyList;
-    assertEquals(notifyList.size(), listSize);
-    msc.createDatabase(db);
-    listSize++;
-    assertEquals(listSize, notifyList.size());
-    CreateDatabaseEvent dbEvent = (CreateDatabaseEvent)(notifyList.get(listSize - 1));
-    assert dbEvent.getStatus();
-
-    Log.debug("Creating table");
-    msc.createTable(table, envContext);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    CreateTableEvent tblEvent = (CreateTableEvent)(notifyList.get(listSize - 1));
-    assert tblEvent.getStatus();
-    assertEquals(envContext, tblEvent.getEnvironmentContext());
-
-    table = msc.getTable(dbName, tblName);
-
-    Log.debug("Adding partition");
-    partition.getSd().setLocation(table.getSd().getLocation() + "/part1");
-    msc.add_partition(partition, envContext);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    AddPartitionEvent partEvent = (AddPartitionEvent)(notifyList.get(listSize-1));
-    assert partEvent.getStatus();
-    assertEquals(envContext, partEvent.getEnvironmentContext());
-
-    Log.debug("Appending partition");
-    List<String> partVals = new ArrayList<String>();
-    partVals.add("2012");
-    msc.appendPartition(dbName, tblName, partVals, envContext);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    AddPartitionEvent appendPartEvent = (AddPartitionEvent)(notifyList.get(listSize-1));
-    assert appendPartEvent.getStatus();
-    assertEquals(envContext, appendPartEvent.getEnvironmentContext());
-
-    Log.debug("Renaming table");
-    table.setTableName(renamed);
-    msc.alter_table(dbName, tblName, table, envContext);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    AlterTableEvent alterTableEvent = (AlterTableEvent) notifyList.get(listSize-1);
-    assert alterTableEvent.getStatus();
-    assertEquals(envContext, alterTableEvent.getEnvironmentContext());
-
-    Log.debug("Renaming table back");
-    table.setTableName(tblName);
-    msc.alter_table(dbName, renamed, table, envContext);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-
-    Log.debug("Dropping partition");
-    List<String> dropPartVals = new ArrayList<String>();
-    dropPartVals.add("2011");
-    msc.dropPartition(dbName, tblName, dropPartVals, envContext);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    DropPartitionEvent dropPartEvent = (DropPartitionEvent)notifyList.get(listSize - 1);
-    assert dropPartEvent.getStatus();
-    assertEquals(envContext, dropPartEvent.getEnvironmentContext());
-
-    Log.debug("Dropping partition by name");
-    msc.dropPartition(dbName, tblName, "b=2012", true, envContext);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    DropPartitionEvent dropPartByNameEvent = (DropPartitionEvent)notifyList.get(listSize - 1);
-    assert dropPartByNameEvent.getStatus();
-    assertEquals(envContext, dropPartByNameEvent.getEnvironmentContext());
-
-    Log.debug("Dropping table");
-    msc.dropTable(dbName, tblName, true, false, envContext);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    DropTableEvent dropTblEvent = (DropTableEvent)notifyList.get(listSize-1);
-    assert dropTblEvent.getStatus();
-    assertEquals(envContext, dropTblEvent.getEnvironmentContext());
-
-    msc.dropDatabase(dbName);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-
-    DropDatabaseEvent dropDB = (DropDatabaseEvent)notifyList.get(listSize-1);
-    assert dropDB.getStatus();
-  }
-
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaTool.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaTool.java
deleted file mode 100644
index 1b688bd..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaTool.java
+++ /dev/null
@@ -1,247 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import java.io.ByteArrayOutputStream;
-import java.io.OutputStream;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.HashMap;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.InvalidOperationException;
-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
-import org.apache.hadoop.hive.metastore.api.SerDeInfo;
-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
-import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.metastore.api.Type;
-import org.apache.hadoop.hive.metastore.tools.HiveMetaTool;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils;
-import org.apache.hadoop.util.StringUtils;
-
-public class TestHiveMetaTool extends TestCase {
-
-  private HiveMetaStoreClient client;
-
-  private PrintStream originalOut;
-  private OutputStream os;
-  private PrintStream ps;
-  private String locationUri;
-  private final String dbName = "TestHiveMetaToolDB";
-  private final String typeName = "Person";
-  private final String tblName = "simpleTbl";
-  private final String badTblName = "badSimpleTbl";
-
-
-  private void dropDatabase(String dbName) throws Exception {
-    try {
-      client.dropDatabase(dbName);
-    } catch (NoSuchObjectException e) {
-    } catch (InvalidOperationException e) {
-    } catch (Exception e) {
-      throw e;
-    }
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-
-    try {
-      HiveConf hiveConf = new HiveConf(HiveMetaTool.class);
-      client = new HiveMetaStoreClient(hiveConf, null);
-
-      // Setup output stream to redirect output to
-      os = new ByteArrayOutputStream();
-      ps = new PrintStream(os);
-
-      // create a dummy database and a couple of dummy tables
-      Database db = new Database();
-      db.setName(dbName);
-      client.dropTable(dbName, tblName);
-      client.dropTable(dbName, badTblName);
-      dropDatabase(dbName);
-      client.createDatabase(db);
-      locationUri = db.getLocationUri();
-      String avroUri = "hdfs://nn.example.com/warehouse/hive/ab.avsc";
-      String badAvroUri = new String("hdfs:/hive");
-
-      client.dropType(typeName);
-      Type typ1 = new Type();
-      typ1.setName(typeName);
-      typ1.setFields(new ArrayList<FieldSchema>(2));
-      typ1.getFields().add(
-          new FieldSchema("name", serdeConstants.STRING_TYPE_NAME, ""));
-      typ1.getFields().add(
-          new FieldSchema("income", serdeConstants.INT_TYPE_NAME, ""));
-      client.createType(typ1);
-
-      Table tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(tblName);
-      StorageDescriptor sd = new StorageDescriptor();
-      tbl.setSd(sd);
-      sd.setCols(typ1.getFields());
-      sd.setCompressed(false);
-      sd.setNumBuckets(1);
-      sd.setParameters(new HashMap<String, String>());
-      sd.getParameters().put("test_param_1", "Use this for comments etc");
-      sd.setBucketCols(new ArrayList<String>(2));
-      sd.getBucketCols().add("name");
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters().put(
-          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
-      sd.getParameters().put(AvroSerdeUtils.SCHEMA_URL, avroUri);
-      sd.getSerdeInfo().setSerializationLib(
-          org.apache.hadoop.hive.serde2.avro.AvroSerDe.class.getName());
-      tbl.setPartitionKeys(new ArrayList<FieldSchema>());
-      client.createTable(tbl);
-
-      //create a table with bad avro uri
-      tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(badTblName);
-      sd = new StorageDescriptor();
-      tbl.setSd(sd);
-      sd.setCols(typ1.getFields());
-      sd.setCompressed(false);
-      sd.setNumBuckets(1);
-      sd.setParameters(new HashMap<String, String>());
-      sd.getParameters().put("test_param_1", "Use this for comments etc");
-      sd.setBucketCols(new ArrayList<String>(2));
-      sd.getBucketCols().add("name");
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters().put(
-          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
-      sd.getParameters().put(AvroSerdeUtils.SCHEMA_URL, badAvroUri);
-      sd.getSerdeInfo().setSerializationLib(
-          org.apache.hadoop.hive.serde2.avro.AvroSerDe.class.getName());
-      tbl.setPartitionKeys(new ArrayList<FieldSchema>());
-      client.createTable(tbl);
-      client.close();
-    } catch (Exception e) {
-      System.err.println("Unable to setup the hive metatool test");
-      System.err.println(StringUtils.stringifyException(e));
-      throw new Exception(e);
-    }
-  }
-
-  private void redirectOutputStream() {
-
-    originalOut = System.out;
-    System.setOut(ps);
-
-  }
-
-  private void restoreOutputStream() {
-
-    System.setOut(originalOut);
-  }
-
-  public void testListFSRoot() throws Exception {
-
-    redirectOutputStream();
-    String[] args = new String[1];
-    args[0] = new String("-listFSRoot");
-
-    try {
-      HiveMetaTool.main(args);
-      String out = os.toString();
-      boolean b = out.contains(locationUri);
-      assertTrue(b);
-    } finally {
-      restoreOutputStream();
-      System.out.println("Completed testListFSRoot");
-    }
-  }
-
-  public void testExecuteJDOQL() throws Exception {
-
-    redirectOutputStream();
-    String[] args = new String[2];
-    args[0] = new String("-executeJDOQL");
-    args[1] = new String("select locationUri from org.apache.hadoop.hive.metastore.model.MDatabase");
-
-    try {
-      HiveMetaTool.main(args);
-      String out = os.toString();
-      boolean b = out.contains(locationUri);
-      assertTrue(b);
-    } finally {
-      restoreOutputStream();
-      System.out.println("Completed testExecuteJDOQL");
-    }
-  }
-
-  public void testUpdateFSRootLocation() throws Exception {
-
-    redirectOutputStream();
-    String oldLocationUri = "hdfs://nn.example.com/";
-    String newLocationUri = "hdfs://nn-ha-uri/";
-    String[] args = new String[5];
-    args[0] = new String("-updateLocation");
-    args[1] = new String(newLocationUri);
-    args[2] = new String(oldLocationUri);
-    args[3] = new String("-tablePropKey");
-    args[4] = new String("avro.schema.url");
-
-    try {
-      // perform HA upgrade
-      HiveMetaTool.main(args);
-      String out = os.toString();
-      boolean b = out.contains(newLocationUri);
-      restoreOutputStream();
-      assertTrue(b);
-
-      //restore the original HDFS root
-      args[1] = new String(oldLocationUri);
-      args[2] = new String(newLocationUri);
-      redirectOutputStream();
-      HiveMetaTool.main(args);
-      restoreOutputStream();
-    } finally {
-      restoreOutputStream();
-      System.out.println("Completed testUpdateFSRootLocation..");
-    }
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    try {
-      client.dropTable(dbName, tblName);
-      client.dropTable(dbName, badTblName);
-      dropDatabase(dbName);
-      super.tearDown();
-      client.close();
-    } catch (Throwable e) {
-      System.err.println("Unable to close metastore");
-      System.err.println(StringUtils.stringifyException(e));
-      throw new Exception(e);
-    }
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartition.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartition.java
deleted file mode 100644
index 57a5e6b..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartition.java
+++ /dev/null
@@ -1,107 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.metastore.api.InvalidPartitionException;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
-import org.apache.hadoop.hive.metastore.api.PartitionEventType;
-import org.apache.hadoop.hive.metastore.api.UnknownDBException;
-import org.apache.hadoop.hive.metastore.api.UnknownPartitionException;
-import org.apache.hadoop.hive.metastore.api.UnknownTableException;
-import org.apache.hadoop.hive.ql.CommandNeedRetryException;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.thrift.TException;
-
-public class TestMarkPartition extends TestCase{
-
-  protected HiveConf hiveConf;
-  private Driver driver;
-
-  @Override
-  protected void setUp() throws Exception {
-
-    super.setUp();
-    System.setProperty("hive.metastore.event.clean.freq", "2");
-    System.setProperty("hive.metastore.event.expiry.duration", "5");
-    hiveConf = new HiveConf(this.getClass());
-    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    SessionState.start(new CliSessionState(hiveConf));
-
-  }
-
-  public void testMarkingPartitionSet() throws CommandNeedRetryException, MetaException,
-  TException, NoSuchObjectException, UnknownDBException, UnknownTableException,
-  InvalidPartitionException, UnknownPartitionException, InterruptedException {
-    HiveMetaStoreClient msc = new HiveMetaStoreClient(hiveConf, null);
-    driver = new Driver(hiveConf);
-    driver.run("drop database if exists hive2215 cascade");
-    driver.run("create database hive2215");
-    driver.run("use hive2215");
-    driver.run("drop table if exists tmptbl");
-    driver.run("create table tmptbl (a string) partitioned by (b string)");
-    driver.run("alter table tmptbl add partition (b='2011')");
-    Map<String,String> kvs = new HashMap<String, String>();
-    kvs.put("b", "'2011'");
-    msc.markPartitionForEvent("hive2215", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
-    assert msc.isPartitionMarkedForEvent("hive2215", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
-    Thread.sleep(10000);
-    assert !msc.isPartitionMarkedForEvent("hive2215", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
-
-    kvs.put("b", "'2012'");
-    assert !msc.isPartitionMarkedForEvent("hive2215", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
-    try{
-      msc.markPartitionForEvent("hive2215", "tmptbl2", kvs, PartitionEventType.LOAD_DONE);
-      assert false;
-    } catch(Exception e){
-      assert e instanceof UnknownTableException;
-    }
-    try{
-      msc.isPartitionMarkedForEvent("hive2215", "tmptbl2", kvs, PartitionEventType.LOAD_DONE);
-      assert false;
-    } catch(Exception e){
-      assert e instanceof UnknownTableException;
-    }
-    kvs.put("a", "'2012'");
-    try{
-      msc.isPartitionMarkedForEvent("hive2215", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
-      assert false;
-    } catch(Exception e){
-      assert e instanceof InvalidPartitionException;
-    }
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    driver.run("drop database if exists hive2215 cascade");
-    super.tearDown();
-  }
-
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java
deleted file mode 100644
index 7576f39..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-
-public class TestMarkPartitionRemote extends TestMarkPartition {
-
-  private static class RunMS implements Runnable {
-
-    private final int port;
-
-    public RunMS(int port) {
-      this.port = port;
-    }
-    @Override
-    public void run() {
-      try {
-        HiveMetaStore.main(new String[] { String.valueOf(port) });
-      } catch (Throwable e) {
-        e.printStackTrace(System.err);
-        assert false;
-      }
-    }
-
-  }
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    int port = MetaStoreUtils.findFreePort();
-    Thread t = new Thread(new RunMS(port));
-    t.setDaemon(true);
-    t.start();
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    Thread.sleep(30000);
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreAuthorization.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreAuthorization.java
deleted file mode 100644
index a6a038a..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreAuthorization.java
+++ /dev/null
@@ -1,125 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import java.io.IOException;
-import java.net.ServerSocket;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
-import org.apache.hadoop.hive.shims.ShimLoader;
-
-
-public class TestMetaStoreAuthorization extends TestCase {
-  protected HiveConf conf = new HiveConf();
-
-  private int port;
-
-  public void setup() throws Exception {
-    port = findFreePort();
-    System.setProperty(HiveConf.ConfVars.METASTORE_AUTHORIZATION_STORAGE_AUTH_CHECKS.varname,
-        "true");
-    conf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    conf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    conf.setIntVar(ConfVars.METASTORE_CLIENT_CONNECT_RETRY_DELAY, 60);
-  }
-
-  public void testIsWritable() throws Exception {
-    setup();
-    conf = new HiveConf(this.getClass());
-    String testDir = System.getProperty("test.warehouse.dir", "/tmp");
-    Path testDirPath = new Path(testDir);
-    FileSystem fs = testDirPath.getFileSystem(conf);
-    Path top = new Path(testDirPath, "_foobarbaz12_");
-    try {
-      fs.mkdirs(top);
-
-      Warehouse wh = new Warehouse(conf);
-      FsPermission writePerm = FsPermission.createImmutable((short)0777);
-      FsPermission noWritePerm = FsPermission.createImmutable((short)0555);
-
-      fs.setPermission(top, writePerm);
-      assertTrue("Expected " + top + " to be writable", wh.isWritable(top));
-
-      fs.setPermission(top, noWritePerm);
-      assertTrue("Expected " + top + " to be not writable", !wh.isWritable(top));
-    } finally {
-      fs.delete(top, true);
-    }
-  }
-
-  public void testMetaStoreAuthorization() throws Exception {
-    setup();
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-    HiveMetaStoreClient client = new HiveMetaStoreClient(conf);
-
-    FileSystem fs = null;
-    String dbName = "simpdb";
-    Database db1 = null;
-    Path p = null;
-    try {
-      try {
-        db1 = client.getDatabase(dbName);
-        client.dropDatabase(dbName);
-      } catch (NoSuchObjectException noe) {}
-      if (db1 != null) {
-        p = new Path(db1.getLocationUri());
-        fs = p.getFileSystem(conf);
-        fs.delete(p, true);
-      }
-      db1 = new Database();
-      db1.setName(dbName);
-      client.createDatabase(db1);
-      Database db = client.getDatabase(dbName);
-
-      assertTrue("Databases do not match", db1.getName().equals(db.getName()));
-      p = new Path(db.getLocationUri());
-      if (fs == null) {
-        fs = p.getFileSystem(conf);
-      }
-      fs.setPermission(p.getParent(), FsPermission.createImmutable((short)0555));
-      try {
-        client.dropDatabase(dbName);
-        throw new Exception("Expected dropDatabase call to fail");
-      } catch (MetaException me) {
-      }
-      fs.setPermission(p.getParent(), FsPermission.createImmutable((short)0755));
-      client.dropDatabase(dbName);
-    } finally {
-      if (p != null) {
-        fs.delete(p, true);
-      }
-    }
-  }
-
-  private int findFreePort() throws IOException {
-    ServerSocket socket= new ServerSocket(0);
-    int port = socket.getLocalPort();
-    socket.close();
-    return port;
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreConnectionUrlHook.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreConnectionUrlHook.java
deleted file mode 100644
index 91a2888..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreConnectionUrlHook.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.session.SessionState;
-
-/**
- * TestMetaStoreConnectionUrlHook
- * Verifies that when an instance of an implementation of RawStore is initialized, the connection
- * URL has already been updated by any metastore connect URL hooks.
- */
-public class TestMetaStoreConnectionUrlHook extends TestCase {
-  private HiveConf hiveConf;
-
-  @Override
-  protected void setUp() throws Exception {
-
-    super.setUp();
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-  public void testUrlHook() throws Exception {
-    hiveConf = new HiveConf(this.getClass());
-    hiveConf.setVar(HiveConf.ConfVars.METASTORECONNECTURLHOOK,
-        DummyJdoConnectionUrlHook.class.getName());
-    hiveConf.setVar(HiveConf.ConfVars.METASTORECONNECTURLKEY,
-        DummyJdoConnectionUrlHook.initialUrl);
-    hiveConf.setVar(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL,
-        DummyRawStoreForJdoConnection.class.getName());
-    hiveConf.setBoolean("hive.metastore.checkForDefaultDb", true);
-    SessionState.start(new CliSessionState(hiveConf));
-
-    // Instantiating the HMSHandler with hive.metastore.checkForDefaultDb will cause it to
-    // initialize an instance of the DummyRawStoreForJdoConnection
-    HiveMetaStore.HMSHandler hms = new HiveMetaStore.HMSHandler(
-        "test_metastore_connection_url_hook_hms_handler", hiveConf);
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEndFunctionListener.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEndFunctionListener.java
deleted file mode 100644
index e2c860c..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEndFunctionListener.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.shims.ShimLoader;
-/**
- * TestMetaStoreEventListener. Test case for
- * {@link org.apache.hadoop.hive.metastore.MetaStoreEndFunctionListener}
- */
-public class TestMetaStoreEndFunctionListener extends TestCase {
-  private HiveConf hiveConf;
-  private HiveMetaStoreClient msc;
-  private Driver driver;
-
-  @Override
-  protected void setUp() throws Exception {
-
-    super.setUp();
-    System.setProperty("hive.metastore.event.listeners",
-        DummyListener.class.getName());
-    System.setProperty("hive.metastore.pre.event.listeners",
-        DummyPreListener.class.getName());
-    System.setProperty("hive.metastore.end.function.listeners",
-        DummyEndFunctionListener.class.getName());
-    int port = MetaStoreUtils.findFreePort();
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-    hiveConf = new HiveConf(this.getClass());
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    SessionState.start(new CliSessionState(hiveConf));
-    msc = new HiveMetaStoreClient(hiveConf, null);
-    driver = new Driver(hiveConf);
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-  public void testEndFunctionListener() throws Exception {
-    /* Objective here is to ensure that when exceptions are thrown in HiveMetaStore in API methods
-     * they bubble up and are stored in the MetaStoreEndFunctionContext objects
-     */
-    String dbName = "hive3524";
-    String tblName = "tmptbl";
-    int listSize = 0;
-
-    driver.run("create database " + dbName);
-
-    try {
-      msc.getDatabase("UnknownDB");
-    }
-    catch (Exception e) {
-    }
-    listSize = DummyEndFunctionListener.funcNameList.size();
-    String func_name = DummyEndFunctionListener.funcNameList.get(listSize-1);
-    MetaStoreEndFunctionContext context = DummyEndFunctionListener.contextList.get(listSize-1);
-    assertEquals(func_name,"get_database");
-    assertFalse(context.isSuccess());
-    Exception e = context.getException();
-    assertTrue((e!=null));
-    assertTrue((e instanceof NoSuchObjectException));
-    assertEquals(context.getInputTableName(), null);
-
-    driver.run("use " + dbName);
-    driver.run(String.format("create table %s (a string) partitioned by (b string)", tblName));
-    String tableName = "Unknown";
-    try {
-      msc.getTable(dbName, tableName);
-    }
-    catch (Exception e1) {
-    }
-    listSize = DummyEndFunctionListener.funcNameList.size();
-    func_name = DummyEndFunctionListener.funcNameList.get(listSize-1);
-    context = DummyEndFunctionListener.contextList.get(listSize-1);
-    assertEquals(func_name,"get_table");
-    assertFalse(context.isSuccess());
-    e = context.getException();
-    assertTrue((e!=null));
-    assertTrue((e instanceof NoSuchObjectException));
-    assertEquals(context.getInputTableName(), tableName);
-
-    try {
-      msc.getPartition("hive3524", tblName, "b=2012");
-    }
-    catch (Exception e2) {
-    }
-    listSize = DummyEndFunctionListener.funcNameList.size();
-    func_name = DummyEndFunctionListener.funcNameList.get(listSize-1);
-    context = DummyEndFunctionListener.contextList.get(listSize-1);
-    assertEquals(func_name,"get_partition_by_name");
-    assertFalse(context.isSuccess());
-    e = context.getException();
-    assertTrue((e!=null));
-    assertTrue((e instanceof NoSuchObjectException));
-    assertEquals(context.getInputTableName(), tblName);
-    try {
-      driver.run("drop table Unknown");
-    }
-    catch (Exception e4) {
-    }
-    listSize = DummyEndFunctionListener.funcNameList.size();
-    func_name = DummyEndFunctionListener.funcNameList.get(listSize-1);
-    context = DummyEndFunctionListener.contextList.get(listSize-1);
-    assertEquals(func_name,"get_table");
-    assertFalse(context.isSuccess());
-    e = context.getException();
-    assertTrue((e!=null));
-    assertTrue((e instanceof NoSuchObjectException));
-    assertEquals(context.getInputTableName(), "Unknown");
-
-  }
-
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
deleted file mode 100644
index 4951c94..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
+++ /dev/null
@@ -1,362 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.Partition;
-import org.apache.hadoop.hive.metastore.api.PartitionEventType;
-import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.metastore.events.AddPartitionEvent;
-import org.apache.hadoop.hive.metastore.events.AlterPartitionEvent;
-import org.apache.hadoop.hive.metastore.events.AlterTableEvent;
-import org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent;
-import org.apache.hadoop.hive.metastore.events.CreateTableEvent;
-import org.apache.hadoop.hive.metastore.events.DropDatabaseEvent;
-import org.apache.hadoop.hive.metastore.events.DropPartitionEvent;
-import org.apache.hadoop.hive.metastore.events.DropTableEvent;
-import org.apache.hadoop.hive.metastore.events.ListenerEvent;
-import org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent;
-import org.apache.hadoop.hive.metastore.events.PreAddPartitionEvent;
-import org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent;
-import org.apache.hadoop.hive.metastore.events.PreAlterTableEvent;
-import org.apache.hadoop.hive.metastore.events.PreCreateDatabaseEvent;
-import org.apache.hadoop.hive.metastore.events.PreCreateTableEvent;
-import org.apache.hadoop.hive.metastore.events.PreDropDatabaseEvent;
-import org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent;
-import org.apache.hadoop.hive.metastore.events.PreDropTableEvent;
-import org.apache.hadoop.hive.metastore.events.PreEventContext;
-import org.apache.hadoop.hive.metastore.events.PreLoadPartitionDoneEvent;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.shims.ShimLoader;
-
-/**
- * TestMetaStoreEventListener. Test case for
- * {@link org.apache.hadoop.hive.metastore.MetaStoreEventListener} and
- * {@link org.apache.hadoop.hive.metastore.MetaStorePreEventListener}
- */
-public class TestMetaStoreEventListener extends TestCase {
-  private HiveConf hiveConf;
-  private HiveMetaStoreClient msc;
-  private Driver driver;
-
-  private static final String dbName = "hive2038";
-  private static final String tblName = "tmptbl";
-  private static final String renamed = "tmptbl2";
-
-  @Override
-  protected void setUp() throws Exception {
-
-    super.setUp();
-
-    System.setProperty("hive.metastore.event.listeners",
-        DummyListener.class.getName());
-    System.setProperty("hive.metastore.pre.event.listeners",
-        DummyPreListener.class.getName());
-
-    int port = MetaStoreUtils.findFreePort();
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-
-    hiveConf = new HiveConf(this.getClass());
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    SessionState.start(new CliSessionState(hiveConf));
-    msc = new HiveMetaStoreClient(hiveConf, null);
-    driver = new Driver(hiveConf);
-
-    driver.run("drop database if exists " + dbName + " cascade");
-
-    DummyListener.notifyList.clear();
-    DummyPreListener.notifyList.clear();
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-  private void validateCreateDb(Database expectedDb, Database actualDb) {
-    assertEquals(expectedDb.getName(), actualDb.getName());
-    assertEquals(expectedDb.getLocationUri(), actualDb.getLocationUri());
-  }
-
-  private void validateTable(Table expectedTable, Table actualTable) {
-    assertEquals(expectedTable.getTableName(), actualTable.getTableName());
-    assertEquals(expectedTable.getDbName(), actualTable.getDbName());
-    assertEquals(expectedTable.getSd().getLocation(), actualTable.getSd().getLocation());
-  }
-
-  private void validateCreateTable(Table expectedTable, Table actualTable) {
-    validateTable(expectedTable, actualTable);
-  }
-
-  private void validateAddPartition(Partition expectedPartition, Partition actualPartition) {
-    assertEquals(expectedPartition, actualPartition);
-  }
-
-  private void validateTableInAddPartition(Table expectedTable, Table actualTable) {
-    assertEquals(expectedTable, actualTable);
-  }
-
-  private void validatePartition(Partition expectedPartition, Partition actualPartition) {
-    assertEquals(expectedPartition.getValues(), actualPartition.getValues());
-    assertEquals(expectedPartition.getDbName(), actualPartition.getDbName());
-    assertEquals(expectedPartition.getTableName(), actualPartition.getTableName());
-  }
-
-  private void validateAlterPartition(Partition expectedOldPartition,
-      Partition expectedNewPartition, String actualOldPartitionDbName,
-      String actualOldPartitionTblName,List<String> actualOldPartitionValues,
-      Partition actualNewPartition) {
-    assertEquals(expectedOldPartition.getValues(), actualOldPartitionValues);
-    assertEquals(expectedOldPartition.getDbName(), actualOldPartitionDbName);
-    assertEquals(expectedOldPartition.getTableName(), actualOldPartitionTblName);
-
-    validatePartition(expectedNewPartition, actualNewPartition);
-  }
-
-  private void validateAlterTable(Table expectedOldTable, Table expectedNewTable,
-      Table actualOldTable, Table actualNewTable) {
-    validateTable(expectedOldTable, actualOldTable);
-    validateTable(expectedNewTable, actualNewTable);
-  }
-
-  private void validateAlterTableColumns(Table expectedOldTable, Table expectedNewTable,
-      Table actualOldTable, Table actualNewTable) {
-    validateAlterTable(expectedOldTable, expectedNewTable, actualOldTable, actualNewTable);
-
-    assertEquals(expectedOldTable.getSd().getCols(), actualOldTable.getSd().getCols());
-    assertEquals(expectedNewTable.getSd().getCols(), actualNewTable.getSd().getCols());
-  }
-
-  private void validateLoadPartitionDone(String expectedTableName,
-      Map<String,String> expectedPartitionName, String actualTableName,
-      Map<String,String> actualPartitionName) {
-    assertEquals(expectedPartitionName, actualPartitionName);
-    assertEquals(expectedTableName, actualTableName);
-  }
-
-  private void validateDropPartition(Partition expectedPartition, Partition actualPartition) {
-    validatePartition(expectedPartition, actualPartition);
-  }
-
-  private void validateTableInDropPartition(Table expectedTable, Table actualTable) {
-    validateTable(expectedTable, actualTable);
-  }
-
-  private void validateDropTable(Table expectedTable, Table actualTable) {
-    validateTable(expectedTable, actualTable);
-  }
-
-  private void validateDropDb(Database expectedDb, Database actualDb) {
-    assertEquals(expectedDb, actualDb);
-  }
-
-  public void testListener() throws Exception {
-    int listSize = 0;
-
-    List<ListenerEvent> notifyList = DummyListener.notifyList;
-    assertEquals(notifyList.size(), listSize);
-    List<PreEventContext> preNotifyList = DummyPreListener.notifyList;
-    assertEquals(preNotifyList.size(), listSize);
-
-    driver.run("create database " + dbName);
-    listSize++;
-    Database db = msc.getDatabase(dbName);
-    assertEquals(listSize, notifyList.size());
-    assertEquals(listSize, preNotifyList.size());
-
-    CreateDatabaseEvent dbEvent = (CreateDatabaseEvent)(notifyList.get(listSize - 1));
-    assert dbEvent.getStatus();
-    validateCreateDb(db, dbEvent.getDatabase());
-
-    PreCreateDatabaseEvent preDbEvent = (PreCreateDatabaseEvent)(preNotifyList.get(listSize - 1));
-    validateCreateDb(db, preDbEvent.getDatabase());
-
-    driver.run("use " + dbName);
-    driver.run(String.format("create table %s (a string) partitioned by (b string)", tblName));
-    listSize++;
-    Table tbl = msc.getTable(dbName, tblName);
-    assertEquals(notifyList.size(), listSize);
-    assertEquals(preNotifyList.size(), listSize);
-
-    CreateTableEvent tblEvent = (CreateTableEvent)(notifyList.get(listSize - 1));
-    assert tblEvent.getStatus();
-    validateCreateTable(tbl, tblEvent.getTable());
-
-    PreCreateTableEvent preTblEvent = (PreCreateTableEvent)(preNotifyList.get(listSize - 1));
-    validateCreateTable(tbl, preTblEvent.getTable());
-
-    driver.run("alter table tmptbl add partition (b='2011')");
-    listSize++;
-    Partition part = msc.getPartition("hive2038", "tmptbl", "b=2011");
-    assertEquals(notifyList.size(), listSize);
-    assertEquals(preNotifyList.size(), listSize);
-
-    AddPartitionEvent partEvent = (AddPartitionEvent)(notifyList.get(listSize-1));
-    assert partEvent.getStatus();
-    validateAddPartition(part, partEvent.getPartition());
-    validateTableInAddPartition(tbl, partEvent.getTable());
-
-    PreAddPartitionEvent prePartEvent = (PreAddPartitionEvent)(preNotifyList.get(listSize-1));
-    validateAddPartition(part, prePartEvent.getPartition());
-
-    driver.run(String.format("alter table %s touch partition (%s)", tblName, "b='2011'"));
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    assertEquals(preNotifyList.size(), listSize);
-
-    //the partition did not change,
-    // so the new partition should be similar to the original partition
-    Partition origP = msc.getPartition(dbName, tblName, "b=2011");
-
-    AlterPartitionEvent alterPartEvent = (AlterPartitionEvent)notifyList.get(listSize - 1);
-    assert alterPartEvent.getStatus();
-    validateAlterPartition(origP, origP, alterPartEvent.getOldPartition().getDbName(),
-        alterPartEvent.getOldPartition().getTableName(),
-        alterPartEvent.getOldPartition().getValues(), alterPartEvent.getNewPartition());
-
-    PreAlterPartitionEvent preAlterPartEvent =
-        (PreAlterPartitionEvent)preNotifyList.get(listSize - 1);
-    validateAlterPartition(origP, origP, preAlterPartEvent.getDbName(),
-        preAlterPartEvent.getTableName(), preAlterPartEvent.getNewPartition().getValues(),
-        preAlterPartEvent.getNewPartition());
-
-    List<String> part_vals = new ArrayList<String>();
-    part_vals.add("c=2012");
-    Partition newPart = msc.appendPartition(dbName, tblName, part_vals);
-
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    assertEquals(preNotifyList.size(), listSize);
-
-    AddPartitionEvent appendPartEvent =
-        (AddPartitionEvent)(notifyList.get(listSize-1));
-    validateAddPartition(newPart, appendPartEvent.getPartition());
-
-    PreAddPartitionEvent preAppendPartEvent =
-        (PreAddPartitionEvent)(preNotifyList.get(listSize-1));
-    validateAddPartition(newPart, preAppendPartEvent.getPartition());
-
-    driver.run(String.format("alter table %s rename to %s", tblName, renamed));
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    assertEquals(preNotifyList.size(), listSize);
-
-    Table renamedTable = msc.getTable(dbName, renamed);
-
-    AlterTableEvent alterTableE = (AlterTableEvent) notifyList.get(listSize-1);
-    assert alterTableE.getStatus();
-    validateAlterTable(tbl, renamedTable, alterTableE.getOldTable(), alterTableE.getNewTable());
-
-    PreAlterTableEvent preAlterTableE = (PreAlterTableEvent) preNotifyList.get(listSize-1);
-    validateAlterTable(tbl, renamedTable, preAlterTableE.getOldTable(),
-        preAlterTableE.getNewTable());
-
-    //change the table name back
-    driver.run(String.format("alter table %s rename to %s", renamed, tblName));
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    assertEquals(preNotifyList.size(), listSize);
-
-    driver.run(String.format("alter table %s ADD COLUMNS (c int)", tblName));
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    assertEquals(preNotifyList.size(), listSize);
-
-    Table altTable = msc.getTable(dbName, tblName);
-
-    alterTableE = (AlterTableEvent) notifyList.get(listSize-1);
-    assert alterTableE.getStatus();
-    validateAlterTableColumns(tbl, altTable, alterTableE.getOldTable(), alterTableE.getNewTable());
-
-    preAlterTableE = (PreAlterTableEvent) preNotifyList.get(listSize-1);
-    validateAlterTableColumns(tbl, altTable, preAlterTableE.getOldTable(),
-        preAlterTableE.getNewTable());
-
-    Map<String,String> kvs = new HashMap<String, String>(1);
-    kvs.put("b", "2011");
-    msc.markPartitionForEvent("hive2038", "tmptbl", kvs, PartitionEventType.LOAD_DONE);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    assertEquals(preNotifyList.size(), listSize);
-
-    LoadPartitionDoneEvent partMarkEvent = (LoadPartitionDoneEvent)notifyList.get(listSize - 1);
-    assert partMarkEvent.getStatus();
-    validateLoadPartitionDone("tmptbl", kvs, partMarkEvent.getTable().getTableName(),
-        partMarkEvent.getPartitionName());
-
-    PreLoadPartitionDoneEvent prePartMarkEvent =
-        (PreLoadPartitionDoneEvent)preNotifyList.get(listSize - 1);
-    validateLoadPartitionDone("tmptbl", kvs, prePartMarkEvent.getTableName(),
-        prePartMarkEvent.getPartitionName());
-
-    driver.run(String.format("alter table %s drop partition (b='2011')", tblName));
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    assertEquals(preNotifyList.size(), listSize);
-
-    DropPartitionEvent dropPart = (DropPartitionEvent)notifyList.get(listSize - 1);
-    assert dropPart.getStatus();
-    validateDropPartition(part, dropPart.getPartition());
-    validateTableInDropPartition(tbl, dropPart.getTable());
-
-    PreDropPartitionEvent preDropPart = (PreDropPartitionEvent)preNotifyList.get(listSize - 1);
-    validateDropPartition(part, preDropPart.getPartition());
-    validateTableInDropPartition(tbl, preDropPart.getTable());
-
-    driver.run("drop table " + tblName);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    assertEquals(preNotifyList.size(), listSize);
-
-    DropTableEvent dropTbl = (DropTableEvent)notifyList.get(listSize-1);
-    assert dropTbl.getStatus();
-    validateDropTable(tbl, dropTbl.getTable());
-
-    PreDropTableEvent preDropTbl = (PreDropTableEvent)preNotifyList.get(listSize-1);
-    validateDropTable(tbl, preDropTbl.getTable());
-
-    driver.run("drop database " + dbName);
-    listSize++;
-    assertEquals(notifyList.size(), listSize);
-    assertEquals(preNotifyList.size(), listSize);
-
-    DropDatabaseEvent dropDB = (DropDatabaseEvent)notifyList.get(listSize-1);
-    assert dropDB.getStatus();
-    validateDropDb(db, dropDB.getDatabase());
-
-    PreDropDatabaseEvent preDropDB = (PreDropDatabaseEvent)preNotifyList.get(listSize-1);
-    assert dropDB.getStatus();
-    validateDropDb(db, preDropDB.getDatabase());
-  }
-
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEventListenerOnlyOnCommit.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEventListenerOnlyOnCommit.java
deleted file mode 100644
index 6a14982..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEventListenerOnlyOnCommit.java
+++ /dev/null
@@ -1,104 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import java.util.List;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.events.ListenerEvent;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.shims.ShimLoader;
-
-/**
- * Ensure that the status of MetaStore events depend on the RawStore's commit status.
- */
-public class TestMetaStoreEventListenerOnlyOnCommit extends TestCase {
-
-  private HiveConf hiveConf;
-  private HiveMetaStoreClient msc;
-  private Driver driver;
-
-  @Override
-  protected void setUp() throws Exception {
-
-    super.setUp();
-
-    DummyRawStoreControlledCommit.setCommitSucceed(true);
-
-    System.setProperty(HiveConf.ConfVars.METASTORE_EVENT_LISTENERS.varname,
-            DummyListener.class.getName());
-    System.setProperty(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL.varname,
-            DummyRawStoreControlledCommit.class.getName());
-
-    int port = MetaStoreUtils.findFreePort();
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-
-    hiveConf = new HiveConf(this.getClass());
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    SessionState.start(new CliSessionState(hiveConf));
-    msc = new HiveMetaStoreClient(hiveConf, null);
-    driver = new Driver(hiveConf);
-
-    DummyListener.notifyList.clear();
-  }
-
-  public void testEventStatus() throws Exception {
-    int listSize = 0;
-    List<ListenerEvent> notifyList = DummyListener.notifyList;
-    assertEquals(notifyList.size(), listSize);
-
-    driver.run("CREATE DATABASE tmpDb");
-    listSize += 1;
-    notifyList = DummyListener.notifyList;
-    assertEquals(notifyList.size(), listSize);
-    assertTrue(DummyListener.getLastEvent().getStatus());
-
-    driver.run("CREATE TABLE unittest_TestMetaStoreEventListenerOnlyOnCommit (id INT) " +
-                "PARTITIONED BY (ds STRING)");
-    listSize += 1;
-    notifyList = DummyListener.notifyList;
-    assertEquals(notifyList.size(), listSize);
-    assertTrue(DummyListener.getLastEvent().getStatus());
-
-    driver.run("ALTER TABLE unittest_TestMetaStoreEventListenerOnlyOnCommit " +
-                "ADD PARTITION(ds='foo1')");
-    listSize += 1;
-    notifyList = DummyListener.notifyList;
-    assertEquals(notifyList.size(), listSize);
-    assertTrue(DummyListener.getLastEvent().getStatus());
-
-    DummyRawStoreControlledCommit.setCommitSucceed(false);
-
-    driver.run("ALTER TABLE unittest_TestMetaStoreEventListenerOnlyOnCommit " +
-                "ADD PARTITION(ds='foo2')");
-    listSize += 1;
-    notifyList = DummyListener.notifyList;
-    assertEquals(notifyList.size(), listSize);
-    assertFalse(DummyListener.getLastEvent().getStatus());
-
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreInitListener.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreInitListener.java
deleted file mode 100644
index 42232fd..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreInitListener.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.shims.ShimLoader;
-
-/**
- * TestMetaStoreInitListener. Test case for
- * {@link org.apache.hadoop.hive.metastore.MetaStoreInitListener}
- */
-public class TestMetaStoreInitListener extends TestCase {
-  private HiveConf hiveConf;
-  private HiveMetaStoreClient msc;
-  private Driver driver;
-
-  @Override
-  protected void setUp() throws Exception {
-
-    super.setUp();
-    System.setProperty("hive.metastore.init.hooks",
-        DummyMetaStoreInitListener.class.getName());
-    int port = MetaStoreUtils.findFreePort();
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-    hiveConf = new HiveConf(this.getClass());
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-    hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    SessionState.start(new CliSessionState(hiveConf));
-    msc = new HiveMetaStoreClient(hiveConf, null);
-    driver = new Driver(hiveConf);
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-  public void testMetaStoreInitListener() throws Exception {
-    // DummyMataStoreInitListener's onInit will be called at HMSHandler
-    // initialization, and set this to true
-    assertTrue(DummyMetaStoreInitListener.wasCalled);
-  }
-
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreListenersError.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreListenersError.java
deleted file mode 100644
index d074028..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreListenersError.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.shims.ShimLoader;
-
-/**
- * Test for unwrapping InvocationTargetException, which is thrown from
- * constructor of listener class
- */
-public class TestMetaStoreListenersError extends TestCase {
-
-  public void testInitListenerException() throws Throwable {
-
-    System.setProperty("hive.metastore.init.hooks", ErrorInitListener.class.getName());
-    int port = MetaStoreUtils.findFreePort();
-    try {
-      HiveMetaStore.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-    } catch (Throwable throwable) {
-      Assert.assertEquals(MetaException.class, throwable.getClass());
-      Assert.assertEquals(
-          "Failed to instantiate listener named: " +
-              "org.apache.hadoop.hive.metastore.TestMetaStoreListenersError$ErrorInitListener, " +
-              "reason: java.lang.IllegalArgumentException: exception on constructor",
-          throwable.getMessage());
-    }
-  }
-
-  public void testEventListenerException() throws Throwable {
-
-    System.setProperty("hive.metastore.init.hooks", "");
-    System.setProperty("hive.metastore.event.listeners", ErrorEventListener.class.getName());
-    int port = MetaStoreUtils.findFreePort();
-    try {
-      HiveMetaStore.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-    } catch (Throwable throwable) {
-      Assert.assertEquals(MetaException.class, throwable.getClass());
-      Assert.assertEquals(
-          "Failed to instantiate listener named: " +
-              "org.apache.hadoop.hive.metastore.TestMetaStoreListenersError$ErrorEventListener, " +
-              "reason: java.lang.IllegalArgumentException: exception on constructor",
-          throwable.getMessage());
-    }
-  }
-
-  public static class ErrorInitListener extends MetaStoreInitListener {
-
-    public ErrorInitListener(Configuration config) {
-      super(config);
-      throw new IllegalArgumentException("exception on constructor");
-    }
-
-    public void onInit(MetaStoreInitContext context) throws MetaException {
-    }
-  }
-
-  public static class ErrorEventListener extends MetaStoreEventListener {
-
-    public ErrorEventListener(Configuration config) {
-      super(config);
-      throw new IllegalArgumentException("exception on constructor");
-    }
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
deleted file mode 100644
index 3efec79..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
+++ /dev/null
@@ -1,188 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.metastore;
-
-import java.io.File;
-import java.lang.reflect.Field;
-import java.util.Random;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
-import org.apache.hadoop.hive.ql.session.SessionState;
-
-public class TestMetastoreVersion extends TestCase {
-
-  protected HiveConf hiveConf;
-  private Driver driver;
-  private String metaStoreRoot;
-  private String testMetastoreDB;
-  Random randomNum = new Random();
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    Field defDb = HiveMetaStore.HMSHandler.class.getDeclaredField("createDefaultDB");
-    defDb.setAccessible(true);
-    defDb.setBoolean(null, false);
-    hiveConf = new HiveConf(this.getClass());
-    System.setProperty("hive.metastore.event.listeners",
-        DummyListener.class.getName());
-    System.setProperty("hive.metastore.pre.event.listeners",
-        DummyPreListener.class.getName());
-    testMetastoreDB = System.getProperty("java.io.tmpdir") +
-    File.separator + "test_metastore-" + randomNum.nextInt();
-    System.setProperty(HiveConf.ConfVars.METASTORECONNECTURLKEY.varname,
-        "jdbc:derby:" + testMetastoreDB + ";create=true");
-    metaStoreRoot = System.getProperty("test.tmp.dir");
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    File metaStoreDir = new File(testMetastoreDB);
-    if (metaStoreDir.exists()) {
-      FileUtils.deleteDirectory(metaStoreDir);
-    }
-  }
-
-  /***
-   * Test config defaults
-   */
-  public void testDefaults() {
-    hiveConf = new HiveConf(this.getClass());
-    assertFalse(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION));
-    assertTrue(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_AUTO_CREATE_SCHEMA));
-    assertFalse(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_FIXED_DATASTORE));
-  }
-
-  /***
-   * Test schema verification property
-   * @throws Exception
-   */
-  public void testVersionRestriction () throws Exception {
-    System.setProperty(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION.toString(), "true");
-    hiveConf = new HiveConf(this.getClass());
-    assertTrue(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION));
-    assertFalse(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_AUTO_CREATE_SCHEMA));
-    assertTrue(hiveConf.getBoolVar(HiveConf.ConfVars.METASTORE_FIXED_DATASTORE));
-
-    SessionState.start(new CliSessionState(hiveConf));
-    driver = new Driver(hiveConf);
-    // driver execution should fail since the schema didn't get created
-    CommandProcessorResponse proc = driver.run("show tables");
-    assertFalse(proc.getResponseCode() == 0);
-   }
-
-  /***
-   * Test that with no verification, hive populates the schema and version correctly
-   * @throws Exception
-   */
-  public void testMetastoreVersion () throws Exception {
-    // let the schema and version be auto created
-    System.setProperty(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION.toString(), "false");
-    hiveConf = new HiveConf(this.getClass());
-    SessionState.start(new CliSessionState(hiveConf));
-    driver = new Driver(hiveConf);
-    driver.run("show tables");
-
-    // correct version stored by Metastore during startup
-    assertEquals(MetaStoreSchemaInfo.getHiveSchemaVersion(), getVersion(hiveConf));
-    setVersion(hiveConf, "foo");
-    assertEquals("foo", getVersion(hiveConf));
-  }
-
-  /***
-   * Test that with verification enabled, hive works when the correct schema is already populated
-   * @throws Exception
-   */
-  public void testVersionMatching () throws Exception {
-    System.setProperty(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION.toString(), "false");
-    hiveConf = new HiveConf(this.getClass());
-    SessionState.start(new CliSessionState(hiveConf));
-    driver = new Driver(hiveConf);
-    driver.run("show tables");
-
-    hiveConf.setBoolVar(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION, true);
-    setVersion(hiveConf, MetaStoreSchemaInfo.getHiveSchemaVersion());
-    driver = new Driver(hiveConf);
-    CommandProcessorResponse proc = driver.run("show tables");
-    assertTrue(proc.getResponseCode() == 0);
-  }
-
-  /**
-   * Store garbage version in metastore and verify that hive fails when verification is on
-   * @throws Exception
-   */
-  public void testVersionMisMatch () throws Exception {
-    System.setProperty(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION.toString(), "false");
-    hiveConf = new HiveConf(this.getClass());
-    SessionState.start(new CliSessionState(hiveConf));
-    driver = new Driver(hiveConf);
-    driver.run("show tables");
-
-    System.setProperty(HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION.toString(), "true");
-    hiveConf = new HiveConf(this.getClass());
-    setVersion(hiveConf, "fooVersion");
-    SessionState.start(new CliSessionState(hiveConf));
-    driver = new Driver(hiveConf);
-    CommandProcessorResponse proc = driver.run("show tables");
-    assertFalse(proc.getResponseCode() == 0);
-  }
-
-  //  write the given version to metastore
-  private String getVersion(HiveConf conf) throws HiveMetaException {
-    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, conf, "derby");
-    return getMetaStoreVersion();
-  }
-
-  //  write the given version to metastore
-  private void setVersion(HiveConf conf, String version) throws HiveMetaException {
-    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, conf, "derby");
-    setMetaStoreVersion(version, "setVersion test");
-  }
-
-  // Load the version stored in the metastore db
-  public String getMetaStoreVersion() throws HiveMetaException {
-    ObjectStore objStore = new ObjectStore();
-    objStore.setConf(hiveConf);
-    try {
-      return objStore.getMetaStoreSchemaVersion();
-    } catch (MetaException e) {
-      throw new HiveMetaException("Failed to get version", e);
-    }
-  }
-
-  // Store the given version and comment in the metastore
-  public void setMetaStoreVersion(String newVersion, String comment) throws HiveMetaException {
-    ObjectStore objStore = new ObjectStore();
-    objStore.setConf(hiveConf);
-    try {
-      objStore.setMetaStoreSchemaVersion(newVersion, comment);
-    } catch (MetaException e) {
-      throw new HiveMetaException("Failed to set version", e);
-    }
-  }
-
-
-}
-
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestPartitionNameWhitelistValidation.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestPartitionNameWhitelistValidation.java
deleted file mode 100644
index 61d08cd..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestPartitionNameWhitelistValidation.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.junit.Test;
-
-// Validate the metastore client call validatePartitionNameCharacters to ensure it throws
-// an exception if partition fields contain Unicode characters or commas
-
-public class TestPartitionNameWhitelistValidation extends TestCase {
-
-  private static final String partitionValidationPattern = "[\\x20-\\x7E&&[^,]]*";
-
-  private HiveConf hiveConf;
-  private HiveMetaStoreClient msc;
-  private Driver driver;
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    System.setProperty(HiveConf.ConfVars.METASTORE_PARTITION_NAME_WHITELIST_PATTERN.varname,
-        partitionValidationPattern);
-    int port = MetaStoreUtils.findFreePort();
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-    hiveConf = new HiveConf(this.getClass());
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    SessionState.start(new CliSessionState(hiveConf));
-    msc = new HiveMetaStoreClient(hiveConf, null);
-    driver = new Driver(hiveConf);
-  }
-
-  // Runs an instance of DisallowUnicodePreEventListener
-  // Returns whether or not it succeeded
-  private boolean runValidation(List<String> partVals) {
-
-    try {
-      msc.validatePartitionNameCharacters(partVals);
-    } catch (Exception e) {
-      return false;
-    }
-
-    return true;
- }
-
-  // Sample data
-  private List<String> getPartValsWithUnicode() {
-
-    List<String> partVals = new ArrayList<String>();
-    partVals.add("klâwen");
-    partVals.add("tägelîch");
-
-    return partVals;
-
-  }
-
-  private List<String> getPartValsWithCommas() {
-
-    List<String> partVals = new ArrayList<String>();
-    partVals.add("a,b");
-    partVals.add("c,d,e,f");
-
-    return partVals;
-
-  }
-
-  private List<String> getPartValsWithValidCharacters() {
-
-    List<String> partVals = new ArrayList<String>();
-    partVals.add("part1");
-    partVals.add("part2");
-
-    return partVals;
-
-  }
-
-  @Test
-  public void testAddPartitionWithCommas() {
-
-    Assert.assertFalse("Add a partition with commas in name",
-        runValidation(getPartValsWithCommas()));
-  }
-
-  @Test
-  public void testAddPartitionWithUnicode() {
-
-    Assert.assertFalse("Add a partition with unicode characters in name",
-        runValidation(getPartValsWithUnicode()));
-  }
-
-  @Test
-  public void testAddPartitionWithValidPartVal() {
-
-    Assert.assertTrue("Add a partition with unicode characters in name",
-        runValidation(getPartValsWithValidCharacters()));
-  }
-
-  @Test
-  public void testAppendPartitionWithUnicode() {
-
-    Assert.assertFalse("Append a partition with unicode characters in name",
-        runValidation(getPartValsWithUnicode()));
-  }
-
-  @Test
-  public void testAppendPartitionWithCommas() {
-
-    Assert.assertFalse("Append a partition with unicode characters in name",
-        runValidation(getPartValsWithCommas()));
-  }
-
-  @Test
-  public void testAppendPartitionWithValidCharacters() {
-
-    Assert.assertTrue("Append a partition with no unicode characters in name",
-        runValidation(getPartValsWithValidCharacters()));
-  }
-
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRawStoreTxn.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRawStoreTxn.java
deleted file mode 100644
index 0b87077..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRawStoreTxn.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import java.lang.reflect.Method;
-import java.util.Arrays;
-import java.util.List;
-
-import javax.jdo.JDOException;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.conf.HiveConf;
-
-public class TestRawStoreTxn extends TestCase {
-
-  public static class DummyRawStoreWithCommitError extends DummyRawStoreForJdoConnection {
-    private static int callCount = 0;
-
-    @Override
-    /***
-     * Throw exception on first try
-     */
-    public boolean commitTransaction() {
-      callCount++;
-      if (callCount == 1 ) {
-        throw new JDOException ("Failed for call count " + callCount);
-      } else {
-        return true;
-      }
-    }
-  }
-
-  private ObjectStore objStore;
-  private HiveConf hiveConf;
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    hiveConf = new HiveConf();
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-  /***
-   * Check annotations of the restricted methods
-   * @throws Exception
-   */
-  public void testCheckNoRetryMethods() throws Exception {
-    List<String> nonExecMethods =
-      Arrays.asList("commitTransaction", "commitTransaction");
-
-    RawStore rawStore = RetryingRawStore.getProxy(hiveConf, new Configuration(hiveConf),
-          hiveConf.getVar(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL), 1);
-    for (Method rawStoreMethod : RawStore.class.getMethods()) {
-      if (nonExecMethods.contains(rawStoreMethod.getName())) {
-        assertNotNull(rawStoreMethod.getAnnotation(RawStore.CanNotRetry.class));
-      }
-    }
-  }
-
-  /***
-   * Invoke commit and verify it doesn't get retried
-   * @throws Exception
-   */
-  public void testVerifyNoRetryMethods() throws Exception {
-    hiveConf.setVar(HiveConf.ConfVars.METASTORECONNECTURLKEY,
-        DummyJdoConnectionUrlHook.newUrl);;
-    hiveConf.setVar(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL,
-        DummyRawStoreWithCommitError.class.getName());
-    RawStore rawStore = RetryingRawStore.getProxy(hiveConf, new Configuration(hiveConf),
-        DummyRawStoreWithCommitError.class.getName(), 1);
-    try {
-      rawStore.commitTransaction();
-      fail("Commit should fail due to no retry");
-    } catch (JDOException e) {
-      // Excepted JDOException
-    }
-  }
-
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStore.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStore.java
deleted file mode 100644
index 491d093..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStore.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.shims.ShimLoader;
-
-
-public class TestRemoteHiveMetaStore extends TestHiveMetaStore {
-  private static boolean isServerStarted = false;
-
-  public TestRemoteHiveMetaStore() {
-    super();
-    isThriftClient = true;
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-
-    if (isServerStarted) {
-      assertNotNull("Unable to connect to the MetaStore server", client);
-      return;
-    }
-
-    int port = MetaStoreUtils.findFreePort();
-    System.out.println("Starting MetaStore Server on port " + port);
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-    isServerStarted = true;
-
-    // This is default case with setugi off for both client and server
-    createClient(false, port);
-  }
-
-  protected void createClient(boolean setugi, int port) throws Exception {
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    hiveConf.setBoolVar(ConfVars.METASTORE_EXECUTE_SET_UGI,setugi);
-    client = new HiveMetaStoreClient(hiveConf);
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreIpAddress.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreIpAddress.java
deleted file mode 100644
index 7600e99..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreIpAddress.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.util.StringUtils;
-
-/**
- *
- * TestRemoteHiveMetaStoreIpAddress.
- *
- * Test which checks that the remote Hive metastore stores the proper IP address using
- * IpAddressListener
- */
-public class TestRemoteHiveMetaStoreIpAddress extends TestCase {
-  private static boolean isServerStarted = false;
-  private static HiveConf hiveConf;
-  private static HiveMetaStoreClient msc;
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    hiveConf = new HiveConf(this.getClass());
-
-    if (isServerStarted) {
-      assertNotNull("Unable to connect to the MetaStore server", msc);
-      return;
-    }
-
-    int port = MetaStoreUtils.findFreePort();
-    System.out.println("Starting MetaStore Server on port " + port);
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-    isServerStarted = true;
-
-    // This is default case with setugi off for both client and server
-    createClient(port);
-  }
-
-  public void testIpAddress() throws Exception {
-    try {
-
-      Database db = new Database();
-      db.setName("testIpAddressIp");
-      msc.createDatabase(db);
-      msc.dropDatabase(db.getName());
-    } catch (Exception e) {
-      System.err.println(StringUtils.stringifyException(e));
-      System.err.println("testIpAddress() failed.");
-      throw e;
-    }
-  }
-
-  protected void createClient(int port) throws Exception {
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    msc = new HiveMetaStoreClient(hiveConf);
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteUGIHiveMetaStoreIpAddress.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteUGIHiveMetaStoreIpAddress.java
deleted file mode 100644
index 8658262..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRemoteUGIHiveMetaStoreIpAddress.java
+++ /dev/null
@@ -1,28 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.metastore;
-
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-
-public class TestRemoteUGIHiveMetaStoreIpAddress extends TestRemoteHiveMetaStoreIpAddress {
-  public TestRemoteUGIHiveMetaStoreIpAddress() {
-    super();
-    System.setProperty(ConfVars.METASTORE_EXECUTE_SET_UGI.varname, "true");
-  }
-
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRetryingHMSHandler.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRetryingHMSHandler.java
deleted file mode 100644
index e059255..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestRetryingHMSHandler.java
+++ /dev/null
@@ -1,116 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Map;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.Order;
-import org.apache.hadoop.hive.metastore.api.SerDeInfo;
-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
-import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.shims.ShimLoader;
-
-/**
- * TestRetryingHMSHandler. Test case for
- * {@link org.apache.hadoop.hive.metastore.RetryingHMSHandler}
- */
-public class TestRetryingHMSHandler extends TestCase {
-  private HiveConf hiveConf;
-  private HiveMetaStoreClient msc;
-
-  @Override
-  protected void setUp() throws Exception {
-
-    super.setUp();
-    System.setProperty("hive.metastore.pre.event.listeners",
-        AlternateFailurePreListener.class.getName());
-    int port = MetaStoreUtils.findFreePort();
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-    hiveConf = new HiveConf(this.getClass());
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    hiveConf.setIntVar(HiveConf.ConfVars.HMSHANDLERATTEMPTS, 2);
-    hiveConf.setIntVar(HiveConf.ConfVars.HMSHANDLERINTERVAL, 0);
-    hiveConf.setBoolVar(HiveConf.ConfVars.HMSHANDLERFORCERELOADCONF, false);
-    msc = new HiveMetaStoreClient(hiveConf, null);
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-  // Create a database and a table in that database.  Because the AlternateFailurePreListener is
-  // being used each attempt to create something should require two calls by the RetryingHMSHandler
-  public void testRetryingHMSHandler() throws Exception {
-    String dbName = "hive4159";
-    String tblName = "tmptbl";
-
-    Database db = new Database();
-    db.setName(dbName);
-    msc.createDatabase(db);
-
-    Assert.assertEquals(2, AlternateFailurePreListener.getCallCount());
-
-    ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
-    cols.add(new FieldSchema("c1", serdeConstants.STRING_TYPE_NAME, ""));
-    cols.add(new FieldSchema("c2", serdeConstants.INT_TYPE_NAME, ""));
-
-    Map<String, String> params = new HashMap<String, String>();
-    params.put("test_param_1", "Use this for comments etc");
-
-    Map<String, String> serdParams = new HashMap<String, String>();
-    serdParams.put(serdeConstants.SERIALIZATION_FORMAT, "1");
-
-    StorageDescriptor sd = new StorageDescriptor();
-
-    sd.setCols(cols);
-    sd.setCompressed(false);
-    sd.setNumBuckets(1);
-    sd.setParameters(params);
-    sd.setBucketCols(new ArrayList<String>(2));
-    sd.getBucketCols().add("name");
-    sd.setSerdeInfo(new SerDeInfo());
-    sd.getSerdeInfo().setName(tblName);
-    sd.getSerdeInfo().setParameters(serdParams);
-    sd.getSerdeInfo().getParameters()
-        .put(serdeConstants.SERIALIZATION_FORMAT, "1");
-    sd.setSortCols(new ArrayList<Order>());
-
-    Table tbl = new Table();
-    tbl.setDbName(dbName);
-    tbl.setTableName(tblName);
-    tbl.setSd(sd);
-    tbl.setLastAccessTime(0);
-
-    msc.createTable(tbl);
-
-    Assert.assertEquals(4, AlternateFailurePreListener.getCallCount());
-  }
-
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnBothClientServer.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnBothClientServer.java
deleted file mode 100644
index 98708a6..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnBothClientServer.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-
-public class TestSetUGIOnBothClientServer extends TestRemoteHiveMetaStore{
-
-  public TestSetUGIOnBothClientServer() {
-    super();
-    isThriftClient = true;
-    // This will turn on setugi on both client and server processes of the test.
-    System.setProperty(ConfVars.METASTORE_EXECUTE_SET_UGI.varname, "true");
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyClient.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyClient.java
deleted file mode 100644
index 2c6d567..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyClient.java
+++ /dev/null
@@ -1,28 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-public class TestSetUGIOnOnlyClient extends TestRemoteHiveMetaStore{
-
-  @Override
-  protected void createClient(boolean setugi, int port) throws Exception {
-    // turn it on for client.
-    super.createClient(true, port);
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyServer.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyServer.java
deleted file mode 100644
index 6c3fbf6..0000000
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestSetUGIOnOnlyServer.java
+++ /dev/null
@@ -1,28 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-public class TestSetUGIOnOnlyServer extends TestSetUGIOnBothClientServer {
-
-  @Override
-  protected void createClient(boolean setugi, int port) throws Exception {
-    // It is turned on for both client and server because of super class. Turn it off for client.
-    super.createClient(false, port);
-  }
-}
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/VerifyingObjectStore.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/VerifyingObjectStore.java
new file mode 100644
index 0000000..c818a00
--- /dev/null
+++ b/src/metastore/src/test/org/apache/hadoop/hive/metastore/VerifyingObjectStore.java
@@ -0,0 +1,178 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import static org.apache.commons.lang.StringUtils.repeat;
+
+import java.lang.reflect.AccessibleObject;
+import java.lang.reflect.Array;
+import java.lang.reflect.Field;
+import java.lang.reflect.Modifier;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.commons.lang.ClassUtils;
+import org.apache.commons.lang.builder.EqualsBuilder;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.thrift.TException;
+
+class VerifyingObjectStore extends ObjectStore {
+  private static final Log LOG = LogFactory.getLog(VerifyingObjectStore.class);
+
+  public VerifyingObjectStore() {
+    super();
+    LOG.warn(getClass().getSimpleName() + " is being used - test run");
+  }
+
+  @Override
+  public List<Partition> getPartitionsByFilter(String dbName, String tblName, String filter,
+      short maxParts) throws MetaException, NoSuchObjectException {
+    List<Partition> sqlResults = getPartitionsByFilterInternal(
+        dbName, tblName, filter, maxParts, true, false);
+    List<Partition> ormResults = getPartitionsByFilterInternal(
+        dbName, tblName, filter, maxParts, false, true);
+    verifyParts(sqlResults, ormResults);
+    return sqlResults;
+  }
+
+  @Override
+  public List<Partition> getPartitionsByNames(String dbName, String tblName,
+      List<String> partNames) throws MetaException, NoSuchObjectException {
+    List<Partition> sqlResults = getPartitionsByNamesInternal(
+        dbName, tblName, partNames, true, false);
+    List<Partition> ormResults = getPartitionsByNamesInternal(
+        dbName, tblName, partNames, false, true);
+    verifyParts(sqlResults, ormResults);
+    return sqlResults;
+  }
+
+  @Override
+  public boolean getPartitionsByExpr(String dbName, String tblName, byte[] expr,
+      String defaultPartitionName, short maxParts, Set<Partition> result) throws TException {
+    Set<Partition> ormParts = new LinkedHashSet<Partition>();
+    boolean sqlResult = getPartitionsByExprInternal(
+        dbName, tblName, expr, defaultPartitionName, maxParts, result, true, false);
+    boolean ormResult = getPartitionsByExprInternal(
+        dbName, tblName, expr, defaultPartitionName, maxParts, ormParts, false, true);
+    if (sqlResult != ormResult) {
+      String msg = "The unknown flag is different - SQL " + sqlResult + ", ORM " + ormResult;
+      LOG.error(msg);
+      throw new MetaException(msg);
+    }
+    verifyParts(result, ormParts);
+    return sqlResult;
+  }
+
+  @Override
+  public List<Partition> getPartitions(
+      String dbName, String tableName, int maxParts) throws MetaException {
+    List<Partition> sqlResults = getPartitionsInternal(dbName, tableName, maxParts, true, false);
+    List<Partition> ormResults = getPartitionsInternal(dbName, tableName, maxParts, false, true);
+    verifyParts(sqlResults, ormResults);
+    return sqlResults;
+  };
+
+  private void verifyParts(Collection<Partition> sqlResults, Collection<Partition> ormResults)
+      throws MetaException {
+    final int MAX_DIFFS = 5;
+    if (sqlResults.size() != ormResults.size()) {
+      String msg = "Lists are not the same size: SQL " + sqlResults.size()
+          + ", ORM " + ormResults.size();
+      LOG.error(msg);
+      throw new MetaException(msg);
+    }
+
+    Iterator<Partition> sqlIter = sqlResults.iterator(), ormIter = ormResults.iterator();
+    StringBuilder errorStr = new StringBuilder();
+    int errors = 0;
+    for (int partIx = 0; partIx < sqlResults.size(); ++partIx) {
+      assert sqlIter.hasNext() && ormIter.hasNext();
+      Partition p1 = sqlIter.next(), p2 = ormIter.next();
+      if (EqualsBuilder.reflectionEquals(p1, p2)) continue;
+      errorStr.append("Results are different at list index " + partIx + ": \n");
+      try {
+        dumpObject(errorStr, "SQL", p1, Partition.class, 0);
+        errorStr.append("\n");
+        dumpObject(errorStr, "ORM", p2, Partition.class, 0);
+        errorStr.append("\n\n");
+      } catch (Throwable t) {
+        String msg = "Error getting the diff at list index " + partIx;
+        errorStr.append("\n\n" + msg);
+        LOG.error(msg, t);
+        break;
+      }
+      if (++errors == MAX_DIFFS) {
+        errorStr.append("\n\nToo many diffs, giving up (lists might be sorted differently)");
+        break;
+      }
+    }
+    if (errorStr.length() > 0) {
+      LOG.error("Different results: \n" + errorStr.toString());
+      throw new MetaException("Different results from SQL and ORM, see log for details");
+    }
+  }
+
+  private void dumpObject(StringBuilder errorStr, String name, Object p, Class<?> c, int level)
+      throws IllegalAccessException {
+    String offsetStr = repeat("  ", level);
+    if (p == null || c == String.class || c.isPrimitive()
+        || ClassUtils.wrapperToPrimitive(c) != null) {
+      errorStr.append(offsetStr).append(name + ": [" + p + "]\n");
+    } else if (ClassUtils.isAssignable(c, Iterable.class)) {
+      errorStr.append(offsetStr).append(name + " is an iterable\n");
+      Iterator<?> i1 = ((Iterable<?>)p).iterator();
+      int i = 0;
+      while (i1.hasNext()) {
+        Object o1 = i1.next();
+        Class<?> t = o1 == null ? Object.class : o1.getClass(); // ...
+        dumpObject(errorStr, name + "[" + (i++) + "]", o1, t, level + 1);
+      }
+    } else if (c.isArray()) {
+      int len = Array.getLength(p);
+      Class<?> t = c.getComponentType();
+      errorStr.append(offsetStr).append(name + " is an array\n");
+      for (int i = 0; i < len; ++i) {
+        dumpObject(errorStr, name + "[" + i + "]", Array.get(p, i), t, level + 1);
+      }
+    } else if (ClassUtils.isAssignable(c, Map.class)) {
+      Map<?,?> c1 = (Map<?,?>)p;
+      errorStr.append(offsetStr).append(name + " is a map\n");
+      dumpObject(errorStr, name + ".keys", c1.keySet(), Set.class, level + 1);
+      dumpObject(errorStr, name + ".vals", c1.values(), Collection.class, level + 1);
+    } else {
+      errorStr.append(offsetStr).append(name + " is of type " + c.getCanonicalName() + "\n");
+      // TODO: this doesn't include superclass.
+      Field[] fields = c.getDeclaredFields();
+      AccessibleObject.setAccessible(fields, true);
+      for (int i = 0; i < fields.length; i++) {
+        Field f = fields[i];
+        if (f.getName().indexOf('$') != -1 || Modifier.isStatic(f.getModifiers())) continue;
+        dumpObject(errorStr, name + "." + f.getName(), f.get(p), f.getType(), level + 1);
+      }
+    }
+  }
+}
diff --git a/src/ql/src/java/conf/hive-exec-log4j.properties b/src/ql/src/java/conf/hive-exec-log4j.properties
deleted file mode 100644
index 37f330c..0000000
--- a/src/ql/src/java/conf/hive-exec-log4j.properties
+++ /dev/null
@@ -1,77 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-# Define some default values that can be overridden by system properties
-hive.log.threshold=ALL
-hive.root.logger=INFO,FA
-hive.log.dir=${java.io.tmpdir}/${user.name}
-hive.query.id=hadoop
-hive.log.file=${hive.query.id}.log
-
-# Define the root logger to the system property "hadoop.root.logger".
-log4j.rootLogger=${hive.root.logger}, EventCounter
-
-# Logging Threshold
-log4j.threshhold=${hive.log.threshold}
-
-#
-# File Appender
-#
-
-log4j.appender.FA=org.apache.log4j.FileAppender
-log4j.appender.FA.File=${hive.log.dir}/${hive.log.file}
-log4j.appender.FA.layout=org.apache.log4j.PatternLayout
-
-# Pattern format: Date LogLevel LoggerName LogMessage
-#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
-# Debugging Pattern format
-log4j.appender.FA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
-
-
-#
-# console
-# Add "console" to rootlogger above if you want to use this
-#
-
-log4j.appender.console=org.apache.log4j.ConsoleAppender
-log4j.appender.console.target=System.err
-log4j.appender.console.layout=org.apache.log4j.PatternLayout
-log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
-
-#custom logging levels
-#log4j.logger.xxx=DEBUG
-
-#
-# Event Counter Appender
-# Sends counts of logging messages at different severity levels to Hadoop Metrics.
-#
-log4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter
-
-
-log4j.category.DataNucleus=ERROR,FA
-log4j.category.Datastore=ERROR,FA
-log4j.category.Datastore.Schema=ERROR,FA
-log4j.category.JPOX.Datastore=ERROR,FA
-log4j.category.JPOX.Plugin=ERROR,FA
-log4j.category.JPOX.MetaData=ERROR,FA
-log4j.category.JPOX.Query=ERROR,FA
-log4j.category.JPOX.General=ERROR,FA
-log4j.category.JPOX.Enhancer=ERROR,FA
-
-
-# Silence useless ZK logs
-log4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN,FA
-log4j.logger.org.apache.zookeeper.ClientCnxnSocketNIO=WARN,FA
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/EnforceReadOnlyTables.java b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/EnforceReadOnlyTables.java
new file mode 100644
index 0000000..1458075
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/EnforceReadOnlyTables.java
@@ -0,0 +1,76 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.hooks;
+
+import static org.apache.hadoop.hive.metastore.MetaStoreUtils.DEFAULT_DATABASE_NAME;
+
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.hadoop.hive.ql.metadata.Table;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.security.UserGroupInformation;
+
+/**
+ * Implementation of a pre execute hook that prevents modifications
+ * of read-only tables used by the test framework
+ */
+public class EnforceReadOnlyTables implements ExecuteWithHookContext {
+
+  private static final Set<String> READ_ONLY_TABLES = new HashSet<String>();
+
+  static {
+    for (String srcTable : System.getProperty("test.src.tables", "").trim().split(",")) {
+      srcTable = srcTable.trim();
+      if (!srcTable.isEmpty()) {
+        READ_ONLY_TABLES.add(srcTable);
+      }
+    }
+    if (READ_ONLY_TABLES.isEmpty()) {
+      throw new AssertionError("Source tables cannot be empty");
+    }
+  }
+
+  @Override
+  public void run(HookContext hookContext) throws Exception {
+    SessionState ss = SessionState.get();
+    Set<ReadEntity> inputs = hookContext.getInputs();
+    Set<WriteEntity> outputs = hookContext.getOutputs();
+    UserGroupInformation ugi = hookContext.getUgi();
+    this.run(ss,inputs,outputs,ugi);
+  }
+
+  public void run(SessionState sess, Set<ReadEntity> inputs,
+      Set<WriteEntity> outputs, UserGroupInformation ugi)
+    throws Exception {
+    if (sess.getConf().getBoolean("hive.test.init.phase", false) == true) {
+      return;
+    }
+    for (WriteEntity w: outputs) {
+      if ((w.getTyp() == WriteEntity.Type.TABLE) ||
+          (w.getTyp() == WriteEntity.Type.PARTITION)) {
+        Table t = w.getTable();
+        if (DEFAULT_DATABASE_NAME.equalsIgnoreCase(t.getDbName())
+            && READ_ONLY_TABLES.contains(t.getTableName())) {
+          throw new RuntimeException ("Cannot overwrite read-only table: " + t.getTableName());
+        }
+      }
+    }
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java
new file mode 100644
index 0000000..4518315
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java
@@ -0,0 +1,175 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
+import org.apache.hadoop.hive.ql.hooks.LineageInfo.BaseColumnInfo;
+import org.apache.hadoop.hive.ql.hooks.LineageInfo.Dependency;
+import org.apache.hadoop.hive.ql.hooks.LineageInfo.DependencyKey;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+import org.apache.hadoop.security.UserGroupInformation;
+
+/**
+ * Implementation of a post execute hook that simply prints out its parameters
+ * to standard output.
+ */
+public class PostExecutePrinter implements ExecuteWithHookContext {
+
+  public class DependencyKeyComp implements
+    Comparator<Map.Entry<DependencyKey, Dependency>> {
+
+    @Override
+    public int compare(Map.Entry<DependencyKey, Dependency> o1,
+        Map.Entry<DependencyKey, Dependency> o2) {
+      if (o1 == null && o2 == null) {
+        return 0;
+      }
+      else if (o1 == null && o2 != null) {
+        return -1;
+      }
+      else if (o1 != null && o2 == null) {
+        return 1;
+      }
+      else {
+        // Both are non null.
+        // First compare the table names.
+        int ret = o1.getKey().getDataContainer().getTable().getTableName()
+          .compareTo(o2.getKey().getDataContainer().getTable().getTableName());
+
+        if (ret != 0) {
+          return ret;
+        }
+
+        // The table names match, so check on the partitions
+        if (!o1.getKey().getDataContainer().isPartition() &&
+            o2.getKey().getDataContainer().isPartition()) {
+          return -1;
+        }
+        else if (o1.getKey().getDataContainer().isPartition() &&
+            !o2.getKey().getDataContainer().isPartition()) {
+          return 1;
+        }
+
+        if (o1.getKey().getDataContainer().isPartition() &&
+            o2.getKey().getDataContainer().isPartition()) {
+          // Both are partitioned tables.
+          ret = o1.getKey().getDataContainer().getPartition().toString()
+          .compareTo(o2.getKey().getDataContainer().getPartition().toString());
+
+          if (ret != 0) {
+            return ret;
+          }
+        }
+
+        // The partitons are also the same so check the fieldschema
+        return (o1.getKey().getFieldSchema().getName().compareTo(
+            o2.getKey().getFieldSchema().getName()));
+      }
+    }
+  }
+
+  @Override
+  public void run(HookContext hookContext) throws Exception {
+    assert(hookContext.getHookType() == HookType.POST_EXEC_HOOK);
+    SessionState ss = SessionState.get();
+    Set<ReadEntity> inputs = hookContext.getInputs();
+    Set<WriteEntity> outputs = hookContext.getOutputs();
+    LineageInfo linfo = hookContext.getLinfo();
+    UserGroupInformation ugi = hookContext.getUgi();
+    this.run(ss,inputs,outputs,linfo,ugi);
+  }
+
+  public void run(SessionState sess, Set<ReadEntity> inputs,
+      Set<WriteEntity> outputs, LineageInfo linfo,
+      UserGroupInformation ugi) throws Exception {
+
+    LogHelper console = SessionState.getConsole();
+
+    if (console == null) {
+      return;
+    }
+
+    if (sess != null) {
+      console.printError("POSTHOOK: query: " + sess.getCmd().trim());
+      console.printError("POSTHOOK: type: " + sess.getCommandType());
+    }
+
+    PreExecutePrinter.printEntities(console, inputs, "POSTHOOK: Input: ");
+    PreExecutePrinter.printEntities(console, outputs, "POSTHOOK: Output: ");
+
+    // Also print out the generic lineage information if there is any
+    if (linfo != null) {
+      LinkedList<Map.Entry<DependencyKey, Dependency>> entry_list =
+        new LinkedList<Map.Entry<DependencyKey, Dependency>>(linfo.entrySet());
+      Collections.sort(entry_list, new DependencyKeyComp());
+      Iterator<Map.Entry<DependencyKey, Dependency>> iter = entry_list.iterator();
+      while(iter.hasNext()) {
+        Map.Entry<DependencyKey, Dependency> it = iter.next();
+        Dependency dep = it.getValue();
+        DependencyKey depK = it.getKey();
+
+        if(dep == null) {
+          continue;
+        }
+
+        StringBuilder sb = new StringBuilder();
+        sb.append("POSTHOOK: Lineage: ");
+        if (depK.getDataContainer().isPartition()) {
+          Partition part = depK.getDataContainer().getPartition();
+          sb.append(part.getTableName());
+          sb.append(" PARTITION(");
+          int i = 0;
+          for (FieldSchema fs : depK.getDataContainer().getTable().getPartitionKeys()) {
+            if (i != 0) {
+              sb.append(",");
+            }
+            sb.append(fs.getName() + "=" + part.getValues().get(i++));
+          }
+          sb.append(")");
+        }
+        else {
+          sb.append(depK.getDataContainer().getTable().getTableName());
+        }
+        sb.append("." + depK.getFieldSchema().getName() + " " +
+            dep.getType() + " ");
+
+        sb.append("[");
+        for(BaseColumnInfo col: dep.getBaseCols()) {
+          sb.append("("+col.getTabAlias().getTable().getTableName() + ")"
+              + col.getTabAlias().getAlias() + "."
+              + col.getColumn() + ", ");
+        }
+        sb.append("]");
+
+        console.printError(sb.toString());
+      }
+    }
+  }
+
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java
new file mode 100644
index 0000000..acde2f6
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java
@@ -0,0 +1,76 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.hooks;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+import org.apache.hadoop.security.UserGroupInformation;
+
+/**
+ * Implementation of a pre execute hook that simply prints out its parameters to
+ * standard output.
+ */
+public class PreExecutePrinter implements ExecuteWithHookContext {
+
+  @Override
+  public void run(HookContext hookContext) throws Exception {
+    assert(hookContext.getHookType() == HookType.PRE_EXEC_HOOK);
+    SessionState ss = SessionState.get();
+    Set<ReadEntity> inputs = hookContext.getInputs();
+    Set<WriteEntity> outputs = hookContext.getOutputs();
+    UserGroupInformation ugi = hookContext.getUgi();
+    this.run(ss,inputs,outputs,ugi);
+  }
+
+  public void run(SessionState sess, Set<ReadEntity> inputs,
+      Set<WriteEntity> outputs, UserGroupInformation ugi)
+    throws Exception {
+
+    LogHelper console = SessionState.getConsole();
+
+    if (console == null) {
+      return;
+    }
+
+    if (sess != null) {
+      console.printError("PREHOOK: query: " + sess.getCmd().trim());
+      console.printError("PREHOOK: type: " + sess.getCommandType());
+    }
+
+    printEntities(console, inputs, "PREHOOK: Input: ");
+    printEntities(console, outputs, "PREHOOK: Output: ");
+  }
+
+  static void printEntities(LogHelper console, Set<?> entities, String prefix) {
+    List<String> strings = new ArrayList<String>();
+    for (Object o : entities) {
+      strings.add(o.toString());
+    }
+    Collections.sort(strings);
+    for (String s : strings) {
+      console.printError(prefix + s);
+    }
+  }
+}
diff --git a/src/ql/src/main/resources/hive-exec-log4j.properties b/src/ql/src/main/resources/hive-exec-log4j.properties
new file mode 100644
index 0000000..37f330c
--- /dev/null
+++ b/src/ql/src/main/resources/hive-exec-log4j.properties
@@ -0,0 +1,77 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Define some default values that can be overridden by system properties
+hive.log.threshold=ALL
+hive.root.logger=INFO,FA
+hive.log.dir=${java.io.tmpdir}/${user.name}
+hive.query.id=hadoop
+hive.log.file=${hive.query.id}.log
+
+# Define the root logger to the system property "hadoop.root.logger".
+log4j.rootLogger=${hive.root.logger}, EventCounter
+
+# Logging Threshold
+log4j.threshhold=${hive.log.threshold}
+
+#
+# File Appender
+#
+
+log4j.appender.FA=org.apache.log4j.FileAppender
+log4j.appender.FA.File=${hive.log.dir}/${hive.log.file}
+log4j.appender.FA.layout=org.apache.log4j.PatternLayout
+
+# Pattern format: Date LogLevel LoggerName LogMessage
+#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+# Debugging Pattern format
+log4j.appender.FA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
+
+
+#
+# console
+# Add "console" to rootlogger above if you want to use this
+#
+
+log4j.appender.console=org.apache.log4j.ConsoleAppender
+log4j.appender.console.target=System.err
+log4j.appender.console.layout=org.apache.log4j.PatternLayout
+log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
+
+#custom logging levels
+#log4j.logger.xxx=DEBUG
+
+#
+# Event Counter Appender
+# Sends counts of logging messages at different severity levels to Hadoop Metrics.
+#
+log4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter
+
+
+log4j.category.DataNucleus=ERROR,FA
+log4j.category.Datastore=ERROR,FA
+log4j.category.Datastore.Schema=ERROR,FA
+log4j.category.JPOX.Datastore=ERROR,FA
+log4j.category.JPOX.Plugin=ERROR,FA
+log4j.category.JPOX.MetaData=ERROR,FA
+log4j.category.JPOX.Query=ERROR,FA
+log4j.category.JPOX.General=ERROR,FA
+log4j.category.JPOX.Enhancer=ERROR,FA
+
+
+# Silence useless ZK logs
+log4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN,FA
+log4j.logger.org.apache.zookeeper.ClientCnxnSocketNIO=WARN,FA
diff --git a/src/ql/src/test/org/apache/hadoop/hive/metastore/VerifyingObjectStore.java b/src/ql/src/test/org/apache/hadoop/hive/metastore/VerifyingObjectStore.java
deleted file mode 100644
index c818a00..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/metastore/VerifyingObjectStore.java
+++ /dev/null
@@ -1,178 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.metastore;
-
-import static org.apache.commons.lang.StringUtils.repeat;
-
-import java.lang.reflect.AccessibleObject;
-import java.lang.reflect.Array;
-import java.lang.reflect.Field;
-import java.lang.reflect.Modifier;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.LinkedHashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.commons.lang.ClassUtils;
-import org.apache.commons.lang.builder.EqualsBuilder;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
-import org.apache.hadoop.hive.metastore.api.Partition;
-import org.apache.thrift.TException;
-
-class VerifyingObjectStore extends ObjectStore {
-  private static final Log LOG = LogFactory.getLog(VerifyingObjectStore.class);
-
-  public VerifyingObjectStore() {
-    super();
-    LOG.warn(getClass().getSimpleName() + " is being used - test run");
-  }
-
-  @Override
-  public List<Partition> getPartitionsByFilter(String dbName, String tblName, String filter,
-      short maxParts) throws MetaException, NoSuchObjectException {
-    List<Partition> sqlResults = getPartitionsByFilterInternal(
-        dbName, tblName, filter, maxParts, true, false);
-    List<Partition> ormResults = getPartitionsByFilterInternal(
-        dbName, tblName, filter, maxParts, false, true);
-    verifyParts(sqlResults, ormResults);
-    return sqlResults;
-  }
-
-  @Override
-  public List<Partition> getPartitionsByNames(String dbName, String tblName,
-      List<String> partNames) throws MetaException, NoSuchObjectException {
-    List<Partition> sqlResults = getPartitionsByNamesInternal(
-        dbName, tblName, partNames, true, false);
-    List<Partition> ormResults = getPartitionsByNamesInternal(
-        dbName, tblName, partNames, false, true);
-    verifyParts(sqlResults, ormResults);
-    return sqlResults;
-  }
-
-  @Override
-  public boolean getPartitionsByExpr(String dbName, String tblName, byte[] expr,
-      String defaultPartitionName, short maxParts, Set<Partition> result) throws TException {
-    Set<Partition> ormParts = new LinkedHashSet<Partition>();
-    boolean sqlResult = getPartitionsByExprInternal(
-        dbName, tblName, expr, defaultPartitionName, maxParts, result, true, false);
-    boolean ormResult = getPartitionsByExprInternal(
-        dbName, tblName, expr, defaultPartitionName, maxParts, ormParts, false, true);
-    if (sqlResult != ormResult) {
-      String msg = "The unknown flag is different - SQL " + sqlResult + ", ORM " + ormResult;
-      LOG.error(msg);
-      throw new MetaException(msg);
-    }
-    verifyParts(result, ormParts);
-    return sqlResult;
-  }
-
-  @Override
-  public List<Partition> getPartitions(
-      String dbName, String tableName, int maxParts) throws MetaException {
-    List<Partition> sqlResults = getPartitionsInternal(dbName, tableName, maxParts, true, false);
-    List<Partition> ormResults = getPartitionsInternal(dbName, tableName, maxParts, false, true);
-    verifyParts(sqlResults, ormResults);
-    return sqlResults;
-  };
-
-  private void verifyParts(Collection<Partition> sqlResults, Collection<Partition> ormResults)
-      throws MetaException {
-    final int MAX_DIFFS = 5;
-    if (sqlResults.size() != ormResults.size()) {
-      String msg = "Lists are not the same size: SQL " + sqlResults.size()
-          + ", ORM " + ormResults.size();
-      LOG.error(msg);
-      throw new MetaException(msg);
-    }
-
-    Iterator<Partition> sqlIter = sqlResults.iterator(), ormIter = ormResults.iterator();
-    StringBuilder errorStr = new StringBuilder();
-    int errors = 0;
-    for (int partIx = 0; partIx < sqlResults.size(); ++partIx) {
-      assert sqlIter.hasNext() && ormIter.hasNext();
-      Partition p1 = sqlIter.next(), p2 = ormIter.next();
-      if (EqualsBuilder.reflectionEquals(p1, p2)) continue;
-      errorStr.append("Results are different at list index " + partIx + ": \n");
-      try {
-        dumpObject(errorStr, "SQL", p1, Partition.class, 0);
-        errorStr.append("\n");
-        dumpObject(errorStr, "ORM", p2, Partition.class, 0);
-        errorStr.append("\n\n");
-      } catch (Throwable t) {
-        String msg = "Error getting the diff at list index " + partIx;
-        errorStr.append("\n\n" + msg);
-        LOG.error(msg, t);
-        break;
-      }
-      if (++errors == MAX_DIFFS) {
-        errorStr.append("\n\nToo many diffs, giving up (lists might be sorted differently)");
-        break;
-      }
-    }
-    if (errorStr.length() > 0) {
-      LOG.error("Different results: \n" + errorStr.toString());
-      throw new MetaException("Different results from SQL and ORM, see log for details");
-    }
-  }
-
-  private void dumpObject(StringBuilder errorStr, String name, Object p, Class<?> c, int level)
-      throws IllegalAccessException {
-    String offsetStr = repeat("  ", level);
-    if (p == null || c == String.class || c.isPrimitive()
-        || ClassUtils.wrapperToPrimitive(c) != null) {
-      errorStr.append(offsetStr).append(name + ": [" + p + "]\n");
-    } else if (ClassUtils.isAssignable(c, Iterable.class)) {
-      errorStr.append(offsetStr).append(name + " is an iterable\n");
-      Iterator<?> i1 = ((Iterable<?>)p).iterator();
-      int i = 0;
-      while (i1.hasNext()) {
-        Object o1 = i1.next();
-        Class<?> t = o1 == null ? Object.class : o1.getClass(); // ...
-        dumpObject(errorStr, name + "[" + (i++) + "]", o1, t, level + 1);
-      }
-    } else if (c.isArray()) {
-      int len = Array.getLength(p);
-      Class<?> t = c.getComponentType();
-      errorStr.append(offsetStr).append(name + " is an array\n");
-      for (int i = 0; i < len; ++i) {
-        dumpObject(errorStr, name + "[" + i + "]", Array.get(p, i), t, level + 1);
-      }
-    } else if (ClassUtils.isAssignable(c, Map.class)) {
-      Map<?,?> c1 = (Map<?,?>)p;
-      errorStr.append(offsetStr).append(name + " is a map\n");
-      dumpObject(errorStr, name + ".keys", c1.keySet(), Set.class, level + 1);
-      dumpObject(errorStr, name + ".vals", c1.values(), Collection.class, level + 1);
-    } else {
-      errorStr.append(offsetStr).append(name + " is of type " + c.getCanonicalName() + "\n");
-      // TODO: this doesn't include superclass.
-      Field[] fields = c.getDeclaredFields();
-      AccessibleObject.setAccessible(fields, true);
-      for (int i = 0; i < fields.length; i++) {
-        Field f = fields[i];
-        if (f.getName().indexOf('$') != -1 || Modifier.isStatic(f.getModifiers())) continue;
-        dumpObject(errorStr, name + "." + f.getName(), f.get(p), f.getType(), level + 1);
-      }
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/BaseTestQueries.java b/src/ql/src/test/org/apache/hadoop/hive/ql/BaseTestQueries.java
deleted file mode 100644
index 40674cf..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/BaseTestQueries.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql;
-
-import java.io.File;
-
-import junit.framework.TestCase;
-
-/**
- * Base class for testing queries.
- */
-public abstract class BaseTestQueries extends TestCase {
-
-  protected final String inpDir = System
-      .getProperty("hive.root") + "/ql/src/test/queries/clientpositive";
-  protected final String resDir = System
-      .getProperty("hive.root") + "/ql/src/test/results/clientpositive";
-  protected final String logDir = System
-      .getProperty("build.dir") + "/junit-qfile-results/clientpositive";
-
-  /**
-   * Create a file for each test name in the inpDir.
-   * @param testNames
-   * @return files corresponding to each test name
-   */
-  protected File[] setupQFiles(String[] testNames) {
-    File[] qfiles = new File[testNames.length];
-    for (int i = 0; i < testNames.length; i++) {
-      qfiles[i] = new File(inpDir, testNames[i]);
-    }
-    return qfiles;
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java b/src/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
deleted file mode 100644
index 8ce1bbb..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
+++ /dev/null
@@ -1,1536 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql;
-
-import static org.apache.hadoop.hive.metastore.MetaStoreUtils.DEFAULT_DATABASE_NAME;
-
-import java.io.BufferedInputStream;
-import java.io.BufferedReader;
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileNotFoundException;
-import java.io.FileOutputStream;
-import java.io.FileReader;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.io.PrintStream;
-import java.io.Serializable;
-import java.io.StringWriter;
-import java.io.UnsupportedEncodingException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Deque;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Set;
-import java.util.TreeMap;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-import org.apache.commons.io.IOUtils;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;
-import org.apache.hadoop.hive.cli.CliDriver;
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.common.io.CachingPrintStream;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.metastore.api.Index;
-import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
-import org.apache.hadoop.hive.ql.exec.Task;
-import org.apache.hadoop.hive.ql.exec.Utilities;
-import org.apache.hadoop.hive.ql.exec.Utilities.StreamPrinter;
-import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
-import org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager;
-import org.apache.hadoop.hive.ql.metadata.Hive;
-import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.parse.ASTNode;
-import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;
-import org.apache.hadoop.hive.ql.parse.ParseDriver;
-import org.apache.hadoop.hive.ql.parse.ParseException;
-import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
-import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer;
-import org.apache.hadoop.hive.serde2.thrift.test.Complex;
-import org.apache.hadoop.hive.shims.HadoopShims;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.mapred.SequenceFileInputFormat;
-import org.apache.hadoop.mapred.SequenceFileOutputFormat;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.util.Shell;
-import org.apache.thrift.protocol.TBinaryProtocol;
-import org.apache.zookeeper.WatchedEvent;
-import org.apache.zookeeper.Watcher;
-import org.apache.zookeeper.ZooKeeper;
-
-/**
- * QTestUtil.
- *
- */
-public class QTestUtil {
-
-  public static final String UTF_8 = "UTF-8";
-  private static final Log LOG = LogFactory.getLog("QTestUtil");
-
-  private String testWarehouse;
-  private final String testFiles;
-  protected final String outDir;
-  protected final String logDir;
-  private final TreeMap<String, String> qMap;
-  private final Set<String> qSkipSet;
-  private final Set<String> qSortSet;
-  private static final String SORT_SUFFIX = ".sorted";
-  public static final HashSet<String> srcTables = new HashSet<String>();
-  private ParseDriver pd;
-  private Hive db;
-  protected HiveConf conf;
-  private Driver drv;
-  private BaseSemanticAnalyzer sem;
-  private FileSystem fs;
-  protected final boolean overWrite;
-  private CliDriver cliDriver;
-  private HadoopShims.MiniMrShim mr = null;
-  private HadoopShims.MiniDFSShim dfs = null;
-  private boolean miniMr = false;
-  private String hadoopVer = null;
-  private QTestSetup setup = null;
-
-  static {
-    for (String srcTable : System.getProperty("test.src.tables", "").trim().split(",")) {
-      srcTable = srcTable.trim();
-      if (!srcTable.isEmpty()) {
-        srcTables.add(srcTable);
-      }
-    }
-    if (srcTables.isEmpty()) {
-      throw new AssertionError("Source tables cannot be empty");
-    }
-  }
-
-  public boolean deleteDirectory(File path) {
-    if (path.exists()) {
-      File[] files = path.listFiles();
-      for (File file : files) {
-        if (file.isDirectory()) {
-          deleteDirectory(file);
-        } else {
-          file.delete();
-        }
-      }
-    }
-    return (path.delete());
-  }
-
-  public void copyDirectoryToLocal(Path src, Path dest) throws Exception {
-
-    FileSystem srcFs = src.getFileSystem(conf);
-    FileSystem destFs = dest.getFileSystem(conf);
-    if (srcFs.exists(src)) {
-      FileStatus[] files = srcFs.listStatus(src);
-      for (FileStatus file : files) {
-        String name = file.getPath().getName();
-        Path dfs_path = file.getPath();
-        Path local_path = new Path(dest, name);
-
-        // If this is a source table we do not copy it out
-        if (srcTables.contains(name)) {
-          continue;
-        }
-
-        if (file.isDir()) {
-          if (!destFs.exists(local_path)) {
-            destFs.mkdirs(local_path);
-          }
-          copyDirectoryToLocal(dfs_path, local_path);
-        } else {
-          srcFs.copyToLocalFile(dfs_path, local_path);
-        }
-      }
-    }
-  }
-
-  static Pattern mapTok = Pattern.compile("(\\.?)(.*)_map_(.*)");
-  static Pattern reduceTok = Pattern.compile("(.*)(reduce_[^\\.]*)((\\..*)?)");
-
-  public void normalizeNames(File path) throws Exception {
-    if (path.isDirectory()) {
-      File[] files = path.listFiles();
-      for (File file : files) {
-        normalizeNames(file);
-      }
-    } else {
-      Matcher m = reduceTok.matcher(path.getName());
-      if (m.matches()) {
-        String name = m.group(1) + "reduce" + m.group(3);
-        path.renameTo(new File(path.getParent(), name));
-      } else {
-        m = mapTok.matcher(path.getName());
-        if (m.matches()) {
-          String name = m.group(1) + "map_" + m.group(3);
-          path.renameTo(new File(path.getParent(), name));
-        }
-      }
-    }
-  }
-
-  public QTestUtil(String outDir, String logDir) throws Exception {
-    this(outDir, logDir, false, "0.20");
-  }
-
-  public String getOutputDirectory() {
-    return outDir;
-  }
-
-  public String getLogDirectory() {
-    return logDir;
-  }
-
-  private String getHadoopMainVersion(String input) {
-    if (input == null) {
-      return null;
-    }
-    Pattern p = Pattern.compile("^(\\d+\\.\\d+).*");
-    Matcher m = p.matcher(input);
-    if (m.matches()) {
-      return m.group(1);
-    }
-    return null;
-  }
-
-  public void initConf() throws Exception {
-
-    if (Shell.WINDOWS) {
-      convertPathsFromWindowsToHdfs();
-    }
-
-    // Plug verifying metastore in for testing.
-    conf.setVar(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL,
-        "org.apache.hadoop.hive.metastore.VerifyingObjectStore");
-
-    if (miniMr) {
-      assert dfs != null;
-      assert mr != null;
-
-      mr.setupConfiguration(conf);
-
-      // set fs.default.name to the uri of mini-dfs
-      String dfsUriString = getHdfsUriString(dfs.getFileSystem().getUri().toString());
-      conf.setVar(HiveConf.ConfVars.HADOOPFS, dfsUriString);
-      // hive.metastore.warehouse.dir needs to be set relative to the mini-dfs
-      conf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE,
-                  (new Path(dfsUriString,
-                            "/build/ql/test/data/warehouse/")).toString());
-    }
-  }
-
-  private void convertPathsFromWindowsToHdfs() {
-    // Following local paths are used as HDFS paths in unit tests.
-    // It works well in Unix as the path notation in Unix and HDFS is more or less same.
-    // But when it comes to Windows, drive letter separator ':' & backslash '\" are invalid
-    // characters in HDFS so we need to converts these local paths to HDFS paths before using them
-    // in unit tests.
-
-    // hive.exec.scratchdir needs to be set relative to the mini-dfs
-    String orgWarehouseDir = conf.getVar(HiveConf.ConfVars.METASTOREWAREHOUSE);
-    conf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE, getHdfsUriString(orgWarehouseDir));
-
-    String orgTestTempDir = System.getProperty("test.tmp.dir");
-    System.setProperty("test.tmp.dir", getHdfsUriString(orgTestTempDir));
-
-    String orgTestDataDir = System.getProperty("test.src.data.dir");
-    System.setProperty("test.src.data.dir", getHdfsUriString(orgTestDataDir));
-
-    String orgScratchDir = conf.getVar(HiveConf.ConfVars.SCRATCHDIR);
-    conf.setVar(HiveConf.ConfVars.SCRATCHDIR, getHdfsUriString(orgScratchDir));
-
-    if (miniMr) {
-      String orgAuxJarFolder = conf.getAuxJars();
-      conf.setAuxJars(getHdfsUriString("file://" + orgAuxJarFolder));
-    }
-  }
-
-  private String getHdfsUriString(String uriStr) {
-    assert uriStr != null;
-    if(Shell.WINDOWS) {
-      // If the URI conversion is from Windows to HDFS then replace the '\' with '/'
-      // and remove the windows single drive letter & colon from absolute path.
-      return uriStr.replace('\\', '/')
-        .replaceFirst("/[c-zC-Z]:", "/")
-        .replaceFirst("^[c-zC-Z]:", "");
-    }
-
-    return uriStr;
-  }
-
-  public QTestUtil(String outDir, String logDir, boolean miniMr, String hadoopVer)
-    throws Exception {
-    this.outDir = outDir;
-    this.logDir = logDir;
-    conf = new HiveConf(Driver.class);
-    this.miniMr = miniMr;
-    this.hadoopVer = getHadoopMainVersion(hadoopVer);
-    qMap = new TreeMap<String, String>();
-    qSkipSet = new HashSet<String>();
-    qSortSet = new HashSet<String>();
-
-    if (miniMr) {
-      dfs = ShimLoader.getHadoopShims().getMiniDfs(conf, 4, true, null);
-      FileSystem fs = dfs.getFileSystem();
-      mr = ShimLoader.getHadoopShims().getMiniMrCluster(conf, 4, getHdfsUriString(fs.getUri().toString()), 1);
-    }
-
-    initConf();
-
-    // Use the current directory if it is not specified
-    String dataDir = conf.get("test.data.files");
-    if (dataDir == null) {
-      dataDir = new File(".").getAbsolutePath() + "/data/files";
-    }
-
-    testFiles = dataDir;
-
-    overWrite = "true".equalsIgnoreCase(System.getProperty("test.output.overwrite"));
-
-    setup = new QTestSetup();
-    setup.preTest(conf);
-    init();
-  }
-
-  public void shutdown() throws Exception {
-    cleanUp();
-    setup.tearDown();
-    if (mr != null) {
-      mr.shutdown();
-      mr = null;
-    }
-    FileSystem.closeAll();
-    if (dfs != null) {
-      dfs.shutdown();
-      dfs = null;
-    }
-  }
-
-  public String readEntireFileIntoString(File queryFile) throws IOException {
-    InputStreamReader isr = new InputStreamReader(
-        new BufferedInputStream(new FileInputStream(queryFile)), QTestUtil.UTF_8);
-    StringWriter sw = new StringWriter();
-    try {
-      IOUtils.copy(isr, sw);
-    } finally {
-      if (isr != null) {
-        isr.close();
-      }
-    }
-    return sw.toString();
-  }
-
-  public void addFile(String queryFile) throws IOException {
-    addFile(new File(queryFile));
-  }
-
-  public void addFile(File qf) throws IOException  {
-    String query = readEntireFileIntoString(qf);
-    qMap.put(qf.getName(), query);
-
-    if(checkHadoopVersionExclude(qf.getName(), query)
-      || checkOSExclude(qf.getName(), query)) {
-      qSkipSet.add(qf.getName());
-    }
-
-    if (checkNeedsSort(qf.getName(), query)) {
-      qSortSet.add(qf.getName());
-    }
-  }
-
-  private boolean checkNeedsSort(String fileName, String query) {
-    Pattern pattern = Pattern.compile("-- SORT_BEFORE_DIFF");
-    Matcher matcher = pattern.matcher(query);
-
-    if (matcher.find()) {
-      return true;
-    }
-    return false;
-  }
-
-  private boolean checkHadoopVersionExclude(String fileName, String query){
-
-    // Look for a hint to not run a test on some Hadoop versions
-    Pattern pattern = Pattern.compile("-- (EX|IN)CLUDE_HADOOP_MAJOR_VERSIONS\\((.*)\\)");
-
-    boolean excludeQuery = false;
-    boolean includeQuery = false;
-    Set<String> versionSet = new HashSet<String>();
-    String hadoopVer = ShimLoader.getMajorVersion();
-
-    Matcher matcher = pattern.matcher(query);
-
-    // Each qfile may include at most one INCLUDE or EXCLUDE directive.
-    //
-    // If a qfile contains an INCLUDE directive, and hadoopVer does
-    // not appear in the list of versions to include, then the qfile
-    // is skipped.
-    //
-    // If a qfile contains an EXCLUDE directive, and hadoopVer is
-    // listed in the list of versions to EXCLUDE, then the qfile is
-    // skipped.
-    //
-    // Otherwise, the qfile is included.
-
-    if (matcher.find()) {
-
-      String prefix = matcher.group(1);
-      if ("EX".equals(prefix)) {
-        excludeQuery = true;
-      } else {
-        includeQuery = true;
-      }
-
-      String versions = matcher.group(2);
-      for (String s : versions.split("\\,")) {
-        s = s.trim();
-        versionSet.add(s);
-      }
-    }
-
-    if (matcher.find()) {
-      //2nd match is not supposed to be there
-      String message = "QTestUtil: qfile " + fileName
-        + " contains more than one reference to (EX|IN)CLUDE_HADOOP_MAJOR_VERSIONS";
-      throw new UnsupportedOperationException(message);
-    }
-
-    if (excludeQuery && versionSet.contains(hadoopVer)) {
-      System.out.println("QTestUtil: " + fileName
-        + " EXCLUDE list contains Hadoop Version " + hadoopVer + ". Skipping...");
-      return true;
-    } else if (includeQuery && !versionSet.contains(hadoopVer)) {
-      System.out.println("QTestUtil: " + fileName
-        + " INCLUDE list does not contain Hadoop Version " + hadoopVer + ". Skipping...");
-      return true;
-    }
-    return false;
-  }
-
-  private boolean checkOSExclude(String fileName, String query){
-    // Look for a hint to not run a test on some Hadoop versions
-    Pattern pattern = Pattern.compile("-- (EX|IN)CLUDE_OS_WINDOWS");
-
-    // detect whether this query wants to be excluded or included
-    // on windows
-    Matcher matcher = pattern.matcher(query);
-    if (matcher.find()) {
-      String prefix = matcher.group(1);
-      if ("EX".equals(prefix)) {
-        //windows is to be exluded
-        if(Shell.WINDOWS){
-          System.out.println("Due to the OS being windows " +
-                             "adding the  query " + fileName +
-                             " to the set of tests to skip");
-          return true;
-        }
-      }
-      else  if(!Shell.WINDOWS){
-        //non windows to be exluded
-        System.out.println("Due to the OS not being windows " +
-                           "adding the  query " + fileName +
-                           " to the set of tests to skip");
-        return true;
-      }
-    }
-    return false;
-  }
-
-
-  /**
-   * Clear out any side effects of running tests
-   */
-  public void clearPostTestEffects() throws Exception {
-    setup.postTest(conf);
-  }
-
-  /**
-   * Clear out any side effects of running tests
-   */
-  public void clearTestSideEffects() throws Exception {
-    // Delete any tables other than the source tables
-    // and any databases other than the default database.
-    for (String dbName : db.getAllDatabases()) {
-      SessionState.get().setCurrentDatabase(dbName);
-      for (String tblName : db.getAllTables()) {
-        if (!DEFAULT_DATABASE_NAME.equals(dbName) || !srcTables.contains(tblName)) {
-          Table tblObj = db.getTable(tblName);
-          // dropping index table can not be dropped directly. Dropping the base
-          // table will automatically drop all its index table
-          if(tblObj.isIndexTable()) {
-            continue;
-          }
-          db.dropTable(dbName, tblName);
-        } else {
-          // this table is defined in srcTables, drop all indexes on it
-         List<Index> indexes = db.getIndexes(dbName, tblName, (short)-1);
-          if (indexes != null && indexes.size() > 0) {
-            for (Index index : indexes) {
-              db.dropIndex(dbName, tblName, index.getIndexName(), true);
-            }
-          }
-        }
-      }
-      if (!DEFAULT_DATABASE_NAME.equals(dbName)) {
-        db.dropDatabase(dbName);
-      }
-    }
-    SessionState.get().setCurrentDatabase(DEFAULT_DATABASE_NAME);
-
-    List<String> roleNames = db.getAllRoleNames();
-      for (String roleName : roleNames) {
-        db.dropRole(roleName);
-    }
-    // allocate and initialize a new conf since a test can
-    // modify conf by using 'set' commands
-    conf = new HiveConf (Driver.class);
-    initConf();
-    db = Hive.get(conf);  // propagate new conf to meta store
-    setup.preTest(conf);
-  }
-
-  public void cleanUp() throws Exception {
-    // Drop any tables that remain due to unsuccessful runs
-    for (String s : new String[] {"src", "src1", "src_json", "src_thrift",
-        "src_sequencefile", "srcpart", "srcbucket", "srcbucket2", "dest1",
-        "dest2", "dest3", "dest4", "dest4_sequencefile", "dest_j1", "dest_j2",
-        "dest_g1", "dest_g2", "fetchtask_ioexception"}) {
-      db.dropTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, s);
-    }
-
-    // delete any contents in the warehouse dir
-    Path p = new Path(testWarehouse);
-    FileSystem fs = p.getFileSystem(conf);
-
-    try {
-      FileStatus [] ls = fs.listStatus(p);
-      for (int i=0; (ls != null) && (i<ls.length); i++) {
-        fs.delete(ls[i].getPath(), true);
-      }
-    } catch (FileNotFoundException e) {
-      // Best effort
-    }
-
-    FunctionRegistry.unregisterTemporaryUDF("test_udaf");
-    FunctionRegistry.unregisterTemporaryUDF("test_error");
-  }
-
-  private void runLoadCmd(String loadCmd) throws Exception {
-    int ecode = 0;
-    ecode = drv.run(loadCmd).getResponseCode();
-    drv.close();
-    if (ecode != 0) {
-      throw new Exception("load command: " + loadCmd
-          + " failed with exit code= " + ecode);
-    }
-    return;
-  }
-
-  private void runCreateTableCmd(String createTableCmd) throws Exception {
-    int ecode = 0;
-    ecode = drv.run(createTableCmd).getResponseCode();
-    if (ecode != 0) {
-      throw new Exception("create table command: " + createTableCmd
-          + " failed with exit code= " + ecode);
-    }
-
-    return;
-  }
-
-  public void createSources() throws Exception {
-
-    startSessionState();
-    conf.setBoolean("hive.test.init.phase", true);
-
-    // Create a bunch of tables with columns key and value
-    LinkedList<String> cols = new LinkedList<String>();
-    cols.add("key");
-    cols.add("value");
-
-    LinkedList<String> part_cols = new LinkedList<String>();
-    part_cols.add("ds");
-    part_cols.add("hr");
-    db.createTable("srcpart", cols, part_cols, TextInputFormat.class,
-        IgnoreKeyTextOutputFormat.class);
-
-    Path fpath;
-    HashMap<String, String> part_spec = new HashMap<String, String>();
-    for (String ds : new String[] {"2008-04-08", "2008-04-09"}) {
-      for (String hr : new String[] {"11", "12"}) {
-        part_spec.clear();
-        part_spec.put("ds", ds);
-        part_spec.put("hr", hr);
-        // System.out.println("Loading partition with spec: " + part_spec);
-        // db.createPartition(srcpart, part_spec);
-        fpath = new Path(testFiles, "kv1.txt");
-        // db.loadPartition(fpath, srcpart.getName(), part_spec, true);
-        runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
-            + "' OVERWRITE INTO TABLE srcpart PARTITION (ds='" + ds + "',hr='"
-            + hr + "')");
-      }
-    }
-    ArrayList<String> bucketCols = new ArrayList<String>();
-    bucketCols.add("key");
-    runCreateTableCmd("CREATE TABLE srcbucket(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE");
-    // db.createTable("srcbucket", cols, null, TextInputFormat.class,
-    // IgnoreKeyTextOutputFormat.class, 2, bucketCols);
-    for (String fname : new String[] {"srcbucket0.txt", "srcbucket1.txt"}) {
-      fpath = new Path(testFiles, fname);
-      runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
-          + "' INTO TABLE srcbucket");
-    }
-
-    runCreateTableCmd("CREATE TABLE srcbucket2(key int, value string) "
-        + "CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE");
-    // db.createTable("srcbucket", cols, null, TextInputFormat.class,
-    // IgnoreKeyTextOutputFormat.class, 2, bucketCols);
-    for (String fname : new String[] {"srcbucket20.txt", "srcbucket21.txt",
-        "srcbucket22.txt", "srcbucket23.txt"}) {
-      fpath = new Path(testFiles, fname);
-      runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
-          + "' INTO TABLE srcbucket2");
-    }
-
-    for (String tname : new String[] {"src", "src1"}) {
-      db.createTable(tname, cols, null, TextInputFormat.class,
-          IgnoreKeyTextOutputFormat.class);
-    }
-    db.createTable("src_sequencefile", cols, null,
-        SequenceFileInputFormat.class, SequenceFileOutputFormat.class);
-
-    Table srcThrift =
-        new Table(SessionState.get().getCurrentDatabase(), "src_thrift");
-    srcThrift.setInputFormatClass(SequenceFileInputFormat.class.getName());
-    srcThrift.setOutputFormatClass(SequenceFileOutputFormat.class.getName());
-    srcThrift.setSerializationLib(ThriftDeserializer.class.getName());
-    srcThrift.setSerdeParam(serdeConstants.SERIALIZATION_CLASS, Complex.class
-        .getName());
-    srcThrift.setSerdeParam(serdeConstants.SERIALIZATION_FORMAT,
-        TBinaryProtocol.class.getName());
-    db.createTable(srcThrift);
-
-    LinkedList<String> json_cols = new LinkedList<String>();
-    json_cols.add("json");
-    db.createTable("src_json", json_cols, null, TextInputFormat.class,
-        IgnoreKeyTextOutputFormat.class);
-
-    // load the input data into the src table
-    fpath = new Path(testFiles, "kv1.txt");
-    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath() + "' INTO TABLE src");
-
-    // load the input data into the src table
-    fpath = new Path(testFiles, "kv3.txt");
-    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath() + "' INTO TABLE src1");
-
-    // load the input data into the src_sequencefile table
-    fpath = new Path(testFiles, "kv1.seq");
-    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
-        + "' INTO TABLE src_sequencefile");
-
-    // load the input data into the src_thrift table
-    fpath = new Path(testFiles, "complex.seq");
-    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
-        + "' INTO TABLE src_thrift");
-
-    // load the json data into the src_json table
-    fpath = new Path(testFiles, "json.txt");
-    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
-        + "' INTO TABLE src_json");
-    conf.setBoolean("hive.test.init.phase", false);
-  }
-
-  public void init() throws Exception {
-    // System.out.println(conf.toString());
-    testWarehouse = conf.getVar(HiveConf.ConfVars.METASTOREWAREHOUSE);
-    // conf.logVars(System.out);
-    // System.out.flush();
-
-    SessionState.start(conf);
-    db = Hive.get(conf);
-    fs = FileSystem.get(conf);
-    drv = new Driver(conf);
-    drv.init();
-    pd = new ParseDriver();
-    sem = new SemanticAnalyzer(conf);
-  }
-
-  public void init(String tname) throws Exception {
-    cleanUp();
-    createSources();
-
-    LinkedList<String> cols = new LinkedList<String>();
-    cols.add("key");
-    cols.add("value");
-
-    LinkedList<String> part_cols = new LinkedList<String>();
-    part_cols.add("ds");
-    part_cols.add("hr");
-
-    db.createTable("dest1", cols, null, TextInputFormat.class,
-        IgnoreKeyTextOutputFormat.class);
-    db.createTable("dest2", cols, null, TextInputFormat.class,
-        IgnoreKeyTextOutputFormat.class);
-
-    db.createTable("dest3", cols, part_cols, TextInputFormat.class,
-        IgnoreKeyTextOutputFormat.class);
-    Table dest3 = db.getTable("dest3");
-
-    HashMap<String, String> part_spec = new HashMap<String, String>();
-    part_spec.put("ds", "2008-04-08");
-    part_spec.put("hr", "12");
-    db.createPartition(dest3, part_spec);
-
-    db.createTable("dest4", cols, null, TextInputFormat.class,
-        IgnoreKeyTextOutputFormat.class);
-    db.createTable("dest4_sequencefile", cols, null,
-        SequenceFileInputFormat.class, SequenceFileOutputFormat.class);
-  }
-
-  public void cliInit(String tname) throws Exception {
-    cliInit(tname, true);
-  }
-
-  public void cliInit(String tname, boolean recreate) throws Exception {
-    if (recreate) {
-      cleanUp();
-      createSources();
-    }
-
-    HiveConf.setVar(conf, HiveConf.ConfVars.HIVE_AUTHENTICATOR_MANAGER,
-    "org.apache.hadoop.hive.ql.security.DummyAuthenticator");
-    CliSessionState ss = new CliSessionState(conf);
-    assert ss != null;
-    ss.in = System.in;
-
-    File qf = new File(outDir, tname);
-    File outf = null;
-    outf = new File(logDir);
-    outf = new File(outf, qf.getName().concat(".out"));
-    FileOutputStream fo = new FileOutputStream(outf);
-    ss.out = new PrintStream(fo, true, "UTF-8");
-    ss.err = new CachingPrintStream(fo, true, "UTF-8");
-    ss.setIsSilent(true);
-    SessionState oldSs = SessionState.get();
-    if (oldSs != null && oldSs.out != null && oldSs.out != System.out) {
-      oldSs.out.close();
-    }
-    SessionState.start(ss);
-
-    cliDriver = new CliDriver();
-    if (tname.equals("init_file.q")) {
-      ss.initFiles.add("../../data/scripts/test_init_file.sql");
-    }
-    cliDriver.processInitFiles(ss);
-  }
-
-  private CliSessionState startSessionState()
-      throws FileNotFoundException, UnsupportedEncodingException {
-
-    HiveConf.setVar(conf, HiveConf.ConfVars.HIVE_AUTHENTICATOR_MANAGER,
-        "org.apache.hadoop.hive.ql.security.DummyAuthenticator");
-
-    CliSessionState ss = new CliSessionState(conf);
-    assert ss != null;
-
-    SessionState.start(ss);
-    return ss;
-  }
-
-  public int executeOne(String tname) {
-    String q = qMap.get(tname);
-
-    if (q.indexOf(";") == -1) {
-      return -1;
-    }
-
-    String q1 = q.substring(0, q.indexOf(";") + 1);
-    String qrest = q.substring(q.indexOf(";") + 1);
-    qMap.put(tname, qrest);
-
-    System.out.println("Executing " + q1);
-    return cliDriver.processLine(q1);
-  }
-
-  public int execute(String tname) {
-    try {
-      return drv.run(qMap.get(tname)).getResponseCode();
-    } catch (CommandNeedRetryException e) {
-      // TODO Auto-generated catch block
-      e.printStackTrace();
-      return -1;
-    }
-  }
-
-  public int executeClient(String tname) {
-    String commands = qMap.get(tname);
-    StringBuilder newCommands = new StringBuilder(commands.length());
-    int lastMatchEnd = 0;
-    Matcher commentMatcher = Pattern.compile("^--.*$", Pattern.MULTILINE).matcher(commands);
-    while (commentMatcher.find()) {
-      newCommands.append(commands.substring(lastMatchEnd, commentMatcher.start()));
-      newCommands.append(commentMatcher.group().replaceAll("(?<!\\\\);", "\\\\;"));
-      lastMatchEnd = commentMatcher.end();
-    }
-    newCommands.append(commands.substring(lastMatchEnd, commands.length()));
-    commands = newCommands.toString();
-    return cliDriver.processLine(commands);
-  }
-
-  public boolean shouldBeSkipped(String tname) {
-    return qSkipSet.contains(tname);
-  }
-
-  public void convertSequenceFileToTextFile() throws Exception {
-    // Create an instance of hive in order to create the tables
-    testWarehouse = conf.getVar(HiveConf.ConfVars.METASTOREWAREHOUSE);
-    db = Hive.get(conf);
-    // Create dest4 to replace dest4_sequencefile
-    LinkedList<String> cols = new LinkedList<String>();
-    cols.add("key");
-    cols.add("value");
-
-    // Move all data from dest4_sequencefile to dest4
-    drv
-        .run("FROM dest4_sequencefile INSERT OVERWRITE TABLE dest4 SELECT dest4_sequencefile.*");
-
-    // Drop dest4_sequencefile
-    db.dropTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, "dest4_sequencefile",
-        true, true);
-  }
-
-  public int checkNegativeResults(String tname, Exception e) throws Exception {
-
-    File qf = new File(outDir, tname);
-    String expf = outPath(outDir.toString(), tname.concat(".out"));
-
-    File outf = null;
-    outf = new File(logDir);
-    outf = new File(outf, qf.getName().concat(".out"));
-
-    FileWriter outfd = new FileWriter(outf);
-    if (e instanceof ParseException) {
-      outfd.write("Parse Error: ");
-    } else if (e instanceof SemanticException) {
-      outfd.write("Semantic Exception: \n");
-    } else {
-      throw e;
-    }
-
-    outfd.write(e.getMessage());
-    outfd.close();
-
-    int exitVal = executeDiffCommand(outf.getPath(), expf, false,
-                                     qSortSet.contains(qf.getName()));
-    if (exitVal != 0 && overWrite) {
-      exitVal = overwriteResults(outf.getPath(), expf);
-    }
-
-    return exitVal;
-  }
-
-  public int checkParseResults(String tname, ASTNode tree) throws Exception {
-
-    if (tree != null) {
-      File parseDir = new File(outDir, "parse");
-      String expf = outPath(parseDir.toString(), tname.concat(".out"));
-
-      File outf = null;
-      outf = new File(logDir);
-      outf = new File(outf, tname.concat(".out"));
-
-      FileWriter outfd = new FileWriter(outf);
-      outfd.write(tree.toStringTree());
-      outfd.close();
-
-      int exitVal = executeDiffCommand(outf.getPath(), expf, false, false);
-
-      if (exitVal != 0 && overWrite) {
-        exitVal = overwriteResults(outf.getPath(), expf);
-      }
-
-      return exitVal;
-    } else {
-      throw new Exception("Parse tree is null");
-    }
-  }
-
-  private final Pattern[] xmlPlanMask = toPattern(new String[] {
-      "<java version=\".*\" class=\"java.beans.XMLDecoder\">",
-      "<string>.*/tmp/.*</string>",
-      "<string>file:.*</string>",
-      "<string>pfile:.*</string>",
-      "<string>[0-9]{10}</string>",
-      "<string>/.*/warehouse/.*</string>"
-  });
-
-  public int checkPlan(String tname, List<Task<? extends Serializable>> tasks) throws Exception {
-
-    if (tasks == null) {
-      throw new Exception("Plan is null");
-    }
-    File planDir = new File(outDir, "plan");
-    String planFile = outPath(planDir.toString(), tname + ".xml");
-
-    File outf = null;
-    outf = new File(logDir);
-    outf = new File(outf, tname.concat(".xml"));
-
-    FileOutputStream ofs = new FileOutputStream(outf);
-    try {
-      conf.set(HiveConf.ConfVars.PLAN_SERIALIZATION.varname, "javaXML");
-      for (Task<? extends Serializable> plan : tasks) {
-        Utilities.serializePlan(plan, ofs, conf);
-      }
-
-      fixXml4JDK7(outf.getPath());
-      maskPatterns(xmlPlanMask, outf.getPath());
-
-      int exitVal = executeDiffCommand(outf.getPath(), planFile, true, false);
-
-      if (exitVal != 0 && overWrite) {
-        exitVal = overwriteResults(outf.getPath(), planFile);
-      }
-      return exitVal;
-    } finally {
-      conf.set(HiveConf.ConfVars.PLAN_SERIALIZATION.varname, "kryo");
-    }
-  }
-
-  /**
-   * Given the current configurations (e.g., hadoop version and execution mode), return
-   * the correct file name to compare with the current test run output.
-   * @param outDir The directory where the reference log files are stored.
-   * @param testName The test file name (terminated by ".out").
-   * @return The file name appended with the configuration values if it exists.
-   */
-  public String outPath(String outDir, String testName) {
-    String ret = (new File(outDir, testName)).getPath();
-    // List of configurations. Currently the list consists of hadoop version and execution mode only
-    List<String> configs = new ArrayList<String>();
-    configs.add(this.hadoopVer);
-
-    Deque<String> stack = new LinkedList<String>();
-    StringBuilder sb = new StringBuilder();
-    sb.append(testName);
-    stack.push(sb.toString());
-
-    // example file names are input1.q.out_0.20.0_minimr or input2.q.out_0.17
-    for (String s: configs) {
-      sb.append('_');
-      sb.append(s);
-      stack.push(sb.toString());
-    }
-    while (stack.size() > 0) {
-      String fileName = stack.pop();
-      File f = new File(outDir, fileName);
-      if (f.exists()) {
-        ret = f.getPath();
-        break;
-      }
-    }
-   return ret;
-  }
-
-  /**
-   * Fix the XML generated by JDK7 which is slightly different from what's generated by JDK6,
-   * causing 40+ test failures. There are mainly two problems:
-   *
-   * 1. object element's properties, id and class, are in reverse order, i.e.
-   *    <object class="org.apache.hadoop.hive.ql.exec.MapRedTask" id="MapRedTask0">
-   *    which needs to be fixed to
-   *    <object id="MapRedTask0" class="org.apache.hadoop.hive.ql.exec.MapRedTask">
-   * 2. JDK introduces Enum as class, i.e.
-   *    <object id="GenericUDAFEvaluator$Mode0" class="java.lang.Enum">
-   *      <class>org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator$Mode</class>
-   *    which needs to be fixed to
-   *    <object id="GenericUDAFEvaluator$Mode0" class="org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator$Mode"
-   *     method="valueOf">
-   *
-   * Though not elegant, this allows these test cases to pass until we have a better serialization mechanism.
-   *
-   * Did I mention this is test code?
-   *
-   * @param fname the name of the file to fix
-   * @throws Exception in case of IO error
-   */
-  private static void fixXml4JDK7(String fname) throws Exception {
-    String version = System.getProperty("java.version");
-    if (!version.startsWith("1.7")) {
-      return;
-    }
-
-    BufferedReader in = new BufferedReader(new FileReader(fname));
-    BufferedWriter out = new BufferedWriter(new FileWriter(fname + ".orig"));
-    String line = null;
-    while (null != (line = in.readLine())) {
-      out.write(line);
-      out.write('\n');
-    }
-    in.close();
-    out.close();
-
-    in = new BufferedReader(new FileReader(fname + ".orig"));
-    out = new BufferedWriter(new FileWriter(fname));
-
-    while (null != (line = in.readLine())) {
-      if (line.indexOf("<object ") == -1 || line.indexOf("class=") == -1) {
-        out.write(line);
-      } else {
-        StringBuilder sb = new StringBuilder();
-        String prefix = line.substring(0, line.indexOf("<object") + 7);
-        sb.append( prefix );
-        String postfix = line.substring(line.lastIndexOf('"') + 1);
-        String id = getPropertyValue(line, "id");
-        if (id != null) {
-          sb.append(" id=" + id);
-        }
-        String cls = getPropertyValue(line, "class");
-        assert(cls != null);
-        if (cls.equals("\"java.lang.Enum\"")) {
-          line = in.readLine();
-          cls = "\"" + getElementValue(line, "class") + "\"";
-          sb.append(" class=" + cls + " method=\"valueOf\"" );
-        } else {
-          sb.append(" class=" + cls);
-        }
-
-        sb.append(postfix);
-        out.write(sb.toString());
-      }
-
-      out.write('\n');
-    }
-
-    in.close();
-    out.close();
-  }
-
-  /**
-   * Get the value of a property in line. The returned value has original quotes
-   */
-  private static String getPropertyValue(String line, String name) {
-    int start = line.indexOf( name + "=" );
-    if (start == -1) {
-      return null;
-    }
-    start += name.length() + 1;
-    int end = line.indexOf("\"", start + 1);
-    return line.substring( start, end + 1 );
-  }
-
-  /**
-   * Get the value of the element in input. (Note: the returned value has no quotes.)
-   */
-  private static String getElementValue(String line, String name) {
-    assert(line.contains("<" + name + ">"));
-    int start = line.indexOf("<" + name + ">") + name.length() + 2;
-    int end = line.indexOf("</" + name + ">");
-    return line.substring(start, end);
-  }
-
-  private Pattern[] toPattern(String[] patternStrs) {
-    Pattern[] patterns = new Pattern[patternStrs.length];
-    for (int i = 0; i < patternStrs.length; i++) {
-      patterns[i] = Pattern.compile(patternStrs[i]);
-    }
-    return patterns;
-  }
-
-  private void maskPatterns(Pattern[] patterns, String fname) throws Exception {
-    String maskPattern = "#### A masked pattern was here ####";
-
-    String line;
-    BufferedReader in;
-    BufferedWriter out;
-
-    in = new BufferedReader(new FileReader(fname));
-    out = new BufferedWriter(new FileWriter(fname + ".orig"));
-    while (null != (line = in.readLine())) {
-      // Ignore the empty lines on windows
-      if(line.isEmpty() && Shell.WINDOWS) {
-        continue;
-      }
-      out.write(line);
-      out.write('\n');
-    }
-    in.close();
-    out.close();
-
-    in = new BufferedReader(new FileReader(fname + ".orig"));
-    out = new BufferedWriter(new FileWriter(fname));
-
-    boolean lastWasMasked = false;
-    while (null != (line = in.readLine())) {
-      for (Pattern pattern : patterns) {
-        line = pattern.matcher(line).replaceAll(maskPattern);
-      }
-
-      if (line.equals(maskPattern)) {
-        // We're folding multiple masked lines into one.
-        if (!lastWasMasked) {
-          out.write(line);
-          out.write("\n");
-          lastWasMasked = true;
-        }
-      } else {
-        out.write(line);
-        out.write("\n");
-        lastWasMasked = false;
-      }
-    }
-
-    in.close();
-    out.close();
-  }
-
-  private final Pattern[] planMask = toPattern(new String[] {
-      ".*file:.*",
-      ".*pfile:.*",
-      ".*hdfs:.*",
-      ".*/tmp/.*",
-      ".*invalidscheme:.*",
-      ".*lastUpdateTime.*",
-      ".*lastAccessTime.*",
-      ".*lastModifiedTime.*",
-      ".*[Oo]wner.*",
-      ".*CreateTime.*",
-      ".*LastAccessTime.*",
-      ".*Location.*",
-      ".*LOCATION '.*",
-      ".*transient_lastDdlTime.*",
-      ".*last_modified_.*",
-      ".*at org.*",
-      ".*at sun.*",
-      ".*at java.*",
-      ".*at junit.*",
-      ".*Caused by:.*",
-      ".*LOCK_QUERYID:.*",
-      ".*LOCK_TIME:.*",
-      ".*grantTime.*",
-      ".*[.][.][.] [0-9]* more.*",
-      ".*job_[0-9_]*.*",
-      ".*job_local[0-9_]*.*",
-      ".*USING 'java -cp.*",
-      "^Deleted.*",
-  });
-
-  public int checkCliDriverResults(String tname) throws Exception {
-    assert(qMap.containsKey(tname));
-
-    String outFileName = outPath(outDir, tname + ".out");
-
-    File f = new File(logDir, tname + ".out");
-
-    maskPatterns(planMask, f.getPath());
-    int exitVal = executeDiffCommand(f.getPath(),
-                                     outFileName, false,
-                                     qSortSet.contains(tname));
-
-    if (exitVal != 0 && overWrite) {
-      exitVal = overwriteResults(f.getPath(), outFileName);
-    }
-
-    return exitVal;
-  }
-
-  private static int overwriteResults(String inFileName, String outFileName) throws Exception {
-    // This method can be replaced with Files.copy(source, target, REPLACE_EXISTING)
-    // once Hive uses JAVA 7.
-    System.out.println("Overwriting results " + inFileName + " to " + outFileName);
-    return executeCmd(new String[] {
-        "cp",
-        getQuotedString(inFileName),
-        getQuotedString(outFileName)
-      });
-  }
-
-  private static int executeDiffCommand(String inFileName,
-      String outFileName,
-      boolean ignoreWhiteSpace,
-      boolean sortResults
-      ) throws Exception {
-
-    int result = 0;
-
-    if (sortResults) {
-      // sort will try to open the output file in write mode on windows. We need to
-      // close it first.
-      SessionState ss = SessionState.get();
-      if (ss != null && ss.out != null && ss.out != System.out) {
-	ss.out.close();
-      }
-
-      String inSorted = inFileName + SORT_SUFFIX;
-      String outSorted = outFileName + SORT_SUFFIX;
-
-      result = sortFiles(inFileName, inSorted);
-      result |= sortFiles(outFileName, outSorted);
-      if (result != 0) {
-        System.err.println("ERROR: Could not sort files before comparing");
-        return result;
-      }
-      inFileName = inSorted;
-      outFileName = outSorted;
-    }
-
-    ArrayList<String> diffCommandArgs = new ArrayList<String>();
-    diffCommandArgs.add("diff");
-
-    // Text file comparison
-    diffCommandArgs.add("-a");
-
-    // Ignore changes in the amount of white space
-    if (ignoreWhiteSpace || Shell.WINDOWS) {
-      diffCommandArgs.add("-b");
-    }
-
-    // Files created on Windows machines have different line endings
-    // than files created on Unix/Linux. Windows uses carriage return and line feed
-    // ("\r\n") as a line ending, whereas Unix uses just line feed ("\n").
-    // Also StringBuilder.toString(), Stream to String conversions adds extra
-    // spaces at the end of the line.
-    if (Shell.WINDOWS) {
-      diffCommandArgs.add("--strip-trailing-cr"); // Strip trailing carriage return on input
-      diffCommandArgs.add("-B"); // Ignore changes whose lines are all blank
-    }
-    // Add files to compare to the arguments list
-    diffCommandArgs.add(getQuotedString(inFileName));
-    diffCommandArgs.add(getQuotedString(outFileName));
-
-    result = executeCmd(diffCommandArgs);
-
-    if (sortResults) {
-      new File(inFileName).delete();
-      new File(outFileName).delete();
-    }
-
-    return result;
-  }
-
-  private static int sortFiles(String in, String out) throws Exception {
-    return executeCmd(new String[] {
-        "sort",
-        getQuotedString(in),
-      }, out, null);
-  }
-
-  private static int executeCmd(Collection<String> args) throws Exception {
-    return executeCmd(args, null, null);
-  }
-
-  private static int executeCmd(String[] args) throws Exception {
-    return executeCmd(args, null, null);
-  }
-
-  private static int executeCmd(Collection<String> args, String outFile, String errFile) throws Exception {
-    String[] cmdArray = (String[]) args.toArray(new String[args.size()]);
-    return executeCmd(cmdArray, outFile, errFile);
-  }
-
-  private static int executeCmd(String[] args, String outFile, String errFile) throws Exception {
-    System.out.println("Running: " + org.apache.commons.lang.StringUtils.join(args, ' '));
-
-    PrintStream out = outFile == null ?
-      SessionState.getConsole().getChildOutStream() :
-      new PrintStream(new FileOutputStream(outFile), true);
-    PrintStream err = errFile == null ?
-      SessionState.getConsole().getChildErrStream() :
-      new PrintStream(new FileOutputStream(errFile), true);
-
-    Process executor = Runtime.getRuntime().exec(args);
-
-    StreamPrinter errPrinter = new StreamPrinter(executor.getErrorStream(), null, err);
-    StreamPrinter outPrinter = new StreamPrinter(executor.getInputStream(), null, out);
-
-    outPrinter.start();
-    errPrinter.start();
-
-    int result = executor.waitFor();
-
-    outPrinter.join();
-    errPrinter.join();
-
-    if (outFile != null) {
-      out.close();
-    }
-
-    if (errFile != null) {
-      err.close();
-    }
-
-    return result;
-  }
-
-  private static String getQuotedString(String str){
-    return Shell.WINDOWS ? String.format("\"%s\"", str) : str;
-  }
-
-  public ASTNode parseQuery(String tname) throws Exception {
-    return pd.parse(qMap.get(tname));
-  }
-
-  public void resetParser() throws SemanticException {
-    drv.init();
-    pd = new ParseDriver();
-    sem = new SemanticAnalyzer(conf);
-  }
-
-
-  public List<Task<? extends Serializable>> analyzeAST(ASTNode ast) throws Exception {
-
-    // Do semantic analysis and plan generation
-    Context ctx = new Context(conf);
-    while ((ast.getToken() == null) && (ast.getChildCount() > 0)) {
-      ast = (ASTNode) ast.getChild(0);
-    }
-    sem.getOutputs().clear();
-    sem.getInputs().clear();
-    sem.analyze(ast, ctx);
-    ctx.clear();
-    return sem.getRootTasks();
-  }
-
-  public TreeMap<String, String> getQMap() {
-    return qMap;
-  }
-
-  /**
-   * QTestSetup defines test fixtures which are reused across testcases,
-   * and are needed before any test can be run
-   */
-  public static class QTestSetup
-  {
-    private MiniZooKeeperCluster zooKeeperCluster = null;
-    private int zkPort;
-    private ZooKeeper zooKeeper;
-
-    public QTestSetup() {
-    }
-
-    public void preTest(HiveConf conf) throws Exception {
-
-      if (zooKeeperCluster == null) {
-        String tmpdir =  System.getProperty("test.tmp.dir");
-        zooKeeperCluster = new MiniZooKeeperCluster();
-        zkPort = zooKeeperCluster.startup(new File(tmpdir, "zookeeper"));
-      }
-
-      if (zooKeeper != null) {
-        zooKeeper.close();
-      }
-
-      int sessionTimeout = conf.getIntVar(HiveConf.ConfVars.HIVE_ZOOKEEPER_SESSION_TIMEOUT);
-      zooKeeper = new ZooKeeper("localhost:" + zkPort, sessionTimeout, new Watcher() {
-        @Override
-        public void process(WatchedEvent arg0) {
-        }
-      });
-
-      String zkServer = "localhost";
-      conf.set("hive.zookeeper.quorum", zkServer);
-      conf.set("hive.zookeeper.client.port", "" + zkPort);
-    }
-
-    public void postTest(HiveConf conf) throws Exception {
-      if (zooKeeperCluster == null) {
-        return;
-      }
-
-      if (zooKeeper != null) {
-        zooKeeper.close();
-      }
-
-      ZooKeeperHiveLockManager.releaseAllLocks(conf);
-    }
-
-    public void tearDown() throws Exception {
-      if (zooKeeperCluster != null) {
-        zooKeeperCluster.shutdown();
-        zooKeeperCluster = null;
-      }
-    }
-  }
-
-  /**
-   * QTRunner: Runnable class for running a a single query file.
-   *
-   **/
-  public static class QTRunner implements Runnable {
-    private final QTestUtil qt;
-    private final String fname;
-
-    public QTRunner(QTestUtil qt, String fname) {
-      this.qt = qt;
-      this.fname = fname;
-    }
-
-    public void run() {
-      try {
-        // assumption is that environment has already been cleaned once globally
-        // hence each thread does not call cleanUp() and createSources() again
-        qt.cliInit(fname, false);
-        qt.executeClient(fname);
-      } catch (Throwable e) {
-        System.err.println("Query file " + fname + " failed with exception "
-            + e.getMessage());
-        e.printStackTrace();
-        outputTestFailureHelpMessage();
-      }
-    }
-  }
-
-  /**
-   * Setup to execute a set of query files. Uses QTestUtil to do so.
-   *
-   * @param qfiles
-   *          array of input query files containing arbitrary number of hive
-   *          queries
-   * @param resDir
-   *          output directory
-   * @param logDir
-   *          log directory
-   * @return one QTestUtil for each query file
-   */
-  public static QTestUtil[] queryListRunnerSetup(File[] qfiles, String resDir,
-      String logDir) throws Exception
-  {
-    QTestUtil[] qt = new QTestUtil[qfiles.length];
-    for (int i = 0; i < qfiles.length; i++) {
-      qt[i] = new QTestUtil(resDir, logDir, false, "0.20");
-      qt[i].addFile(qfiles[i]);
-      qt[i].clearTestSideEffects();
-    }
-
-    return qt;
-  }
-
-  /**
-   * Executes a set of query files in sequence.
-   *
-   * @param qfiles
-   *          array of input query files containing arbitrary number of hive
-   *          queries
-   * @param qt
-   *          array of QTestUtils, one per qfile
-   * @return true if all queries passed, false otw
-   */
-  public static boolean queryListRunnerSingleThreaded(File[] qfiles, QTestUtil[] qt)
-    throws Exception
-  {
-    boolean failed = false;
-    qt[0].cleanUp();
-    qt[0].createSources();
-    for (int i = 0; i < qfiles.length && !failed; i++) {
-      qt[i].clearTestSideEffects();
-      qt[i].cliInit(qfiles[i].getName(), false);
-      qt[i].executeClient(qfiles[i].getName());
-      int ecode = qt[i].checkCliDriverResults(qfiles[i].getName());
-      if (ecode != 0) {
-        failed = true;
-        System.err.println("Test " + qfiles[i].getName()
-            + " results check failed with error code " + ecode);
-        outputTestFailureHelpMessage();
-      }
-      qt[i].clearPostTestEffects();
-    }
-    return (!failed);
-  }
-
-  /**
-   * Executes a set of query files parallel.
-   *
-   * Each query file is run in a separate thread. The caller has to arrange
-   * that different query files do not collide (in terms of destination tables)
-   *
-   * @param qfiles
-   *          array of input query files containing arbitrary number of hive
-   *          queries
-   * @param qt
-   *          array of QTestUtils, one per qfile
-   * @return true if all queries passed, false otw
-   *
-   */
-  public static boolean queryListRunnerMultiThreaded(File[] qfiles, QTestUtil[] qt)
-    throws Exception
-  {
-    boolean failed = false;
-
-    // in multithreaded mode - do cleanup/initialization just once
-
-    qt[0].cleanUp();
-    qt[0].createSources();
-    qt[0].clearTestSideEffects();
-
-    QTRunner[] qtRunners = new QTestUtil.QTRunner[qfiles.length];
-    Thread[] qtThread = new Thread[qfiles.length];
-
-    for (int i = 0; i < qfiles.length; i++) {
-      qtRunners[i] = new QTestUtil.QTRunner(qt[i], qfiles[i].getName());
-      qtThread[i] = new Thread(qtRunners[i]);
-    }
-
-    for (int i = 0; i < qfiles.length; i++) {
-      qtThread[i].start();
-    }
-
-    for (int i = 0; i < qfiles.length; i++) {
-      qtThread[i].join();
-      int ecode = qt[i].checkCliDriverResults(qfiles[i].getName());
-      if (ecode != 0) {
-        failed = true;
-        System.err.println("Test " + qfiles[i].getName()
-            + " results check failed with error code " + ecode);
-        outputTestFailureHelpMessage();
-      }
-    }
-    return (!failed);
-  }
-
-  public static void outputTestFailureHelpMessage() {
-    System.err.println("See build/ql/tmp/hive.log, "
-        + "or try \"ant test ... -Dtest.silent=false\" to get more logs.");
-    System.err.flush();
-  }
-
-  public static String ensurePathEndsInSlash(String path) {
-    if(path == null) {
-      throw new NullPointerException("Path cannot be null");
-    }
-    if(path.endsWith(File.separator)) {
-      return path;
-    } else {
-      return path + File.separator;
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/TestLocationQueries.java b/src/ql/src/test/org/apache/hadoop/hive/ql/TestLocationQueries.java
deleted file mode 100644
index 9384a35..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/TestLocationQueries.java
+++ /dev/null
@@ -1,116 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileReader;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-/**
- * Suite for testing location. e.g. if "alter table alter partition
- * location" is run, do the partitions end up in the correct location.
- *
- *  This is a special case of the regular queries as paths are typically
- *  ignored.
- */
-public class TestLocationQueries extends BaseTestQueries {
-
-  public TestLocationQueries() {
-    File logDirFile = new File(logDir);
-    if (!(logDirFile.exists() || logDirFile.mkdirs())) {
-      fail("Could not create " + logDir);
-    }
-  }
-
-  /**
-   * Our own checker - validate the location of the partition.
-   */
-  public static class CheckResults extends QTestUtil {
-    private final String locationSubdir;
-
-    /**
-     * Validate only that the location is correct.
-     * @return non-zero if it failed
-     */
-    @Override
-    public int checkCliDriverResults(String tname) throws Exception {
-      File logFile = new File(logDir, tname + ".out");
-
-      int failedCount = 0;
-      FileReader fr = new FileReader(logFile);
-      BufferedReader in = new BufferedReader(fr);
-      try {
-        String line;
-        int locationCount = 0;
-        Pattern p = Pattern.compile("location:([^,)]+)");
-        while((line = in.readLine()) != null) {
-          Matcher m = p.matcher(line);
-          if (m.find()) {
-            File f = new File(m.group(1));
-            if (!f.getName().equals(locationSubdir)) {
-              failedCount++;
-            }
-            locationCount++;
-          }
-        }
-        // we always have to find at least one location, otw the test is useless
-        if (locationCount == 0) {
-          return Integer.MAX_VALUE;
-        }
-      } finally {
-        in.close();
-      }
-
-      return failedCount;
-    }
-
-    public CheckResults(String outDir, String logDir, boolean miniMr,
-        String hadoopVer, String locationSubdir)
-      throws Exception
-    {
-      super(outDir, logDir, miniMr, hadoopVer);
-      this.locationSubdir = locationSubdir;
-    }
-  }
-
-  /**
-   * Verify that the location of the partition is valid. In this case
-   * the path should end in "parta" and not "dt=a" (the default).
-   *
-   */
-  public void testAlterTablePartitionLocation_alter5() throws Exception {
-    String[] testNames = new String[] {"alter5.q"};
-
-    File[] qfiles = setupQFiles(testNames);
-
-    QTestUtil[] qt = new QTestUtil[qfiles.length];
-    for (int i = 0; i < qfiles.length; i++) {
-      qt[i] = new CheckResults(resDir, logDir, false, "0.20", "parta");
-      qt[i].addFile(qfiles[i]);
-      qt[i].clearTestSideEffects();
-    }
-
-    boolean success = QTestUtil.queryListRunnerSingleThreaded(qfiles, qt);
-    if (!success) {
-      fail("One or more queries failed");
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/TestMTQueries.java b/src/ql/src/test/org/apache/hadoop/hive/ql/TestMTQueries.java
deleted file mode 100644
index 378de03..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/TestMTQueries.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql;
-
-import java.io.File;
-
-/**
- * Suite for testing running of queries in multi-threaded mode.
- */
-public class TestMTQueries extends BaseTestQueries {
-
-  public TestMTQueries() {
-    File logDirFile = new File(logDir);
-    if (!(logDirFile.exists() || logDirFile.mkdirs())) {
-      fail("Could not create " + logDir);
-    }
-  }
-
-  public void testMTQueries1() throws Exception {
-    String[] testNames = new String[] {"join1.q", "join2.q", "groupby1.q",
-        "groupby2.q", "join3.q", "input1.q", "input19.q"};
-
-    File[] qfiles = setupQFiles(testNames);
-
-    QTestUtil[] qts = QTestUtil.queryListRunnerSetup(qfiles, resDir, logDir);
-    boolean success = QTestUtil.queryListRunnerMultiThreaded(qfiles, qts);
-    if (!success) {
-      fail("One or more queries failed");
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/history/TestHiveHistory.java b/src/ql/src/test/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
deleted file mode 100644
index 8beef09..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
+++ /dev/null
@@ -1,232 +0,0 @@
-package org.apache.hadoop.hive.ql.history;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.PrintStream;
-import java.io.UnsupportedEncodingException;
-import java.lang.reflect.Proxy;
-import java.util.LinkedList;
-import java.util.Map;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.common.LogUtils;
-import org.apache.hadoop.hive.common.LogUtils.LogInitializationException;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.history.HiveHistory.Keys;
-import org.apache.hadoop.hive.ql.history.HiveHistory.QueryInfo;
-import org.apache.hadoop.hive.ql.history.HiveHistory.TaskInfo;
-import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
-import org.apache.hadoop.hive.ql.metadata.Hive;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.tools.LineageInfo;
-import org.apache.hadoop.mapred.TextInputFormat;
-
-/**
- * TestHiveHistory.
- *
- */
-public class TestHiveHistory extends TestCase {
-
-  static HiveConf conf;
-
-  private static String tmpdir = System.getProperty("test.tmp.dir");
-  private static Path tmppath = new Path(tmpdir);
-  private static Hive db;
-  private static FileSystem fs;
-  /*
-   * intialize the tables
-   */
-
-  @Override
-  protected void setUp() {
-    try {
-      conf = new HiveConf(HiveHistory.class);
-      SessionState.start(conf);
-
-      fs = FileSystem.get(conf);
-      if (fs.exists(tmppath) && !fs.getFileStatus(tmppath).isDir()) {
-        throw new RuntimeException(tmpdir + " exists but is not a directory");
-      }
-
-      if (!fs.exists(tmppath)) {
-        if (!fs.mkdirs(tmppath)) {
-          throw new RuntimeException("Could not make scratch directory "
-              + tmpdir);
-        }
-      }
-
-      conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
-
-      // copy the test files into hadoop if required.
-      int i = 0;
-      Path[] hadoopDataFile = new Path[2];
-      String[] testFiles = {"kv1.txt", "kv2.txt"};
-      String testFileDir = new Path(conf.get("test.data.files")).toUri().getPath();
-      for (String oneFile : testFiles) {
-        Path localDataFile = new Path(testFileDir, oneFile);
-        hadoopDataFile[i] = new Path(tmppath, oneFile);
-        fs.copyFromLocalFile(false, true, localDataFile, hadoopDataFile[i]);
-        i++;
-      }
-
-      // load the test files into tables
-      i = 0;
-      db = Hive.get(conf);
-      String[] srctables = {"src", "src2"};
-      LinkedList<String> cols = new LinkedList<String>();
-      cols.add("key");
-      cols.add("value");
-      for (String src : srctables) {
-        db.dropTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, src, true, true);
-        db.createTable(src, cols, null, TextInputFormat.class,
-            IgnoreKeyTextOutputFormat.class);
-        db.loadTable(hadoopDataFile[i], src, false, false);
-        i++;
-      }
-
-    } catch (Throwable e) {
-      e.printStackTrace();
-      throw new RuntimeException("Encountered throwable");
-    }
-  }
-
-  /**
-   * Check history file output for this query.
-   */
-  public void testSimpleQuery() {
-    new LineageInfo();
-    try {
-
-      // NOTE: It is critical to do this here so that log4j is reinitialized
-      // before any of the other core hive classes are loaded
-      try {
-        LogUtils.initHiveLog4j();
-      } catch (LogInitializationException e) {
-      }
-      HiveConf hconf = new HiveConf(SessionState.class);
-      hconf.setBoolVar(ConfVars.HIVE_SESSION_HISTORY_ENABLED, true);
-      CliSessionState ss = new CliSessionState(hconf);
-      ss.in = System.in;
-      try {
-        ss.out = new PrintStream(System.out, true, "UTF-8");
-        ss.err = new PrintStream(System.err, true, "UTF-8");
-      } catch (UnsupportedEncodingException e) {
-        System.exit(3);
-      }
-
-      SessionState.start(ss);
-
-      String cmd = "select a.key from src a";
-      Driver d = new Driver(conf);
-      int ret = d.run(cmd).getResponseCode();
-      if (ret != 0) {
-        fail("Failed");
-      }
-      HiveHistoryViewer hv = new HiveHistoryViewer(SessionState.get()
-          .getHiveHistory().getHistFileName());
-      Map<String, QueryInfo> jobInfoMap = hv.getJobInfoMap();
-      Map<String, TaskInfo> taskInfoMap = hv.getTaskInfoMap();
-      if (jobInfoMap.size() != 1) {
-        fail("jobInfo Map size not 1");
-      }
-
-      if (taskInfoMap.size() != 1) {
-        fail("jobInfo Map size not 1");
-      }
-
-      cmd = (String) jobInfoMap.keySet().toArray()[0];
-      QueryInfo ji = jobInfoMap.get(cmd);
-
-      if (!ji.hm.get(Keys.QUERY_NUM_TASKS.name()).equals("1")) {
-        fail("Wrong number of tasks");
-      }
-
-    } catch (Exception e) {
-      e.printStackTrace();
-      fail("Failed");
-    }
-  }
-
-  public void testQueryloglocParentDirNotExist() throws Exception {
-    String parentTmpDir = tmpdir + "/HIVE2654";
-    Path parentDirPath = new Path(parentTmpDir);
-    try {
-      fs.delete(parentDirPath, true);
-    } catch (Exception e) {
-    }
-    try {
-      String actualDir = parentTmpDir + "/test";
-      HiveConf conf = new HiveConf(SessionState.class);
-      conf.set(HiveConf.ConfVars.HIVEHISTORYFILELOC.toString(), actualDir);
-      SessionState ss = new CliSessionState(conf);
-      HiveHistory hiveHistory = new HiveHistoryImpl(ss);
-      Path actualPath = new Path(actualDir);
-      if (!fs.exists(actualPath)) {
-        fail("Query location path is not exist :" + actualPath.toString());
-      }
-    } finally {
-      try {
-        fs.delete(parentDirPath, true);
-      } catch (Exception e) {
-      }
-    }
-  }
-
-  /**
-   * Check if HiveHistoryImpl class is returned when hive history is enabled
-   * @throws Exception
-   */
-  public void testHiveHistoryConfigEnabled() throws Exception {
-      HiveConf conf = new HiveConf(SessionState.class);
-      conf.setBoolVar(ConfVars.HIVE_SESSION_HISTORY_ENABLED, true);
-      SessionState ss = new CliSessionState(conf);
-      SessionState.start(ss);
-      HiveHistory hHistory = ss.getHiveHistory();
-      assertEquals("checking hive history class when history is enabled",
-          hHistory.getClass(), HiveHistoryImpl.class);
-  }
-  /**
-   * Check if HiveHistory class is a Proxy class when hive history is disabled
-   * @throws Exception
-   */
-  public void testHiveHistoryConfigDisabled() throws Exception {
-    HiveConf conf = new HiveConf(SessionState.class);
-    conf.setBoolVar(ConfVars.HIVE_SESSION_HISTORY_ENABLED, false);
-    SessionState ss = new CliSessionState(conf);
-    SessionState.start(ss);
-    HiveHistory hHistory = ss.getHiveHistory();
-    assertTrue("checking hive history class when history is disabled",
-        hHistory.getClass() != HiveHistoryImpl.class);
-    System.err.println("hHistory.getClass" + hHistory.getClass());
-    assertTrue("verifying proxy class is used when history is disabled",
-        Proxy.isProxyClass(hHistory.getClass()));
-
-  }
-
-
-
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java
deleted file mode 100644
index 14fc430..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckColumnAccessHook.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.Arrays;
-import java.util.Map;
-import java.util.HashMap;
-import java.util.Set;
-
-import org.apache.commons.lang.StringUtils;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.QueryPlan;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-
-import org.apache.hadoop.hive.ql.parse.ColumnAccessInfo;
-import org.mortbay.log.Log;
-
-/*
- * This hook is used for verifying the column access information
- * that is generated and maintained in the QueryPlan object by the
- * ColumnAccessAnalyer. All the hook does is print out the columns
- * accessed from each table as recorded in the ColumnAccessInfo
- * in the QueryPlan.
- */
-public class CheckColumnAccessHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    Log.info("Running CheckColumnAccessHook");
-    HiveConf conf = hookContext.getConf();
-    if (conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_COLLECT_SCANCOLS) == false) {
-      return;
-    }
-
-    QueryPlan plan = hookContext.getQueryPlan();
-    if (plan == null) {
-      return;
-    }
-
-    ColumnAccessInfo columnAccessInfo = hookContext.getQueryPlan().getColumnAccessInfo();
-    if (columnAccessInfo == null) {
-      return;
-    }
-
-    LogHelper console = SessionState.getConsole();
-    Map<String, Set<String>> tableToColumnAccessMap =
-      columnAccessInfo.getTableToColumnAccessMap();
-
-    // We need a new map to ensure output is always produced in the same order.
-    // This makes tests that use this hook deterministic.
-    Map<String, String> outputOrderedMap = new HashMap<String, String>();
-
-    for (Map.Entry<String, Set<String>> tableAccess : tableToColumnAccessMap.entrySet()) {
-      StringBuilder perTableInfo = new StringBuilder();
-      perTableInfo.append("Table:").append(tableAccess.getKey()).append("\n");
-      // Sort columns to make output deterministic
-      String[] columns = new String[tableAccess.getValue().size()];
-      tableAccess.getValue().toArray(columns);
-      Arrays.sort(columns);
-      perTableInfo.append("Columns:").append(StringUtils.join(columns, ','))
-        .append("\n");
-      outputOrderedMap.put(tableAccess.getKey(), perTableInfo.toString());
-    }
-
-    for (String perOperatorInfo : outputOrderedMap.values()) {
-      console.printError(perOperatorInfo);
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckQueryPropertiesHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckQueryPropertiesHook.java
deleted file mode 100644
index faea9f1..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckQueryPropertiesHook.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import org.apache.hadoop.hive.ql.QueryProperties;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-
-/**
- *
- * CheckQueryPropertiesHook.
- *
- * This hook prints the values in the QueryProperties object contained in the QueryPlan
- * in the HookContext passed to the hook.
- */
-public class CheckQueryPropertiesHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    LogHelper console = SessionState.getConsole();
-
-    if (console == null) {
-      return;
-    }
-
-    QueryProperties queryProps = hookContext.getQueryPlan().getQueryProperties();
-
-    if (queryProps != null) {
-      console.printError("Has Join: " + queryProps.hasJoin());
-      console.printError("Has Group By: " + queryProps.hasGroupBy());
-      console.printError("Has Sort By: " + queryProps.hasSortBy());
-      console.printError("Has Order By: " + queryProps.hasOrderBy());
-      console.printError("Has Group By After Join: " + queryProps.hasJoinFollowedByGroupBy());
-      console.printError("Uses Script: " + queryProps.usesScript());
-      console.printError("Has Distribute By: " + queryProps.hasDistributeBy());
-      console.printError("Has Cluster By: " + queryProps.hasClusterBy());
-    }
-  }
-}
\ No newline at end of file
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java
deleted file mode 100644
index 8e19fad..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.List;
-import java.util.Map;
-import java.util.HashMap;
-
-import org.apache.commons.lang.StringUtils;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.QueryPlan;
-import org.apache.hadoop.hive.ql.parse.TableAccessInfo;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-
-import org.apache.hadoop.hive.ql.exec.Operator;
-import org.apache.hadoop.hive.ql.plan.OperatorDesc;
-
-/*
- * This hook is used for verifying the table access key information
- * that is generated and maintained in the QueryPlan object by the
- * TableAccessAnalyer. All the hook does is print out the table/keys
- * per operator recorded in the TableAccessInfo in the QueryPlan.
- */
-public class CheckTableAccessHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    HiveConf conf = hookContext.getConf();
-    if (conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_COLLECT_TABLEKEYS) == false) {
-      return;
-    }
-
-    QueryPlan plan = hookContext.getQueryPlan();
-    if (plan == null) {
-      return;
-    }
-
-    TableAccessInfo tableAccessInfo = hookContext.getQueryPlan().getTableAccessInfo();
-    if (tableAccessInfo == null ||
-        tableAccessInfo.getOperatorToTableAccessMap() == null ||
-        tableAccessInfo.getOperatorToTableAccessMap().isEmpty()) {
-      return;
-    }
-
-    LogHelper console = SessionState.getConsole();
-    Map<Operator<? extends OperatorDesc>, Map<String, List<String>>> operatorToTableAccessMap =
-      tableAccessInfo.getOperatorToTableAccessMap();
-
-    // We need a new map to ensure output is always produced in the same order.
-    // This makes tests that use this hook deterministic.
-    Map<String, String> outputOrderedMap = new HashMap<String, String>();
-
-    for (Map.Entry<Operator<? extends OperatorDesc>, Map<String, List<String>>> tableAccess:
-        operatorToTableAccessMap.entrySet()) {
-      StringBuilder perOperatorInfo = new StringBuilder();
-      perOperatorInfo.append("Operator:").append(tableAccess.getKey().getOperatorId())
-        .append("\n");
-      for (Map.Entry<String, List<String>> entry: tableAccess.getValue().entrySet()) {
-        perOperatorInfo.append("Table:").append(entry.getKey()).append("\n");
-        perOperatorInfo.append("Keys:").append(StringUtils.join(entry.getValue(), ','))
-          .append("\n");
-      }
-      outputOrderedMap.put(tableAccess.getKey().getOperatorId(), perOperatorInfo.toString());
-    }
-
-    for (String perOperatorInfo: outputOrderedMap.values()) {
-        console.printError(perOperatorInfo);
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/EnforceReadOnlyTables.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/EnforceReadOnlyTables.java
deleted file mode 100644
index 1458075..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/EnforceReadOnlyTables.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.hooks;
-
-import static org.apache.hadoop.hive.metastore.MetaStoreUtils.DEFAULT_DATABASE_NAME;
-
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.security.UserGroupInformation;
-
-/**
- * Implementation of a pre execute hook that prevents modifications
- * of read-only tables used by the test framework
- */
-public class EnforceReadOnlyTables implements ExecuteWithHookContext {
-
-  private static final Set<String> READ_ONLY_TABLES = new HashSet<String>();
-
-  static {
-    for (String srcTable : System.getProperty("test.src.tables", "").trim().split(",")) {
-      srcTable = srcTable.trim();
-      if (!srcTable.isEmpty()) {
-        READ_ONLY_TABLES.add(srcTable);
-      }
-    }
-    if (READ_ONLY_TABLES.isEmpty()) {
-      throw new AssertionError("Source tables cannot be empty");
-    }
-  }
-
-  @Override
-  public void run(HookContext hookContext) throws Exception {
-    SessionState ss = SessionState.get();
-    Set<ReadEntity> inputs = hookContext.getInputs();
-    Set<WriteEntity> outputs = hookContext.getOutputs();
-    UserGroupInformation ugi = hookContext.getUgi();
-    this.run(ss,inputs,outputs,ugi);
-  }
-
-  public void run(SessionState sess, Set<ReadEntity> inputs,
-      Set<WriteEntity> outputs, UserGroupInformation ugi)
-    throws Exception {
-    if (sess.getConf().getBoolean("hive.test.init.phase", false) == true) {
-      return;
-    }
-    for (WriteEntity w: outputs) {
-      if ((w.getTyp() == WriteEntity.Type.TABLE) ||
-          (w.getTyp() == WriteEntity.Type.PARTITION)) {
-        Table t = w.getTable();
-        if (DEFAULT_DATABASE_NAME.equalsIgnoreCase(t.getDbName())
-            && READ_ONLY_TABLES.contains(t.getTableName())) {
-          throw new RuntimeException ("Cannot overwrite read-only table: " + t.getTableName());
-        }
-      }
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/MapJoinCounterHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/MapJoinCounterHook.java
deleted file mode 100644
index 1b0d57e..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/MapJoinCounterHook.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.List;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.QueryPlan;
-import org.apache.hadoop.hive.ql.exec.Task;
-import org.apache.hadoop.hive.ql.exec.TaskRunner;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-
-public class MapJoinCounterHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    HiveConf conf = hookContext.getConf();
-    boolean enableConvert = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECONVERTJOIN);
-    if (!enableConvert) {
-      return;
-    }
-
-    QueryPlan plan = hookContext.getQueryPlan();
-    String queryID = plan.getQueryId();
-    // String query = SessionState.get().getCmd();
-
-    int convertedMapJoin = 0;
-    int commonJoin = 0;
-    int backupCommonJoin = 0;
-    int convertedLocalMapJoin = 0;
-    int localMapJoin = 0;
-
-    List<TaskRunner> list = hookContext.getCompleteTaskList();
-    for (TaskRunner tskRunner : list) {
-      Task tsk = tskRunner.getTask();
-      int tag = tsk.getTaskTag();
-      switch (tag) {
-      case Task.COMMON_JOIN:
-        commonJoin++;
-        break;
-      case Task.CONVERTED_LOCAL_MAPJOIN:
-        convertedLocalMapJoin++;
-        break;
-      case Task.CONVERTED_MAPJOIN:
-        convertedMapJoin++;
-        break;
-      case Task.BACKUP_COMMON_JOIN:
-        backupCommonJoin++;
-        break;
-      case Task.LOCAL_MAPJOIN:
-         localMapJoin++;
-         break;
-      }
-    }
-    LogHelper console = SessionState.getConsole();
-    console.printError("[MapJoinCounter PostHook] CONVERTED_LOCAL_MAPJOIN: " + convertedLocalMapJoin
-        + " CONVERTED_MAPJOIN: " + convertedMapJoin + " LOCAL_MAPJOIN: "+localMapJoin+ " COMMON_JOIN: "+commonJoin
-        + " BACKUP_COMMON_JOIN: " + backupCommonJoin);
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/OptrStatGroupByHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/OptrStatGroupByHook.java
deleted file mode 100644
index 828de5e..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/OptrStatGroupByHook.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/**
- *  Licensed to the Apache Software Foundation (ASF) under one
- *  or more contributor license agreements.  See the NOTICE file
- *  distributed with this work for additional information
- *  regarding copyright ownership.  The ASF licenses this file
- *  to you under the Apache License, Version 2.0 (the
- *  "License"); you may not use this file except in compliance
- *  with the License.  You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express  or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- **/
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.HashMap;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Queue;
-import java.util.Set;
-import java.io.Serializable;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.exec.Operator;
-import org.apache.hadoop.hive.ql.exec.Task;
-import org.apache.hadoop.hive.ql.exec.TaskRunner;
-import org.apache.hadoop.hive.ql.plan.api.OperatorType;
-import org.apache.hadoop.hive.ql.plan.OperatorDesc;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-
-public class OptrStatGroupByHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    HiveConf conf = hookContext.getConf();
-
-    List<TaskRunner> completedTasks = hookContext.getCompleteTaskList();
-
-    boolean enableProgress = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEJOBPROGRESS);
-
-    /** For each task visit the opeartor tree and and if the operator is GROUPBY
-     *  then print the HASH_OUT Optr level stat value.
-     **/
-    if (completedTasks != null) {
-      for (TaskRunner taskRunner : completedTasks) {
-        Task<? extends Serializable> task = taskRunner.getTask();
-        if (task.isMapRedTask() && !task.isMapRedLocalTask()) {
-          Set<Operator<? extends OperatorDesc>> optrSet = getOptrsForTask(task);
-          for (Operator<? extends OperatorDesc> optr : optrSet) {
-            if (optr.getType() == OperatorType.GROUPBY) {
-               printCounterValue(optr.getCounters());
-            }
-          }
-        }
-      }
-    }
-  }
-
-  private void printCounterValue(HashMap<String, Long> ctrs) {
-    for (String ctrName : ctrs.keySet()) {
-      if (ctrName.contains("HASH_OUT")) {
-        SessionState.getConsole().printError(ctrName+"="+ctrs.get(ctrName));
-      }
-    }
-  }
-
-  private Set<Operator<? extends OperatorDesc>> getOptrsForTask(
-    Task<? extends Serializable> task) {
-
-    Collection<Operator<? extends OperatorDesc>> topOptrs = task.getTopOperators();
-    Set<Operator<? extends OperatorDesc>> allOptrs =
-      new HashSet<Operator<? extends OperatorDesc>>();
-    Queue<Operator<? extends OperatorDesc>> opsToVisit =
-      new LinkedList<Operator<? extends OperatorDesc>>();
-    if(topOptrs != null) {
-      opsToVisit.addAll(topOptrs);
-      addChildOptrs(opsToVisit, allOptrs);
-    }
-
-    return allOptrs;
-  }
-
-  private void addChildOptrs(
-    Queue<Operator<? extends OperatorDesc>> opsToVisit,
-    Set<Operator<? extends OperatorDesc>> opsVisited) {
-
-    if(opsToVisit == null || opsVisited == null) {
-      return;
-    }
-
-    while (opsToVisit.peek() != null) {
-      Operator<? extends OperatorDesc> op = opsToVisit.remove();
-      opsVisited.add(op);
-      if (op.getChildOperators() != null) {
-        for (Operator<? extends OperatorDesc> childOp : op.getChildOperators()) {
-          if (!opsVisited.contains(childOp)) {
-            opsToVisit.add(childOp);
-          }
-        }
-      }
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java
deleted file mode 100644
index 4518315..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java
+++ /dev/null
@@ -1,175 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.LinkedList;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.Partition;
-import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
-import org.apache.hadoop.hive.ql.hooks.LineageInfo.BaseColumnInfo;
-import org.apache.hadoop.hive.ql.hooks.LineageInfo.Dependency;
-import org.apache.hadoop.hive.ql.hooks.LineageInfo.DependencyKey;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-import org.apache.hadoop.security.UserGroupInformation;
-
-/**
- * Implementation of a post execute hook that simply prints out its parameters
- * to standard output.
- */
-public class PostExecutePrinter implements ExecuteWithHookContext {
-
-  public class DependencyKeyComp implements
-    Comparator<Map.Entry<DependencyKey, Dependency>> {
-
-    @Override
-    public int compare(Map.Entry<DependencyKey, Dependency> o1,
-        Map.Entry<DependencyKey, Dependency> o2) {
-      if (o1 == null && o2 == null) {
-        return 0;
-      }
-      else if (o1 == null && o2 != null) {
-        return -1;
-      }
-      else if (o1 != null && o2 == null) {
-        return 1;
-      }
-      else {
-        // Both are non null.
-        // First compare the table names.
-        int ret = o1.getKey().getDataContainer().getTable().getTableName()
-          .compareTo(o2.getKey().getDataContainer().getTable().getTableName());
-
-        if (ret != 0) {
-          return ret;
-        }
-
-        // The table names match, so check on the partitions
-        if (!o1.getKey().getDataContainer().isPartition() &&
-            o2.getKey().getDataContainer().isPartition()) {
-          return -1;
-        }
-        else if (o1.getKey().getDataContainer().isPartition() &&
-            !o2.getKey().getDataContainer().isPartition()) {
-          return 1;
-        }
-
-        if (o1.getKey().getDataContainer().isPartition() &&
-            o2.getKey().getDataContainer().isPartition()) {
-          // Both are partitioned tables.
-          ret = o1.getKey().getDataContainer().getPartition().toString()
-          .compareTo(o2.getKey().getDataContainer().getPartition().toString());
-
-          if (ret != 0) {
-            return ret;
-          }
-        }
-
-        // The partitons are also the same so check the fieldschema
-        return (o1.getKey().getFieldSchema().getName().compareTo(
-            o2.getKey().getFieldSchema().getName()));
-      }
-    }
-  }
-
-  @Override
-  public void run(HookContext hookContext) throws Exception {
-    assert(hookContext.getHookType() == HookType.POST_EXEC_HOOK);
-    SessionState ss = SessionState.get();
-    Set<ReadEntity> inputs = hookContext.getInputs();
-    Set<WriteEntity> outputs = hookContext.getOutputs();
-    LineageInfo linfo = hookContext.getLinfo();
-    UserGroupInformation ugi = hookContext.getUgi();
-    this.run(ss,inputs,outputs,linfo,ugi);
-  }
-
-  public void run(SessionState sess, Set<ReadEntity> inputs,
-      Set<WriteEntity> outputs, LineageInfo linfo,
-      UserGroupInformation ugi) throws Exception {
-
-    LogHelper console = SessionState.getConsole();
-
-    if (console == null) {
-      return;
-    }
-
-    if (sess != null) {
-      console.printError("POSTHOOK: query: " + sess.getCmd().trim());
-      console.printError("POSTHOOK: type: " + sess.getCommandType());
-    }
-
-    PreExecutePrinter.printEntities(console, inputs, "POSTHOOK: Input: ");
-    PreExecutePrinter.printEntities(console, outputs, "POSTHOOK: Output: ");
-
-    // Also print out the generic lineage information if there is any
-    if (linfo != null) {
-      LinkedList<Map.Entry<DependencyKey, Dependency>> entry_list =
-        new LinkedList<Map.Entry<DependencyKey, Dependency>>(linfo.entrySet());
-      Collections.sort(entry_list, new DependencyKeyComp());
-      Iterator<Map.Entry<DependencyKey, Dependency>> iter = entry_list.iterator();
-      while(iter.hasNext()) {
-        Map.Entry<DependencyKey, Dependency> it = iter.next();
-        Dependency dep = it.getValue();
-        DependencyKey depK = it.getKey();
-
-        if(dep == null) {
-          continue;
-        }
-
-        StringBuilder sb = new StringBuilder();
-        sb.append("POSTHOOK: Lineage: ");
-        if (depK.getDataContainer().isPartition()) {
-          Partition part = depK.getDataContainer().getPartition();
-          sb.append(part.getTableName());
-          sb.append(" PARTITION(");
-          int i = 0;
-          for (FieldSchema fs : depK.getDataContainer().getTable().getPartitionKeys()) {
-            if (i != 0) {
-              sb.append(",");
-            }
-            sb.append(fs.getName() + "=" + part.getValues().get(i++));
-          }
-          sb.append(")");
-        }
-        else {
-          sb.append(depK.getDataContainer().getTable().getTableName());
-        }
-        sb.append("." + depK.getFieldSchema().getName() + " " +
-            dep.getType() + " ");
-
-        sb.append("[");
-        for(BaseColumnInfo col: dep.getBaseCols()) {
-          sb.append("("+col.getTabAlias().getTable().getTableName() + ")"
-              + col.getTabAlias().getAlias() + "."
-              + col.getColumn() + ", ");
-        }
-        sb.append("]");
-
-        console.printError(sb.toString());
-      }
-    }
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java
deleted file mode 100644
index acde2f6..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-import org.apache.hadoop.security.UserGroupInformation;
-
-/**
- * Implementation of a pre execute hook that simply prints out its parameters to
- * standard output.
- */
-public class PreExecutePrinter implements ExecuteWithHookContext {
-
-  @Override
-  public void run(HookContext hookContext) throws Exception {
-    assert(hookContext.getHookType() == HookType.PRE_EXEC_HOOK);
-    SessionState ss = SessionState.get();
-    Set<ReadEntity> inputs = hookContext.getInputs();
-    Set<WriteEntity> outputs = hookContext.getOutputs();
-    UserGroupInformation ugi = hookContext.getUgi();
-    this.run(ss,inputs,outputs,ugi);
-  }
-
-  public void run(SessionState sess, Set<ReadEntity> inputs,
-      Set<WriteEntity> outputs, UserGroupInformation ugi)
-    throws Exception {
-
-    LogHelper console = SessionState.getConsole();
-
-    if (console == null) {
-      return;
-    }
-
-    if (sess != null) {
-      console.printError("PREHOOK: query: " + sess.getCmd().trim());
-      console.printError("PREHOOK: type: " + sess.getCommandType());
-    }
-
-    printEntities(console, inputs, "PREHOOK: Input: ");
-    printEntities(console, outputs, "PREHOOK: Output: ");
-  }
-
-  static void printEntities(LogHelper console, Set<?> entities, String prefix) {
-    List<String> strings = new ArrayList<String>();
-    for (Object o : entities) {
-      strings.add(o.toString());
-    }
-    Collections.sort(strings);
-    for (String s : strings) {
-      console.printError(prefix + s);
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyCachingPrintStreamHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyCachingPrintStreamHook.java
deleted file mode 100644
index 6a93c21..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyCachingPrintStreamHook.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import org.apache.hadoop.hive.common.io.CachingPrintStream;
-import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
-import org.apache.hadoop.hive.ql.session.SessionState;
-
-// If this is run as a pre or post execution hook, it writes a message to SessionState.err
-// (causing it to be cached if a CachingPrintStream is being used).  If it is run as a failure
-// hook, it will write what has been cached by the CachingPrintStream to SessionState.out for
-// verification.
-public class VerifyCachingPrintStreamHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    SessionState ss = SessionState.get();
-
-    assert(ss.err instanceof CachingPrintStream);
-
-    if (hookContext.getHookType() == HookType.ON_FAILURE_HOOK) {
-      assert(ss.err instanceof CachingPrintStream);
-      ss.out.println("Begin cached logs.");
-      for (String output : ((CachingPrintStream)ss.err).getOutput()) {
-        ss.out.println(output);
-      }
-      ss.out.println("End cached logs.");
-    } else {
-      ss.err.println("TEST, this should only appear once in the log.");
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyContentSummaryCacheHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyContentSummaryCacheHook.java
deleted file mode 100644
index cdb9d03..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyContentSummaryCacheHook.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.Map;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.fs.ContentSummary;
-import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
-
-public class VerifyContentSummaryCacheHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    Map<String, ContentSummary> inputToCS = hookContext.getInputPathToContentSummary();
-    if (hookContext.getHookType().equals(HookType.PRE_EXEC_HOOK)) {
-      Assert.assertEquals(0, inputToCS.size());
-    } else {
-      Assert.assertEquals(1, inputToCS.size());
-      String tmp_dir = System.getProperty("test.tmp.dir");
-      for (String key : inputToCS.keySet()) {
-        if (!key.equals(tmp_dir + "/VerifyContentSummaryCacheHook") &&
-            !key.equals("pfile:" + tmp_dir + "/VerifyContentSummaryCacheHook")) {
-          Assert.fail("VerifyContentSummaryCacheHook fails the input path check");
-        }
-      }
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyHiveSortedInputFormatUsedHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyHiveSortedInputFormatUsedHook.java
deleted file mode 100644
index a64086b..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyHiveSortedInputFormatUsedHook.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.io.Serializable;
-import java.util.ArrayList;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.hive.ql.exec.Task;
-import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
-import org.apache.hadoop.hive.ql.plan.MapredWork;
-
-public class VerifyHiveSortedInputFormatUsedHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    if (hookContext.getHookType().equals(HookType.POST_EXEC_HOOK)) {
-
-      // Go through the root tasks, and verify the input format of the map reduce task(s) is
-      // HiveSortedInputFormat
-      ArrayList<Task<? extends Serializable>> rootTasks =
-          hookContext.getQueryPlan().getRootTasks();
-      for (Task<? extends Serializable> rootTask : rootTasks) {
-        if (rootTask.getWork() instanceof MapredWork) {
-          Assert.assertTrue("The root map reduce task's input was not marked as sorted.",
-              ((MapredWork)rootTask.getWork()).getMapWork().isInputFormatSorted());
-        }
-      }
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyHooksRunInOrder.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyHooksRunInOrder.java
deleted file mode 100644
index 8a72eca..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyHooksRunInOrder.java
+++ /dev/null
@@ -1,228 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.io.Serializable;
-import java.util.List;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.hive.ql.HiveDriverRunHook;
-import org.apache.hadoop.hive.ql.HiveDriverRunHookContext;
-import org.apache.hadoop.hive.ql.exec.Task;
-import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
-import org.apache.hadoop.hive.ql.parse.ASTNode;
-import org.apache.hadoop.hive.ql.parse.AbstractSemanticAnalyzerHook;
-import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext;
-import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-
-/**
- * VerifyHooksRunInOrder.
- *
- * Has to subclasses RunFirst and RunSecond which can be run as either pre or post hooks.
- * Verifies that RunFirst is executed before RunSecond as the same type of hook.  I.e. if they are
- * run as both Pre and Post hooks, RunSecond checks that RunFirst was run as a Pre or Post hook
- * respectively.
- *
- * When running this, be sure to specify RunFirst before RunSecond in the configuration variable.
- */
-public class VerifyHooksRunInOrder {
-
-  private static boolean preHookRunFirstRan = false;
-  private static boolean postHookRunFirstRan = false;
-  private static boolean staticAnalysisPreHookFirstRan = false;
-  private static boolean staticAnalysisPostHookFirstRan = false;
-  private static boolean driverRunPreHookFirstRan = false;
-  private static boolean driverRunPostHookFirstRan = false;
-
-  public static class RunFirst implements ExecuteWithHookContext {
-    public void run(HookContext hookContext) {
-      LogHelper console = SessionState.getConsole();
-
-      if (console == null) {
-        return;
-      }
-
-      // This is simply to verify that the hooks were in fact run
-      console.printError("Running RunFirst for " + hookContext.getHookType());
-
-      if (hookContext.getHookType() == HookType.PRE_EXEC_HOOK) {
-        preHookRunFirstRan = true;
-      } else {
-        postHookRunFirstRan = true;
-      }
-    }
-  }
-
-  public static class RunSecond implements ExecuteWithHookContext {
-    public void run(HookContext hookContext) throws Exception {
-      LogHelper console = SessionState.getConsole();
-
-      if (console == null) {
-        return;
-      }
-
-      // This is simply to verify that the hooks were in fact run
-      console.printError("Running RunSecond for " + hookContext.getHookType());
-
-      if (hookContext.getHookType() == HookType.PRE_EXEC_HOOK) {
-        Assert.assertTrue("Pre hooks did not run in the order specified.", preHookRunFirstRan);
-      } else {
-        Assert.assertTrue("Post hooks did not run in the order specified.", postHookRunFirstRan);
-      }
-    }
-  }
-
-  public static class RunFirstSemanticAnalysisHook extends AbstractSemanticAnalyzerHook {
-    @Override
-    public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,ASTNode ast)
-        throws SemanticException {
-      LogHelper console = SessionState.getConsole();
-
-      if (console == null) {
-        return ast;
-      }
-
-      // This is simply to verify that the hooks were in fact run
-      console.printError("Running RunFirst for Pre Analysis Hook");
-
-      staticAnalysisPreHookFirstRan = true;
-
-      return ast;
-    }
-
-    @Override
-    public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-        List<Task<? extends Serializable>> rootTasks) throws SemanticException {
-      LogHelper console = SessionState.getConsole();
-
-      if (console == null) {
-        return;
-      }
-
-      // This is simply to verify that the hooks were in fact run
-      console.printError("Running RunFirst for Post Analysis Hook");
-
-      staticAnalysisPostHookFirstRan = true;
-    }
-  }
-
-  public static class RunSecondSemanticAnalysisHook extends AbstractSemanticAnalyzerHook {
-    @Override
-    public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,ASTNode ast)
-        throws SemanticException {
-      LogHelper console = SessionState.getConsole();
-
-      if (console == null) {
-        return ast;
-      }
-
-      // This is simply to verify that the hooks were in fact run
-      console.printError("Running RunSecond for Pre Analysis Hook");
-
-      Assert.assertTrue("Pre Analysis Hooks did not run in the order specified.",
-                        staticAnalysisPreHookFirstRan);
-
-      return ast;
-    }
-
-    @Override
-    public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-        List<Task<? extends Serializable>> rootTasks) throws SemanticException {
-      LogHelper console = SessionState.getConsole();
-
-      if (console == null) {
-        return;
-      }
-
-      // This is simply to verify that the hooks were in fact run
-      console.printError("Running RunSecond for Post Analysis Hook");
-
-      Assert.assertTrue("Post Analysis Hooks did not run in the order specified.",
-                        staticAnalysisPostHookFirstRan);
-    }
-  }
-
-  public static class RunFirstDriverRunHook implements HiveDriverRunHook {
-
-    @Override
-    public void preDriverRun(HiveDriverRunHookContext hookContext) throws Exception {
-      LogHelper console = SessionState.getConsole();
-
-      if (console == null) {
-        return;
-      }
-
-      // This is simply to verify that the hooks were in fact run
-      console.printError("Running RunFirst for Pre Driver Run Hook");
-
-      driverRunPreHookFirstRan = true;
-    }
-
-    @Override
-    public void postDriverRun(HiveDriverRunHookContext hookContext) throws Exception {
-      LogHelper console = SessionState.getConsole();
-
-      if (console == null) {
-        return;
-      }
-
-      // This is simply to verify that the hooks were in fact run
-      console.printError("Running RunFirst for Post Driver Run Hook");
-
-      driverRunPostHookFirstRan = true;
-    }
-
-  }
-
-  public static class RunSecondDriverRunHook implements HiveDriverRunHook {
-
-    @Override
-    public void preDriverRun(HiveDriverRunHookContext hookContext) throws Exception {
-      LogHelper console = SessionState.getConsole();
-
-      if (console == null) {
-        return;
-      }
-
-      // This is simply to verify that the hooks were in fact run
-      console.printError("Running RunSecond for Pre Driver Run Hook");
-
-      Assert.assertTrue("Driver Run Hooks did not run in the order specified.",
-          driverRunPreHookFirstRan);
-    }
-
-    @Override
-    public void postDriverRun(HiveDriverRunHookContext hookContext) throws Exception {
-      LogHelper console = SessionState.getConsole();
-
-      if (console == null) {
-        return;
-      }
-
-      // This is simply to verify that the hooks were in fact run
-      console.printError("Running RunSecond for Post Driver Run Hook");
-
-      Assert.assertTrue("Driver Run Hooks did not run in the order specified.",
-          driverRunPostHookFirstRan);
-    }
-
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyIsLocalModeHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyIsLocalModeHook.java
deleted file mode 100644
index 166bd5b..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyIsLocalModeHook.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.List;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.hive.ql.exec.Task;
-import org.apache.hadoop.hive.ql.exec.TaskRunner;
-import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
-
-public class VerifyIsLocalModeHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    if (hookContext.getHookType().equals(HookType.POST_EXEC_HOOK)) {
-      List<TaskRunner> taskRunners = hookContext.getCompleteTaskList();
-      for (TaskRunner taskRunner : taskRunners) {
-        Task task = taskRunner.getTask();
-        if (task.isMapRedTask()) {
-          Assert.assertTrue("VerifyIsLocalModeHook fails because a isLocalMode was not set for a task.",
-              task.isLocalMode());
-        }
-      }
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyNumReducersHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyNumReducersHook.java
deleted file mode 100644
index 4b2c184..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyNumReducersHook.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.List;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.hive.ql.MapRedStats;
-import org.apache.hadoop.hive.ql.session.SessionState;
-
-/**
- *
- * VerifyNumReducersHook.
- *
- * Provided a query involves exactly 1 map reduce job, this hook can be used to verify that the
- * number of reducers matches what is expected.
- *
- * Use the config VerifyNumReducersHook.num.reducers to specify the expected number of reducers.
- */
-public class VerifyNumReducersHook implements ExecuteWithHookContext {
-
-  public static final String BUCKET_CONFIG = "VerifyNumReducersHook.num.reducers";
-
-  public void run(HookContext hookContext) {
-    SessionState ss = SessionState.get();
-    Assert.assertNotNull("SessionState returned null");
-
-    int expectedReducers = hookContext.getConf().getInt(BUCKET_CONFIG, 0);
-    List<MapRedStats> stats = ss.getLastMapRedStatsList();
-    Assert.assertEquals("Number of MapReduce jobs is incorrect", 1, stats.size());
-
-    Assert.assertEquals("NumReducers is incorrect", expectedReducers, stats.get(0).getNumReduce());
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java
deleted file mode 100644
index 5cc4079..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java
+++ /dev/null
@@ -1,34 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import org.junit.Assert;
-
-
-public class VerifyOutputTableLocationSchemeIsFileHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    for (WriteEntity output : hookContext.getOutputs()) {
-      if (output.getType() == WriteEntity.Type.TABLE) {
-        String scheme = output.getTable().getDataLocation().getScheme();
-        Assert.assertTrue(output.getTable().getTableName() + " has a location which has a " +
-              "scheme other than file: " + scheme, scheme.equals("file"));
-      }
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java
deleted file mode 100644
index 41c178a..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map.Entry;
-
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-
-/**
- *
- * VerifyOverriddenConfigsHook.
- *
- * This hook is meant to be used for testing.  It prints the keys and values of config variables
- * which have been noted by the session state as changed by the user.  It only prints specific
- * variables as others may change and that should not affect this test.
- */
-public class VerifyOverriddenConfigsHook implements ExecuteWithHookContext {
-
-  // A config variable set via a System Propery, a config variable set in the CLI,
-  // a config variable not in the default List of config variables, and a config variable in the
-  // default list of conifg variables, but which has not been overridden
-  private static String[] keysArray =
-    {"mapred.job.tracker", "hive.exec.post.hooks", "hive.config.doesnt.exit",
-     "hive.exec.mode.local.auto"};
-  private static List<String> keysList = Arrays.asList(keysArray);
-
-  public void run(HookContext hookContext) {
-    LogHelper console = SessionState.getConsole();
-    SessionState ss = SessionState.get();
-
-    if (console == null || ss == null) {
-      return;
-    }
-
-    for (Entry<String, String> entry : ss.getOverriddenConfigurations().entrySet()) {
-      if (keysList.contains(entry.getKey())) {
-        console.printError("Key: " + entry.getKey() + ", Value: " + entry.getValue());
-      }
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java
deleted file mode 100644
index ce377be..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.hive.ql.metadata.Partition;
-import org.apache.hadoop.hive.ql.metadata.Table;
-
-// This hook verifies that the location of every partition in the inputs and outputs does not
-// start with the location of the table.  It is a very simple check to make sure the location is
-// not a subdirectory.
-public class VerifyPartitionIsNotSubdirectoryOfTableHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    for (WriteEntity output : hookContext.getOutputs()) {
-      if (output.getType() == WriteEntity.Type.PARTITION) {
-        verify (output.getPartition(), output.getTable());
-      }
-    }
-
-    for (ReadEntity input : hookContext.getInputs()) {
-      if (input.getType() == ReadEntity.Type.PARTITION) {
-        verify (input.getPartition(), input.getTable());
-      }
-    }
-  }
-
-  private void verify(Partition partition, Table table) {
-    Assert.assertFalse("The location of the partition: " + partition.getName() + " was a " +
-        "subdirectory of the location of the table: " + table.getTableName(),
-        partition.getPartitionPath().toString().startsWith(table.getPath().toString()));
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java
deleted file mode 100644
index 4406036..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.hive.ql.metadata.Partition;
-import org.apache.hadoop.hive.ql.metadata.Table;
-
-// This hook verifies that the location of every partition in the inputs and outputs starts with
-// the location of the table.  It is a very simple check to make sure it is a subdirectory.
-public class VerifyPartitionIsSubdirectoryOfTableHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    for (WriteEntity output : hookContext.getOutputs()) {
-      if (output.getType() == WriteEntity.Type.PARTITION) {
-        verify (output.getPartition(), output.getTable());
-      }
-    }
-
-    for (ReadEntity input : hookContext.getInputs()) {
-      if (input.getType() == ReadEntity.Type.PARTITION) {
-        verify (input.getPartition(), input.getTable());
-      }
-    }
-  }
-
-  private void verify(Partition partition, Table table) {
-    Assert.assertTrue("The location of the partition: " + partition.getName() + " was not a " +
-        "subdirectory of the location of the table: " + table.getTableName(),
-        partition.getPartitionPath().toString().startsWith(table.getPath().toString()));
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifySessionStateLocalErrorsHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifySessionStateLocalErrorsHook.java
deleted file mode 100644
index b6ec8ba..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifySessionStateLocalErrorsHook.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.List;
-import java.util.Map.Entry;
-
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-
-/**
- *
- * VerifySessionStateLocalErrorsHook.
- *
- * This hook is intended for testing that the localMapRedErrors variable in SessionState is
- * populated when a local map reduce job fails.  It prints the ID of the stage that failed, and
- * the lines recorded in the variable.
- */
-public class VerifySessionStateLocalErrorsHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    LogHelper console = SessionState.getConsole();
-
-    for (Entry<String, List<String>> entry : SessionState.get().getLocalMapRedErrors().entrySet()) {
-      console.printError("ID: " + entry.getKey());
-
-      for (String line : entry.getValue()) {
-        console.printError(line);
-      }
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifySessionStateStackTracesHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifySessionStateStackTracesHook.java
deleted file mode 100644
index 129d57a..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifySessionStateStackTracesHook.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.util.List;
-import java.util.Map.Entry;
-
-import org.apache.commons.lang.StringUtils;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-
-/**
- *
- * VerifySessionStateStackTracesHook.
- *
- * Writes the first line of each stack trace collected to the console, to either verify stack
- * traces were or were not collected.
- */
-public class VerifySessionStateStackTracesHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) {
-    LogHelper console = SessionState.getConsole();
-
-    for (Entry<String, List<List<String>>> entry :
-        SessionState.get().getStackTraces().entrySet()) {
-
-      for (List<String> stackTrace : entry.getValue()) {
-        // Only print the first line of the stack trace as it contains the error message, and other
-        // lines may contain line numbers which are volatile
-        // Also only take the string after the first two spaces, because the prefix is a date and
-        // and time stamp
-        console.printError(StringUtils.substringAfter(
-            StringUtils.substringAfter(stackTrace.get(0), " "), " "));
-      }
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyTableDirectoryIsEmptyHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyTableDirectoryIsEmptyHook.java
deleted file mode 100644
index c2005e4..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/hooks/VerifyTableDirectoryIsEmptyHook.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.hooks;
-
-import java.io.IOException;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.session.SessionState;
-
-// This hook verifies that the location of every output table is empty
-public class VerifyTableDirectoryIsEmptyHook implements ExecuteWithHookContext {
-
-  public void run(HookContext hookContext) throws IOException {
-    for (WriteEntity output : hookContext.getOutputs()) {
-      Path tableLocation = new Path(output.getTable().getDataLocation().toString());
-      FileSystem fs = tableLocation.getFileSystem(SessionState.get().getConf());
-      assert(fs.listStatus(tableLocation).length == 0);
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/udf/Rot13InputFormat.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/udf/Rot13InputFormat.java
deleted file mode 100644
index ddf62c3..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/io/udf/Rot13InputFormat.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.io.udf;
-
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileSplit;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.LineRecordReader;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-
-import java.io.IOException;
-
-/**
-A simple input format that does a rot13 on the inputs
- */
-class Rot13InputFormat extends TextInputFormat {
-
-  public static void rot13(byte[] bytes, int offset, int length) {
-    for(int i=offset; i < offset+length; i++) {
-      if (bytes[i] >= 'A' && bytes[i] <= 'Z') {
-        bytes[i] = (byte) ('A' + (bytes[i] - 'A' + 13) % 26);
-      } else if (bytes[i] >= 'a' && bytes[i] <= 'z') {
-        bytes[i] = (byte) ('a' + (bytes[i] - 'a' + 13) % 26);
-      }
-    }
-  }
-
-  private static class Rot13LineRecordReader extends LineRecordReader {
-    Rot13LineRecordReader(JobConf job, FileSplit split) throws IOException {
-      super(job, split);
-    }
-
-    public synchronized boolean next(LongWritable key,
-                                     Text value) throws IOException {
-      boolean result = super.next(key, value);
-      if (result) {
-        System.out.println("Read " + value);
-        rot13(value.getBytes(), 0, value.getLength());
-        System.out.println("Returned " + value);
-      }
-      return result;
-    }
-  }
-
-  public RecordReader<LongWritable, Text>
-    getRecordReader(InputSplit genericSplit, JobConf job,
-                    Reporter reporter) throws IOException {
-    reporter.setStatus(genericSplit.toString());
-    return new Rot13LineRecordReader(job, (FileSplit) genericSplit);
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/udf/Rot13OutputFormat.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/udf/Rot13OutputFormat.java
deleted file mode 100644
index e2d92a5..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/io/udf/Rot13OutputFormat.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.io.udf;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;
-import org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.util.Progressable;
-
-import java.io.IOException;
-import java.util.Properties;
-
-public class Rot13OutputFormat
-  extends HiveIgnoreKeyTextOutputFormat<LongWritable,Text> {
-
-  @Override
-  public RecordWriter
-    getHiveRecordWriter(JobConf jc,
-                        Path outPath,
-                        Class<? extends Writable> valueClass,
-                        boolean isCompressed,
-                        Properties tableProperties,
-                        Progressable progress) throws IOException {
-    final RecordWriter result =
-      super.getHiveRecordWriter(jc,outPath,valueClass,isCompressed,
-        tableProperties,progress);
-    final Reporter reporter = (Reporter) progress;
-    reporter.setStatus("got here");
-    System.out.println("Got a reporter " + reporter);
-    return new RecordWriter() {
-      @Override
-      public void write(Writable w) throws IOException {
-        if (w instanceof Text) {
-          Text value = (Text) w;
-          Rot13InputFormat.rot13(value.getBytes(), 0, value.getLength());
-          result.write(w);
-        } else if (w instanceof BytesWritable) {
-          BytesWritable value = (BytesWritable) w;
-          Rot13InputFormat.rot13(value.getBytes(), 0, value.getLength());
-          result.write(w);
-        } else {
-          throw new IllegalArgumentException("need text or bytes writable " +
-            " instead of " + w.getClass().getName());
-        }
-      }
-
-      @Override
-      public void close(boolean abort) throws IOException {
-        result.close(abort);
-      }
-    };
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook.java b/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook.java
deleted file mode 100644
index 8d9cad5..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook.java
+++ /dev/null
@@ -1,104 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.metadata;
-
-import java.io.Serializable;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.hadoop.hive.ql.exec.DDLTask;
-import org.apache.hadoop.hive.ql.exec.Task;
-import org.apache.hadoop.hive.ql.parse.ASTNode;
-import org.apache.hadoop.hive.ql.parse.AbstractSemanticAnalyzerHook;
-import org.apache.hadoop.hive.ql.parse.HiveParser;
-import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext;
-import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.ql.plan.CreateTableDesc;
-
-public class DummySemanticAnalyzerHook extends AbstractSemanticAnalyzerHook{
-
-  private AbstractSemanticAnalyzerHook hook;
-
-  @Override
-  public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)
-  throws SemanticException {
-
-    switch (ast.getToken().getType()) {
-
-    case HiveParser.TOK_CREATETABLE:
-      hook = new DummyCreateTableHook();
-      return hook.preAnalyze(context, ast);
-
-    case HiveParser.TOK_DROPTABLE:
-    case HiveParser.TOK_DESCTABLE:
-      return ast;
-
-    default:
-      throw new SemanticException("Operation not supported.");
-    }
-  }
-
-  public DummySemanticAnalyzerHook() {
-
-  }
-
-  @Override
-  public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
-
-    if(hook != null) {
-      hook.postAnalyze(context, rootTasks);
-    }
-  }
-}
-
-class DummyCreateTableHook extends AbstractSemanticAnalyzerHook{
-
-  @Override
-  public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)
-  throws SemanticException {
-
-    int numCh = ast.getChildCount();
-
-    for (int num = 1; num < numCh; num++) {
-      ASTNode child = (ASTNode) ast.getChild(num);
-
-      switch (child.getToken().getType()) {
-
-      case HiveParser.TOK_QUERY:
-        throw new SemanticException("CTAS not supported.");
-      }
-    }
-    return ast;
-  }
-
-  @Override
-  public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
-    CreateTableDesc desc = ((DDLTask)rootTasks.get(rootTasks.size()-1)).getWork().getCreateTblDesc();
-    Map<String,String> tblProps = desc.getTblProps();
-    if(tblProps == null) {
-      tblProps = new HashMap<String, String>();
-    }
-    tblProps.put("createdBy", DummyCreateTableHook.class.getName());
-    tblProps.put("Message", "Open Source rocks!!");
-    desc.setTblProps(tblProps);
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook1.java b/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook1.java
deleted file mode 100644
index 9f86637..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook1.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.metadata;
-
-import java.io.Serializable;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.hadoop.hive.ql.exec.DDLTask;
-import org.apache.hadoop.hive.ql.exec.Task;
-import org.apache.hadoop.hive.ql.parse.ASTNode;
-import org.apache.hadoop.hive.ql.parse.AbstractSemanticAnalyzerHook;
-import org.apache.hadoop.hive.ql.parse.HiveParser;
-import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext;
-import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.ql.plan.CreateTableDesc;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-
-public class DummySemanticAnalyzerHook1 extends AbstractSemanticAnalyzerHook {
-  static int count = 0;
-  int myCount = -1;
-  boolean isCreateTable;
-
-  @Override
-  public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)
-      throws SemanticException {
-    LogHelper console = SessionState.getConsole();
-    isCreateTable = (ast.getToken().getType() == HiveParser.TOK_CREATETABLE);
-    myCount = count++;
-    if (isCreateTable) {
-      console.printError("DummySemanticAnalyzerHook1 Pre: Count " + myCount);
-    }
-    return ast;
-  }
-
-  public DummySemanticAnalyzerHook1() {
-  }
-
-  @Override
-  public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
-    count = 0;
-    if (!isCreateTable) {
-      return;
-    }
-
-    CreateTableDesc desc = ((DDLTask) rootTasks.get(rootTasks.size() - 1)).getWork()
-        .getCreateTblDesc();
-    Map<String, String> tblProps = desc.getTblProps();
-    if (tblProps == null) {
-      tblProps = new HashMap<String, String>();
-    }
-    tblProps.put("createdBy", DummyCreateTableHook.class.getName());
-    tblProps.put("Message", "Hive rocks!! Count: " + myCount);
-
-    LogHelper console = SessionState.getConsole();
-    console.printError("DummySemanticAnalyzerHook1 Post: Hive rocks!! Count: " + myCount);
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java b/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java
deleted file mode 100644
index 3027ef4..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.metadata;
-
-import java.util.Map;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
-import org.apache.hadoop.hive.ql.session.SessionState;
-
-public class TestSemanticAnalyzerHookLoading extends TestCase {
-
-  public void testHookLoading() throws Exception{
-
-    HiveConf conf = new HiveConf(this.getClass());
-    conf.set(ConfVars.SEMANTIC_ANALYZER_HOOK.varname, DummySemanticAnalyzerHook.class.getName());
-    conf.set(ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    SessionState.start(conf);
-    Driver driver = new Driver(conf);
-
-    driver.run("drop table testDL");
-    CommandProcessorResponse resp = driver.run("create table testDL (a int) as select * from tbl2");
-    assertEquals(40000, resp.getResponseCode());
-
-    resp = driver.run("create table testDL (a int)");
-    assertEquals(0, resp.getResponseCode());
-    assertNull(resp.getErrorMessage());
-
-    Map<String,String> params = Hive.get(conf).getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, "testDL").getParameters();
-
-    assertEquals(DummyCreateTableHook.class.getName(),params.get("createdBy"));
-    assertEquals("Open Source rocks!!", params.get("Message"));
-
-    driver.run("drop table testDL");
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/security/DummyAuthenticator.java b/src/ql/src/test/org/apache/hadoop/hive/ql/security/DummyAuthenticator.java
deleted file mode 100644
index 578a177..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/security/DummyAuthenticator.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.security;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-
-public class DummyAuthenticator implements HiveAuthenticationProvider {
-
-  private List<String> groupNames;
-  private String userName;
-  private Configuration conf;
-
-  public DummyAuthenticator() {
-    this.groupNames = new ArrayList<String>();
-    groupNames.add("hive_test_group1");
-    groupNames.add("hive_test_group2");
-    userName = "hive_test_user";
-  }
-
-  @Override
-  public void destroy() throws HiveException{
-    return;
-  }
-
-  @Override
-  public List<String> getGroupNames() {
-    return groupNames;
-  }
-
-  @Override
-  public String getUserName() {
-    return userName;
-  }
-
-  @Override
-  public void setConf(Configuration conf) {
-    this.conf = conf;
-  }
-
-  public Configuration getConf() {
-    return this.conf;
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/security/DummyHiveMetastoreAuthorizationProvider.java b/src/ql/src/test/org/apache/hadoop/hive/ql/security/DummyHiveMetastoreAuthorizationProvider.java
deleted file mode 100644
index 195a5a4..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/security/DummyHiveMetastoreAuthorizationProvider.java
+++ /dev/null
@@ -1,204 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.security;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.ql.metadata.AuthorizationException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.metadata.Partition;
-import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider;
-import org.apache.hadoop.hive.ql.security.authorization.Privilege;
-
-public class DummyHiveMetastoreAuthorizationProvider implements HiveMetastoreAuthorizationProvider {
-
-
-  protected HiveAuthenticationProvider authenticator;
-
-  public enum AuthCallContextType {
-    USER,
-    DB,
-    TABLE,
-    PARTITION,
-    TABLE_AND_PARTITION
-  };
-
-  class AuthCallContext {
-
-    public AuthCallContextType type;
-    public List<Object> authObjects;
-    public Privilege[] readRequiredPriv;
-    public Privilege[] writeRequiredPriv;
-
-    AuthCallContext(AuthCallContextType typeOfCall,
-        Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv) {
-      this.type = typeOfCall;
-      this.authObjects = new ArrayList<Object>();
-      this.readRequiredPriv = readRequiredPriv;
-      this.writeRequiredPriv = writeRequiredPriv;
-    }
-    AuthCallContext(AuthCallContextType typeOfCall, Object authObject,
-        Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv) {
-      this(typeOfCall,readRequiredPriv,writeRequiredPriv);
-      this.authObjects.add(authObject);
-    }
-    AuthCallContext(AuthCallContextType typeOfCall, List<? extends Object> authObjects,
-        Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv) {
-      this(typeOfCall,readRequiredPriv,writeRequiredPriv);
-      this.authObjects.addAll(authObjects);
-    }
-  }
-
-  public static final List<AuthCallContext> authCalls = new ArrayList<AuthCallContext>();
-
-  private Configuration conf;
-  public static final Log LOG = LogFactory.getLog(
-      DummyHiveMetastoreAuthorizationProvider.class);;
-
-  @Override
-  public Configuration getConf() {
-    return this.conf;
-  }
-
-  @Override
-  public void setConf(Configuration conf) {
-    this.conf = conf;
-    try {
-      init(conf);
-    } catch (HiveException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  @Override
-  public HiveAuthenticationProvider getAuthenticator() {
-    return authenticator;
-  }
-
-  @Override
-  public void setAuthenticator(HiveAuthenticationProvider authenticator) {
-    this.authenticator = authenticator;
-  }
-
-  @Override
-  public void init(Configuration conf) throws HiveException {
-    debugLog("DHMAP.init");
-  }
-
-  @Override
-  public void authorize(Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-      throws HiveException, AuthorizationException {
-    debugLog("DHMAP.authorize " +
-      "read:" + debugPrivPrint(readRequiredPriv) +
-      " , write:" + debugPrivPrint(writeRequiredPriv)
-      );
-    authCalls.add(new AuthCallContext(AuthCallContextType.USER,
-        readRequiredPriv, writeRequiredPriv));
-  }
-
-  @Override
-  public void authorize(Database db, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-      throws HiveException, AuthorizationException {
-    debugLog("DHMAP.authorizedb " +
-        "db:" + db.getName() +
-        " , read:" + debugPrivPrint(readRequiredPriv) +
-        " , write:" + debugPrivPrint(writeRequiredPriv)
-        );
-    authCalls.add(new AuthCallContext(AuthCallContextType.DB,
-        db, readRequiredPriv, writeRequiredPriv));
-  }
-
-  @Override
-  public void authorize(Table table, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-      throws HiveException, AuthorizationException {
-    debugLog("DHMAP.authorizetbl " +
-        "tbl:" + table.getCompleteName() +
-        " , read:" + debugPrivPrint(readRequiredPriv) +
-        " , write:" + debugPrivPrint(writeRequiredPriv)
-        );
-    authCalls.add(new AuthCallContext(AuthCallContextType.TABLE,
-        table, readRequiredPriv, writeRequiredPriv));
-
-  }
-
-  @Override
-  public void authorize(Partition part, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-      throws HiveException, AuthorizationException {
-    debugLog("DHMAP.authorizepart " +
-        "tbl:" + part.getTable().getCompleteName() +
-        " , part: " + part.getName() +
-        " , read:" + debugPrivPrint(readRequiredPriv) +
-        " , write:" + debugPrivPrint(writeRequiredPriv)
-        );
-    authCalls.add(new AuthCallContext(AuthCallContextType.PARTITION,
-        part, readRequiredPriv, writeRequiredPriv));
-
-  }
-
-  @Override
-  public void authorize(Table table, Partition part, List<String> columns,
-      Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv) throws HiveException,
-      AuthorizationException {
-    debugLog("DHMAP.authorizecols " +
-        "tbl:" + table.getCompleteName() +
-        " , part: " + part.getName() +
-        " . cols: " + columns.toString() +
-        " , read:" + debugPrivPrint(readRequiredPriv) +
-        " , write:" + debugPrivPrint(writeRequiredPriv)
-        );
-    List<Object> authObjects = new ArrayList<Object>();
-    authObjects.add(table);
-    authObjects.add(part);
-    authCalls.add(new AuthCallContext(AuthCallContextType.TABLE_AND_PARTITION,
-        authObjects, readRequiredPriv, writeRequiredPriv));
-
-  }
-
-  private void debugLog(String s) {
-    LOG.debug(s);
-  }
-
-  private String debugPrivPrint(Privilege[] privileges) {
-    StringBuffer sb = new StringBuffer();
-    sb.append("Privileges{");
-    if (privileges != null){
-    for (Privilege p : privileges){
-      sb.append(p.toString());
-    }
-    }else{
-      sb.append("null");
-    }
-    sb.append("}");
-    return sb.toString();
-  }
-
-  @Override
-  public void setMetaStoreHandler(HMSHandler handler) {
-    debugLog("DHMAP.setMetaStoreHandler");
-  }
-
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java b/src/ql/src/test/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java
deleted file mode 100644
index 2dd225e..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.security;
-
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-
-/**
- *
- * InjectableDummyAuthenticator - An implementation of HiveMetastoreAuthenticationProvider
- * that wraps another Authenticator, but when asked to inject a user provided username
- * and groupnames, does so. This can be toggled back and forth to use in testing
- */
-public class InjectableDummyAuthenticator implements HiveMetastoreAuthenticationProvider {
-
-  private static String userName;
-  private static List<String> groupNames;
-  private static boolean injectMode;
-  private static Class<? extends HiveMetastoreAuthenticationProvider> hmapClass =
-      HadoopDefaultMetastoreAuthenticator.class;
-  private HiveMetastoreAuthenticationProvider hmap;
-
-  public static void injectHmapClass(Class<? extends HiveMetastoreAuthenticationProvider> clazz){
-    hmapClass = clazz;
-  }
-
-  public static void injectUserName(String user){
-    userName = user;
-  }
-
-  public static void injectGroupNames(List<String> groups){
-    groupNames = groups;
-  }
-
-  public static void injectMode(boolean mode){
-    injectMode = mode;
-  }
-
-  @Override
-  public String getUserName() {
-    if (injectMode){
-      return userName;
-    } else {
-      return hmap.getUserName();
-    }
-  }
-
-  @Override
-  public List<String> getGroupNames() {
-    if (injectMode) {
-      return groupNames;
-    } else {
-      return hmap.getGroupNames();
-    }
-  }
-
-  @Override
-  public Configuration getConf() {
-    return hmap.getConf();
-  }
-
-  @Override
-  public void setConf(Configuration config) {
-    try {
-      hmap = (HiveMetastoreAuthenticationProvider) hmapClass.newInstance();
-    } catch (InstantiationException e) {
-      throw new RuntimeException("Whoops, could not create an Authenticator of class " +
-          hmapClass.getName());
-    } catch (IllegalAccessException e) {
-      throw new RuntimeException("Whoops, could not create an Authenticator of class " +
-          hmapClass.getName());
-    }
-
-    hmap.setConf(config);
-  }
-
-  @Override
-  public void setMetaStoreHandler(HMSHandler handler) {
-    hmap.setMetaStoreHandler(handler);
-  }
-
-  @Override
-  public void destroy() throws HiveException {
-    hmap.destroy();
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestAuthorizationPreEventListener.java b/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestAuthorizationPreEventListener.java
deleted file mode 100644
index d2595dd..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestAuthorizationPreEventListener.java
+++ /dev/null
@@ -1,303 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.security;
-
-import java.io.IOException;
-import java.net.ServerSocket;
-import java.util.ArrayList;
-import java.util.List;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.Partition;
-import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.AuthCallContext;
-import org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.shims.ShimLoader;
-
-/**
- * TestAuthorizationPreEventListener. Test case for
- * {@link org.apache.hadoop.hive.metastore.AuthorizationPreEventListener} and
- * {@link org.apache.hadoop.hive.metastore.MetaStorePreEventListener}
- */
-public class TestAuthorizationPreEventListener extends TestCase {
-  private HiveConf clientHiveConf;
-  private HiveMetaStoreClient msc;
-  private Driver driver;
-
-  @Override
-  protected void setUp() throws Exception {
-
-    super.setUp();
-
-    int port = MetaStoreUtils.findFreePort();
-
-    System.setProperty(HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS.varname,
-        AuthorizationPreEventListener.class.getName());
-    System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHORIZATION_MANAGER.varname,
-        DummyHiveMetastoreAuthorizationProvider.class.getName());
-    System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHENTICATOR_MANAGER.varname,
-        HadoopDefaultMetastoreAuthenticator.class.getName());
-
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-
-    clientHiveConf = new HiveConf(this.getClass());
-
-    clientHiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    clientHiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    clientHiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-
-    clientHiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    clientHiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-
-
-    SessionState.start(new CliSessionState(clientHiveConf));
-    msc = new HiveMetaStoreClient(clientHiveConf, null);
-    driver = new Driver(clientHiveConf);
-  }
-
-  private static String getFreeAvailablePort() throws IOException {
-    ServerSocket socket = new ServerSocket(0);
-    socket.setReuseAddress(true);
-    int port = socket.getLocalPort();
-    socket.close();
-    return "" + port;
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-  private void validateCreateDb(Database expectedDb, Database actualDb) {
-    assertEquals(expectedDb.getName(), actualDb.getName());
-    assertEquals(expectedDb.getLocationUri(), actualDb.getLocationUri());
-  }
-
-  private void validateTable(Table expectedTable, Table actualTable) {
-    assertEquals(expectedTable.getTableName(), actualTable.getTableName());
-    assertEquals(expectedTable.getDbName(), actualTable.getDbName());
-
-    // We won't try to be too strict in checking this because we're comparing
-    // table create intents with observed tables created.
-    // If it does have a location though, we will compare, as with external tables
-    if ((actualTable.getSd() != null) && (actualTable.getSd().getLocation() != null)){
-      assertEquals(expectedTable.getSd().getLocation(), actualTable.getSd().getLocation());
-    }
-  }
-
-  private void validateCreateTable(Table expectedTable, Table actualTable) {
-    validateTable(expectedTable, actualTable);
-  }
-
-  private void validateAddPartition(Partition expectedPartition, Partition actualPartition) {
-    validatePartition(expectedPartition,actualPartition);
-  }
-
-  private void validatePartition(Partition expectedPartition, Partition actualPartition) {
-    assertEquals(expectedPartition.getValues(),
-        actualPartition.getValues());
-    assertEquals(expectedPartition.getDbName(),
-        actualPartition.getDbName());
-    assertEquals(expectedPartition.getTableName(),
-        actualPartition.getTableName());
-
-    // assertEquals(expectedPartition.getSd().getLocation(),
-    //     actualPartition.getSd().getLocation());
-    // we don't compare locations, because the location can still be empty in
-    // the pre-event listener before it is created.
-
-    assertEquals(expectedPartition.getSd().getInputFormat(),
-        actualPartition.getSd().getInputFormat());
-    assertEquals(expectedPartition.getSd().getOutputFormat(),
-        actualPartition.getSd().getOutputFormat());
-    assertEquals(expectedPartition.getSd().getSerdeInfo(),
-        actualPartition.getSd().getSerdeInfo());
-
-  }
-
-  private void validateAlterPartition(Partition expectedOldPartition,
-      Partition expectedNewPartition, String actualOldPartitionDbName,
-      String actualOldPartitionTblName,List<String> actualOldPartitionValues,
-      Partition actualNewPartition) {
-    assertEquals(expectedOldPartition.getValues(), actualOldPartitionValues);
-    assertEquals(expectedOldPartition.getDbName(), actualOldPartitionDbName);
-    assertEquals(expectedOldPartition.getTableName(), actualOldPartitionTblName);
-
-    validatePartition(expectedNewPartition, actualNewPartition);
-  }
-
-  private void validateAlterTable(Table expectedOldTable, Table expectedNewTable,
-      Table actualOldTable, Table actualNewTable) {
-    validateTable(expectedOldTable, actualOldTable);
-    validateTable(expectedNewTable, actualNewTable);
-  }
-
-  private void validateDropPartition(Partition expectedPartition, Partition actualPartition) {
-    validatePartition(expectedPartition, actualPartition);
-  }
-
-  private void validateDropTable(Table expectedTable, Table actualTable) {
-    validateTable(expectedTable, actualTable);
-  }
-
-  private void validateDropDb(Database expectedDb, Database actualDb) {
-    assertEquals(expectedDb, actualDb);
-  }
-
-  public void testListener() throws Exception {
-    String dbName = "hive3705";
-    String tblName = "tmptbl";
-    String renamed = "tmptbl2";
-    int listSize = 0;
-
-    List<AuthCallContext> authCalls = DummyHiveMetastoreAuthorizationProvider.authCalls;
-    assertEquals(authCalls.size(),listSize);
-
-    driver.run("create database " + dbName);
-    listSize++;
-    Database db = msc.getDatabase(dbName);
-
-    Database dbFromEvent = (Database)assertAndExtractSingleObjectFromEvent(listSize, authCalls,
-        DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.DB);
-    validateCreateDb(db,dbFromEvent);
-
-    driver.run("use " + dbName);
-    driver.run(String.format("create table %s (a string) partitioned by (b string)", tblName));
-    listSize++;
-    Table tbl = msc.getTable(dbName, tblName);
-
-    Table tblFromEvent = (
-        (org.apache.hadoop.hive.ql.metadata.Table)
-        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
-            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.TABLE))
-            .getTTable();
-    validateCreateTable(tbl, tblFromEvent);
-
-    driver.run("alter table tmptbl add partition (b='2011')");
-    listSize++;
-    Partition part = msc.getPartition("hive3705", "tmptbl", "b=2011");
-
-    Partition ptnFromEvent = (
-        (org.apache.hadoop.hive.ql.metadata.Partition)
-        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
-            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION))
-            .getTPartition();
-    validateAddPartition(part,ptnFromEvent);
-
-    driver.run(String.format("alter table %s touch partition (%s)", tblName, "b='2011'"));
-    listSize++;
-
-    //the partition did not change,
-    // so the new partition should be similar to the original partition
-    Partition modifiedP = msc.getPartition(dbName, tblName, "b=2011");
-
-    Partition ptnFromEventAfterAlter = (
-        (org.apache.hadoop.hive.ql.metadata.Partition)
-        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
-            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION))
-            .getTPartition();
-
-    validateAlterPartition(part, modifiedP, ptnFromEventAfterAlter.getDbName(),
-        ptnFromEventAfterAlter.getTableName(), ptnFromEventAfterAlter.getValues(),
-        ptnFromEventAfterAlter);
-
-
-    List<String> part_vals = new ArrayList<String>();
-    part_vals.add("c=2012");
-    Partition newPart = msc.appendPartition(dbName, tblName, part_vals);
-
-    listSize++;
-
-    Partition newPtnFromEvent = (
-        (org.apache.hadoop.hive.ql.metadata.Partition)
-        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
-            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION))
-            .getTPartition();
-    validateAddPartition(newPart,newPtnFromEvent);
-
-
-    driver.run(String.format("alter table %s rename to %s", tblName, renamed));
-    listSize++;
-
-    Table renamedTable = msc.getTable(dbName, renamed);
-    Table renamedTableFromEvent = (
-        (org.apache.hadoop.hive.ql.metadata.Table)
-        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
-            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.TABLE))
-            .getTTable();
-
-    validateAlterTable(tbl, renamedTable, renamedTableFromEvent,
-        renamedTable);
-    assertFalse(tbl.getTableName().equals(renamedTable.getTableName()));
-
-
-    //change the table name back
-    driver.run(String.format("alter table %s rename to %s", renamed, tblName));
-    listSize++;
-
-    driver.run(String.format("alter table %s drop partition (b='2011')", tblName));
-    listSize++;
-
-    Partition ptnFromDropPartition = (
-        (org.apache.hadoop.hive.ql.metadata.Partition)
-        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
-            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.PARTITION))
-            .getTPartition();
-
-    validateDropPartition(modifiedP, ptnFromDropPartition);
-
-    driver.run("drop table " + tblName);
-    listSize++;
-    Table tableFromDropTableEvent = (
-        (org.apache.hadoop.hive.ql.metadata.Table)
-        assertAndExtractSingleObjectFromEvent(listSize, authCalls,
-            DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.TABLE))
-            .getTTable();
-
-
-    validateDropTable(tbl, tableFromDropTableEvent);
-
-    driver.run("drop database " + dbName);
-    listSize++;
-    Database dbFromDropDatabaseEvent =
-        (Database)assertAndExtractSingleObjectFromEvent(listSize, authCalls,
-        DummyHiveMetastoreAuthorizationProvider.AuthCallContextType.DB);
-
-    validateDropDb(db, dbFromDropDatabaseEvent);
-  }
-
-  public Object assertAndExtractSingleObjectFromEvent(int listSize,
-      List<AuthCallContext> authCalls,
-      DummyHiveMetastoreAuthorizationProvider.AuthCallContextType callType) {
-    assertEquals(listSize, authCalls.size());
-    assertEquals(1,authCalls.get(listSize-1).authObjects.size());
-
-    assertEquals(callType,authCalls.get(listSize-1).type);
-    return (authCalls.get(listSize-1).authObjects.get(0));
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestClientSideAuthorizationProvider.java b/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestClientSideAuthorizationProvider.java
deleted file mode 100644
index 48e8b23..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestClientSideAuthorizationProvider.java
+++ /dev/null
@@ -1,214 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.security;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
-import org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.security.UserGroupInformation;
-
-/**
- * TestClientSideAuthorizationProvider : Simple base test for client side
- * Authorization Providers. By default, tests DefaultHiveAuthorizationProvider
- */
-public class TestClientSideAuthorizationProvider extends TestCase {
-  protected HiveConf clientHiveConf;
-  protected HiveMetaStoreClient msc;
-  protected Driver driver;
-  protected UserGroupInformation ugi;
-
-
-  protected String getAuthorizationProvider(){
-    return DefaultHiveAuthorizationProvider.class.getName();
-  }
-
-
-  @Override
-  protected void setUp() throws Exception {
-
-    super.setUp();
-
-    int port = MetaStoreUtils.findFreePort();
-
-    // Turn off metastore-side authorization
-    System.setProperty(HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS.varname,
-        "");
-
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-
-    clientHiveConf = new HiveConf(this.getClass());
-
-    // Turn on client-side authorization
-    clientHiveConf.setBoolVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED,true);
-    clientHiveConf.set(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER.varname,
-        getAuthorizationProvider());
-    clientHiveConf.set(HiveConf.ConfVars.HIVE_AUTHENTICATOR_MANAGER.varname,
-        InjectableDummyAuthenticator.class.getName());
-    clientHiveConf.set(HiveConf.ConfVars.HIVE_AUTHORIZATION_TABLE_OWNER_GRANTS.varname, "");
-
-    clientHiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    clientHiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    clientHiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-
-    clientHiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    clientHiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-
-    ugi = ShimLoader.getHadoopShims().getUGIForConf(clientHiveConf);
-
-    SessionState.start(new CliSessionState(clientHiveConf));
-    msc = new HiveMetaStoreClient(clientHiveConf, null);
-    driver = new Driver(clientHiveConf);
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-  private void validateCreateDb(Database expectedDb, String dbName) {
-    assertEquals(expectedDb.getName().toLowerCase(), dbName.toLowerCase());
-  }
-
-  private void validateCreateTable(Table expectedTable, String tblName, String dbName) {
-    assertNotNull(expectedTable);
-    assertEquals(expectedTable.getTableName().toLowerCase(),tblName.toLowerCase());
-    assertEquals(expectedTable.getDbName().toLowerCase(),dbName.toLowerCase());
-  }
-
-  protected String getTestDbName(){
-    return "smp_cl_db";
-  }
-
-  protected String getTestTableName(){
-    return "smp_cl_tbl";
-  }
-
-  public void testSimplePrivileges() throws Exception {
-    String dbName = getTestDbName();
-    String tblName = getTestTableName();
-
-    String userName = ugi.getUserName();
-
-    CommandProcessorResponse ret = driver.run("create database " + dbName);
-    assertEquals(0,ret.getResponseCode());
-    Database db = msc.getDatabase(dbName);
-    String dbLocn = db.getLocationUri();
-
-    validateCreateDb(db,dbName);
-    disallowCreateInDb(dbName, userName, dbLocn);
-
-    driver.run("use " + dbName);
-    ret = driver.run(
-        String.format("create table %s (a string) partitioned by (b string)", tblName));
-
-    // failure from not having permissions to create table
-    assertNoPrivileges(ret);
-
-    allowCreateInDb(dbName, userName, dbLocn);
-
-    driver.run("use " + dbName);
-    ret = driver.run(
-        String.format("create table %s (a string) partitioned by (b string)", tblName));
-
-    assertEquals(0,ret.getResponseCode()); // now it succeeds.
-    Table tbl = msc.getTable(dbName, tblName);
-
-    validateCreateTable(tbl,tblName, dbName);
-
-    String fakeUser = "mal";
-    List<String> fakeGroupNames = new ArrayList<String>();
-    fakeGroupNames.add("groupygroup");
-
-    InjectableDummyAuthenticator.injectUserName(fakeUser);
-    InjectableDummyAuthenticator.injectGroupNames(fakeGroupNames);
-    InjectableDummyAuthenticator.injectMode(true);
-
-    ret = driver.run(
-        String.format("create table %s (a string) partitioned by (b string)", tblName+"mal"));
-
-    assertNoPrivileges(ret);
-
-    disallowCreateInTbl(tbl.getTableName(), userName, tbl.getSd().getLocation());
-    ret = driver.run("alter table "+tblName+" add partition (b='2011')");
-    assertNoPrivileges(ret);
-
-    InjectableDummyAuthenticator.injectMode(false);
-    allowCreateInTbl(tbl.getTableName(), userName, tbl.getSd().getLocation());
-
-    ret = driver.run("alter table "+tblName+" add partition (b='2011')");
-    assertEquals(0,ret.getResponseCode());
-
-    allowDropOnTable(tblName, userName, tbl.getSd().getLocation());
-    allowDropOnDb(dbName,userName,db.getLocationUri());
-    driver.run("drop database if exists "+getTestDbName()+" cascade");
-
-  }
-
-  protected void allowCreateInTbl(String tableName, String userName, String location)
-      throws Exception{
-    driver.run("grant create on table "+tableName+" to user "+userName);
-  }
-
-  protected void disallowCreateInTbl(String tableName, String userName, String location)
-      throws Exception {
-    // nothing needed here by default
-  }
-
-
-  protected void allowCreateInDb(String dbName, String userName, String location)
-      throws Exception {
-    driver.run("grant create on database "+dbName+" to user "+userName);
-  }
-
-  protected void disallowCreateInDb(String dbName, String userName, String location)
-      throws Exception {
-    // nothing needed here by default
-  }
-
-  protected void allowDropOnTable(String tblName, String userName, String location)
-      throws Exception {
-    driver.run("grant drop on table "+tblName+" to user "+userName);
-  }
-
-  protected void allowDropOnDb(String dbName, String userName, String location)
-      throws Exception {
-    driver.run("grant drop on database "+dbName+" to user "+userName);
-  }
-
-  protected void assertNoPrivileges(CommandProcessorResponse ret){
-    assertNotNull(ret);
-    assertFalse(0 == ret.getResponseCode());
-    assertTrue(ret.getErrorMessage().indexOf("No privilege") != -1);
-  }
-
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestMetastoreAuthorizationProvider.java b/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestMetastoreAuthorizationProvider.java
deleted file mode 100644
index f19e3df..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestMetastoreAuthorizationProvider.java
+++ /dev/null
@@ -1,287 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.security;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.metastore.api.Partition;
-import org.apache.hadoop.hive.metastore.api.SerDeInfo;
-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
-import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
-import org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener;
-import org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.security.UserGroupInformation;
-
-/**
- * TestHiveMetastoreAuthorizationProvider. Test case for
- * HiveMetastoreAuthorizationProvider, and by default,
- * for DefaultHiveMetaStoreAuthorizationProvider
- * using {@link org.apache.hadoop.hive.metastore.AuthorizationPreEventListener}
- * and {@link org.apache.hadoop.hive.}
- *
- * Note that while we do use the hive driver to test, that is mostly for test
- * writing ease, and it has the same effect as using a metastore client directly
- * because we disable hive client-side authorization for this test, and only
- * turn on server-side auth.
- *
- * This test is also intended to be extended to provide tests for other
- * authorization providers like StorageBasedAuthorizationProvider
- */
-public class TestMetastoreAuthorizationProvider extends TestCase {
-  protected HiveConf clientHiveConf;
-  protected HiveMetaStoreClient msc;
-  protected Driver driver;
-  protected UserGroupInformation ugi;
-
-
-  protected String getAuthorizationProvider(){
-    return DefaultHiveMetastoreAuthorizationProvider.class.getName();
-  }
-
-
-  @Override
-  protected void setUp() throws Exception {
-
-    super.setUp();
-
-    int port = MetaStoreUtils.findFreePort();
-
-    // Turn on metastore-side authorization
-    System.setProperty(HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS.varname,
-        AuthorizationPreEventListener.class.getName());
-    System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHORIZATION_MANAGER.varname,
-        getAuthorizationProvider());
-    System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHENTICATOR_MANAGER.varname,
-        InjectableDummyAuthenticator.class.getName());
-    System.setProperty(HiveConf.ConfVars.HIVE_AUTHORIZATION_TABLE_OWNER_GRANTS.varname, "");
-
-
-    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
-
-    clientHiveConf = new HiveConf(this.getClass());
-
-    // Turn off client-side authorization
-    clientHiveConf.setBoolVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED,false);
-
-    clientHiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
-    clientHiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
-    clientHiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-
-    clientHiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
-    clientHiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
-
-    ugi = ShimLoader.getHadoopShims().getUGIForConf(clientHiveConf);
-
-    SessionState.start(new CliSessionState(clientHiveConf));
-    msc = new HiveMetaStoreClient(clientHiveConf, null);
-    driver = new Driver(clientHiveConf);
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-  private void validateCreateDb(Database expectedDb, String dbName) {
-    assertEquals(expectedDb.getName().toLowerCase(), dbName.toLowerCase());
-  }
-
-  private void validateCreateTable(Table expectedTable, String tblName, String dbName) {
-    assertNotNull(expectedTable);
-    assertEquals(expectedTable.getTableName().toLowerCase(),tblName.toLowerCase());
-    assertEquals(expectedTable.getDbName().toLowerCase(),dbName.toLowerCase());
-  }
-
-  protected String getTestDbName(){
-    return "smp_ms_db";
-  }
-
-  protected String getTestTableName(){
-    return "smp_ms_tbl";
-  }
-
-  public void testSimplePrivileges() throws Exception {
-    String dbName = getTestDbName();
-    String tblName = getTestTableName();
-    String userName = ugi.getUserName();
-
-    CommandProcessorResponse ret = driver.run("create database " + dbName);
-    assertEquals(0,ret.getResponseCode());
-    Database db = msc.getDatabase(dbName);
-    String dbLocn = db.getLocationUri();
-
-    validateCreateDb(db,dbName);
-    disallowCreateInDb(dbName, userName, dbLocn);
-
-    driver.run("use " + dbName);
-    ret = driver.run(
-        String.format("create table %s (a string) partitioned by (b string)", tblName));
-
-    assertEquals(1,ret.getResponseCode());
-    // failure from not having permissions to create table
-
-    ArrayList<FieldSchema> fields = new ArrayList<FieldSchema>(2);
-    fields.add(new FieldSchema("a", serdeConstants.STRING_TYPE_NAME, ""));
-
-    Table ttbl = new Table();
-    ttbl.setDbName(dbName);
-    ttbl.setTableName(tblName);
-    StorageDescriptor sd = new StorageDescriptor();
-    ttbl.setSd(sd);
-    sd.setCols(fields);
-    sd.setParameters(new HashMap<String, String>());
-    sd.getParameters().put("test_param_1", "Use this for comments etc");
-    sd.setSerdeInfo(new SerDeInfo());
-    sd.getSerdeInfo().setName(ttbl.getTableName());
-    sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-    sd.getSerdeInfo().getParameters().put(
-        org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT, "1");
-    sd.getSerdeInfo().setSerializationLib(
-        org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
-    ttbl.setPartitionKeys(new ArrayList<FieldSchema>());
-
-    MetaException me = null;
-    try {
-      msc.createTable(ttbl);
-    } catch (MetaException e){
-      me = e;
-    }
-    assertNoPrivileges(me);
-
-    allowCreateInDb(dbName, userName, dbLocn);
-
-    driver.run("use " + dbName);
-    ret = driver.run(
-        String.format("create table %s (a string) partitioned by (b string)", tblName));
-
-    assertEquals(0,ret.getResponseCode()); // now it succeeds.
-    Table tbl = msc.getTable(dbName, tblName);
-
-    validateCreateTable(tbl,tblName, dbName);
-
-    String fakeUser = "mal";
-    List<String> fakeGroupNames = new ArrayList<String>();
-    fakeGroupNames.add("groupygroup");
-
-    InjectableDummyAuthenticator.injectUserName(fakeUser);
-    InjectableDummyAuthenticator.injectGroupNames(fakeGroupNames);
-    InjectableDummyAuthenticator.injectMode(true);
-
-    ret = driver.run(
-        String.format("create table %s (a string) partitioned by (b string)", tblName+"mal"));
-
-    assertEquals(1,ret.getResponseCode());
-
-    ttbl.setTableName(tblName+"mal");
-    me = null;
-    try {
-      msc.createTable(ttbl);
-    } catch (MetaException e){
-      me = e;
-    }
-    assertNoPrivileges(me);
-
-    disallowCreateInTbl(tbl.getTableName(), userName, tbl.getSd().getLocation());
-    ret = driver.run("alter table "+tblName+" add partition (b='2011')");
-    assertEquals(1,ret.getResponseCode());
-
-    List<String> ptnVals = new ArrayList<String>();
-    ptnVals.add("b=2011");
-    Partition tpart = new Partition();
-    tpart.setDbName(dbName);
-    tpart.setTableName(tblName);
-    tpart.setValues(ptnVals);
-    tpart.setParameters(new HashMap<String, String>());
-    tpart.setSd(tbl.getSd().deepCopy());
-    tpart.getSd().setSerdeInfo(tbl.getSd().getSerdeInfo().deepCopy());
-    tpart.getSd().setLocation(tbl.getSd().getLocation() + "/tpart");
-
-    me = null;
-    try {
-      msc.add_partition(tpart);
-    } catch (MetaException e){
-      me = e;
-    }
-    assertNoPrivileges(me);
-
-    InjectableDummyAuthenticator.injectMode(false);
-    allowCreateInTbl(tbl.getTableName(), userName, tbl.getSd().getLocation());
-
-    ret = driver.run("alter table "+tblName+" add partition (b='2011')");
-    assertEquals(0,ret.getResponseCode());
-
-    allowDropOnTable(tblName, userName, tbl.getSd().getLocation());
-    allowDropOnDb(dbName,userName,db.getLocationUri());
-    driver.run("drop database if exists "+getTestDbName()+" cascade");
-
-  }
-
-  protected void allowCreateInTbl(String tableName, String userName, String location)
-      throws Exception{
-    driver.run("grant create on table "+tableName+" to user "+userName);
-  }
-
-  protected void disallowCreateInTbl(String tableName, String userName, String location)
-      throws Exception {
-    driver.run("revoke create on table "+tableName+" from user "+userName);
-  }
-
-
-  protected void allowCreateInDb(String dbName, String userName, String location)
-      throws Exception {
-    driver.run("grant create on database "+dbName+" to user "+userName);
-  }
-
-  protected void disallowCreateInDb(String dbName, String userName, String location)
-      throws Exception {
-    driver.run("revoke create on database "+dbName+" from user "+userName);
-  }
-
-  protected void allowDropOnTable(String tblName, String userName, String location)
-      throws Exception {
-    driver.run("grant drop on table "+tblName+" to user "+userName);
-  }
-
-  protected void allowDropOnDb(String dbName, String userName, String location)
-      throws Exception {
-    driver.run("grant drop on database "+dbName+" to user "+userName);
-  }
-
-  protected void assertNoPrivileges(MetaException me){
-    assertNotNull(me);
-    assertTrue(me.getMessage().indexOf("No privilege") != -1);
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestStorageBasedClientSideAuthorizationProvider.java b/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestStorageBasedClientSideAuthorizationProvider.java
deleted file mode 100644
index 61dc211..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestStorageBasedClientSideAuthorizationProvider.java
+++ /dev/null
@@ -1,102 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.security;
-
-import java.net.URI;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
-import org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
-
-/**
- * TestStorageBasedClientSideAuthorizationProvider : Overrides
- * TestClientSideAuthorizationProvider to test StorageBasedAuthorizationProvider
- * on the client side.
- */
-
-public class TestStorageBasedClientSideAuthorizationProvider extends
-    TestClientSideAuthorizationProvider {
-
-  @Override
-  protected String getAuthorizationProvider(){
-    return StorageBasedAuthorizationProvider.class.getName();
-  }
-
-  @Override
-  protected void allowCreateInDb(String dbName, String userName, String location)
-      throws Exception {
-    setPermissions(location,"-rwxr--r--");
-  }
-
-  @Override
-  protected void disallowCreateInDb(String dbName, String userName, String location)
-      throws Exception {
-    setPermissions(location,"-r--r--r--");
-  }
-
-  @Override
-  protected void allowCreateInTbl(String tableName, String userName, String location)
-      throws Exception{
-    setPermissions(location,"-rwxr--r--");
-  }
-
-
-  @Override
-  protected void disallowCreateInTbl(String tableName, String userName, String location)
-      throws Exception {
-    setPermissions(location,"-r--r--r--");
-  }
-
-  @Override
-  protected void allowDropOnTable(String tblName, String userName, String location)
-      throws Exception {
-    setPermissions(location,"-rwxr--r--");
-  }
-
-  @Override
-  protected void allowDropOnDb(String dbName, String userName, String location)
-      throws Exception {
-    setPermissions(location,"-rwxr--r--");
-  }
-
-  private void setPermissions(String locn, String permissions) throws Exception {
-    FileSystem fs = FileSystem.get(new URI(locn), clientHiveConf);
-    fs.setPermission(new Path(locn), FsPermission.valueOf(permissions));
-  }
-
-  @Override
-  protected void assertNoPrivileges(CommandProcessorResponse ret){
-    assertNotNull(ret);
-    assertFalse(0 == ret.getResponseCode());
-    assertTrue(ret.getErrorMessage().indexOf("not permitted") != -1);
-  }
-
-
-  @Override
-  protected String getTestDbName(){
-    return super.getTestDbName() + "_SBAP";
-  }
-
-  @Override
-  protected String getTestTableName(){
-    return super.getTestTableName() + "_SBAP";
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationProvider.java b/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationProvider.java
deleted file mode 100644
index 223f155..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationProvider.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.security;
-
-import java.net.URI;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
-
-/**
- * TestStorageBasedMetastoreAuthorizationProvider. Test case for
- * StorageBasedAuthorizationProvider, by overriding methods defined in
- * TestMetastoreAuthorizationProvider
- *
- * Note that while we do use the hive driver to test, that is mostly for test
- * writing ease, and it has the same effect as using a metastore client directly
- * because we disable hive client-side authorization for this test, and only
- * turn on server-side auth.
- */
-public class TestStorageBasedMetastoreAuthorizationProvider extends
-    TestMetastoreAuthorizationProvider {
-
-  @Override
-  protected String getAuthorizationProvider(){
-    return StorageBasedAuthorizationProvider.class.getName();
-  }
-
-  @Override
-  protected void allowCreateInDb(String dbName, String userName, String location)
-      throws Exception {
-    setPermissions(location,"-rwxr--r--");
-  }
-
-  @Override
-  protected void disallowCreateInDb(String dbName, String userName, String location)
-      throws Exception {
-    setPermissions(location,"-r--r--r--");
-  }
-
-  @Override
-  protected void allowCreateInTbl(String tableName, String userName, String location)
-      throws Exception{
-    setPermissions(location,"-rwxr--r--");
-  }
-
-
-  @Override
-  protected void disallowCreateInTbl(String tableName, String userName, String location)
-      throws Exception {
-    setPermissions(location,"-r--r--r--");
-  }
-
-  @Override
-  protected void allowDropOnTable(String tblName, String userName, String location)
-      throws Exception {
-    setPermissions(location,"-rwxr--r--");
-  }
-
-  @Override
-  protected void allowDropOnDb(String dbName, String userName, String location)
-      throws Exception {
-    setPermissions(location,"-rwxr--r--");
-  }
-
-  private void setPermissions(String locn, String permissions) throws Exception {
-    FileSystem fs = FileSystem.get(new URI(locn), clientHiveConf);
-    fs.setPermission(new Path(locn), FsPermission.valueOf(permissions));
-  }
-
-  @Override
-  protected void assertNoPrivileges(MetaException me){
-    assertNotNull(me);
-    assertTrue(me.getMessage().indexOf("not permitted") != -1);
-  }
-
-  @Override
-  protected String getTestDbName(){
-    return super.getTestDbName() + "_SBAP";
-  }
-
-  @Override
-  protected String getTestTableName(){
-    return super.getTestTableName() + "_SBAP";
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java b/src/ql/src/test/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java
deleted file mode 100644
index 969e5b9..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.stats;
-
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * An test implementation for StatsAggregator.
- * The method corresponding to the configuration parameter
- * hive.test.dummystats.aggregator fail, whereas all
- * other methods succeed.
- */
-
-public class DummyStatsAggregator implements StatsAggregator {
-  String errorMethod = null;
-
-  // This is a test. The parameter hive.test.dummystats.aggregator's value
-  // denotes the method which needs to throw an error.
-  public boolean connect(Configuration hconf) {
-    errorMethod = hconf.get("hive.test.dummystats.aggregator", "");
-    if (errorMethod.equalsIgnoreCase("connect")) {
-      return false;
-    }
-
-    return true;
-  }
-
-  public String aggregateStats(String keyPrefix, String statType) {
-    return null;
-  }
-
-  public boolean closeConnection() {
-    if (errorMethod.equalsIgnoreCase("closeConnection")) {
-      return false;
-    }
-    return true;
-  }
-
-  public boolean cleanUp(String keyPrefix) {
-    if (errorMethod.equalsIgnoreCase("cleanUp")) {
-      return false;
-    }
-    return true;
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java b/src/ql/src/test/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java
deleted file mode 100644
index 4dd632d..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.stats;
-
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * An test implementation for StatsPublisher.
- * The method corresponding to the configuration parameter
- * hive.test.dummystats.publisher fail, whereas all
- * other methods succeed
- */
-
-public class DummyStatsPublisher implements StatsPublisher {
-
-  String errorMethod = null;
-
-  // This is a test. The parameter hive.test.dummystats.publisher's value
-  // denotes the method which needs to throw an error.
-  public boolean init(Configuration hconf) {
-    errorMethod = hconf.get("hive.test.dummystats.publisher", "");
-    if (errorMethod.equalsIgnoreCase("init")) {
-      return false;
-    }
-
-    return true;
-  }
-
-  public boolean connect(Configuration hconf) {
-    errorMethod = hconf.get("hive.test.dummystats.publisher", "");
-    if (errorMethod.equalsIgnoreCase("connect")) {
-      return false;
-    }
-
-    return true;
-  }
-
-  public boolean publishStat(String fileID, Map<String, String> stats) {
-    if (errorMethod.equalsIgnoreCase("publishStat")) {
-      return false;
-    }
-    return true;
-  }
-
-  public boolean closeConnection() {
-    if (errorMethod.equalsIgnoreCase("closeConnection")) {
-      return false;
-    }
-    return true;
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java b/src/ql/src/test/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java
deleted file mode 100644
index fafd68b..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.stats;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.ql.session.SessionState;
-
-/**
- * An test implementation for StatsAggregator.
- * aggregateStats prints the length of the keyPrefix to SessionState's out stream
- * All other methods are no-ops.
- */
-
-public class KeyVerifyingStatsAggregator implements StatsAggregator {
-
-  public boolean connect(Configuration hconf) {
-    return true;
-  }
-
-  public String aggregateStats(String keyPrefix, String statType) {
-    SessionState ss = SessionState.get();
-    // Have to use the length instead of the actual prefix because the prefix is location dependent
-    // 17 is 16 (16 byte MD5 hash) + 1 for the path separator
-    // Can be less than 17 due to unicode characters
-    ss.out.println("Stats prefix is hashed: " + new Boolean(keyPrefix.length() <= 17));
-    return null;
-  }
-
-  public boolean closeConnection() {
-    return true;
-  }
-
-  public boolean cleanUp(String keyPrefix) {
-    return true;
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDAFTestMax.java b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDAFTestMax.java
deleted file mode 100644
index eda2aa4..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDAFTestMax.java
+++ /dev/null
@@ -1,295 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf;
-
-import org.apache.hadoop.hive.ql.exec.UDAF;
-import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-
-/**
- * UDAFTestMax.
- *
- */
-public class UDAFTestMax extends UDAF {
-
-  /**
-   * MaxShortEvaluator.
-   *
-   */
-  public static class MaxShortEvaluator implements UDAFEvaluator {
-    private short mMax;
-    private boolean mEmpty;
-
-    public MaxShortEvaluator() {
-      super();
-      init();
-    }
-
-    public void init() {
-      mMax = 0;
-      mEmpty = true;
-    }
-
-    public boolean iterate(ShortWritable o) {
-      if (o != null) {
-        if (mEmpty) {
-          mMax = o.get();
-          mEmpty = false;
-        } else {
-          mMax = (short) Math.max(mMax, o.get());
-        }
-      }
-      return true;
-    }
-
-    public ShortWritable terminatePartial() {
-      return mEmpty ? null : new ShortWritable(mMax);
-    }
-
-    public boolean merge(ShortWritable o) {
-      return iterate(o);
-    }
-
-    public ShortWritable terminate() {
-      return mEmpty ? null : new ShortWritable(mMax);
-    }
-  }
-
-  /**
-   * MaxIntEvaluator.
-   *
-   */
-  public static class MaxIntEvaluator implements UDAFEvaluator {
-    private int mMax;
-    private boolean mEmpty;
-
-    public MaxIntEvaluator() {
-      super();
-      init();
-    }
-
-    public void init() {
-      mMax = 0;
-      mEmpty = true;
-    }
-
-    public boolean iterate(IntWritable o) {
-      if (o != null) {
-        if (mEmpty) {
-          mMax = o.get();
-          mEmpty = false;
-        } else {
-          mMax = Math.max(mMax, o.get());
-        }
-      }
-      return true;
-    }
-
-    public IntWritable terminatePartial() {
-      return mEmpty ? null : new IntWritable(mMax);
-    }
-
-    public boolean merge(IntWritable o) {
-      return iterate(o);
-    }
-
-    public IntWritable terminate() {
-      return mEmpty ? null : new IntWritable(mMax);
-    }
-  }
-
-  /**
-   * MaxLongEvaluator.
-   *
-   */
-  public static class MaxLongEvaluator implements UDAFEvaluator {
-    private long mMax;
-    private boolean mEmpty;
-
-    public MaxLongEvaluator() {
-      super();
-      init();
-    }
-
-    public void init() {
-      mMax = 0;
-      mEmpty = true;
-    }
-
-    public boolean iterate(LongWritable o) {
-      if (o != null) {
-        if (mEmpty) {
-          mMax = o.get();
-          mEmpty = false;
-        } else {
-          mMax = Math.max(mMax, o.get());
-        }
-      }
-      return true;
-    }
-
-    public LongWritable terminatePartial() {
-      return mEmpty ? null : new LongWritable(mMax);
-    }
-
-    public boolean merge(LongWritable o) {
-      return iterate(o);
-    }
-
-    public LongWritable terminate() {
-      return mEmpty ? null : new LongWritable(mMax);
-    }
-  }
-
-  /**
-   * MaxFloatEvaluator.
-   *
-   */
-  public static class MaxFloatEvaluator implements UDAFEvaluator {
-    private float mMax;
-    private boolean mEmpty;
-
-    public MaxFloatEvaluator() {
-      super();
-      init();
-    }
-
-    public void init() {
-      mMax = 0;
-      mEmpty = true;
-    }
-
-    public boolean iterate(FloatWritable o) {
-      if (o != null) {
-        if (mEmpty) {
-          mMax = o.get();
-          mEmpty = false;
-        } else {
-          mMax = Math.max(mMax, o.get());
-        }
-      }
-      return true;
-    }
-
-    public FloatWritable terminatePartial() {
-      return mEmpty ? null : new FloatWritable(mMax);
-    }
-
-    public boolean merge(FloatWritable o) {
-      return iterate(o);
-    }
-
-    public FloatWritable terminate() {
-      return mEmpty ? null : new FloatWritable(mMax);
-    }
-  }
-
-  /**
-   * MaxDoubleEvaluator.
-   *
-   */
-  public static class MaxDoubleEvaluator implements UDAFEvaluator {
-    private double mMax;
-    private boolean mEmpty;
-
-    public MaxDoubleEvaluator() {
-      super();
-      init();
-    }
-
-    public void init() {
-      mMax = 0;
-      mEmpty = true;
-    }
-
-    public boolean iterate(DoubleWritable o) {
-      if (o != null) {
-        if (mEmpty) {
-          mMax = o.get();
-          mEmpty = false;
-        } else {
-          mMax = Math.max(mMax, o.get());
-        }
-      }
-      return true;
-    }
-
-    public DoubleWritable terminatePartial() {
-      return mEmpty ? null : new DoubleWritable(mMax);
-    }
-
-    public boolean merge(DoubleWritable o) {
-      return iterate(o);
-    }
-
-    public DoubleWritable terminate() {
-      return mEmpty ? null : new DoubleWritable(mMax);
-    }
-  }
-
-  /**
-   * MaxStringEvaluator.
-   *
-   */
-  public static class MaxStringEvaluator implements UDAFEvaluator {
-    private Text mMax;
-    private boolean mEmpty;
-
-    public MaxStringEvaluator() {
-      super();
-      init();
-    }
-
-    public void init() {
-      mMax = null;
-      mEmpty = true;
-    }
-
-    public boolean iterate(Text o) {
-      if (o != null) {
-        if (mEmpty) {
-          mMax = new Text(o);
-          mEmpty = false;
-        } else if (ShimLoader.getHadoopShims().compareText(mMax, o) < 0) {
-          mMax.set(o);
-        }
-      }
-      return true;
-    }
-
-    public Text terminatePartial() {
-      return mEmpty ? null : mMax;
-    }
-
-    public boolean merge(Text o) {
-      return iterate(o);
-    }
-
-    public Text terminate() {
-      return mEmpty ? null : mMax;
-    }
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestErrorOnFalse.java b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestErrorOnFalse.java
deleted file mode 100644
index 66a30ab..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestErrorOnFalse.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf;
-
-import org.apache.hadoop.hive.ql.exec.UDF;
-
-/**
- * A UDF for testing, which throws RuntimeException if  the length of a string.
- */
-public class UDFTestErrorOnFalse extends UDF {
-
-  public int evaluate(Boolean b) {
-    if (b) {
-      return 1;
-    } else {
-      throw new RuntimeException("UDFTestErrorOnFalse got b=false");
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestLength.java b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestLength.java
deleted file mode 100644
index 9e75c51..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestLength.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf;
-
-import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Text;
-
-/**
- * A UDF for testing, which evaluates the length of a string.
- */
-public class UDFTestLength extends UDF {
-
-  IntWritable result = new IntWritable();
-
-  public IntWritable evaluate(Text s) {
-    if (s == null) {
-      return null;
-    }
-    result.set(s.toString().length());
-    return result;
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestLength2.java b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestLength2.java
deleted file mode 100644
index b1aab45..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestLength2.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf;
-
-import org.apache.hadoop.hive.ql.exec.UDF;
-
-/**
- * A UDF for testing, which evaluates the length of a string. This UDF uses Java
- * Primitive classes for parameters.
- */
-public class UDFTestLength2 extends UDF {
-
-  public Integer evaluate(String s) {
-    if (s == null) {
-      return null;
-    }
-    return Integer.valueOf(s.length());
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/DummyContextUDF.java b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/DummyContextUDF.java
deleted file mode 100644
index d3b525e..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/DummyContextUDF.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf.generic;
-
-import org.apache.hadoop.hive.ql.exec.MapredContext;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.mapred.Counters;
-import org.apache.hadoop.mapred.Reporter;
-
-public class DummyContextUDF extends GenericUDF {
-
-  private MapredContext context;
-  private LongWritable result = new LongWritable();
-
-  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    return PrimitiveObjectInspectorFactory.writableLongObjectInspector;
-  }
-
-  public Object evaluate(DeferredObject[] arguments) throws HiveException {
-    Reporter reporter = context.getReporter();
-    Counters.Counter counter = reporter.getCounter("org.apache.hadoop.mapred.Task$Counter", "MAP_INPUT_RECORDS");
-    result.set(counter.getValue());
-    return result;
-  }
-
-  public String getDisplayString(String[] children) {
-    return "dummy-func()";
-  }
-
-  @Override
-    public void configure(MapredContext context) {
-    this.context = context;
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSumList.java b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSumList.java
deleted file mode 100644
index 55d7912..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSumList.java
+++ /dev/null
@@ -1,160 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.udf.generic;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.ql.util.JavaDataModel;
-import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
-import org.apache.hadoop.io.LongWritable;
-
-/**
- * GenericUDAFSum.
- *
- */
-@Description(name = "sum_list", value = "_FUNC_(x) - Returns the sum of a set of numbers")
-public class GenericUDAFSumList extends AbstractGenericUDAFResolver {
-
-  static final Log LOG = LogFactory.getLog(GenericUDAFSumList.class.getName());
-
-  @Override
-  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info)
-    throws SemanticException {
-    ObjectInspector[] inspectors = info.getParameterObjectInspectors();
-    if (inspectors.length != 1) {
-      throw new UDFArgumentTypeException(inspectors.length - 1,
-          "Exactly one argument is expected.");
-    }
-
-    if (inspectors[0].getCategory() != ObjectInspector.Category.LIST) {
-      throw new UDFArgumentTypeException(0, "Argument should be a list type");
-    }
-
-    ListObjectInspector listOI = (ListObjectInspector) inspectors[0];
-    ObjectInspector elementOI = listOI.getListElementObjectInspector();
-
-    if (elementOI.getCategory() != ObjectInspector.Category.PRIMITIVE) {
-      throw new UDFArgumentTypeException(0,
-          "Only primitive type arguments are accepted but "
-          + elementOI.getTypeName() + " is passed.");
-    }
-    PrimitiveObjectInspector.PrimitiveCategory pcat =
-        ((PrimitiveObjectInspector)elementOI).getPrimitiveCategory();
-    return new GenericUDAFSumLong();
-  }
-
-  /**
-   * GenericUDAFSumLong.
-   *
-   */
-  public static class GenericUDAFSumLong extends GenericUDAFEvaluator {
-    private ListObjectInspector listOI;
-    private PrimitiveObjectInspector elementOI;
-    private ObjectInspectorConverters.Converter toLong;
-    private PrimitiveObjectInspector inputOI;
-    private LongWritable result;
-
-    @Override
-    public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException {
-      assert (parameters.length == 1);
-      super.init(m, parameters);
-      result = new LongWritable(0);
-      if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) {
-        listOI = (ListObjectInspector) parameters[0];
-        elementOI = (PrimitiveObjectInspector) listOI.getListElementObjectInspector();
-        toLong = ObjectInspectorConverters.getConverter(elementOI,
-            PrimitiveObjectInspectorFactory.javaLongObjectInspector);
-      } else {
-        inputOI = (PrimitiveObjectInspector) parameters[0];
-      }
-      return PrimitiveObjectInspectorFactory.writableLongObjectInspector;
-    }
-
-    /** class for storing double sum value. */
-    @AggregationType(estimable = true)
-    static class SumLongAgg extends AbstractAggregationBuffer {
-      boolean empty;
-      long sum;
-      @Override
-      public int estimate() { return JavaDataModel.PRIMITIVES1 + JavaDataModel.PRIMITIVES2; }
-    }
-
-    @Override
-    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
-      SumLongAgg result = new SumLongAgg();
-      reset(result);
-      return result;
-    }
-
-    @Override
-    public void reset(AggregationBuffer agg) throws HiveException {
-      SumLongAgg myagg = (SumLongAgg) agg;
-      myagg.empty = true;
-      myagg.sum = 0;
-    }
-
-    @Override
-    public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {
-      assert (parameters.length == 1);
-      SumLongAgg myagg = (SumLongAgg) agg;
-      int length = listOI.getListLength(parameters[0]);
-      for (int i = 0; i < length; i++) {
-        Object element = listOI.getListElement(parameters[0], i);
-        if (element != null) {
-          myagg.sum += (Long)toLong.convert(element);
-          myagg.empty = false;
-        }
-      }
-    }
-
-    @Override
-    public Object terminatePartial(AggregationBuffer agg) throws HiveException {
-      return terminate(agg);
-    }
-
-    @Override
-    public void merge(AggregationBuffer agg, Object partial) throws HiveException {
-      if (partial != null) {
-        SumLongAgg myagg = (SumLongAgg) agg;
-        myagg.sum += PrimitiveObjectInspectorUtils.getLong(partial, inputOI);
-        myagg.empty = false;
-      }
-    }
-
-    @Override
-    public Object terminate(AggregationBuffer agg) throws HiveException {
-      SumLongAgg myagg = (SumLongAgg) agg;
-      if (myagg.empty) {
-        return null;
-      }
-      result.set(myagg.sum);
-      return result;
-    }
-
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFEvaluateNPE.java b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFEvaluateNPE.java
deleted file mode 100644
index 4080c9f..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFEvaluateNPE.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf.generic;
-
-import java.lang.NullPointerException;
-
-import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Text;
-
-/**
- * GenericUDFEvaluateNPE
- * This UDF is to throw an Null Pointer Exception
- * It is used to test hive failure handling
- *
- */
-@Description(name = "evaluate_npe", value = "_FUNC_(string) - "
-  + "Throws an NPE in the GenericUDF.evaluate() method. "
-  + "Used for testing GenericUDF error handling.")
-public class GenericUDFEvaluateNPE extends GenericUDF {
-  private ObjectInspector[] argumentOIs;
-  private final Text result= new Text();
-
-  @Override
-  public ObjectInspector initialize(ObjectInspector[] arguments)
-      throws UDFArgumentException {
-    if (arguments.length != 1) {
-      throw new UDFArgumentLengthException(
-          "The function evaluate_npe(string)"
-            + "needs only one argument.");
-    }
-
-    if (!arguments[0].getTypeName().equals(serdeConstants.STRING_TYPE_NAME)) {
-      throw new UDFArgumentTypeException(0,
-        "Argument 1 of function evaluate_npe must be \""
-        + serdeConstants.STRING_TYPE_NAME + "but \""
-        + arguments[0].getTypeName() + "\" was found.");
-    }
-
-    argumentOIs = arguments;
-    return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
-  }
-
-  @Override
-  public Object evaluate(DeferredObject[] arguments) throws HiveException {
-    if (true) {
-        throw new NullPointerException("evaluate null pointer exception");
-    }
-    return result;
-  }
-
-  @Override
-  public String getDisplayString(String[] children) {
-    assert (children.length == 1);
-    return "evaluate_npe(" + children[0] + ")";
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java
deleted file mode 100644
index 4ec7431..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf.generic;
-
-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
-
-/**
- * A test GenericUDF to return native Java's boolean type
- */
-public class GenericUDFTestGetJavaBoolean extends GenericUDF {
-  ObjectInspector[] argumentOIs;
-
-  @Override
-  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    argumentOIs = arguments;
-    return PrimitiveObjectInspectorFactory.javaBooleanObjectInspector;
-  }
-
-  @Override
-  public Object evaluate(DeferredObject[] arguments) throws HiveException {
-    String input = ((StringObjectInspector) argumentOIs[0]).getPrimitiveJavaObject(arguments[0].get());
-    if (input.equalsIgnoreCase("true")) {
-      return Boolean.TRUE;
-    } else if (input.equalsIgnoreCase("false")) {
-      return false;
-    } else {
-      return null;
-    }
-  }
-
-  @Override
-  public String getDisplayString(String[] children) {
-    assert (children.length == 1);
-    return "TestGetJavaBoolean(" + children[0] + ")";
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java
deleted file mode 100644
index ead45ae..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf.generic;
-
-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
-
-/**
- * A test GenericUDF to return native Java's string type
- */
-public class GenericUDFTestGetJavaString extends GenericUDF {
-  ObjectInspector[] argumentOIs;
-
-  @Override
-  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    argumentOIs = arguments;
-    return PrimitiveObjectInspectorFactory.javaStringObjectInspector;
-  }
-
-  @Override
-  public Object evaluate(DeferredObject[] arguments) throws HiveException {
-    if (arguments[0].get() == null) {
-      return null;
-    }
-    return ((StringObjectInspector) argumentOIs[0]).getPrimitiveJavaObject(arguments[0].get());
-  }
-
-  @Override
-  public String getDisplayString(String[] children) {
-    assert (children.length == 1);
-    return "GenericUDFTestGetJavaString(" + children[0] + ")";
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java
deleted file mode 100644
index dedf91d..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf.generic;
-
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
-import org.apache.hadoop.io.Text;
-
-/**
- * Mimics oracle's function translate(str1, str2, str3).
- */
-public class GenericUDFTestTranslate extends GenericUDF {
-  private transient ObjectInspector[] argumentOIs;
-
-  /**
-   * Return a corresponding ordinal from an integer.
-   */
-  static String getOrdinal(int i) {
-    int unit = i % 10;
-    return (i <= 0) ? "" : (i != 11 && unit == 1) ? i + "st"
-        : (i != 12 && unit == 2) ? i + "nd" : (i != 13 && unit == 3) ? i + "rd"
-        : i + "th";
-  }
-
-  @Override
-  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    if (arguments.length != 3) {
-      throw new UDFArgumentLengthException(
-          "The function TRANSLATE(expr,from_string,to_string) accepts exactly 3 arguments, but "
-          + arguments.length + " arguments is found.");
-    }
-
-    for (int i = 0; i < 3; i++) {
-      if (arguments[i].getTypeName() != serdeConstants.STRING_TYPE_NAME
-          && arguments[i].getTypeName() != serdeConstants.VOID_TYPE_NAME) {
-        throw new UDFArgumentTypeException(i, "The " + getOrdinal(i + 1)
-            + " argument of function TRANSLATE is expected to \""
-            + serdeConstants.STRING_TYPE_NAME + "\", but \""
-            + arguments[i].getTypeName() + "\" is found");
-      }
-    }
-
-    argumentOIs = arguments;
-    return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
-  }
-
-  private final Text resultText = new Text();
-
-  @Override
-  public Object evaluate(DeferredObject[] arguments) throws HiveException {
-    if (arguments[0].get() == null || arguments[1].get() == null
-        || arguments[2].get() == null) {
-      return null;
-    }
-    String exprString = ((StringObjectInspector) argumentOIs[0])
-        .getPrimitiveJavaObject(arguments[0].get());
-    String fromString = ((StringObjectInspector) argumentOIs[1])
-        .getPrimitiveJavaObject(arguments[1].get());
-    String toString = ((StringObjectInspector) argumentOIs[2])
-        .getPrimitiveJavaObject(arguments[2].get());
-
-    char[] expr = exprString.toCharArray();
-    char[] from = fromString.toCharArray();
-    char[] to = toString.toCharArray();
-    char[] result = new char[expr.length];
-    System.arraycopy(expr, 0, result, 0, expr.length);
-    Set<Character> seen = new HashSet<Character>();
-
-    for (int i = 0; i < from.length; i++) {
-      if (seen.contains(from[i])) {
-        continue;
-      }
-      seen.add(from[i]);
-      for (int j = 0; j < expr.length; j++) {
-        if (expr[j] == from[i]) {
-          result[j] = (i < to.length) ? to[i] : 0;
-        }
-      }
-    }
-
-    int pos = 0;
-    for (int i = 0; i < result.length; i++) {
-      if (result[i] != 0) {
-        result[pos++] = result[i];
-      }
-    }
-    resultText.set(new String(result, 0, pos));
-    return resultText;
-  }
-
-  @Override
-  public String getDisplayString(String[] children) {
-    assert (children.length == 3);
-    return "translate(" + children[0] + "," + children[1] + "," + children[2]
-        + ")";
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/scripts/extracturl.java b/src/ql/src/test/org/apache/hadoop/hive/scripts/extracturl.java
deleted file mode 100644
index 2d9b037..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/scripts/extracturl.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.scripts;
-
-import java.io.BufferedReader;
-import java.io.InputStreamReader;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-/**
- * extracturl.
- *
- */
-public final class extracturl {
-
-  protected static final Pattern pattern = Pattern.compile(
-      "<a href=\"http://([\\w\\d]+\\.html)\">link</a>",
-      Pattern.CASE_INSENSITIVE);
-  static InputStreamReader converter = new InputStreamReader(System.in);
-  static BufferedReader in = new BufferedReader(converter);
-
-  public static void main(String[] args) {
-    String input;
-    try {
-      while ((input = in.readLine()) != null) {
-        Matcher m = pattern.matcher(input);
-
-        while (m.find()) {
-          String url = input.substring(m.start(1), m.end(1));
-          System.out.println(url + "\t" + "1");
-        }
-      }
-    } catch (Exception e) {
-      e.printStackTrace();
-      System.exit(1);
-    }
-  }
-
-  private extracturl() {
-    // prevent instantiation
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableListObjectInspector1.java b/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableListObjectInspector1.java
deleted file mode 100644
index 245e4f5..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableListObjectInspector1.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.serde2;
-
-import java.util.List;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-
-public class CustomNonSettableListObjectInspector1  implements ListObjectInspector {
-
-  private ObjectInspector listElementObjectInspector;
-
-  protected CustomNonSettableListObjectInspector1() {
-    super();
-  }
-  protected CustomNonSettableListObjectInspector1(
-      ObjectInspector listElementObjectInspector) {
-    this.listElementObjectInspector = listElementObjectInspector;
-  }
-
-  public final Category getCategory() {
-    return Category.LIST;
-  }
-
-  // without data
-  public ObjectInspector getListElementObjectInspector() {
-    return listElementObjectInspector;
-  }
-
-  // Not supported for the test case
-  public Object getListElement(Object data, int index) {
-    return null;
-  }
-
-  // Not supported for the test case
-  public int getListLength(Object data) {
-    return 0;
-  }
-
-  // Not supported for the test case
-  public List<?> getList(Object data) {
-    return null;
-  }
-
-  public String getTypeName() {
-    return org.apache.hadoop.hive.serde.serdeConstants.LIST_TYPE_NAME + "<"
-        + listElementObjectInspector.getTypeName() + ">";
-  }
-}
-
diff --git a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableStructObjectInspector1.java b/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableStructObjectInspector1.java
deleted file mode 100644
index c09fd61..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableStructObjectInspector1.java
+++ /dev/null
@@ -1,137 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.serde2;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
-import org.apache.hadoop.hive.serde2.objectinspector.StructField;
-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-
-public class CustomNonSettableStructObjectInspector1 extends
-StructObjectInspector {
-  public static final Log LOG = LogFactory
-      .getLog(CustomNonSettableStructObjectInspector1.class.getName());
-
-  protected static class MyField implements StructField {
-    protected int fieldID;
-    protected String fieldName;
-    protected ObjectInspector fieldObjectInspector;
-    protected String fieldComment;
-
-    protected MyField() {
-      super();
-    }
-
-    public MyField(int fieldID, String fieldName,
-        ObjectInspector fieldObjectInspector) {
-      this.fieldID = fieldID;
-      this.fieldName = fieldName.toLowerCase();
-      this.fieldObjectInspector = fieldObjectInspector;
-    }
-
-    public MyField(int fieldID, String fieldName,
-        ObjectInspector fieldObjectInspector, String fieldComment) {
-      this(fieldID, fieldName, fieldObjectInspector);
-      this.fieldComment = fieldComment;
-    }
-
-    public int getFieldID() {
-      return fieldID;
-    }
-
-    public String getFieldName() {
-      return fieldName;
-    }
-
-    public ObjectInspector getFieldObjectInspector() {
-      return fieldObjectInspector;
-    }
-
-    public String getFieldComment() {
-      return fieldComment;
-    }
-
-    @Override
-    public String toString() {
-      return "" + fieldID + ":" + fieldName;
-    }
-  }
-
-  protected List<MyField> fields;
-
-  protected CustomNonSettableStructObjectInspector1() {
-    super();
-  }
-  /**
-   * Call ObjectInspectorFactory.getNonSettableStructObjectInspector instead.
-   */
-  protected CustomNonSettableStructObjectInspector1(List<String> structFieldNames,
-      List<ObjectInspector> structFieldObjectInspectors) {
-    init(structFieldNames, structFieldObjectInspectors);
-  }
-
-  protected void init(List<String> structFieldNames,
-      List<ObjectInspector> structFieldObjectInspectors) {
-    assert (structFieldNames.size() == structFieldObjectInspectors.size());
-
-    fields = new ArrayList<MyField>(structFieldNames.size());
-    for (int i = 0; i < structFieldNames.size(); i++) {
-      fields.add(new MyField(i, structFieldNames.get(i),
-          structFieldObjectInspectors.get(i), null));
-    }
-  }
-
-  public String getTypeName() {
-    return ObjectInspectorUtils.getStandardStructTypeName(this);
-  }
-
-  public final Category getCategory() {
-    return Category.STRUCT;
-  }
-
-  // Without Data
-  @Override
-  public StructField getStructFieldRef(String fieldName) {
-    return ObjectInspectorUtils.getStandardStructFieldRef(fieldName, fields);
-  }
-
-  @Override
-  public List<? extends StructField> getAllStructFieldRefs() {
-    return fields;
-  }
-
-  // With Data - Unsupported for the test case
-  @Override
-  @SuppressWarnings("unchecked")
-  public Object getStructFieldData(Object data, StructField fieldRef) {
-    return null;
-  }
-
-  // Unsupported for the test case
-  @Override
-  @SuppressWarnings("unchecked")
-  public List<Object> getStructFieldsDataAsList(Object data) {
-    return null;
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableUnionObjectInspector1.java b/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableUnionObjectInspector1.java
deleted file mode 100644
index 8e9ce78..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomNonSettableUnionObjectInspector1.java
+++ /dev/null
@@ -1,82 +0,0 @@
-package org.apache.hadoop.hive.serde2;
-
-import java.util.List;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.UnionObject;
-import org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector;
-
-public class CustomNonSettableUnionObjectInspector1 implements UnionObjectInspector{
-
-  private final List<ObjectInspector> children;
-
-  public CustomNonSettableUnionObjectInspector1(List<ObjectInspector> children) {
-    this.children = children;
-  }
-
-  public static class StandardUnion implements UnionObject {
-    protected byte tag;
-    protected Object object;
-
-    public StandardUnion() {
-    }
-
-    public StandardUnion(byte tag, Object object) {
-      this.tag = tag;
-      this.object = object;
-    }
-
-    @Override
-    public Object getObject() {
-      return object;
-    }
-
-    @Override
-    public byte getTag() {
-      return tag;
-    }
-
-    @Override
-    public String toString() {
-      return tag + ":" + object;
-    }
-  }
-
-  /**
-   * Return the tag of the object.
-   */
-  public byte getTag(Object o) {
-    if (o == null) {
-      return -1;
-    }
-    return ((UnionObject) o).getTag();
-  }
-
-  /**
-   * Return the field based on the tag value associated with the Object.
-   */
-  public Object getField(Object o) {
-    if (o == null) {
-      return null;
-    }
-    return ((UnionObject) o).getObject();
-  }
-
-  public Category getCategory() {
-    return Category.UNION;
-  }
-
-  public String getTypeName() {
-    return null;
-  }
-
-  @Override
-  public String toString() {
-    return null;
-  }
-
-  @Override
-  public List<ObjectInspector> getObjectInspectors() {
-    return children;
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe1.java b/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe1.java
deleted file mode 100644
index e1352c3..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe1.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.serde2;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Properties;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-
-public class CustomSerDe1 extends AbstractSerDe {
-
-  int numColumns;
-
-  StructObjectInspector rowOI;
-  ArrayList<String> row;
-
-  @Override
-  public void initialize(Configuration conf, Properties tbl)
-      throws SerDeException {
-
-    // Read the configuration parameters
-    String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
-    String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
-
-    // The input column can either be a string or a list of integer values.
-    List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
-    List<TypeInfo> columnTypes = TypeInfoUtils
-        .getTypeInfosFromTypeString(columnTypeProperty);
-    assert columnNames.size() == columnTypes.size();
-    numColumns = columnNames.size();
-
-    // No exception for type checking for simplicity
-    // Constructing the row ObjectInspector:
-    // The row consists of some string columns, some Array<int> columns.
-    List<ObjectInspector> columnOIs = new ArrayList<ObjectInspector>(
-        columnNames.size());
-    for (int c = 0; c < numColumns; c++) {
-      if (columnTypes.get(c).equals(TypeInfoFactory.stringTypeInfo)) {
-        columnOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
-      } else {
-        // Blindly add this as a integer list, should be sufficient for the test case.
-        // Use the non-settable list object inspector.
-        columnOIs.add(new CustomNonSettableListObjectInspector1(
-            PrimitiveObjectInspectorFactory.javaIntObjectInspector));
-      }
-    }
-    // Use non-settable struct object inspector.
-    rowOI = new CustomNonSettableStructObjectInspector1(
-        columnNames, columnOIs);
-
-    // Constructing the row object, etc, which will be reused for all rows.
-    row = new ArrayList<String>(numColumns);
-    for (int c = 0; c < numColumns; c++) {
-      row.add(null);
-    }
-  }
-
-  @Override
-  public ObjectInspector getObjectInspector() throws SerDeException {
-    return rowOI;
-  }
-
-  @Override
-  public Class<? extends Writable> getSerializedClass() {
-    return Text.class;
-  }
-
-  @Override
-  public Object deserialize(Writable blob) throws SerDeException {
-    // Now all the column values should always return NULL!
-    return row;
-  }
-
-  @Override
-  public Writable serialize(Object obj, ObjectInspector objInspector)
-      throws SerDeException {
-    return null;
-  }
-
-  @Override
-  public SerDeStats getSerDeStats() {
-    // no support for statistics
-    return null;
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe2.java b/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe2.java
deleted file mode 100644
index f85439c..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe2.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.serde2;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Properties;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-
-public class CustomSerDe2 extends AbstractSerDe {
-
-  int numColumns;
-
-  StructObjectInspector rowOI;
-  ArrayList<String> row;
-
-  @Override
-  public void initialize(Configuration conf, Properties tbl)
-      throws SerDeException {
-
-    // Read the configuration parameters
-    String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
-    String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
-
-    // The input column can either be a string or a list of integer values.
-    List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
-    List<TypeInfo> columnTypes = TypeInfoUtils
-        .getTypeInfosFromTypeString(columnTypeProperty);
-    assert columnNames.size() == columnTypes.size();
-    numColumns = columnNames.size();
-
-    // No exception for type checking for simplicity
-    // Constructing the row ObjectInspector:
-    // The row consists of some string columns, some Array<int> columns.
-    List<ObjectInspector> columnOIs = new ArrayList<ObjectInspector>(
-        columnNames.size());
-    for (int c = 0; c < numColumns; c++) {
-      if (columnTypes.get(c).equals(TypeInfoFactory.stringTypeInfo)) {
-        columnOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
-      } else {
-        // Blindly add this as a integer list! Should be sufficient for the test case.
-        columnOIs.add(ObjectInspectorFactory.getStandardListObjectInspector(
-            PrimitiveObjectInspectorFactory.javaIntObjectInspector));
-      }
-    }
-    // StandardStruct uses ArrayList to store the row.
-    rowOI = ObjectInspectorFactory.getStandardStructObjectInspector(
-        columnNames, columnOIs);
-
-    // Constructing the row object, etc, which will be reused for all rows.
-    row = new ArrayList<String>(numColumns);
-    for (int c = 0; c < numColumns; c++) {
-      row.add(null);
-    }
-  }
-
-  @Override
-  public ObjectInspector getObjectInspector() throws SerDeException {
-    return rowOI;
-  }
-
-  @Override
-  public Class<? extends Writable> getSerializedClass() {
-    return Text.class;
-  }
-
-  @Override
-  public Object deserialize(Writable blob) throws SerDeException {
-    // Now all the column values should always return NULL!
-    return row;
-  }
-
-  @Override
-  public Writable serialize(Object obj, ObjectInspector objInspector)
-      throws SerDeException {
-    return null;
-  }
-
-  @Override
-  public SerDeStats getSerDeStats() {
-    // no support for statistics
-    return null;
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe3.java b/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe3.java
deleted file mode 100644
index 311718e..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe3.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.serde2;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Properties;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-
-public class CustomSerDe3 extends CustomSerDe1 {
-  @Override
-  public void initialize(Configuration conf, Properties tbl)
-      throws SerDeException {
-
-    // Read the configuration parameters
-    String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
-    String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
-
-    // The input column can either be a string or a list of list of integer values.
-    List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
-    List<TypeInfo> columnTypes = TypeInfoUtils
-        .getTypeInfosFromTypeString(columnTypeProperty);
-    assert columnNames.size() == columnTypes.size();
-    numColumns = columnNames.size();
-
-    // No exception for type checking for simplicity
-    // Constructing the row ObjectInspector:
-    // The row consists of some string columns, some Array<Array<int> > columns.
-    List<ObjectInspector> columnOIs = new ArrayList<ObjectInspector>(
-        columnNames.size());
-    for (int c = 0; c < numColumns; c++) {
-      if (columnTypes.get(c).equals(TypeInfoFactory.stringTypeInfo)) {
-        columnOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
-      } else {
-        // Blindly add this as a non settable list of list of integers,
-        // should be sufficient for the test case.
-        // Use the standard list object inspector.
-        columnOIs.add(ObjectInspectorFactory.getStandardListObjectInspector(
-            new CustomNonSettableListObjectInspector1(PrimitiveObjectInspectorFactory.javaIntObjectInspector)));
-      }
-    }
-    // Use non-settable struct object inspector.
-    rowOI = new CustomNonSettableStructObjectInspector1(
-        columnNames, columnOIs);
-
-    // Constructing the row object, etc, which will be reused for all rows.
-    row = new ArrayList<String>(numColumns);
-    for (int c = 0; c < numColumns; c++) {
-      row.add(null);
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe4.java b/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe4.java
deleted file mode 100644
index 281d959..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe4.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.hadoop.hive.serde2;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Properties;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-
-public class CustomSerDe4 extends CustomSerDe2 {
-
-    @Override
-    public void initialize(Configuration conf, Properties tbl)
-        throws SerDeException {
-
-      // Read the configuration parameters
-      String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
-      String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
-
-      // The input column can either be a string or a list of integer values.
-      List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
-      List<TypeInfo> columnTypes = TypeInfoUtils
-          .getTypeInfosFromTypeString(columnTypeProperty);
-      assert columnNames.size() == columnTypes.size();
-      numColumns = columnNames.size();
-
-      // No exception for type checking for simplicity
-      // Constructing the row ObjectInspector:
-      // The row consists of string columns, double columns, some union<int, double> columns only.
-      List<ObjectInspector> columnOIs = new ArrayList<ObjectInspector>(
-          columnNames.size());
-      for (int c = 0; c < numColumns; c++) {
-        if (columnTypes.get(c).equals(TypeInfoFactory.stringTypeInfo)) {
-          columnOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
-        }
-        else if (columnTypes.get(c).equals(TypeInfoFactory.doubleTypeInfo)) {
-          columnOIs.add(PrimitiveObjectInspectorFactory.javaDoubleObjectInspector);
-        }
-        else {
-
-          // Blindly add this as a union type containing int and double!
-          // Should be sufficient for the test case.
-         List<ObjectInspector> unionOI =  new ArrayList<ObjectInspector>();
-         unionOI.add(PrimitiveObjectInspectorFactory.javaIntObjectInspector);
-         unionOI.add(PrimitiveObjectInspectorFactory.javaDoubleObjectInspector);
-          columnOIs.add(ObjectInspectorFactory.getStandardUnionObjectInspector(unionOI));
-        }
-      }
-      // StandardList uses ArrayList to store the row.
-      rowOI = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, columnOIs);
-
-      // Constructing the row object, etc, which will be reused for all rows.
-      row = new ArrayList<String>(numColumns);
-      for (int c = 0; c < numColumns; c++) {
-        row.add(null);
-      }
-    }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe5.java b/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe5.java
deleted file mode 100644
index 169c8ab..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/serde2/CustomSerDe5.java
+++ /dev/null
@@ -1,63 +0,0 @@
-package org.apache.hadoop.hive.serde2;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Properties;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-
-public class CustomSerDe5 extends CustomSerDe4 {
-  @Override
-    public void initialize(Configuration conf, Properties tbl)
-        throws SerDeException {
-      // Read the configuration parameters
-      String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
-      String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
-
-      // The input column can either be a string or a list of integer values.
-      List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
-      List<TypeInfo> columnTypes = TypeInfoUtils
-          .getTypeInfosFromTypeString(columnTypeProperty);
-      assert columnNames.size() == columnTypes.size();
-      numColumns = columnNames.size();
-
-      // No exception for type checking for simplicity
-      // Constructing the row ObjectInspector:
-      // The row consists of string columns, double columns, some union<int, double> columns only.
-      List<ObjectInspector> columnOIs = new ArrayList<ObjectInspector>(
-          columnNames.size());
-      for (int c = 0; c < numColumns; c++) {
-        if (columnTypes.get(c).equals(TypeInfoFactory.stringTypeInfo)) {
-          columnOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
-        }
-        else if (columnTypes.get(c).equals(TypeInfoFactory.doubleTypeInfo)) {
-          columnOIs.add(PrimitiveObjectInspectorFactory.javaDoubleObjectInspector);
-        }
-        else {
-
-          // Blindly add this as a union type containing int and double!
-          // Should be sufficient for the test case.
-         List<ObjectInspector> unionOI =  new ArrayList<ObjectInspector>();
-         unionOI.add(PrimitiveObjectInspectorFactory.javaIntObjectInspector);
-         unionOI.add(PrimitiveObjectInspectorFactory.javaDoubleObjectInspector);
-          columnOIs.add(new CustomNonSettableUnionObjectInspector1(unionOI));
-        }
-      }
-      // StandardList uses ArrayList to store the row.
-      rowOI = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, columnOIs);
-
-      // Constructing the row object, etc, which will be reused for all rows.
-      row = new ArrayList<String>(numColumns);
-      for (int c = 0; c < numColumns; c++) {
-        row.add(null);
-      }
-    }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/serde2/TestSerDe.java b/src/ql/src/test/org/apache/hadoop/hive/serde2/TestSerDe.java
deleted file mode 100644
index 23e67e3..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/serde2/TestSerDe.java
+++ /dev/null
@@ -1,199 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.serde2;
-
-import java.nio.charset.CharacterCodingException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Properties;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.StructField;
-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-
-/**
- * TestSerDe.
- *
- */
-public class TestSerDe extends AbstractSerDe {
-
-  public static final Log LOG = LogFactory.getLog(TestSerDe.class.getName());
-
-  public String getShortName() {
-    return shortName();
-  }
-
-  public static String shortName() {
-    return "test_meta";
-  }
-
-  public static final String DefaultSeparator = "\002";
-
-  private String separator;
-  // constant for now, will make it configurable later.
-  private final String nullString = "\\N";
-  private List<String> columnNames;
-  private ObjectInspector cachedObjectInspector;
-
-  @Override
-  public String toString() {
-    return "TestSerDe[" + separator + "," + columnNames + "]";
-  }
-
-  public TestSerDe() throws SerDeException {
-    separator = DefaultSeparator;
-  }
-
-  @Override
-  public void initialize(Configuration job, Properties tbl) throws SerDeException {
-    separator = DefaultSeparator;
-    String altSep = tbl.getProperty("testserde.default.serialization.format");
-    if (altSep != null && altSep.length() > 0) {
-      try {
-        byte[] b = new byte[1];
-        b[0] = Byte.valueOf(altSep).byteValue();
-        separator = new String(b);
-      } catch (NumberFormatException e) {
-        separator = altSep;
-      }
-    }
-
-    String columnProperty = tbl.getProperty("columns");
-    if (columnProperty == null || columnProperty.length() == 0) {
-      // Hack for tables with no columns
-      // Treat it as a table with a single column called "col"
-      cachedObjectInspector = ObjectInspectorFactory
-          .getReflectionObjectInspector(ColumnSet.class,
-          ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
-    } else {
-      columnNames = Arrays.asList(columnProperty.split(","));
-      cachedObjectInspector = MetadataListStructObjectInspector
-          .getInstance(columnNames);
-    }
-    LOG.info(getClass().getName() + ": initialized with columnNames: "
-        + columnNames);
-  }
-
-  public static Object deserialize(ColumnSet c, String row, String sep,
-      String nullString) throws Exception {
-    if (c.col == null) {
-      c.col = new ArrayList<String>();
-    } else {
-      c.col.clear();
-    }
-    String[] l1 = row.split(sep, -1);
-
-    for (String s : l1) {
-      if (s.equals(nullString)) {
-        c.col.add(null);
-      } else {
-        c.col.add(s);
-      }
-    }
-    return (c);
-  }
-
-  ColumnSet deserializeCache = new ColumnSet();
-
-  @Override
-  public Object deserialize(Writable field) throws SerDeException {
-    String row = null;
-    if (field instanceof BytesWritable) {
-      BytesWritable b = (BytesWritable) field;
-      try {
-        row = Text.decode(b.get(), 0, b.getSize());
-      } catch (CharacterCodingException e) {
-        throw new SerDeException(e);
-      }
-    } else if (field instanceof Text) {
-      row = field.toString();
-    }
-    try {
-      deserialize(deserializeCache, row, separator, nullString);
-      if (columnNames != null) {
-        assert (columnNames.size() == deserializeCache.col.size());
-      }
-      return deserializeCache;
-    } catch (ClassCastException e) {
-      throw new SerDeException(this.getClass().getName()
-          + " expects Text or BytesWritable", e);
-    } catch (Exception e) {
-      throw new SerDeException(e);
-    }
-  }
-
-  @Override
-  public ObjectInspector getObjectInspector() throws SerDeException {
-    return cachedObjectInspector;
-  }
-
-  @Override
-  public Class<? extends Writable> getSerializedClass() {
-    return Text.class;
-  }
-
-  Text serializeCache = new Text();
-
-  @Override
-  public Writable serialize(Object obj, ObjectInspector objInspector) throws SerDeException {
-
-    if (objInspector.getCategory() != Category.STRUCT) {
-      throw new SerDeException(getClass().toString()
-          + " can only serialize struct types, but we got: "
-          + objInspector.getTypeName());
-    }
-    StructObjectInspector soi = (StructObjectInspector) objInspector;
-    List<? extends StructField> fields = soi.getAllStructFieldRefs();
-
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < fields.size(); i++) {
-      if (i > 0) {
-        sb.append(separator);
-      }
-      Object column = soi.getStructFieldData(obj, fields.get(i));
-      if (fields.get(i).getFieldObjectInspector().getCategory() == Category.PRIMITIVE) {
-        // For primitive object, serialize to plain string
-        sb.append(column == null ? nullString : column.toString());
-      } else {
-        // For complex object, serialize to JSON format
-        sb.append(SerDeUtils.getJSONString(column, fields.get(i)
-            .getFieldObjectInspector()));
-      }
-    }
-    serializeCache.set(sb.toString());
-    return serializeCache;
-  }
-
-  @Override
-  public SerDeStats getSerDeStats() {
-    // no support for statistics
-    return null;
-  }
-
-}
diff --git a/src/serde/src/test/org/apache/hadoop/hive/serde2/TestSerdeWithFieldComments.java b/src/serde/src/test/org/apache/hadoop/hive/serde2/TestSerdeWithFieldComments.java
deleted file mode 100644
index bb96a89..0000000
--- a/src/serde/src/test/org/apache/hadoop/hive/serde2/TestSerdeWithFieldComments.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.serde2;
-
-import junit.framework.TestCase;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.StructField;
-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-public class TestSerdeWithFieldComments extends TestCase {
-
-  private StructField mockedStructField(String name, String oiTypeName,
-                                        String comment) {
-    StructField m = mock(StructField.class);
-    when(m.getFieldName()).thenReturn(name);
-
-    ObjectInspector oi = mock(ObjectInspector.class);
-    when(oi.getTypeName()).thenReturn(oiTypeName);
-
-    when(m.getFieldObjectInspector()).thenReturn(oi);
-    when(m.getFieldComment()).thenReturn(comment);
-
-    return m;
-  }
-
-  public void testFieldComments() throws MetaException, SerDeException {
-    StructObjectInspector mockSOI = mock(StructObjectInspector.class);
-    when(mockSOI.getCategory()).thenReturn(ObjectInspector.Category.STRUCT);
-    List fieldRefs = new ArrayList<StructField>();
-    // Add field with a comment...
-    fieldRefs.add(mockedStructField("first", "type name 1", "this is a comment"));
-    // ... and one without
-    fieldRefs.add(mockedStructField("second", "type name 2", null));
-
-    when(mockSOI.getAllStructFieldRefs()).thenReturn(fieldRefs);
-
-    Deserializer mockDe = mock(Deserializer.class);
-    when(mockDe.getObjectInspector()).thenReturn(mockSOI);
-    List<FieldSchema> result =
-        MetaStoreUtils.getFieldsFromDeserializer("testTable", mockDe);
-
-    assertEquals(2, result.size());
-    assertEquals("first", result.get(0).getName());
-    assertEquals("this is a comment", result.get(0).getComment());
-    assertEquals("second", result.get(1).getName());
-    assertEquals("from deserializer", result.get(1).getComment());
-  }
-}
diff --git a/src/serde/src/test/org/apache/hadoop/hive/serde2/dynamic_type/TestDynamicSerDe.java b/src/serde/src/test/org/apache/hadoop/hive/serde2/dynamic_type/TestDynamicSerDe.java
deleted file mode 100644
index 1e326ff..0000000
--- a/src/serde/src/test/org/apache/hadoop/hive/serde2/dynamic_type/TestDynamicSerDe.java
+++ /dev/null
@@ -1,861 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.serde2.dynamic_type;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import java.util.Random;
-import java.util.Map.Entry;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol;
-import org.apache.hadoop.io.BytesWritable;
-
-/**
- * TestDynamicSerDe.
- *
- */
-public class TestDynamicSerDe extends TestCase {
-
-  public static HashMap<String, String> makeHashMap(String... params) {
-    HashMap<String, String> r = new HashMap<String, String>();
-    for (int i = 0; i < params.length; i += 2) {
-      r.put(params[i], params[i + 1]);
-    }
-    return r;
-  }
-
-  public void testDynamicSerDe() throws Throwable {
-    try {
-
-      // Try to construct an object
-      ArrayList<String> bye = new ArrayList<String>();
-      bye.add("firstString");
-      bye.add("secondString");
-      HashMap<String, Integer> another = new HashMap<String, Integer>();
-      another.put("firstKey", 1);
-      another.put("secondKey", 2);
-      ArrayList<Object> struct = new ArrayList<Object>();
-      struct.add(Integer.valueOf(234));
-      struct.add(bye);
-      struct.add(another);
-      struct.add(Integer.valueOf(-234));
-      struct.add(Double.valueOf(1.0));
-      struct.add(Double.valueOf(-2.5));
-
-      // All protocols
-      ArrayList<String> protocols = new ArrayList<String>();
-      ArrayList<Boolean> isBinaries = new ArrayList<Boolean>();
-      ArrayList<HashMap<String, String>> additionalParams = new ArrayList<HashMap<String, String>>();
-
-      protocols
-          .add(org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.class
-          .getName());
-      isBinaries.add(true);
-      additionalParams.add(makeHashMap("serialization.sort.order", "++++++"));
-      protocols
-          .add(org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.class
-          .getName());
-      isBinaries.add(true);
-      additionalParams.add(makeHashMap("serialization.sort.order", "------"));
-
-      protocols.add(org.apache.thrift.protocol.TBinaryProtocol.class.getName());
-      isBinaries.add(true);
-      additionalParams.add(null);
-
-      protocols.add(org.apache.thrift.protocol.TJSONProtocol.class.getName());
-      isBinaries.add(false);
-      additionalParams.add(null);
-
-      // TSimpleJSONProtocol does not support deserialization.
-      // protocols.add(org.apache.thrift.protocol.TSimpleJSONProtocol.class.getName());
-      // isBinaries.add(false);
-      // additionalParams.add(null);
-
-      // TCTLSeparatedProtocol is not done yet.
-      protocols
-          .add(org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
-          .getName());
-      isBinaries.add(false);
-      additionalParams.add(null);
-
-      System.out.println("input struct = " + struct);
-
-      for (int pp = 0; pp < protocols.size(); pp++) {
-
-        String protocol = protocols.get(pp);
-        boolean isBinary = isBinaries.get(pp);
-
-        System.out.println("Testing protocol: " + protocol);
-        Properties schema = new Properties();
-        schema.setProperty(serdeConstants.SERIALIZATION_FORMAT, protocol);
-        schema.setProperty(
-            org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
-            "test");
-        schema
-            .setProperty(
-                serdeConstants.SERIALIZATION_DDL,
-                "struct test { i32 _hello, list<string> 2bye, map<string,i32> another, i32 nhello, double d, double nd}");
-        schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
-            .getClass().toString());
-        HashMap<String, String> p = additionalParams.get(pp);
-        if (p != null) {
-          for (Entry<String, String> e : p.entrySet()) {
-            schema.setProperty(e.getKey(), e.getValue());
-          }
-        }
-
-        DynamicSerDe serde = new DynamicSerDe();
-        serde.initialize(new Configuration(), schema);
-
-        // Try getObjectInspector
-        ObjectInspector oi = serde.getObjectInspector();
-        System.out.println("TypeName = " + oi.getTypeName());
-
-        // Try to serialize
-        BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
-        System.out.println("bytes =" + hexString(bytes));
-
-        if (!isBinary) {
-          System.out.println("bytes in text ="
-              + new String(bytes.get(), 0, bytes.getSize()));
-        }
-
-        // Try to deserialize
-        Object o = serde.deserialize(bytes);
-        System.out.println("o class = " + o.getClass());
-        List<?> olist = (List<?>) o;
-        System.out.println("o size = " + olist.size());
-        System.out.println("o[0] class = " + olist.get(0).getClass());
-        System.out.println("o[1] class = " + olist.get(1).getClass());
-        System.out.println("o[2] class = " + olist.get(2).getClass());
-        System.out.println("o = " + o);
-
-        assertEquals(struct, o);
-      }
-
-    } catch (Throwable e) {
-      e.printStackTrace();
-      throw e;
-    }
-
-  }
-
-  public String hexString(BytesWritable bytes) {
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < bytes.getSize(); i++) {
-      byte b = bytes.get()[i];
-      int v = (b < 0 ? 256 + b : b);
-      sb.append(String.format("x%02x", v));
-    }
-    return sb.toString();
-  }
-
-  private void testTBinarySortableProtocol(Object[] structs, String ddl,
-      boolean ascending) throws Throwable {
-    int fields = ((List) structs[structs.length - 1]).size();
-    String order = "";
-    for (int i = 0; i < fields; i++) {
-      order = order + (ascending ? "+" : "-");
-    }
-
-    Properties schema = new Properties();
-    schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
-        org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.class
-        .getName());
-    schema.setProperty(
-        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME, "test");
-    schema.setProperty(serdeConstants.SERIALIZATION_DDL, ddl);
-    schema.setProperty(serdeConstants.SERIALIZATION_LIB, DynamicSerDe.class
-        .getName());
-    schema.setProperty(serdeConstants.SERIALIZATION_SORT_ORDER, order);
-
-    DynamicSerDe serde = new DynamicSerDe();
-    serde.initialize(new Configuration(), schema);
-
-    ObjectInspector oi = serde.getObjectInspector();
-
-    // Try to serialize
-    BytesWritable bytes[] = new BytesWritable[structs.length];
-    for (int i = 0; i < structs.length; i++) {
-      bytes[i] = new BytesWritable();
-      BytesWritable s = (BytesWritable) serde.serialize(structs[i], oi);
-      bytes[i].set(s);
-      if (i > 0) {
-        int compareResult = bytes[i - 1].compareTo(bytes[i]);
-        if ((compareResult < 0 && !ascending)
-            || (compareResult > 0 && ascending)) {
-          System.out.println("Test failed in "
-              + (ascending ? "ascending" : "descending") + " order.");
-          System.out.println("serialized data of " + structs[i - 1] + " = "
-              + hexString(bytes[i - 1]));
-          System.out.println("serialized data of " + structs[i] + " = "
-              + hexString(bytes[i]));
-          fail("Sort order of serialized " + structs[i - 1] + " and "
-              + structs[i] + " are reversed!");
-        }
-      }
-    }
-
-    // Try to deserialize
-    Object[] deserialized = new Object[structs.length];
-    for (int i = 0; i < structs.length; i++) {
-      deserialized[i] = serde.deserialize(bytes[i]);
-      if (!structs[i].equals(deserialized[i])) {
-        System.out.println("structs[i] = " + structs[i]);
-        System.out.println("deserialized[i] = " + deserialized[i]);
-        System.out.println("serialized[i] = " + hexString(bytes[i]));
-        assertEquals(structs[i], deserialized[i]);
-      }
-    }
-  }
-
-  static int compare(Object a, Object b) {
-    if (a == null && b == null) {
-      return 0;
-    }
-    if (a == null) {
-      return -1;
-    }
-    if (b == null) {
-      return 1;
-    }
-    if (a instanceof List) {
-      List la = (List) a;
-      List lb = (List) b;
-      assert (la.size() == lb.size());
-      for (int i = 0; i < la.size(); i++) {
-        int r = compare(la.get(i), lb.get(i));
-        if (r != 0) {
-          return r;
-        }
-      }
-      return 0;
-    } else if (a instanceof Number) {
-      Number na = (Number) a;
-      Number nb = (Number) b;
-      if (na.doubleValue() < nb.doubleValue()) {
-        return -1;
-      }
-      if (na.doubleValue() > nb.doubleValue()) {
-        return 1;
-      }
-      return 0;
-    } else if (a instanceof String) {
-      String sa = (String) a;
-      String sb = (String) b;
-      return sa.compareTo(sb);
-    }
-    return 0;
-  }
-
-  private void sort(Object[] structs) {
-    for (int i = 0; i < structs.length; i++) {
-      for (int j = i + 1; j < structs.length; j++) {
-        if (compare(structs[i], structs[j]) > 0) {
-          Object t = structs[i];
-          structs[i] = structs[j];
-          structs[j] = t;
-        }
-      }
-    }
-  }
-
-  public void testTBinarySortableProtocol() throws Throwable {
-    try {
-
-      System.out.println("Beginning Test testTBinarySortableProtocol:");
-
-      int num = 100;
-      Random r = new Random(1234);
-      Object structs[] = new Object[num];
-      String ddl;
-
-      // Test double
-      for (int i = 0; i < num; i++) {
-        ArrayList<Object> struct = new ArrayList<Object>();
-        if (i == 0) {
-          struct.add(null);
-        } else {
-          struct.add(Double.valueOf((r.nextDouble() - 0.5) * 10));
-        }
-        structs[i] = struct;
-      }
-      sort(structs);
-      ddl = "struct test { double hello}";
-      System.out.println("Testing " + ddl);
-      testTBinarySortableProtocol(structs, ddl, true);
-      testTBinarySortableProtocol(structs, ddl, false);
-
-      // Test integer
-      for (int i = 0; i < num; i++) {
-        ArrayList<Object> struct = new ArrayList<Object>();
-        if (i == 0) {
-          struct.add(null);
-        } else {
-          struct.add((int) ((r.nextDouble() - 0.5) * 1.5 * Integer.MAX_VALUE));
-        }
-        structs[i] = struct;
-      }
-      sort(structs);
-      // Null should be smaller than any other value, so put a null at the front
-      // end
-      // to test whether that is held.
-      ((List) structs[0]).set(0, null);
-      ddl = "struct test { i32 hello}";
-      System.out.println("Testing " + ddl);
-      testTBinarySortableProtocol(structs, ddl, true);
-      testTBinarySortableProtocol(structs, ddl, false);
-
-      // Test long
-      for (int i = 0; i < num; i++) {
-        ArrayList<Object> struct = new ArrayList<Object>();
-        if (i == 0) {
-          struct.add(null);
-        } else {
-          struct.add((long) ((r.nextDouble() - 0.5) * 1.5 * Long.MAX_VALUE));
-        }
-        structs[i] = struct;
-      }
-      sort(structs);
-      // Null should be smaller than any other value, so put a null at the front
-      // end
-      // to test whether that is held.
-      ((List) structs[0]).set(0, null);
-      ddl = "struct test { i64 hello}";
-      System.out.println("Testing " + ddl);
-      testTBinarySortableProtocol(structs, ddl, true);
-      testTBinarySortableProtocol(structs, ddl, false);
-
-      // Test string
-      for (int i = 0; i < num; i++) {
-        ArrayList<Object> struct = new ArrayList<Object>();
-        if (i == 0) {
-          struct.add(null);
-        } else {
-          struct.add(String.valueOf((r.nextDouble() - 0.5) * 1000));
-        }
-        structs[i] = struct;
-      }
-      sort(structs);
-      // Null should be smaller than any other value, so put a null at the front
-      // end
-      // to test whether that is held.
-      ((List) structs[0]).set(0, null);
-      ddl = "struct test { string hello}";
-      System.out.println("Testing " + ddl);
-      testTBinarySortableProtocol(structs, ddl, true);
-      testTBinarySortableProtocol(structs, ddl, false);
-
-      // Test string + double
-      for (int i = 0; i < num; i++) {
-        ArrayList<Object> struct = new ArrayList<Object>();
-        if (i % 9 == 0) {
-          struct.add(null);
-        } else {
-          struct.add("str" + (i / 5));
-        }
-        if (i % 7 == 0) {
-          struct.add(null);
-        } else {
-          struct.add(Double.valueOf((r.nextDouble() - 0.5) * 10));
-        }
-        structs[i] = struct;
-      }
-      sort(structs);
-      // Null should be smaller than any other value, so put a null at the front
-      // end
-      // to test whether that is held.
-      ((List) structs[0]).set(0, null);
-      ddl = "struct test { string hello, double another}";
-      System.out.println("Testing " + ddl);
-      testTBinarySortableProtocol(structs, ddl, true);
-      testTBinarySortableProtocol(structs, ddl, false);
-
-      System.out.println("Test testTBinarySortableProtocol passed!");
-    } catch (Throwable e) {
-      e.printStackTrace();
-      throw e;
-    }
-  }
-
-  public void testConfigurableTCTLSeparated() throws Throwable {
-    try {
-
-      // Try to construct an object
-      ArrayList<String> bye = new ArrayList<String>();
-      bye.add("firstString");
-      bye.add("secondString");
-      LinkedHashMap<String, Integer> another = new LinkedHashMap<String, Integer>();
-      another.put("firstKey", 1);
-      another.put("secondKey", 2);
-      ArrayList<Object> struct = new ArrayList<Object>();
-      struct.add(Integer.valueOf(234));
-      struct.add(bye);
-      struct.add(another);
-
-      Properties schema = new Properties();
-      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
-          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
-          .getName());
-      schema.setProperty(
-          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
-          "test");
-      schema
-          .setProperty(serdeConstants.SERIALIZATION_DDL,
-          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
-      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
-          .getClass().toString());
-
-      schema.setProperty(serdeConstants.FIELD_DELIM, "9");
-      schema.setProperty(serdeConstants.COLLECTION_DELIM, "1");
-      schema.setProperty(serdeConstants.LINE_DELIM, "2");
-      schema.setProperty(serdeConstants.MAPKEY_DELIM, "4");
-
-      DynamicSerDe serde = new DynamicSerDe();
-      serde.initialize(new Configuration(), schema);
-
-      TCTLSeparatedProtocol prot = (TCTLSeparatedProtocol) serde.oprot_;
-      assertTrue(prot.getPrimarySeparator().equals("\u0009"));
-
-      ObjectInspector oi = serde.getObjectInspector();
-
-      // Try to serialize
-      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
-
-      hexString(bytes);
-
-      String compare = "234" + "\u0009" + "firstString" + "\u0001"
-          + "secondString" + "\u0009" + "firstKey" + "\u0004" + "1" + "\u0001"
-          + "secondKey" + "\u0004" + "2";
-
-      System.out.println("bytes in text ="
-          + new String(bytes.get(), 0, bytes.getSize()) + ">");
-      System.out.println("compare to    =" + compare + ">");
-
-      assertTrue(compare.equals(new String(bytes.get(), 0, bytes.getSize())));
-
-      // Try to deserialize
-      Object o = serde.deserialize(bytes);
-      System.out.println("o class = " + o.getClass());
-      List<?> olist = (List<?>) o;
-      System.out.println("o size = " + olist.size());
-      System.out.println("o[0] class = " + olist.get(0).getClass());
-      System.out.println("o[1] class = " + olist.get(1).getClass());
-      System.out.println("o[2] class = " + olist.get(2).getClass());
-      System.out.println("o = " + o);
-
-      assertEquals(o, struct);
-
-    } catch (Throwable e) {
-      e.printStackTrace();
-      throw e;
-    }
-
-  }
-
-  /**
-   * Tests a single null list within a struct with return nulls on.
-   */
-
-  public void testNulls1() throws Throwable {
-    try {
-
-      // Try to construct an object
-      ArrayList<String> bye = null;
-      HashMap<String, Integer> another = new HashMap<String, Integer>();
-      another.put("firstKey", 1);
-      another.put("secondKey", 2);
-      ArrayList<Object> struct = new ArrayList<Object>();
-      struct.add(Integer.valueOf(234));
-      struct.add(bye);
-      struct.add(another);
-
-      Properties schema = new Properties();
-      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
-          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
-          .getName());
-      schema.setProperty(
-          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
-          "test");
-      schema
-          .setProperty(serdeConstants.SERIALIZATION_DDL,
-          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
-      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
-          .getClass().toString());
-      schema.setProperty(TCTLSeparatedProtocol.ReturnNullsKey, "true");
-
-      DynamicSerDe serde = new DynamicSerDe();
-      serde.initialize(new Configuration(), schema);
-
-      ObjectInspector oi = serde.getObjectInspector();
-
-      // Try to serialize
-      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
-
-      hexString(bytes);
-
-      // Try to deserialize
-      Object o = serde.deserialize(bytes);
-      assertEquals(struct, o);
-
-    } catch (Throwable e) {
-      e.printStackTrace();
-      throw e;
-    }
-
-  }
-
-  /**
-   * Tests all elements of a struct being null with return nulls on.
-   */
-
-  public void testNulls2() throws Throwable {
-    try {
-
-      // Try to construct an object
-      ArrayList<String> bye = null;
-      HashMap<String, Integer> another = null;
-      ArrayList<Object> struct = new ArrayList<Object>();
-      struct.add(null);
-      struct.add(bye);
-      struct.add(another);
-
-      Properties schema = new Properties();
-      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
-          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
-          .getName());
-      schema.setProperty(
-          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
-          "test");
-      schema
-          .setProperty(serdeConstants.SERIALIZATION_DDL,
-          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
-      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
-          .getClass().toString());
-      schema.setProperty(TCTLSeparatedProtocol.ReturnNullsKey, "true");
-
-      DynamicSerDe serde = new DynamicSerDe();
-      serde.initialize(new Configuration(), schema);
-
-      ObjectInspector oi = serde.getObjectInspector();
-
-      // Try to serialize
-      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
-
-      hexString(bytes);
-
-      // Try to deserialize
-      Object o = serde.deserialize(bytes);
-      List<?> olist = (List<?>) o;
-
-      assertTrue(olist.size() == 3);
-      assertEquals(null, olist.get(0));
-      assertEquals(null, olist.get(1));
-      assertEquals(null, olist.get(2));
-
-      // assertEquals(o, struct); Cannot do this because types of null lists are
-      // wrong.
-
-    } catch (Throwable e) {
-      e.printStackTrace();
-      throw e;
-    }
-
-  }
-
-  /**
-   * Tests map and list being empty with return nulls on.
-   */
-
-  public void testNulls3() throws Throwable {
-    try {
-
-      // Try to construct an object
-      ArrayList<String> bye = new ArrayList<String>();
-      HashMap<String, Integer> another = null;
-      ArrayList<Object> struct = new ArrayList<Object>();
-      struct.add(null);
-      struct.add(bye);
-      struct.add(another);
-
-      Properties schema = new Properties();
-      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
-          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
-          .getName());
-      schema.setProperty(
-          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
-          "test");
-      schema
-          .setProperty(serdeConstants.SERIALIZATION_DDL,
-          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
-      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
-          .getClass().toString());
-
-      schema.setProperty(TCTLSeparatedProtocol.ReturnNullsKey, "true");
-      DynamicSerDe serde = new DynamicSerDe();
-      serde.initialize(new Configuration(), schema);
-
-      ObjectInspector oi = serde.getObjectInspector();
-
-      // Try to serialize
-      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
-
-      hexString(bytes);
-
-      // Try to deserialize
-      Object o = serde.deserialize(bytes);
-      List<?> olist = (List<?>) o;
-
-      assertTrue(olist.size() == 3);
-      assertEquals(null, olist.get(0));
-      assertEquals(0, ((List<?>) olist.get(1)).size());
-      assertEquals(null, olist.get(2));
-
-      // assertEquals(o, struct); Cannot do this because types of null lists are
-      // wrong.
-
-    } catch (Throwable e) {
-      e.printStackTrace();
-      throw e;
-    }
-
-  }
-
-  /**
-   * Tests map and list null/empty with return nulls *off*.
-   */
-
-  public void testNulls4() throws Throwable {
-    try {
-
-      // Try to construct an object
-      ArrayList<String> bye = new ArrayList<String>();
-      HashMap<String, Integer> another = null;
-      ArrayList<Object> struct = new ArrayList<Object>();
-      struct.add(null);
-      struct.add(bye);
-      struct.add(another);
-
-      Properties schema = new Properties();
-      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
-          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
-          .getName());
-      schema.setProperty(
-          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
-          "test");
-      schema
-          .setProperty(serdeConstants.SERIALIZATION_DDL,
-          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
-      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
-          .getClass().toString());
-
-      schema.setProperty(TCTLSeparatedProtocol.ReturnNullsKey, "false");
-      DynamicSerDe serde = new DynamicSerDe();
-      serde.initialize(new Configuration(), schema);
-
-      ObjectInspector oi = serde.getObjectInspector();
-
-      // Try to serialize
-      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
-
-      hexString(bytes);
-
-      // Try to deserialize
-      Object o = serde.deserialize(bytes);
-      List<?> olist = (List<?>) o;
-
-      assertTrue(olist.size() == 3);
-      assertEquals(new Integer(0), (Integer) olist.get(0));
-      List<?> num1 = (List<?>) olist.get(1);
-      assertTrue(num1.size() == 0);
-      Map<?, ?> num2 = (Map<?, ?>) olist.get(2);
-      assertTrue(num2.size() == 0);
-
-      // assertEquals(o, struct); Cannot do this because types of null lists are
-      // wrong.
-
-    } catch (Throwable e) {
-      e.printStackTrace();
-      throw e;
-    }
-
-  }
-
-  /**
-   * Tests map and list null/empty with return nulls *off*.
-   */
-
-  public void testStructsinStructs() throws Throwable {
-    try {
-
-      Properties schema = new Properties();
-      // schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
-      // org.apache.thrift.protocol.TJSONProtocol.class.getName());
-      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
-          org.apache.thrift.protocol.TBinaryProtocol.class.getName());
-      schema.setProperty(
-          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
-          "test");
-      schema.setProperty(
-          serdeConstants.SERIALIZATION_DDL,
-      "struct inner { i32 field1, string field2 },struct  test {inner foo,  i32 hello, list<string> bye, map<string,i32> another}");
-      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
-          .getClass().toString());
-
-      //
-      // construct object of above type
-      //
-
-      // construct the inner struct
-      ArrayList<Object> innerStruct = new ArrayList<Object>();
-      innerStruct.add(new Integer(22));
-      innerStruct.add(new String("hello world"));
-
-      // construct outer struct
-      ArrayList<String> bye = new ArrayList<String>();
-      bye.add("firstString");
-      bye.add("secondString");
-      HashMap<String, Integer> another = new HashMap<String, Integer>();
-      another.put("firstKey", 1);
-      another.put("secondKey", 2);
-
-      ArrayList<Object> struct = new ArrayList<Object>();
-
-      struct.add(innerStruct);
-      struct.add(Integer.valueOf(234));
-      struct.add(bye);
-      struct.add(another);
-
-      DynamicSerDe serde = new DynamicSerDe();
-      serde.initialize(new Configuration(), schema);
-
-      ObjectInspector oi = serde.getObjectInspector();
-
-      // Try to serialize
-      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
-
-      // Try to deserialize
-      Object o = serde.deserialize(bytes);
-      List<?> olist = (List<?>) o;
-
-      assertEquals(4, olist.size());
-      assertEquals(innerStruct, olist.get(0));
-      assertEquals(new Integer(234), olist.get(1));
-      assertEquals(bye, olist.get(2));
-      assertEquals(another, olist.get(3));
-
-    } catch (Throwable e) {
-      e.printStackTrace();
-      throw e;
-    }
-
-  }
-
-  public void testSkip() throws Throwable {
-    try {
-
-      // Try to construct an object
-      ArrayList<String> bye = new ArrayList<String>();
-      bye.add("firstString");
-      bye.add("secondString");
-      LinkedHashMap<String, Integer> another = new LinkedHashMap<String, Integer>();
-      another.put("firstKey", 1);
-      another.put("secondKey", 2);
-      ArrayList<Object> struct = new ArrayList<Object>();
-      struct.add(Integer.valueOf(234));
-      struct.add(bye);
-      struct.add(another);
-
-      Properties schema = new Properties();
-      schema.setProperty(serdeConstants.SERIALIZATION_FORMAT,
-          org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
-          .getName());
-      schema.setProperty(
-          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
-          "test");
-      schema
-          .setProperty(serdeConstants.SERIALIZATION_DDL,
-          "struct test { i32 hello, list<string> bye, map<string,i32> another}");
-      schema.setProperty(serdeConstants.SERIALIZATION_LIB, new DynamicSerDe()
-          .getClass().toString());
-
-      schema.setProperty(serdeConstants.FIELD_DELIM, "9");
-      schema.setProperty(serdeConstants.COLLECTION_DELIM, "1");
-      schema.setProperty(serdeConstants.LINE_DELIM, "2");
-      schema.setProperty(serdeConstants.MAPKEY_DELIM, "4");
-
-      DynamicSerDe serde = new DynamicSerDe();
-      serde.initialize(new Configuration(), schema);
-
-      TCTLSeparatedProtocol prot = (TCTLSeparatedProtocol) serde.oprot_;
-      assertTrue(prot.getPrimarySeparator().equals("\u0009"));
-
-      ObjectInspector oi = serde.getObjectInspector();
-
-      // Try to serialize
-      BytesWritable bytes = (BytesWritable) serde.serialize(struct, oi);
-
-      hexString(bytes);
-
-      String compare = "234" + "\u0009" + "firstString" + "\u0001"
-          + "secondString" + "\u0009" + "firstKey" + "\u0004" + "1" + "\u0001"
-          + "secondKey" + "\u0004" + "2";
-
-      System.out.println("bytes in text ="
-          + new String(bytes.get(), 0, bytes.getSize()) + ">");
-      System.out.println("compare to    =" + compare + ">");
-
-      assertTrue(compare.equals(new String(bytes.get(), 0, bytes.getSize())));
-
-      schema
-          .setProperty(serdeConstants.SERIALIZATION_DDL,
-          "struct test { i32 hello, skip list<string> bye, map<string,i32> another}");
-
-      serde.initialize(new Configuration(), schema);
-
-      // Try to deserialize
-      Object o = serde.deserialize(bytes);
-      System.out.println("o class = " + o.getClass());
-      List<?> olist = (List<?>) o;
-      System.out.println("o size = " + olist.size());
-      System.out.println("o = " + o);
-
-      assertEquals(null, olist.get(1));
-
-      // set the skipped field to null
-      struct.set(1, null);
-
-      assertEquals(o, struct);
-
-    } catch (Throwable e) {
-      e.printStackTrace();
-      throw e;
-    }
-
-  }
-
-}
diff --git a/src/service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java b/src/service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java
deleted file mode 100644
index 6188b19..0000000
--- a/src/service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java
+++ /dev/null
@@ -1,424 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.service;
-
-import java.util.List;
-import java.util.Properties;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.common.ServerUtils;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.Schema;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.thrift.protocol.TBinaryProtocol;
-import org.apache.thrift.protocol.TProtocol;
-import org.apache.thrift.transport.TSocket;
-import org.apache.thrift.transport.TTransport;
-
-/**
- * TestHiveServer.
- *
- */
-public class TestHiveServer extends TestCase {
-
-  private HiveInterface client;
-  private static final String host = "localhost";
-  private static final int port = 10000;
-  private final Path dataFilePath;
-
-  private static String tableName = "testhivedrivertable";
-  private final HiveConf conf;
-  private boolean standAloneServer = false;
-  private TTransport transport;
-  private final String invalidPath;
-
-  public TestHiveServer(String name) {
-    super(name);
-    conf = new HiveConf(TestHiveServer.class);
-    String dataFileDir = conf.get("test.data.files").replace('\\', '/')
-        .replace("c:", "");
-    invalidPath = dataFileDir+"/invalidpath/";
-    dataFilePath = new Path(dataFileDir, "kv1.txt");
-    // See data/conf/hive-site.xml
-    String paramStr = System.getProperty("test.service.standalone.server");
-    if (paramStr != null && paramStr.equals("true")) {
-      standAloneServer = true;
-    }
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-
-    if (standAloneServer) {
-      try {
-        transport = new TSocket(host, port);
-        TProtocol protocol = new TBinaryProtocol(transport);
-        client = new HiveClient(protocol);
-        transport.open();
-      } catch (Throwable e) {
-        e.printStackTrace();
-      }
-    } else {
-      client = new HiveServer.HiveServerHandler();
-    }
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    super.tearDown();
-    if (standAloneServer) {
-      try {
-        client.clean();
-      } catch (Exception e) {
-        e.printStackTrace();
-      }
-      transport.close();
-    }
-  }
-
-  public void testExecute() throws Exception {
-    try {
-      client.execute("set hive.support.concurrency = false");
-      client.execute("drop table " + tableName);
-    } catch (Exception ex) {
-    }
-
-    try {
-      client.execute("create table " + tableName + " (num int)");
-      client.execute("load data local inpath '" + dataFilePath.toString()
-          + "' into table " + tableName);
-      client.execute("select count(1) as cnt from " + tableName);
-      String row = client.fetchOne();
-      assertEquals(row, "500");
-
-      Schema hiveSchema = client.getSchema();
-      List<FieldSchema> listFields = hiveSchema.getFieldSchemas();
-      assertEquals(listFields.size(), 1);
-      assertEquals(listFields.get(0).getName(), "cnt");
-      assertEquals(listFields.get(0).getType(), "bigint");
-
-      Schema thriftSchema = client.getThriftSchema();
-      List<FieldSchema> listThriftFields = thriftSchema.getFieldSchemas();
-      assertEquals(listThriftFields.size(), 1);
-      assertEquals(listThriftFields.get(0).getName(), "cnt");
-      assertEquals(listThriftFields.get(0).getType(), "i64");
-
-      client.execute("drop table " + tableName);
-    } catch (Throwable t) {
-      t.printStackTrace();
-    }
-  }
-
-  public void notestExecute() throws Exception {
-    try {
-      client.execute("set hive.support.concurrency = false");
-      client.execute("drop table " + tableName);
-    } catch (Exception ex) {
-    }
-
-    client.execute("create table " + tableName + " (num int)");
-    client.execute("load data local inpath '" + dataFilePath.toString()
-        + "' into table " + tableName);
-    client.execute("select count(1) from " + tableName);
-    String row = client.fetchOne();
-    assertEquals(row, "500");
-    client.execute("drop table " + tableName);
-    transport.close();
-  }
-
-  public void testNonHiveCommand() throws Exception {
-    try {
-      client.execute("set hive.support.concurrency = false");
-      client.execute("drop table " + tableName);
-    } catch (Exception ex) {
-    }
-
-    client.execute("create table " + tableName + " (num int)");
-    client.execute("load data local inpath '" + dataFilePath.toString()
-        + "' into table " + tableName);
-
-    // Command not part of HiveQL - verify no results
-    client.execute("SET hive.mapred.mode = nonstrict");
-
-    Schema schema = client.getSchema();
-    assertEquals(schema.getFieldSchemasSize(), 0);
-    assertEquals(schema.getPropertiesSize(), 0);
-
-    Schema thriftschema = client.getThriftSchema();
-    assertEquals(thriftschema.getFieldSchemasSize(), 0);
-    assertEquals(thriftschema.getPropertiesSize(), 0);
-
-    try {
-      String ret = client.fetchOne();
-      assertTrue(false);
-    } catch (HiveServerException e) {
-      assertEquals(e.getErrorCode(), 0);
-    }
-    assertEquals(client.fetchN(10).size(), 0);
-    assertEquals(client.fetchAll().size(), 0);
-
-    // Execute Hive query and fetch
-    client.execute("select * from " + tableName + " limit 10");
-    client.fetchOne();
-
-    // Re-execute command not part of HiveQL - verify still no results
-    client.execute("SET hive.mapred.mode = nonstrict");
-
-    schema = client.getSchema();
-    assertEquals(schema.getFieldSchemasSize(), 0);
-    assertEquals(schema.getPropertiesSize(), 0);
-
-    thriftschema = client.getThriftSchema();
-    assertEquals(thriftschema.getFieldSchemasSize(), 0);
-    assertEquals(thriftschema.getPropertiesSize(), 0);
-
-    try {
-      String ret = client.fetchOne();
-      assertTrue(false);
-    } catch (HiveServerException e) {
-      assertEquals(e.getErrorCode(), 0);
-    }
-    assertEquals(client.fetchN(10).size(), 0);
-    assertEquals(client.fetchAll().size(), 0);
-
-    // Cleanup
-    client.execute("drop table " + tableName);
-  }
-
-  /**
-   * Test metastore call.
-   */
-  public void testMetastore() throws Exception {
-    try {
-      client.execute("set hive.support.concurrency = false");
-      client.execute("drop table " + tableName);
-    } catch (Exception ex) {
-    }
-
-    client.execute("create table " + tableName + " (num int)");
-    List<String> tabs = client.get_tables("default", tableName);
-    assertEquals(tabs.get(0), tableName);
-    client.execute("drop table " + tableName);
-  }
-
-  /**
-   * Test cluster status retrieval.
-   */
-  public void testGetClusterStatus() throws Exception {
-    HiveClusterStatus clusterStatus = client.getClusterStatus();
-    assertNotNull(clusterStatus);
-    assertTrue(clusterStatus.getTaskTrackers() >= 0);
-    assertTrue(clusterStatus.getMapTasks() >= 0);
-    assertTrue(clusterStatus.getReduceTasks() >= 0);
-    assertTrue(clusterStatus.getMaxMapTasks() >= 0);
-    assertTrue(clusterStatus.getMaxReduceTasks() >= 0);
-    assertTrue(clusterStatus.getState() == JobTrackerState.INITIALIZING
-        || clusterStatus.getState() == JobTrackerState.RUNNING);
-  }
-
-  /**
-   *
-   */
-  public void testFetch() throws Exception {
-    // create and populate a table with 500 rows.
-    try {
-      client.execute("set hive.support.concurrency = false");
-      client.execute("drop table " + tableName);
-    } catch (Exception ex) {
-    }
-    client.execute("create table " + tableName + " (key int, value string)");
-    client.execute("load data local inpath '" + dataFilePath.toString()
-        + "' into table " + tableName);
-
-    try {
-      // fetchAll test
-      client.execute("select key, value from " + tableName);
-      assertEquals(client.fetchAll().size(), 500);
-      assertEquals(client.fetchAll().size(), 0);
-
-      // fetchOne test
-      client.execute("select key, value from " + tableName);
-      for (int i = 0; i < 500; i++) {
-        try {
-          String str = client.fetchOne();
-        } catch (HiveServerException e) {
-          assertTrue(false);
-        }
-      }
-      try {
-        client.fetchOne();
-      } catch (HiveServerException e) {
-        assertEquals(e.getErrorCode(), 0);
-      }
-
-      // fetchN test
-      client.execute("select key, value from " + tableName);
-      assertEquals(client.fetchN(499).size(), 499);
-      assertEquals(client.fetchN(499).size(), 1);
-      assertEquals(client.fetchN(499).size(), 0);
-    } catch (Throwable e) {
-      e.printStackTrace();
-    }
-  }
-
-  public void testDynamicSerde() throws Exception {
-    try {
-      client.execute("set hive.support.concurrency = false");
-      client.execute("drop table " + tableName);
-    } catch (Exception ex) {
-    }
-
-    client.execute("create table " + tableName + " (key int, value string)");
-    client.execute("load data local inpath '" + dataFilePath.toString()
-        + "' into table " + tableName);
-    // client.execute("select key, count(1) from " + tableName +
-    // " where key > 10 group by key");
-    String sql = "select key, value from " + tableName + " where key > 10";
-    client.execute(sql);
-
-    // Instantiate DynamicSerDe
-    DynamicSerDe ds = new DynamicSerDe();
-    Properties dsp = new Properties();
-    dsp.setProperty(serdeConstants.SERIALIZATION_FORMAT,
-        org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.class
-            .getName());
-    dsp.setProperty(
-        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,
-        "result");
-    String serDDL = new String("struct result { ");
-    List<FieldSchema> schema = client.getThriftSchema().getFieldSchemas();
-    for (int pos = 0; pos < schema.size(); pos++) {
-      if (pos != 0) {
-        serDDL = serDDL.concat(",");
-      }
-      serDDL = serDDL.concat(schema.get(pos).getType());
-      serDDL = serDDL.concat(" ");
-      serDDL = serDDL.concat(schema.get(pos).getName());
-    }
-    serDDL = serDDL.concat("}");
-
-    dsp.setProperty(serdeConstants.SERIALIZATION_DDL, serDDL);
-    dsp.setProperty(serdeConstants.SERIALIZATION_LIB, ds.getClass().toString());
-    dsp.setProperty(serdeConstants.FIELD_DELIM, "9");
-    ds.initialize(new Configuration(), dsp);
-
-    String row = client.fetchOne();
-    Object o = ds.deserialize(new BytesWritable(row.getBytes()));
-
-    assertEquals(o.getClass().toString(), "class java.util.ArrayList");
-    List<?> lst = (List<?>) o;
-    assertEquals(lst.get(0), 238);
-
-    // TODO: serde doesn't like underscore -- struct result { string _c0}
-    sql = "select count(1) as c from " + tableName;
-    client.execute(sql);
-    row = client.fetchOne();
-
-    serDDL = new String("struct result { ");
-    schema = client.getThriftSchema().getFieldSchemas();
-    for (int pos = 0; pos < schema.size(); pos++) {
-      if (pos != 0) {
-        serDDL = serDDL.concat(",");
-      }
-      serDDL = serDDL.concat(schema.get(pos).getType());
-      serDDL = serDDL.concat(" ");
-      serDDL = serDDL.concat(schema.get(pos).getName());
-    }
-    serDDL = serDDL.concat("}");
-
-    dsp.setProperty(serdeConstants.SERIALIZATION_DDL, serDDL);
-    // Need a new DynamicSerDe instance - re-initialization is not supported.
-    ds = new DynamicSerDe();
-    ds.initialize(new Configuration(), dsp);
-    o = ds.deserialize(new BytesWritable(row.getBytes()));
-  }
-
-  public void testAddJarShouldFailIfJarNotExist() throws Exception {
-    boolean queryExecutionFailed = false;
-    try {
-      client.execute("add jar " + invalidPath + "sample.jar");
-    } catch (Exception e) {
-      queryExecutionFailed = true;
-    }
-    if (!queryExecutionFailed) {
-      fail("It should throw exception since jar does not exist");
-    }
-  }
-
-  public void testAddFileShouldFailIfFileNotExist() throws Exception {
-    boolean queryExecutionFailed = false;
-    try {
-      client.execute("add file " + invalidPath + "sample.txt");
-    } catch (Exception e) {
-      queryExecutionFailed = true;
-    }
-    if (!queryExecutionFailed) {
-      fail("It should throw exception since file does not exist");
-    }
-  }
-
-  public void testAddArchiveShouldFailIfFileNotExist() throws Exception {
-    boolean queryExecutionFailed = false;
-    try {
-      client.execute("add archive " + invalidPath + "sample.zip");
-    } catch (Exception e) {
-      queryExecutionFailed = true;
-    }
-    if (!queryExecutionFailed) {
-      fail("It should trow exception since archive does not exist");
-    }
-  }
-
-  public void testScratchDirShouldNotClearWhileStartup() throws Exception {
-    FileSystem fs = FileSystem.get(conf);
-    Path scratchDirPath = new Path(HiveConf.getVar(conf,
-        HiveConf.ConfVars.SCRATCHDIR));
-    boolean fileExists = fs.exists(scratchDirPath);
-    if (!fileExists) {
-      fileExists = fs.mkdirs(scratchDirPath);
-    }
-    ServerUtils.cleanUpScratchDir(conf);
-    assertTrue("Scratch dir is not available after startup", fs.exists(scratchDirPath));
-  }
-
-  public void testScratchDirShouldClearWhileStartup() throws Exception {
-    FileSystem fs = FileSystem.get(conf);
-    Path scratchDirPath = new Path(HiveConf.getVar(conf,
-        HiveConf.ConfVars.SCRATCHDIR));
-    boolean fileExists = fs.exists(scratchDirPath);
-    if (!fileExists) {
-      fileExists = fs.mkdirs(scratchDirPath);
-    }
-    try {
-      conf.setBoolVar(HiveConf.ConfVars.HIVE_START_CLEANUP_SCRATCHDIR, true);
-      ServerUtils.cleanUpScratchDir(conf);
-    } finally {
-      conf.setBoolVar(HiveConf.ConfVars.HIVE_START_CLEANUP_SCRATCHDIR, false);
-    }
-    assertFalse("Scratch dir is available after startup", fs.exists(scratchDirPath));
-  }
-
-}
diff --git a/src/service/src/test/org/apache/hive/service/auth/TestCustomAuthentication.java b/src/service/src/test/org/apache/hive/service/auth/TestCustomAuthentication.java
deleted file mode 100644
index ece54a8..0000000
--- a/src/service/src/test/org/apache/hive/service/auth/TestCustomAuthentication.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hive.service.auth;
-
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hive.service.server.HiveServer2;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import javax.security.sasl.AuthenticationException;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.sql.Connection;
-import java.sql.DriverManager;
-import java.sql.SQLException;
-import java.util.HashMap;
-import java.util.Map;
-
-public class TestCustomAuthentication {
-
-  private static HiveServer2 hiveserver2;
-  private static HiveConf hiveConf;
-  private static byte[] hiveConfBackup;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    hiveConf = new HiveConf();
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    hiveConf.writeXml(baos);
-    baos.close();
-    hiveConfBackup = baos.toByteArray();
-    hiveConf.set("hive.server2.authentication", "CUSTOM");
-    hiveConf.set("hive.server2.custom.authentication.class",
-        "org.apache.hive.service.auth.TestCustomAuthentication$SimpleAuthenticationProviderImpl");
-    FileOutputStream fos = new FileOutputStream(new File(hiveConf.getHiveSiteLocation().toURI()));
-    hiveConf.writeXml(fos);
-    fos.close();
-    hiveserver2 = new HiveServer2();
-    hiveserver2.init(hiveConf);
-    hiveserver2.start();
-    Thread.sleep(1000);
-    System.out.println("hiveServer2 start ......");
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    if(hiveConf != null && hiveConfBackup != null) {
-      FileOutputStream fos = new FileOutputStream(new File(hiveConf.getHiveSiteLocation().toURI()));
-      fos.write(hiveConfBackup);
-      fos.close();
-    }
-    if (hiveserver2 != null) {
-      hiveserver2.stop();
-      hiveserver2 = null;
-    }
-    Thread.sleep(1000);
-    System.out.println("hiveServer2 stop ......");
-  }
-
-  @Test
-  public void testCustomAuthentication() throws Exception {
-
-    String url = "jdbc:hive2://localhost:10000/default";
-    Class.forName("org.apache.hive.jdbc.HiveDriver");
-
-    try {
-      DriverManager.getConnection(url, "wronguser", "pwd");
-      Assert.fail("Expected Exception");
-    } catch(SQLException e) {
-      Assert.assertNotNull(e.getMessage());
-      Assert.assertTrue(e.getMessage(), e.getMessage().contains("Peer indicated failure: Error validating the login"));
-    }
-
-    Connection connection = DriverManager.getConnection(url, "hiveuser", "hive");
-    connection.close();
-
-    System.out.println(">>> PASSED testCustomAuthentication");
-  }
-
-  public static class SimpleAuthenticationProviderImpl implements PasswdAuthenticationProvider {
-
-    private Map<String, String> userMap = new HashMap<String, String>();
-
-    public SimpleAuthenticationProviderImpl() {
-      init();
-    }
-
-    private void init(){
-      userMap.put("hiveuser","hive");
-    }
-
-    @Override
-    public void Authenticate(String user, String password) throws AuthenticationException {
-
-      if(!userMap.containsKey(user)){
-        throw new AuthenticationException("Invalid user : "+user);
-      }
-      if(!userMap.get(user).equals(password)){
-        throw new AuthenticationException("Invalid passwd : "+password);
-      }
-    }
-  }
-}
diff --git a/src/service/src/test/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java b/src/service/src/test/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java
deleted file mode 100644
index ebda296..0000000
--- a/src/service/src/test/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hive.service.cli;
-
-import org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService;
-import org.apache.hive.service.cli.thrift.ThriftCLIService;
-import org.apache.hive.service.cli.thrift.ThriftCLIServiceClient;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.BeforeClass;
-
-/**
- * TestEmbeddedThriftBinaryCLIService.
- *
- */
-public class TestEmbeddedThriftBinaryCLIService extends CLIServiceTest {
-  protected static ThriftCLIService service;
-
-  @BeforeClass
-  public static void setUpBeforeClass() throws Exception {
-    service = new EmbeddedThriftBinaryCLIService();
-    client = new ThriftCLIServiceClient(service);
-  }
-
-  /* (non-Javadoc)
-   * @see org.apache.hive.service.cli.CLIServiceTest#setUp()
-   */
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-  }
-
-  /* (non-Javadoc)
-   * @see org.apache.hive.service.cli.CLIServiceTest#tearDown()
-   */
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-}
diff --git a/src/service/src/test/org/apache/hive/service/cli/thrift/TestThriftBinaryCLIService.java b/src/service/src/test/org/apache/hive/service/cli/thrift/TestThriftBinaryCLIService.java
deleted file mode 100644
index a632277..0000000
--- a/src/service/src/test/org/apache/hive/service/cli/thrift/TestThriftBinaryCLIService.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hive.service.cli.thrift;
-
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.fail;
-
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hive.service.auth.HiveAuthFactory.AuthTypes;
-import org.apache.thrift.transport.TTransport;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-
-
-/**
- *
- * TestThriftBinaryCLIService.
- * This tests ThriftCLIService started in binary mode.
- *
- */
-
-public class TestThriftBinaryCLIService extends ThriftCLIServiceTest {
-
-  private static String transportMode = "binary";
-  private static TTransport transport;
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @BeforeClass
-  public static void setUpBeforeClass() throws Exception {
-    // Set up the base class
-    ThriftCLIServiceTest.setUpBeforeClass();
-
-    assertNotNull(port);
-    assertNotNull(hiveServer2);
-    assertNotNull(hiveConf);
-
-    hiveConf.setBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS, false);
-    hiveConf.setVar(ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST, host);
-    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_PORT, port);
-    hiveConf.setVar(ConfVars.HIVE_SERVER2_AUTHENTICATION, AuthTypes.NOSASL.toString());
-    hiveConf.setVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE, transportMode);
-
-    startHiveServer2WithConf(hiveConf);
-
-    // Open a binary transport
-    // Fail if the transport doesn't open
-    transport = createBinaryTransport();
-    try {
-      transport.open();
-    }
-    catch (Exception e) {
-      fail("Exception: " + e);
-    }
-  }
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @AfterClass
-  public static void tearDownAfterClass() throws Exception {
-    ThriftCLIServiceTest.tearDownAfterClass();
-  }
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    // Create and set the client
-    initClient(transport);
-    assertNotNull(client);
-  }
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @Override
-  @After
-  public void tearDown() throws Exception {
-
-  }
-
-
-}
\ No newline at end of file
diff --git a/src/service/src/test/org/apache/hive/service/cli/thrift/TestThriftHttpCLIService.java b/src/service/src/test/org/apache/hive/service/cli/thrift/TestThriftHttpCLIService.java
deleted file mode 100644
index 65177dd..0000000
--- a/src/service/src/test/org/apache/hive/service/cli/thrift/TestThriftHttpCLIService.java
+++ /dev/null
@@ -1,216 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hive.service.cli.thrift;
-
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.fail;
-
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hive.jdbc.HttpBasicAuthInterceptor;
-import org.apache.hive.service.auth.HiveAuthFactory.AuthTypes;
-import org.apache.hive.service.server.HiveServer2;
-import org.apache.http.impl.client.DefaultHttpClient;
-import org.apache.thrift.transport.THttpClient;
-import org.apache.thrift.transport.TTransport;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
-*
-* TestThriftHttpCLIService.
-* This tests ThriftCLIService started in http mode.
-*
-*/
-
-public class TestThriftHttpCLIService extends ThriftCLIServiceTest {
-
-  private static String transportMode = "http";
-  private static String thriftHttpPath = "cliservice";
-  private static TTransport transport;
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @BeforeClass
-  public static void setUpBeforeClass() throws Exception {
-    // Set up the base class
-    ThriftCLIServiceTest.setUpBeforeClass();
-
-    assertNotNull(port);
-    assertNotNull(hiveServer2);
-    assertNotNull(hiveConf);
-
-    hiveConf.setBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS, false);
-    hiveConf.setVar(ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST, host);
-    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_HTTP_PORT, port);
-    hiveConf.setVar(ConfVars.HIVE_SERVER2_AUTHENTICATION, AuthTypes.NOSASL.toString());
-    hiveConf.setVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE, transportMode);
-    hiveConf.setVar(ConfVars.HIVE_SERVER2_THRIFT_HTTP_PATH, thriftHttpPath);
-
-    startHiveServer2WithConf(hiveConf);
-
-    // Open an http transport
-    // Fail if the transport doesn't open
-    transport = createHttpTransport();
-    try {
-      transport.open();
-    }
-    catch (Exception e) {
-      fail("Exception: " + e);
-    }
-  }
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @AfterClass
-  public static void tearDownAfterClass() throws Exception {
-    ThriftCLIServiceTest.tearDownAfterClass();
-  }
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    // Create and set the client before every test from the transport
-    initClient(transport);
-    assertNotNull(client);
-  }
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @Override
-  @After
-  public void tearDown() throws Exception {
-
-  }
-
-  @Test
-  public void testIncompatibeClientServer() throws Exception {
-    // A binary client communicating with an http server should throw an exception
-    // Close the older http client transport
-    // The server is already running in Http mode
-    if (transport != null) {
-      transport.close();
-    }
-    // Create a binary transport and init the client
-    transport = createBinaryTransport();
-    // Create and set the client
-    initClient(transport);
-    assertNotNull(client);
-
-    // This will throw an expected exception since client-server modes are incompatible
-    testOpenSessionExpectedException();
-
-    // Close binary client transport
-    if (transport != null) {
-      transport.close();
-    }
-    // Create http transport (client is inited in setUp before every test from the transport)
-    transport = createHttpTransport();
-    try {
-      transport.open();
-    }
-    catch (Exception e) {
-      fail("Exception: " + e);
-    }
-  }
-
-  @Test
-  public void testIncorrectHttpPath() throws Exception {
-    // Close the older http client transport
-    if (transport != null) {
-      transport.close();
-    }
-    // Create an http transport with incorrect http path endpoint
-    thriftHttpPath = "wrong_path";
-    transport = createHttpTransport();
-    // Create and set the client
-    initClient(transport);
-    assertNotNull(client);
-
-    // This will throw an expected exception since
-    // client is communicating with the wrong http service endpoint
-    testOpenSessionExpectedException();
-
-    // Close incorrect client transport
-    // Reinit http client transport
-    thriftHttpPath = "cliservice";
-    if (transport != null) {
-      transport.close();
-    }
-    transport = createHttpTransport();
-    try {
-      transport.open();
-    }
-    catch (Exception e) {
-      fail("Exception: " + e);
-    }
-  }
-
-
-  private void testWithAuthMode(AuthTypes authType) throws Exception {
-    // Stop and restart HiveServer2 in given incorrect auth mode
-    stopHiveServer2();
-    hiveConf.setVar(ConfVars.HIVE_SERVER2_AUTHENTICATION, authType.toString());
-    hiveServer2 = new HiveServer2();
-    // HiveServer2 in Http mode will not start using KERBEROS/LDAP/CUSTOM auth types
-    startHiveServer2WithConf(hiveConf);
-
-    // This will throw an expected exception since Http server is not running
-    testOpenSessionExpectedException();
-
-    // Stop and restart back with the original config
-    stopHiveServer2();
-    hiveConf.setVar(ConfVars.HIVE_SERVER2_AUTHENTICATION, AuthTypes.NOSASL.toString());
-    hiveServer2 = new HiveServer2();
-    startHiveServer2WithConf(hiveConf);
-  }
-
-  @Test
-  public void testKerberosMode()  throws Exception {
-    testWithAuthMode(AuthTypes.KERBEROS);
-  }
-
-  @Test
-  public void testLDAPMode()  throws Exception {
-    testWithAuthMode(AuthTypes.LDAP);
-  }
-
-  @Test
-  public void testCustomMode()  throws Exception {
-    testWithAuthMode(AuthTypes.CUSTOM);
-  }
-
-  private static TTransport createHttpTransport() throws Exception {
-    DefaultHttpClient httpClient = new DefaultHttpClient();
-    String httpUrl = transportMode + "://" + host + ":" + port +
-        "/" + thriftHttpPath + "/";
-    httpClient.addRequestInterceptor(
-        new HttpBasicAuthInterceptor(anonymousUser, anonymousPasswd));
-    return new THttpClient(httpUrl, httpClient);
-  }
-
-}
\ No newline at end of file
diff --git a/src/service/src/test/org/apache/hive/service/server/TestHS2ThreadAllocation.java b/src/service/src/test/org/apache/hive/service/server/TestHS2ThreadAllocation.java
deleted file mode 100644
index 225bf20..0000000
--- a/src/service/src/test/org/apache/hive/service/server/TestHS2ThreadAllocation.java
+++ /dev/null
@@ -1,256 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-//The tests here are heavily based on some timing, so there is some chance to fail.
-package org.apache.hive.service.server;
-
-import java.sql.ResultSet;
-import java.sql.SQLException;
-import java.sql.Statement;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Properties;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext;
-import org.apache.hadoop.hive.ql.hooks.HookContext;
-import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
-import org.apache.hive.jdbc.HiveConnection;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestHS2ThreadAllocation {
-  // Hook to verify ipaddress
-  public static class IpHookImpl implements ExecuteWithHookContext {
-    public static String userName = "";
-    public static String ipAddress = "";
-    public void run(HookContext hookContext) {
-      if (hookContext.getHookType().equals(HookType.POST_EXEC_HOOK)) {
-        Assert.assertNotNull(hookContext.getIpAddress(), "IP Address is null");
-        ipAddress = hookContext.getIpAddress();
-        Assert.assertNotNull(hookContext.getUserName(), "Username is null");
-        userName = hookContext.getUserName();
-      }
-    }
-  }
-
-  private static HiveServer2 hiveServer2;
-  private static final int MAX_THREADS = 10;
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @BeforeClass
-  public static void setUpBeforeClass() throws Exception {
-    HiveConf hiveConf = new HiveConf();
-    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_MIN_WORKER_THREADS, 1);
-    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_MAX_WORKER_THREADS, MAX_THREADS);
-    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_LOGIN_TIMEOUT, 2);
-
-    hiveServer2 = new HiveServer2();
-    hiveServer2.init(hiveConf);
-    hiveServer2.start();
-    Thread.sleep(2000);
-  }
-
-  @AfterClass
-  public static void tearDownAfterClass() throws Exception {
-    if (hiveServer2 != null) {
-      hiveServer2.stop();
-    }
-  }
-
-  class QueryRunner implements Runnable {
-
-    private int waitTime = 0;
-
-    QueryRunner(int delay) {
-      waitTime = delay;
-    }
-
-    QueryRunner() {
-    }
-
-
-    @Override
-    public void run() {
-
-      HiveConnection connection = null;
-      Statement statement = null;
-      try {
-        connection = new HiveConnection("jdbc:hive2://localhost:10000/default", new Properties());
-
-        statement = connection.createStatement();
-        if (statement.execute("show tables")) {
-          ResultSet results = statement.getResultSet();
-          while (results.next()) {
-            System.out.println(results.getString(1));
-          }
-          results.close();
-        }
-        statement.close();
-
-        if (waitTime > 0) {
-          Thread.sleep(waitTime);
-        }
-
-        statement = connection.createStatement();
-        if (statement.execute("show tables")) {
-          ResultSet results = statement.getResultSet();
-          while (results.next()) {
-            System.out.println(results.getString(1));
-          }
-          results.close();
-        }
-        statement.close();
-        connection.close();
-
-      } catch (SQLException e) {
-        e.printStackTrace();
-
-      } catch (InterruptedException e) {
-        e.printStackTrace();
-
-      } finally {
-        try {
-          if (statement != null) {
-            statement.close();
-          }
-
-          if (connection != null) {
-            connection.close();
-          }
-        } catch (Throwable e) {
-          e.printStackTrace();
-        }
-      }
-    }
-  }
-
-  @Test
-  public void testConnection() throws SQLException {
-    int i = MAX_THREADS;
-    for (; i > 0; i--) {
-
-      HiveConnection connection = new HiveConnection("jdbc:hive2://localhost:10000/default", new Properties());
-      Statement statement = connection.createStatement();
-
-      if (statement.execute("show tables")) {
-        ResultSet results = statement.getResultSet();
-        while (results.next()) {
-          System.out.println(results.getString(1));
-        }
-        results.close();
-      }
-      statement.close();
-      connection.close();
-    }
-    Assert.assertEquals(0, i);
-  }
-
-
-  @Test
-  public void testParallelConnections() throws InterruptedException {
-    List<Thread> threadList = new ArrayList<Thread>(10);
-    for (int i = 0; i < MAX_THREADS; i++) {
-      Thread thread = new Thread(new QueryRunner());
-      thread.setName("HiveConnectionTest-" + i);
-      thread.start();
-      threadList.add(thread);
-      Thread.sleep(100);
-    }
-
-    // Wait for all threads to complete/die
-    for (Thread thread : threadList) {
-      thread.join();
-      System.out.println(thread.getName() + " " + thread.isAlive());
-    }
-  }
-
-  @Test
-  public void testHS2StabilityOnLargeConnections() throws InterruptedException {
-    List<Thread> threadList = new ArrayList<Thread>(10);
-    HiveConnection connection = null;
-
-    for (int i = 0; i < MAX_THREADS; i++) {
-      Thread thread = new Thread(new QueryRunner(5000));
-      thread.setName("HiveConnectionTest-" + i);
-      threadList.add(thread);
-      thread.start();
-    }
-
-    // Above threads should have exploited the threads in HS2
-    Thread.sleep(100);
-    // Create another thread and see if the connection goes through.
-
-    boolean connectionFailed = false;
-    try {
-      connection = new HiveConnection("jdbc:hive2://localhost:10000/default", new Properties());
-    } catch (SQLException e) {
-      //e.printStackTrace();
-      System.out.println("Expected connection failure:" + e.getMessage());
-      connectionFailed = true;
-    }
-    Assert.assertEquals(connectionFailed, true);
-
-    // Hope all threads should be active at this time  -- based on timing, not a good test
-    for (Thread thread : threadList) {
-      Assert.assertEquals(thread.isAlive(), true);
-    }
-
-    // Wait for all threads to complete/die
-    for (Thread thread : threadList) {
-      System.out.println(thread.getName() + " " + thread.isAlive());
-      thread.join();
-    }
-
-    // Check if a new connection is possible
-    try {
-      connection = new HiveConnection("jdbc:hive2://localhost:10000/default", new Properties());
-    } catch (SQLException e) {
-      e.printStackTrace();
-      Assert.fail("Cannot create connection with free threads");
-    }
-    Assert.assertNotNull(connection);
-    try {
-      connection.close();
-    } catch (SQLException e) {
-      e.printStackTrace();
-      Assert.fail("Something went wrong with connection closure");
-    }
-  }
-
-  @Test
-  public void testExecuteStatementWithHook() throws Exception {
-    Properties connProp = new Properties();
-    connProp.setProperty("user", System.getProperty("user.name"));
-    connProp.setProperty("password", "");
-    HiveConnection connection = new HiveConnection("jdbc:hive2://localhost:10000/default", connProp);
-    connection.createStatement().execute("SET hive.exec.post.hooks =  " + IpHookImpl.class.getName());
-    connection.createStatement().execute("show tables");
-    Assert.assertEquals(System.getProperty("user.name"), IpHookImpl.userName);
-    Assert.assertTrue(IpHookImpl.ipAddress.contains("127.0.0.1"));
-    connection.close();
-  }
-
-}
-
diff --git a/src/service/src/test/org/apache/hive/service/server/TestHiveServer2Concurrency.java b/src/service/src/test/org/apache/hive/service/server/TestHiveServer2Concurrency.java
deleted file mode 100644
index 2231f40..0000000
--- a/src/service/src/test/org/apache/hive/service/server/TestHiveServer2Concurrency.java
+++ /dev/null
@@ -1,103 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hive.service.server;
-
-import static org.junit.Assert.fail;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.QTestUtil;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * TestHiveServer2Concurrency.
- *
- */
-public class TestHiveServer2Concurrency {
-
-  private static QTestUtil.QTestSetup miniZKCluster = null;
-  private static HiveServer2 hiveServer2;
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @BeforeClass
-  public static void setUpBeforeClass() throws Exception {
-    HiveConf hiveConf = new HiveConf();
-
-    miniZKCluster = new QTestUtil.QTestSetup();
-    miniZKCluster.preTest(hiveConf);
-
-    hiveServer2 = new HiveServer2();
-    hiveServer2.init(hiveConf);
-    hiveServer2.start();
-    Thread.sleep(5000);
-  }
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @AfterClass
-  public static void tearDownAfterClass() throws Exception {
-    if (hiveServer2 != null) {
-      hiveServer2.stop();
-    }
-    if (miniZKCluster != null) {
-      try {
-        miniZKCluster.tearDown();
-      } catch (Exception e) {
-        e.printStackTrace();
-      }
-    }
-  }
-
-  class QueryRunner implements Runnable {
-
-    @Override
-    public void run() {
-      // TODO Auto-generated method stub
-
-    }
-
-  }
-
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @Before
-  public void setUp() throws Exception {
-  }
-
-  /**
-   * @throws java.lang.Exception
-   */
-  @After
-  public void tearDown() throws Exception {
-  }
-
-  @Test
-  public void test() {
-    fail("Not yet implemented");
-  }
-
-}
diff --git a/src/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java b/src/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
new file mode 100644
index 0000000..7bf5293
--- /dev/null
+++ b/src/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
@@ -0,0 +1,762 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.shims;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.lang.reflect.Constructor;
+import java.net.MalformedURLException;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.net.URL;
+import java.security.PrivilegedActionException;
+import java.security.PrivilegedExceptionAction;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import javax.security.auth.Subject;
+import javax.security.auth.login.LoginException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.fs.ProxyFileSystem;
+import org.apache.hadoop.fs.Trash;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobContext;
+import org.apache.hadoop.mapred.JobStatus;
+import org.apache.hadoop.mapred.MiniMRCluster;
+import org.apache.hadoop.mapred.OutputCommitter;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.hadoop.mapred.TaskAttemptContext;
+import org.apache.hadoop.mapred.TaskCompletionEvent;
+import org.apache.hadoop.mapred.TaskID;
+import org.apache.hadoop.mapred.TaskLogServlet;
+import org.apache.hadoop.mapred.lib.CombineFileInputFormat;
+import org.apache.hadoop.mapred.lib.CombineFileSplit;
+import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobID;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.UnixUserGroupInformation;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.tools.HadoopArchives;
+import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.util.VersionInfo;
+
+/**
+ * Implemention of shims against Hadoop 0.20.0.
+ */
+public class Hadoop20Shims implements HadoopShims {
+
+  public boolean usesJobShell() {
+    return false;
+  }
+
+  public boolean fileSystemDeleteOnExit(FileSystem fs, Path path)
+      throws IOException {
+
+    return fs.deleteOnExit(path);
+  }
+
+  public void inputFormatValidateInput(InputFormat fmt, JobConf conf)
+      throws IOException {
+    // gone in 0.18+
+  }
+
+  public boolean isJobPreparing(RunningJob job) throws IOException {
+    return job.getJobState() == JobStatus.PREP;
+  }
+  /**
+   * Workaround for hadoop-17 - jobclient only looks at commandlineconfig.
+   */
+  public void setTmpFiles(String prop, String files) {
+    // gone in 20+
+  }
+
+   public String getKerberosShortName(String kerberosName) throws IOException {
+    // raise an exception
+    throw new IOException("Authentication is not supported with 0.20");
+  }
+
+
+  /**
+   * Returns a shim to wrap MiniMrCluster
+   */
+  public MiniMrShim getMiniMrCluster(Configuration conf, int numberOfTaskTrackers,
+                                     String nameNode, int numDir) throws IOException {
+    return new MiniMrShim(conf, numberOfTaskTrackers, nameNode, numDir);
+  }
+
+  /**
+   * Shim for MiniMrCluster
+   */
+  public class MiniMrShim implements HadoopShims.MiniMrShim {
+
+    private final MiniMRCluster mr;
+
+    public MiniMrShim(Configuration conf, int numberOfTaskTrackers,
+        String nameNode, int numDir) throws IOException {
+      this.mr = new MiniMRCluster(numberOfTaskTrackers, nameNode, numDir);
+    }
+
+    @Override
+    public int getJobTrackerPort() throws UnsupportedOperationException {
+      return mr.getJobTrackerPort();
+    }
+
+    @Override
+    public void shutdown() throws IOException {
+      mr.shutdown();
+    }
+
+    @Override
+    public void setupConfiguration(Configuration conf) {
+      setJobLauncherRpcAddress(conf, "localhost:" + mr.getJobTrackerPort());
+    }
+  }
+
+  public HadoopShims.MiniDFSShim getMiniDfs(Configuration conf,
+      int numDataNodes,
+      boolean format,
+      String[] racks) throws IOException {
+    return new MiniDFSShim(new MiniDFSCluster(conf, numDataNodes, format, racks));
+  }
+
+  /**
+   * MiniDFSShim.
+   *
+   */
+  public class MiniDFSShim implements HadoopShims.MiniDFSShim {
+    private final MiniDFSCluster cluster;
+
+    public MiniDFSShim(MiniDFSCluster cluster) {
+      this.cluster = cluster;
+    }
+
+    public FileSystem getFileSystem() throws IOException {
+      return cluster.getFileSystem();
+    }
+
+    public void shutdown() {
+      cluster.shutdown();
+    }
+  }
+
+  /**
+   * We define this function here to make the code compatible between
+   * hadoop 0.17 and hadoop 0.20.
+   *
+   * Hive binary that compiled Text.compareTo(Text) with hadoop 0.20 won't
+   * work with hadoop 0.17 because in hadoop 0.20, Text.compareTo(Text) is
+   * implemented in org.apache.hadoop.io.BinaryComparable, and Java compiler
+   * references that class, which is not available in hadoop 0.17.
+   */
+  public int compareText(Text a, Text b) {
+    return a.compareTo(b);
+  }
+
+  @Override
+  public long getAccessTime(FileStatus file) {
+    return file.getAccessTime();
+  }
+
+  public HadoopShims.CombineFileInputFormatShim getCombineFileInputFormat() {
+    return new CombineFileInputFormatShim() {
+      @Override
+      public RecordReader getRecordReader(InputSplit split,
+          JobConf job, Reporter reporter) throws IOException {
+        throw new IOException("CombineFileInputFormat.getRecordReader not needed.");
+      }
+    };
+  }
+
+  public void setTotalOrderPartitionFile(JobConf jobConf, Path partitionFile){
+    TotalOrderPartitioner.setPartitionFile(jobConf, partitionFile);
+  }
+
+  public static class InputSplitShim extends CombineFileSplit implements HadoopShims.InputSplitShim {
+    long shrinkedLength;
+    boolean _isShrinked;
+    public InputSplitShim() {
+      super();
+      _isShrinked = false;
+    }
+
+    public InputSplitShim(CombineFileSplit old) throws IOException {
+      super(old.getJob(), old.getPaths(), old.getStartOffsets(),
+          old.getLengths(), dedup(old.getLocations()));
+      _isShrinked = false;
+    }
+
+    private static String[] dedup(String[] locations) {
+      Set<String> dedup = new HashSet<String>();
+      Collections.addAll(dedup, locations);
+      return dedup.toArray(new String[dedup.size()]);
+    }
+
+    @Override
+    public void shrinkSplit(long length) {
+      _isShrinked = true;
+      shrinkedLength = length;
+    }
+
+    public boolean isShrinked() {
+      return _isShrinked;
+    }
+
+    public long getShrinkedLength() {
+      return shrinkedLength;
+    }
+
+    @Override
+    public void readFields(DataInput in) throws IOException {
+      super.readFields(in);
+      _isShrinked = in.readBoolean();
+      if (_isShrinked) {
+        shrinkedLength = in.readLong();
+      }
+    }
+
+    @Override
+    public void write(DataOutput out) throws IOException {
+      super.write(out);
+      out.writeBoolean(_isShrinked);
+      if (_isShrinked) {
+        out.writeLong(shrinkedLength);
+      }
+    }
+  }
+
+  /* This class should be replaced with org.apache.hadoop.mapred.lib.CombineFileRecordReader class, once
+   * https://issues.apache.org/jira/browse/MAPREDUCE-955 is fixed. This code should be removed - it is a copy
+   * of org.apache.hadoop.mapred.lib.CombineFileRecordReader
+   */
+  public static class CombineFileRecordReader<K, V> implements RecordReader<K, V> {
+
+    static final Class[] constructorSignature = new Class[] {
+        InputSplit.class,
+        Configuration.class,
+        Reporter.class,
+        Integer.class
+        };
+
+    protected CombineFileSplit split;
+    protected JobConf jc;
+    protected Reporter reporter;
+    protected Class<RecordReader<K, V>> rrClass;
+    protected Constructor<RecordReader<K, V>> rrConstructor;
+    protected FileSystem fs;
+
+    protected int idx;
+    protected long progress;
+    protected RecordReader<K, V> curReader;
+    protected boolean isShrinked;
+    protected long shrinkedLength;
+
+    public boolean next(K key, V value) throws IOException {
+
+      while ((curReader == null)
+          || !doNextWithExceptionHandler((K) ((CombineHiveKey) key).getKey(),
+              value)) {
+        if (!initNextRecordReader(key)) {
+          return false;
+        }
+      }
+      return true;
+    }
+
+    public K createKey() {
+      K newKey = curReader.createKey();
+      return (K)(new CombineHiveKey(newKey));
+    }
+
+    public V createValue() {
+      return curReader.createValue();
+    }
+
+    /**
+     * Return the amount of data processed.
+     */
+    public long getPos() throws IOException {
+      return progress;
+    }
+
+    public void close() throws IOException {
+      if (curReader != null) {
+        curReader.close();
+        curReader = null;
+      }
+    }
+
+    /**
+     * Return progress based on the amount of data processed so far.
+     */
+    public float getProgress() throws IOException {
+      long subprogress = 0;    // bytes processed in current split
+      if (null != curReader) {
+        // idx is always one past the current subsplit's true index.
+        subprogress = (long)(curReader.getProgress() * split.getLength(idx - 1));
+      }
+      return Math.min(1.0f, (progress + subprogress) / (float) (split.getLength()));
+    }
+
+    /**
+     * A generic RecordReader that can hand out different recordReaders
+     * for each chunk in the CombineFileSplit.
+     */
+    public CombineFileRecordReader(JobConf job, CombineFileSplit split,
+        Reporter reporter,
+        Class<RecordReader<K, V>> rrClass)
+        throws IOException {
+      this.split = split;
+      this.jc = job;
+      this.rrClass = rrClass;
+      this.reporter = reporter;
+      this.idx = 0;
+      this.curReader = null;
+      this.progress = 0;
+
+      isShrinked = false;
+
+      assert (split instanceof InputSplitShim);
+      if (((InputSplitShim) split).isShrinked()) {
+        isShrinked = true;
+        shrinkedLength = ((InputSplitShim) split).getShrinkedLength();
+      }
+
+      try {
+        rrConstructor = rrClass.getDeclaredConstructor(constructorSignature);
+        rrConstructor.setAccessible(true);
+      } catch (Exception e) {
+        throw new RuntimeException(rrClass.getName() +
+            " does not have valid constructor", e);
+      }
+      initNextRecordReader(null);
+    }
+
+    /**
+     * do next and handle exception inside it.
+     * @param key
+     * @param value
+     * @return
+     * @throws IOException
+     */
+    private boolean doNextWithExceptionHandler(K key, V value) throws IOException {
+      try {
+        return curReader.next(key, value);
+      } catch (Exception e) {
+        return HiveIOExceptionHandlerUtil.handleRecordReaderNextException(e, jc);
+      }
+    }
+
+    /**
+     * Get the record reader for the next chunk in this CombineFileSplit.
+     */
+    protected boolean initNextRecordReader(K key) throws IOException {
+
+      if (curReader != null) {
+        curReader.close();
+        curReader = null;
+        if (idx > 0) {
+          progress += split.getLength(idx - 1); // done processing so far
+        }
+      }
+
+      // if all chunks have been processed or reached the length, nothing more to do.
+      if (idx == split.getNumPaths() || (isShrinked && progress > shrinkedLength)) {
+        return false;
+      }
+
+      // get a record reader for the idx-th chunk
+      try {
+        curReader = rrConstructor.newInstance(new Object[]
+            {split, jc, reporter, Integer.valueOf(idx)});
+
+        // change the key if need be
+        if (key != null) {
+          K newKey = curReader.createKey();
+          ((CombineHiveKey)key).setKey(newKey);
+        }
+
+        // setup some helper config variables.
+        jc.set("map.input.file", split.getPath(idx).toString());
+        jc.setLong("map.input.start", split.getOffset(idx));
+        jc.setLong("map.input.length", split.getLength(idx));
+      } catch (Exception e) {
+        curReader=HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(e, jc);
+      }
+      idx++;
+      return true;
+    }
+  }
+
+  public abstract static class CombineFileInputFormatShim<K, V> extends
+      CombineFileInputFormat<K, V>
+      implements HadoopShims.CombineFileInputFormatShim<K, V> {
+
+    public Path[] getInputPathsShim(JobConf conf) {
+      try {
+        return FileInputFormat.getInputPaths(conf);
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+    }
+
+    @Override
+    public void createPool(JobConf conf, PathFilter... filters) {
+      super.createPool(conf, filters);
+    }
+
+    @Override
+    public InputSplitShim[] getSplits(JobConf job, int numSplits) throws IOException {
+      long minSize = job.getLong("mapred.min.split.size", 0);
+
+      // For backward compatibility, let the above parameter be used
+      if (job.getLong("mapred.min.split.size.per.node", 0) == 0) {
+        super.setMinSplitSizeNode(minSize);
+      }
+
+      if (job.getLong("mapred.min.split.size.per.rack", 0) == 0) {
+        super.setMinSplitSizeRack(minSize);
+      }
+
+      if (job.getLong("mapred.max.split.size", 0) == 0) {
+        super.setMaxSplitSize(minSize);
+      }
+
+      CombineFileSplit[] splits = (CombineFileSplit[]) super.getSplits(job, numSplits);
+
+      InputSplitShim[] isplits = new InputSplitShim[splits.length];
+      for (int pos = 0; pos < splits.length; pos++) {
+        isplits[pos] = new InputSplitShim(splits[pos]);
+      }
+
+      return isplits;
+    }
+
+    public InputSplitShim getInputSplitShim() throws IOException {
+      return new InputSplitShim();
+    }
+
+    public RecordReader getRecordReader(JobConf job, HadoopShims.InputSplitShim split,
+        Reporter reporter,
+        Class<RecordReader<K, V>> rrClass)
+        throws IOException {
+      CombineFileSplit cfSplit = (CombineFileSplit) split;
+      return new CombineFileRecordReader(job, cfSplit, reporter, rrClass);
+    }
+
+  }
+
+  public String getInputFormatClassName() {
+    return "org.apache.hadoop.hive.ql.io.CombineHiveInputFormat";
+  }
+
+  String[] ret = new String[2];
+
+  @Override
+  public String[] getTaskJobIDs(TaskCompletionEvent t) {
+    TaskID tid = t.getTaskAttemptId().getTaskID();
+    ret[0] = tid.toString();
+    ret[1] = tid.getJobID().toString();
+    return ret;
+  }
+
+  public void setFloatConf(Configuration conf, String varName, float val) {
+    conf.setFloat(varName, val);
+  }
+
+  @Override
+  public int createHadoopArchive(Configuration conf, Path sourceDir, Path destDir,
+      String archiveName) throws Exception {
+
+    HadoopArchives har = new HadoopArchives(conf);
+    List<String> args = new ArrayList<String>();
+
+    args.add("-archiveName");
+    args.add(archiveName);
+    args.add(sourceDir.toString());
+    args.add(destDir.toString());
+
+    return ToolRunner.run(har, args.toArray(new String[0]));
+  }
+
+  /*
+   *(non-Javadoc)
+   * @see org.apache.hadoop.hive.shims.HadoopShims#getHarUri(java.net.URI, java.net.URI, java.net.URI)
+   * This particular instance is for Hadoop 20 which creates an archive
+   * with the entire directory path from which one created the archive as
+   * compared against the one used by Hadoop 1.0 (within HadoopShimsSecure)
+   * where a relative path is stored within the archive.
+   */
+  public URI getHarUri (URI original, URI base, URI originalBase)
+    throws URISyntaxException {
+    URI relative = null;
+
+    String dirInArchive = original.getPath();
+    if (dirInArchive.length() > 1 && dirInArchive.charAt(0) == '/') {
+      dirInArchive = dirInArchive.substring(1);
+    }
+
+    relative = new URI(null, null, dirInArchive, null);
+
+    return base.resolve(relative);
+  }
+
+  public static class NullOutputCommitter extends OutputCommitter {
+    @Override
+    public void setupJob(JobContext jobContext) { }
+    @Override
+    public void cleanupJob(JobContext jobContext) { }
+
+    @Override
+    public void setupTask(TaskAttemptContext taskContext) { }
+    @Override
+    public boolean needsTaskCommit(TaskAttemptContext taskContext) {
+      return false;
+    }
+    @Override
+    public void commitTask(TaskAttemptContext taskContext) { }
+    @Override
+    public void abortTask(TaskAttemptContext taskContext) { }
+  }
+
+  public void prepareJobOutput(JobConf conf) {
+    conf.setOutputCommitter(Hadoop20Shims.NullOutputCommitter.class);
+
+    // option to bypass job setup and cleanup was introduced in hadoop-21 (MAPREDUCE-463)
+    // but can be backported. So we disable setup/cleanup in all versions >= 0.19
+    conf.setBoolean("mapred.committer.job.setup.cleanup.needed", false);
+
+    // option to bypass task cleanup task was introduced in hadoop-23 (MAPREDUCE-2206)
+    // but can be backported. So we disable setup/cleanup in all versions >= 0.19
+    conf.setBoolean("mapreduce.job.committer.task.cleanup.needed", false);
+  }
+
+  @Override
+  public UserGroupInformation getUGIForConf(Configuration conf) throws LoginException {
+    UserGroupInformation ugi =
+      UnixUserGroupInformation.readFromConf(conf, UnixUserGroupInformation.UGI_PROPERTY_NAME);
+    if(ugi == null) {
+      ugi = UserGroupInformation.login(conf);
+    }
+    return ugi;
+  }
+
+  @Override
+  public boolean isSecureShimImpl() {
+    return false;
+  }
+
+  @Override
+  public String getShortUserName(UserGroupInformation ugi) {
+    return ugi.getUserName();
+  }
+
+  @Override
+  public String getTokenStrForm(String tokenSignature) throws IOException {
+    throw new UnsupportedOperationException("Tokens are not supported in current hadoop version");
+  }
+
+  @Override
+  public void setTokenStr(UserGroupInformation ugi, String tokenStr, String tokenService)
+    throws IOException {
+    throw new UnsupportedOperationException("Tokens are not supported in current hadoop version");
+  }
+
+  @Override
+  public <T> T doAs(UserGroupInformation ugi, PrivilegedExceptionAction<T> pvea) throws
+    IOException, InterruptedException {
+    try {
+      return Subject.doAs(SecurityUtil.getSubject(ugi),pvea);
+    } catch (PrivilegedActionException e) {
+      throw new IOException(e);
+    }
+  }
+
+  @Override
+  public Path createDelegationTokenFile(Configuration conf) throws IOException {
+    throw new UnsupportedOperationException("Tokens are not supported in current hadoop version");
+  }
+
+  @Override
+  public UserGroupInformation createRemoteUser(String userName, List<String> groupNames) {
+    return new UnixUserGroupInformation(userName, groupNames.toArray(new String[0]));
+  }
+
+  @Override
+  public void loginUserFromKeytab(String principal, String keytabFile) throws IOException {
+    throwKerberosUnsupportedError();
+  }
+
+  @Override
+  public void reLoginUserFromKeytab() throws IOException{
+    throwKerberosUnsupportedError();
+  }
+
+  private void throwKerberosUnsupportedError() throws UnsupportedOperationException{
+    throw new UnsupportedOperationException("Kerberos login is not supported" +
+        " in this hadoop version (" + VersionInfo.getVersion() + ")");
+  }
+
+  @Override
+  public UserGroupInformation createProxyUser(String userName) throws IOException {
+    return createRemoteUser(userName, null);
+  }
+
+  @Override
+  public boolean isSecurityEnabled() {
+    return false;
+  }
+
+  @Override
+  public String getTaskAttemptLogUrl(JobConf conf,
+    String taskTrackerHttpAddress, String taskAttemptId)
+    throws MalformedURLException {
+    URL taskTrackerHttpURL = new URL(taskTrackerHttpAddress);
+    return TaskLogServlet.getTaskLogUrl(
+      taskTrackerHttpURL.getHost(),
+      Integer.toString(taskTrackerHttpURL.getPort()),
+      taskAttemptId);
+  }
+
+  @Override
+  public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception {
+    JobTrackerState state;
+    switch (clusterStatus.getJobTrackerState()) {
+    case INITIALIZING:
+      return JobTrackerState.INITIALIZING;
+    case RUNNING:
+      return JobTrackerState.RUNNING;
+    default:
+      String errorMsg = "Unrecognized JobTracker state: " + clusterStatus.getJobTrackerState();
+      throw new Exception(errorMsg);
+    }
+  }
+
+  @Override
+  public String unquoteHtmlChars(String item) {
+    return item;
+  }
+
+
+  @Override
+  public org.apache.hadoop.mapreduce.TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable) {
+    return new org.apache.hadoop.mapreduce.TaskAttemptContext(conf, new TaskAttemptID()) {
+      @Override
+      public void progress() {
+        progressable.progress();
+      }
+    };
+  }
+
+  @Override
+  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id) {
+    return new TaskAttemptID(jobId.getJtIdentifier(), jobId.getId(), isMap, taskId, id);
+  }
+
+  @Override
+  public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job) {
+    return new org.apache.hadoop.mapreduce.JobContext(job.getConfiguration(), job.getJobID());
+  }
+
+  @Override
+  public void closeAllForUGI(UserGroupInformation ugi) {
+    // No such functionality in ancient hadoop
+    return;
+  }
+
+  @Override
+  public boolean isLocalMode(Configuration conf) {
+    return "local".equals(getJobLauncherRpcAddress(conf));
+  }
+
+  @Override
+  public String getJobLauncherRpcAddress(Configuration conf) {
+    return conf.get("mapred.job.tracker");
+  }
+
+  @Override
+  public void setJobLauncherRpcAddress(Configuration conf, String val) {
+    conf.set("mapred.job.tracker", val);
+  }
+
+  @Override
+  public String getJobLauncherHttpAddress(Configuration conf) {
+    return conf.get("mapred.job.tracker.http.address");
+  }
+
+  @Override
+  public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration conf)
+          throws IOException {
+    // older versions of Hadoop don't have a Trash constructor based on the
+    // Path or FileSystem. So need to achieve this by creating a dummy conf.
+    // this needs to be filtered out based on version
+
+    Configuration dupConf = new Configuration(conf);
+    FileSystem.setDefaultUri(dupConf, fs.getUri());
+    Trash trash = new Trash(dupConf);
+    return trash.moveToTrash(path);
+  }
+
+  @Override
+  public long getDefaultBlockSize(FileSystem fs, Path path) {
+    return fs.getDefaultBlockSize();
+  }
+
+  @Override
+  public short getDefaultReplication(FileSystem fs, Path path) {
+    return fs.getDefaultReplication();
+  }
+
+  @Override
+  public String getTokenFileLocEnvName() {
+    throw new UnsupportedOperationException(
+        "Kerberos not supported in current hadoop version");
+  }
+  @Override
+  public HCatHadoopShims getHCatShim() {
+      throw new UnsupportedOperationException("HCatalog does not support Hadoop 0.20.x");
+  }
+  @Override
+  public WebHCatJTShim getWebHCatShim(Configuration conf, UserGroupInformation ugi) throws IOException {
+      throw new UnsupportedOperationException("WebHCat does not support Hadoop 0.20.x");
+  }
+  @Override
+  public FileSystem createProxyFileSystem(FileSystem fs, URI uri) {
+    return new ProxyFileSystem(fs, uri);
+  }
+}
diff --git a/src/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Jetty20Shims.java b/src/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Jetty20Shims.java
new file mode 100644
index 0000000..13c6b31
--- /dev/null
+++ b/src/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Jetty20Shims.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.shims;
+
+import java.io.IOException;
+
+import org.mortbay.jetty.bio.SocketConnector;
+import org.mortbay.jetty.handler.RequestLogHandler;
+import org.mortbay.jetty.webapp.WebAppContext;
+
+/**
+ * Jetty20Shims.
+ *
+ */
+public class Jetty20Shims implements JettyShims {
+  public Server startServer(String listen, int port) throws IOException {
+    Server s = new Server();
+    s.setupListenerHostPort(listen, port);
+    return s;
+  }
+
+  private static class Server extends org.mortbay.jetty.Server implements JettyShims.Server {
+    public void addWar(String war, String contextPath) {
+      WebAppContext wac = new WebAppContext();
+      wac.setContextPath(contextPath);
+      wac.setWar(war);
+      RequestLogHandler rlh = new RequestLogHandler();
+      rlh.setHandler(wac);
+      this.addHandler(rlh);
+    }
+
+    public void setupListenerHostPort(String listen, int port)
+        throws IOException {
+
+      SocketConnector connector = new SocketConnector();
+      connector.setPort(port);
+      connector.setHost(listen);
+      this.addConnector(connector);
+    }
+  }
+}
diff --git a/src/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java b/src/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
new file mode 100644
index 0000000..d159142
--- /dev/null
+++ b/src/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
@@ -0,0 +1,350 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.shims;
+
+import java.io.IOException;
+import java.net.InetSocketAddress;
+import java.net.MalformedURLException;
+import java.net.URL;
+import java.net.URI;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.ProxyFileSystem;
+import org.apache.hadoop.fs.Trash;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.mapred.JobTracker;
+import org.apache.hadoop.mapred.MiniMRCluster;
+import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.TaskLogServlet;
+import org.apache.hadoop.mapred.WebHCatJTShim20S;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.JobID;
+import org.apache.hadoop.mapreduce.JobStatus;
+import org.apache.hadoop.mapreduce.OutputFormat;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.TaskID;
+import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.KerberosName;
+
+/**
+ * Implemention of shims against Hadoop 0.20 with Security.
+ */
+public class Hadoop20SShims extends HadoopShimsSecure {
+
+  @Override
+  public String getTaskAttemptLogUrl(JobConf conf,
+    String taskTrackerHttpAddress, String taskAttemptId)
+    throws MalformedURLException {
+    URL taskTrackerHttpURL = new URL(taskTrackerHttpAddress);
+    return TaskLogServlet.getTaskLogUrl(
+      taskTrackerHttpURL.getHost(),
+      Integer.toString(taskTrackerHttpURL.getPort()),
+      taskAttemptId);
+  }
+
+  @Override
+  public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception {
+    JobTrackerState state;
+    switch (clusterStatus.getJobTrackerState()) {
+    case INITIALIZING:
+      return JobTrackerState.INITIALIZING;
+    case RUNNING:
+      return JobTrackerState.RUNNING;
+    default:
+      String errorMsg = "Unrecognized JobTracker state: " + clusterStatus.getJobTrackerState();
+      throw new Exception(errorMsg);
+    }
+  }
+
+  @Override
+  public org.apache.hadoop.mapreduce.TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable) {
+    return new org.apache.hadoop.mapreduce.TaskAttemptContext(conf, new TaskAttemptID()) {
+      @Override
+      public void progress() {
+        progressable.progress();
+      }
+    };
+  }
+
+  public String getKerberosShortName(String kerberosLongName) throws IOException {
+    KerberosName kerberosName = new KerberosName(kerberosLongName);
+    return kerberosName.getShortName();
+  }
+
+  @Override
+  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id) {
+    return new TaskAttemptID(jobId.getJtIdentifier(), jobId.getId(), isMap, taskId, id);
+  }
+
+  @Override
+  public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job) {
+    return new org.apache.hadoop.mapreduce.JobContext(job.getConfiguration(), job.getJobID());
+  }
+
+  @Override
+  public boolean isLocalMode(Configuration conf) {
+    return "local".equals(getJobLauncherRpcAddress(conf));
+  }
+
+  @Override
+  public String getJobLauncherRpcAddress(Configuration conf) {
+    return conf.get("mapred.job.tracker");
+  }
+
+  @Override
+  public void setJobLauncherRpcAddress(Configuration conf, String val) {
+    conf.set("mapred.job.tracker", val);
+  }
+
+  @Override
+  public String getJobLauncherHttpAddress(Configuration conf) {
+    return conf.get("mapred.job.tracker.http.address");
+  }
+
+  @Override
+  public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration conf)
+          throws IOException {
+    // older versions of Hadoop don't have a Trash constructor based on the
+    // Path or FileSystem. So need to achieve this by creating a dummy conf.
+    // this needs to be filtered out based on version
+
+    Configuration dupConf = new Configuration(conf);
+    FileSystem.setDefaultUri(dupConf, fs.getUri());
+    Trash trash = new Trash(dupConf);
+    return trash.moveToTrash(path);
+  }
+  @Override
+  public long getDefaultBlockSize(FileSystem fs, Path path) {
+    return fs.getDefaultBlockSize();
+  }
+
+  @Override
+  public short getDefaultReplication(FileSystem fs, Path path) {
+    return fs.getDefaultReplication();
+  }
+
+  @Override
+  public void setTotalOrderPartitionFile(JobConf jobConf, Path partitionFile){
+    TotalOrderPartitioner.setPartitionFile(jobConf, partitionFile);
+  }
+
+  /**
+   * Returns a shim to wrap MiniMrCluster
+   */
+  public MiniMrShim getMiniMrCluster(Configuration conf, int numberOfTaskTrackers,
+                                     String nameNode, int numDir) throws IOException {
+    return new MiniMrShim(conf, numberOfTaskTrackers, nameNode, numDir);
+  }
+
+  /**
+   * Shim for MiniMrCluster
+   */
+  public class MiniMrShim implements HadoopShims.MiniMrShim {
+
+    private final MiniMRCluster mr;
+
+    public MiniMrShim(Configuration conf, int numberOfTaskTrackers,
+        String nameNode, int numDir) throws IOException {
+      this.mr = new MiniMRCluster(numberOfTaskTrackers, nameNode, numDir);
+    }
+
+    @Override
+    public int getJobTrackerPort() throws UnsupportedOperationException {
+      return mr.getJobTrackerPort();
+    }
+
+    @Override
+    public void shutdown() throws IOException {
+      mr.shutdown();
+    }
+
+    @Override
+    public void setupConfiguration(Configuration conf) {
+      setJobLauncherRpcAddress(conf, "localhost:" + mr.getJobTrackerPort());
+    }
+  }
+
+  // Don't move this code to the parent class. There's a binary
+  // incompatibility between hadoop 1 and 2 wrt MiniDFSCluster and we
+  // need to have two different shim classes even though they are
+  // exactly the same.
+  public HadoopShims.MiniDFSShim getMiniDfs(Configuration conf,
+      int numDataNodes,
+      boolean format,
+      String[] racks) throws IOException {
+    return new MiniDFSShim(new MiniDFSCluster(conf, numDataNodes, format, racks));
+  }
+
+  /**
+   * MiniDFSShim.
+   *
+   */
+  public class MiniDFSShim implements HadoopShims.MiniDFSShim {
+    private final MiniDFSCluster cluster;
+
+    public MiniDFSShim(MiniDFSCluster cluster) {
+      this.cluster = cluster;
+    }
+
+    public FileSystem getFileSystem() throws IOException {
+      return cluster.getFileSystem();
+    }
+
+    public void shutdown() {
+      cluster.shutdown();
+    }
+  }
+  private volatile HCatHadoopShims hcatShimInstance;
+  @Override
+  public HCatHadoopShims getHCatShim() {
+    if(hcatShimInstance == null) {
+      hcatShimInstance = new HCatHadoopShims20S();
+    }
+    return hcatShimInstance;
+  }
+  private final class HCatHadoopShims20S implements HCatHadoopShims {
+    @Override
+    public TaskID createTaskID() {
+      return new TaskID();
+    }
+
+    @Override
+    public TaskAttemptID createTaskAttemptID() {
+      return new TaskAttemptID();
+    }
+
+    @Override
+    public TaskAttemptContext createTaskAttemptContext(Configuration conf, TaskAttemptID taskId) {
+      return new TaskAttemptContext(conf, taskId);
+    }
+
+    @Override
+    public org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf conf,
+                 org.apache.hadoop.mapred.TaskAttemptID taskId, Progressable progressable) {
+      org.apache.hadoop.mapred.TaskAttemptContext newContext = null;
+      try {
+        java.lang.reflect.Constructor construct = org.apache.hadoop.mapred.TaskAttemptContext.class.getDeclaredConstructor(
+                org.apache.hadoop.mapred.JobConf.class, org.apache.hadoop.mapred.TaskAttemptID.class,
+                Progressable.class);
+        construct.setAccessible(true);
+        newContext = (org.apache.hadoop.mapred.TaskAttemptContext)construct.newInstance(conf, taskId, progressable);
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+      return newContext;
+    }
+
+    @Override
+    public JobContext createJobContext(Configuration conf,
+                                       JobID jobId) {
+      return new JobContext(conf, jobId);
+    }
+
+    @Override
+    public org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf conf,
+                                   org.apache.hadoop.mapreduce.JobID jobId, Progressable progressable) {
+      org.apache.hadoop.mapred.JobContext newContext = null;
+      try {
+        java.lang.reflect.Constructor construct = org.apache.hadoop.mapred.JobContext.class.getDeclaredConstructor(
+                org.apache.hadoop.mapred.JobConf.class, org.apache.hadoop.mapreduce.JobID.class,
+                Progressable.class);
+        construct.setAccessible(true);
+        newContext = (org.apache.hadoop.mapred.JobContext)construct.newInstance(conf, jobId, progressable);
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+      return newContext;
+    }
+
+    @Override
+    public void commitJob(OutputFormat outputFormat, Job job) throws IOException {
+      if( job.getConfiguration().get("mapred.job.tracker", "").equalsIgnoreCase("local") ) {
+        try {
+          //In local mode, mapreduce will not call OutputCommitter.cleanupJob.
+          //Calling it from here so that the partition publish happens.
+          //This call needs to be removed after MAPREDUCE-1447 is fixed.
+          outputFormat.getOutputCommitter(createTaskAttemptContext(
+                  job.getConfiguration(), createTaskAttemptID())).commitJob(job);
+        } catch (IOException e) {
+          throw new IOException("Failed to cleanup job",e);
+        } catch (InterruptedException e) {
+          throw new IOException("Failed to cleanup job",e);
+        }
+      }
+    }
+
+    @Override
+    public void abortJob(OutputFormat outputFormat, Job job) throws IOException {
+      if (job.getConfiguration().get("mapred.job.tracker", "")
+              .equalsIgnoreCase("local")) {
+        try {
+          // This call needs to be removed after MAPREDUCE-1447 is fixed.
+          outputFormat.getOutputCommitter(createTaskAttemptContext(
+                  job.getConfiguration(), new TaskAttemptID())).abortJob(job, JobStatus.State.FAILED);
+        } catch (IOException e) {
+          throw new IOException("Failed to abort job", e);
+        } catch (InterruptedException e) {
+          throw new IOException("Failed to abort job", e);
+        }
+      }
+    }
+
+    @Override
+    public InetSocketAddress getResourceManagerAddress(Configuration conf)
+    {
+      return JobTracker.getAddress(conf);
+    }
+
+    @Override
+    public String getPropertyName(PropertyName name) {
+      switch (name) {
+        case CACHE_ARCHIVES:
+          return DistributedCache.CACHE_ARCHIVES;
+        case CACHE_FILES:
+          return DistributedCache.CACHE_FILES;
+        case CACHE_SYMLINK:
+          return DistributedCache.CACHE_SYMLINK;
+      }
+
+      return "";
+    }
+
+    @Override
+    public boolean isFileInHDFS(FileSystem fs, Path path) throws IOException {
+      // In hadoop 1.x.x the file system URI is sufficient to determine the uri of the file
+      return "hdfs".equals(fs.getUri().getScheme());
+    }
+  }
+  @Override
+  public WebHCatJTShim getWebHCatShim(Configuration conf, UserGroupInformation ugi) throws IOException {
+    return new WebHCatJTShim20S(conf, ugi);//this has state, so can't be cached
+  }
+
+  @Override
+  public FileSystem createProxyFileSystem(FileSystem fs, URI uri) {
+    return new ProxyFileSystem(fs, uri);
+  }
+}
diff --git a/src/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Jetty20SShims.java b/src/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Jetty20SShims.java
new file mode 100644
index 0000000..75659ff
--- /dev/null
+++ b/src/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Jetty20SShims.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.shims;
+
+
+import org.mortbay.jetty.bio.SocketConnector;
+import org.mortbay.jetty.handler.RequestLogHandler;
+import org.mortbay.jetty.webapp.WebAppContext;
+
+import java.io.IOException;
+
+public class Jetty20SShims implements JettyShims {
+  public Server startServer(String listen, int port) throws IOException {
+    Server s = new Server();
+    s.setupListenerHostPort(listen, port);
+    return s;
+  }
+
+  private static class Server extends org.mortbay.jetty.Server implements JettyShims.Server {
+    public void addWar(String war, String contextPath) {
+      WebAppContext wac = new WebAppContext();
+      wac.setContextPath(contextPath);
+      wac.setWar(war);
+      RequestLogHandler rlh = new RequestLogHandler();
+      rlh.setHandler(wac);
+      this.addHandler(rlh);
+   }
+
+    public void setupListenerHostPort(String listen, int port)
+      throws IOException {
+
+      SocketConnector connector  = new SocketConnector();
+      connector.setPort(port);
+      connector.setHost(listen);
+      this.addConnector(connector);
+    }
+  }
+}
diff --git a/src/shims/0.20S/src/main/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java b/src/shims/0.20S/src/main/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java
new file mode 100644
index 0000000..4c9bd73
--- /dev/null
+++ b/src/shims/0.20S/src/main/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java
@@ -0,0 +1,99 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.shims.HadoopShims.WebHCatJTShim;
+import org.apache.hadoop.ipc.RPC;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.security.UserGroupInformation;
+
+import java.io.IOException;
+import java.net.InetSocketAddress;
+
+/**
+ * This is in org.apache.hadoop.mapred package because it relies on
+ * JobSubmissionProtocol which is package private
+ */
+public class WebHCatJTShim20S implements WebHCatJTShim {
+    private JobSubmissionProtocol cnx;
+
+    /**
+     * Create a connection to the Job Tracker.
+     */
+    public WebHCatJTShim20S(Configuration conf, UserGroupInformation ugi)
+            throws IOException {
+      cnx = (JobSubmissionProtocol)
+              RPC.getProxy(JobSubmissionProtocol.class,
+                      JobSubmissionProtocol.versionID,
+                      getAddress(conf),
+                      ugi,
+                      conf,
+                      NetUtils.getSocketFactory(conf,
+                              JobSubmissionProtocol.class));
+    }
+
+    /**
+     * Grab a handle to a job that is already known to the JobTracker.
+     *
+     * @return Profile of the job, or null if not found.
+     */
+    public JobProfile getJobProfile(org.apache.hadoop.mapred.JobID jobid)
+            throws IOException {
+      return cnx.getJobProfile(jobid);
+    }
+
+    /**
+     * Grab a handle to a job that is already known to the JobTracker.
+     *
+     * @return Status of the job, or null if not found.
+     */
+    public org.apache.hadoop.mapred.JobStatus getJobStatus(org.apache.hadoop.mapred.JobID jobid)
+            throws IOException {
+      return cnx.getJobStatus(jobid);
+    }
+
+
+    /**
+     * Kill a job.
+     */
+    public void killJob(org.apache.hadoop.mapred.JobID jobid)
+            throws IOException {
+      cnx.killJob(jobid);
+    }
+
+    /**
+     * Get all the jobs submitted.
+     */
+    public org.apache.hadoop.mapred.JobStatus[] getAllJobs()
+            throws IOException {
+      return cnx.getAllJobs();
+    }
+
+    /**
+     * Close the connection to the Job Tracker.
+     */
+    public void close() {
+      RPC.stopProxy(cnx);
+    }
+    private InetSocketAddress getAddress(Configuration conf) {
+      String jobTrackerStr = conf.get("mapred.job.tracker", "localhost:8012");
+      return NetUtils.createSocketAddr(jobTrackerStr);
+    }
+  }
+
diff --git a/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java b/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
new file mode 100644
index 0000000..dc5facd
--- /dev/null
+++ b/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
@@ -0,0 +1,390 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.shims;
+
+import java.io.IOException;
+import java.lang.Integer;
+import java.net.InetSocketAddress;
+import java.net.MalformedURLException;
+import java.net.URL;
+import java.util.Map;
+import java.net.URI;
+import java.io.FileNotFoundException;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.ProxyFileSystem;
+import org.apache.hadoop.fs.LocatedFileStatus;
+import org.apache.hadoop.fs.RemoteIterator;
+import org.apache.hadoop.fs.Trash;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.mapred.MiniMRCluster;
+import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.WebHCatJTShim23;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.JobID;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.OutputFormat;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.TaskID;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.task.JobContextImpl;
+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
+import org.apache.hadoop.mapreduce.util.HostUtil;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.authentication.util.KerberosName;
+
+/**
+ * Implemention of shims against Hadoop 0.23.0.
+ */
+public class Hadoop23Shims extends HadoopShimsSecure {
+
+  @Override
+  public String getTaskAttemptLogUrl(JobConf conf,
+    String taskTrackerHttpAddress, String taskAttemptId)
+    throws MalformedURLException {
+    if (conf.get("mapreduce.framework.name") != null
+      && conf.get("mapreduce.framework.name").equals("yarn")) {
+      // if the cluster is running in MR2 mode, return null
+      LOG.warn("Can't fetch tasklog: TaskLogServlet is not supported in MR2 mode.");
+      return null;
+    } else {
+      // if the cluster is running in MR1 mode, using HostUtil to construct TaskLogURL
+      URL taskTrackerHttpURL = new URL(taskTrackerHttpAddress);
+      return HostUtil.getTaskLogUrl(taskTrackerHttpURL.getHost(),
+        Integer.toString(taskTrackerHttpURL.getPort()),
+        taskAttemptId);
+    }
+  }
+
+  @Override
+  public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception {
+    switch (clusterStatus.getJobTrackerStatus()) {
+    case INITIALIZING:
+      return JobTrackerState.INITIALIZING;
+    case RUNNING:
+      return JobTrackerState.RUNNING;
+    default:
+      String errorMsg = "Unrecognized JobTracker state: " + clusterStatus.getJobTrackerStatus();
+      throw new Exception(errorMsg);
+    }
+  }
+
+  @Override
+  public org.apache.hadoop.mapreduce.TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable) {
+    return new TaskAttemptContextImpl(conf, new TaskAttemptID()) {
+      @Override
+      public void progress() {
+        progressable.progress();
+      }
+    };
+  }
+
+  @Override
+  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id) {
+    return new TaskAttemptID(jobId.getJtIdentifier(), jobId.getId(), isMap ?  TaskType.MAP : TaskType.REDUCE, taskId, id);
+  }
+
+  @Override
+  public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job) {
+    return new JobContextImpl(job.getConfiguration(), job.getJobID());
+  }
+
+  @Override
+  public boolean isLocalMode(Configuration conf) {
+    return "local".equals(conf.get("mapreduce.framework.name"));
+  }
+
+  @Override
+  public String getJobLauncherRpcAddress(Configuration conf) {
+    return conf.get("yarn.resourcemanager.address");
+  }
+
+  @Override
+  public void setJobLauncherRpcAddress(Configuration conf, String val) {
+    if (val.equals("local")) {
+      // LocalClientProtocolProvider expects both parameters to be 'local'.
+      conf.set("mapreduce.framework.name", val);
+      conf.set("mapreduce.jobtracker.address", val);
+    }
+    else {
+      conf.set("mapreduce.framework.name", "yarn");
+      conf.set("yarn.resourcemanager.address", val);
+    }
+  }
+
+  public String getKerberosShortName(String kerberosLongName) throws IOException {
+    KerberosName kerberosName = new KerberosName(kerberosLongName);
+    return kerberosName.getShortName();
+  }
+
+  @Override
+  public String getJobLauncherHttpAddress(Configuration conf) {
+    return conf.get("yarn.resourcemanager.webapp.address");
+  }
+
+  @Override
+  public long getDefaultBlockSize(FileSystem fs, Path path) {
+    return fs.getDefaultBlockSize(path);
+  }
+
+  @Override
+  public short getDefaultReplication(FileSystem fs, Path path) {
+    return fs.getDefaultReplication(path);
+  }
+
+  @Override
+  public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration conf)
+          throws IOException {
+    return Trash.moveToAppropriateTrash(fs, path, conf);
+  }
+
+  @Override
+  public void setTotalOrderPartitionFile(JobConf jobConf, Path partitionFile){
+    TotalOrderPartitioner.setPartitionFile(jobConf, partitionFile);
+  }
+
+  /**
+   * Returns a shim to wrap MiniMrCluster
+   */
+  public MiniMrShim getMiniMrCluster(Configuration conf, int numberOfTaskTrackers,
+                                     String nameNode, int numDir) throws IOException {
+    return new MiniMrShim(conf, numberOfTaskTrackers, nameNode, numDir);
+  }
+
+  /**
+   * Shim for MiniMrCluster
+   */
+  public class MiniMrShim implements HadoopShims.MiniMrShim {
+
+    private final MiniMRCluster mr;
+    private final Configuration conf;
+
+    public MiniMrShim(Configuration conf, int numberOfTaskTrackers,
+                      String nameNode, int numDir) throws IOException {
+      this.conf = conf;
+
+      JobConf jConf = new JobConf(conf);
+      jConf.set("yarn.scheduler.capacity.root.queues", "default");
+      jConf.set("yarn.scheduler.capacity.root.default.capacity", "100");
+
+      mr = new MiniMRCluster(numberOfTaskTrackers, nameNode, numDir, null, null, jConf);
+    }
+
+    @Override
+    public int getJobTrackerPort() throws UnsupportedOperationException {
+      String address = conf.get("yarn.resourcemanager.address");
+      address = StringUtils.substringAfterLast(address, ":");
+
+      if (StringUtils.isBlank(address)) {
+        throw new IllegalArgumentException("Invalid YARN resource manager port.");
+      }
+
+      return Integer.parseInt(address);
+    }
+
+    @Override
+    public void shutdown() throws IOException {
+      mr.shutdown();
+    }
+
+    @Override
+    public void setupConfiguration(Configuration conf) {
+      JobConf jConf = mr.createJobConf();
+      for (Map.Entry<String, String> pair: jConf) {
+        conf.set(pair.getKey(), pair.getValue());
+      }
+    }
+  }
+
+  // Don't move this code to the parent class. There's a binary
+  // incompatibility between hadoop 1 and 2 wrt MiniDFSCluster and we
+  // need to have two different shim classes even though they are
+  // exactly the same.
+  public HadoopShims.MiniDFSShim getMiniDfs(Configuration conf,
+      int numDataNodes,
+      boolean format,
+      String[] racks) throws IOException {
+    return new MiniDFSShim(new MiniDFSCluster(conf, numDataNodes, format, racks));
+  }
+
+  /**
+   * MiniDFSShim.
+   *
+   */
+  public class MiniDFSShim implements HadoopShims.MiniDFSShim {
+    private final MiniDFSCluster cluster;
+
+    public MiniDFSShim(MiniDFSCluster cluster) {
+      this.cluster = cluster;
+    }
+
+    public FileSystem getFileSystem() throws IOException {
+      return cluster.getFileSystem();
+    }
+
+    public void shutdown() {
+      cluster.shutdown();
+    }
+  }
+  private volatile HCatHadoopShims hcatShimInstance;
+  @Override
+  public HCatHadoopShims getHCatShim() {
+    if(hcatShimInstance == null) {
+      hcatShimInstance = new HCatHadoopShims23();
+    }
+    return hcatShimInstance;
+  }
+  private final class HCatHadoopShims23 implements HCatHadoopShims {
+    @Override
+    public TaskID createTaskID() {
+      return new TaskID("", 0, TaskType.MAP, 0);
+    }
+
+    @Override
+    public TaskAttemptID createTaskAttemptID() {
+      return new TaskAttemptID("", 0, TaskType.MAP, 0, 0);
+    }
+
+    @Override
+    public org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(Configuration conf,
+                                                                                   org.apache.hadoop.mapreduce.TaskAttemptID taskId) {
+      return new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(
+              conf instanceof JobConf? new JobConf(conf) : conf,
+              taskId);
+    }
+
+    @Override
+    public org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf conf,
+                                                                                org.apache.hadoop.mapred.TaskAttemptID taskId, Progressable progressable) {
+      org.apache.hadoop.mapred.TaskAttemptContext newContext = null;
+      try {
+        java.lang.reflect.Constructor construct = org.apache.hadoop.mapred.TaskAttemptContextImpl.class.getDeclaredConstructor(
+                org.apache.hadoop.mapred.JobConf.class, org.apache.hadoop.mapred.TaskAttemptID.class,
+                Reporter.class);
+        construct.setAccessible(true);
+        newContext = (org.apache.hadoop.mapred.TaskAttemptContext) construct.newInstance(
+                new JobConf(conf), taskId, (Reporter) progressable);
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+      return newContext;
+    }
+
+    @Override
+    public JobContext createJobContext(Configuration conf,
+                                       JobID jobId) {
+      return new JobContextImpl(conf instanceof JobConf? new JobConf(conf) : conf,
+              jobId);
+    }
+
+    @Override
+    public org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf conf,
+                                                                org.apache.hadoop.mapreduce.JobID jobId, Progressable progressable) {
+      return new org.apache.hadoop.mapred.JobContextImpl(
+              new JobConf(conf), jobId, (org.apache.hadoop.mapred.Reporter) progressable);
+    }
+
+    @Override
+    public void commitJob(OutputFormat outputFormat, Job job) throws IOException {
+      // Do nothing as this was fixed by MAPREDUCE-1447.
+    }
+
+    @Override
+    public void abortJob(OutputFormat outputFormat, Job job) throws IOException {
+      // Do nothing as this was fixed by MAPREDUCE-1447.
+    }
+
+    @Override
+    public InetSocketAddress getResourceManagerAddress(Configuration conf) {
+      String addr = conf.get("yarn.resourcemanager.address", "localhost:8032");
+
+      return NetUtils.createSocketAddr(addr);
+    }
+
+    @Override
+    public String getPropertyName(PropertyName name) {
+      switch (name) {
+        case CACHE_ARCHIVES:
+          return MRJobConfig.CACHE_ARCHIVES;
+        case CACHE_FILES:
+          return MRJobConfig.CACHE_FILES;
+        case CACHE_SYMLINK:
+          return MRJobConfig.CACHE_SYMLINK;
+      }
+
+      return "";
+    }
+
+    @Override
+    public boolean isFileInHDFS(FileSystem fs, Path path) throws IOException {
+      // In case of viewfs we need to lookup where the actual file is to know the filesystem in use.
+      // resolvePath is a sure shot way of knowing which file system the file is.
+      return "hdfs".equals(fs.resolvePath(path).toUri().getScheme());
+    }
+  }
+  @Override
+  public WebHCatJTShim getWebHCatShim(Configuration conf, UserGroupInformation ugi) throws IOException {
+    return new WebHCatJTShim23(conf, ugi);//this has state, so can't be cached
+  }
+
+  class ProxyFileSystem23 extends ProxyFileSystem {
+    public ProxyFileSystem23(FileSystem fs) {
+      super(fs);
+    }
+    public ProxyFileSystem23(FileSystem fs, URI uri) {
+      super(fs, uri);
+    }
+
+    @Override
+    public RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path f)
+      throws FileNotFoundException, IOException {
+      return new RemoteIterator<LocatedFileStatus>() {
+        private RemoteIterator<LocatedFileStatus> stats =
+            ProxyFileSystem23.super.listLocatedStatus(
+                ProxyFileSystem23.super.swizzleParamPath(f));
+
+        @Override
+        public boolean hasNext() throws IOException {
+          return stats.hasNext();
+        }
+
+        @Override
+        public LocatedFileStatus next() throws IOException {
+          LocatedFileStatus result = stats.next();
+          return new LocatedFileStatus(
+              ProxyFileSystem23.super.swizzleFileStatus(result, false),
+              result.getBlockLocations());
+        }
+      };
+    }
+  }
+
+  @Override
+  public FileSystem createProxyFileSystem(FileSystem fs, URI uri) {
+    return new ProxyFileSystem23(fs, uri);
+  }
+}
diff --git a/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Jetty23Shims.java b/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Jetty23Shims.java
new file mode 100644
index 0000000..9328749
--- /dev/null
+++ b/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Jetty23Shims.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.shims;
+
+import java.io.IOException;
+
+import org.mortbay.jetty.bio.SocketConnector;
+import org.mortbay.jetty.handler.RequestLogHandler;
+import org.mortbay.jetty.webapp.WebAppContext;
+
+/**
+ * Jetty23Shims.
+ *
+ */
+public class Jetty23Shims implements JettyShims {
+  public Server startServer(String listen, int port) throws IOException {
+    Server s = new Server();
+    s.setupListenerHostPort(listen, port);
+    return s;
+  }
+
+  private static class Server extends org.mortbay.jetty.Server implements JettyShims.Server {
+    public void addWar(String war, String contextPath) {
+      WebAppContext wac = new WebAppContext();
+      wac.setContextPath(contextPath);
+      wac.setWar(war);
+      RequestLogHandler rlh = new RequestLogHandler();
+      rlh.setHandler(wac);
+      this.addHandler(rlh);
+    }
+
+    public void setupListenerHostPort(String listen, int port)
+        throws IOException {
+
+      SocketConnector connector = new SocketConnector();
+      connector.setPort(port);
+      connector.setHost(listen);
+      this.addConnector(connector);
+    }
+  }
+}
diff --git a/src/shims/0.23/src/main/java/org/apache/hadoop/mapred/WebHCatJTShim23.java b/src/shims/0.23/src/main/java/org/apache/hadoop/mapred/WebHCatJTShim23.java
new file mode 100644
index 0000000..abb3911
--- /dev/null
+++ b/src/shims/0.23/src/main/java/org/apache/hadoop/mapred/WebHCatJTShim23.java
@@ -0,0 +1,91 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.hive.shims.HadoopShims.WebHCatJTShim;
+
+import java.io.IOException;
+
+public class WebHCatJTShim23 implements WebHCatJTShim {
+  private JobClient jc;
+
+  /**
+   * Create a connection to the Job Tracker.
+   */
+  public WebHCatJTShim23(Configuration conf, final UserGroupInformation ugi)
+          throws IOException {
+
+    jc = new JobClient(conf);
+  }
+
+  /**
+   * Grab a handle to a job that is already known to the JobTracker.
+   *
+   * @return Profile of the job, or null if not found.
+   */
+  public JobProfile getJobProfile(JobID jobid)
+          throws IOException {
+    RunningJob rj = jc.getJob(jobid);
+    JobStatus jobStatus = rj.getJobStatus();
+    JobProfile jobProfile = new JobProfile(jobStatus.getUsername(), jobStatus.getJobID(),
+            jobStatus.getJobFile(), jobStatus.getTrackingUrl(), jobStatus.getJobName());
+    return jobProfile;
+  }
+
+  /**
+   * Grab a handle to a job that is already known to the JobTracker.
+   *
+   * @return Status of the job, or null if not found.
+   */
+  public JobStatus getJobStatus(JobID jobid)
+          throws IOException {
+    RunningJob rj = jc.getJob(jobid);
+    JobStatus jobStatus = rj.getJobStatus();
+    return jobStatus;
+  }
+
+
+  /**
+   * Kill a job.
+   */
+  public void killJob(JobID jobid)
+          throws IOException {
+    RunningJob rj = jc.getJob(jobid);
+    rj.killJob();
+  }
+
+  /**
+   * Get all the jobs submitted.
+   */
+  public JobStatus[] getAllJobs()
+          throws IOException {
+    return jc.getAllJobs();
+  }
+
+  /**
+   * Close the connection to the Job Tracker.
+   */
+  public void close() {
+    try {
+      jc.close();
+    } catch (IOException e) {
+    }
+  }
+}
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
new file mode 100644
index 0000000..fd274a6
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
@@ -0,0 +1,628 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.shims;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.File;
+import java.io.IOException;
+import java.lang.reflect.Constructor;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.security.PrivilegedExceptionAction;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil;
+import org.apache.hadoop.hive.thrift.DelegationTokenIdentifier;
+import org.apache.hadoop.hive.thrift.DelegationTokenSelector;
+import org.apache.hadoop.http.HtmlQuoting;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobContext;
+import org.apache.hadoop.mapred.JobStatus;
+import org.apache.hadoop.mapred.OutputCommitter;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.hadoop.mapred.TaskAttemptContext;
+import org.apache.hadoop.mapred.TaskCompletionEvent;
+import org.apache.hadoop.mapred.TaskID;
+import org.apache.hadoop.mapred.lib.CombineFileInputFormat;
+import org.apache.hadoop.mapred.lib.CombineFileSplit;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.security.Credentials;
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.security.token.TokenIdentifier;
+import org.apache.hadoop.security.token.TokenSelector;
+import org.apache.hadoop.tools.HadoopArchives;
+import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.util.ToolRunner;
+
+/**
+ * Base implemention for shims against secure Hadoop 0.20.3/0.23.
+ */
+public abstract class HadoopShimsSecure implements HadoopShims {
+
+  static final Log LOG = LogFactory.getLog(HadoopShimsSecure.class);
+
+  public boolean usesJobShell() {
+    return false;
+  }
+
+  public boolean fileSystemDeleteOnExit(FileSystem fs, Path path)
+      throws IOException {
+
+    return fs.deleteOnExit(path);
+  }
+
+  public void inputFormatValidateInput(InputFormat fmt, JobConf conf)
+      throws IOException {
+    // gone in 0.18+
+  }
+
+  @Override
+  public String unquoteHtmlChars(String item) {
+    return HtmlQuoting.unquoteHtmlChars(item);
+  }
+
+  public boolean isJobPreparing(RunningJob job) throws IOException {
+    return job.getJobState() == JobStatus.PREP;
+  }
+  /**
+   * Workaround for hadoop-17 - jobclient only looks at commandlineconfig.
+   */
+  public void setTmpFiles(String prop, String files) {
+    // gone in 20+
+  }
+
+  /**
+   * We define this function here to make the code compatible between
+   * hadoop 0.17 and hadoop 0.20.
+   *
+   * Hive binary that compiled Text.compareTo(Text) with hadoop 0.20 won't
+   * work with hadoop 0.17 because in hadoop 0.20, Text.compareTo(Text) is
+   * implemented in org.apache.hadoop.io.BinaryComparable, and Java compiler
+   * references that class, which is not available in hadoop 0.17.
+   */
+  public int compareText(Text a, Text b) {
+    return a.compareTo(b);
+  }
+
+  @Override
+  public long getAccessTime(FileStatus file) {
+    return file.getAccessTime();
+  }
+
+  public HadoopShims.CombineFileInputFormatShim getCombineFileInputFormat() {
+    return new CombineFileInputFormatShim() {
+      @Override
+      public RecordReader getRecordReader(InputSplit split,
+          JobConf job, Reporter reporter) throws IOException {
+        throw new IOException("CombineFileInputFormat.getRecordReader not needed.");
+      }
+    };
+  }
+
+  public static class InputSplitShim extends CombineFileSplit implements HadoopShims.InputSplitShim {
+    long shrinkedLength;
+    boolean _isShrinked;
+    public InputSplitShim() {
+      super();
+      _isShrinked = false;
+    }
+
+    public InputSplitShim(CombineFileSplit old) throws IOException {
+      super(old.getJob(), old.getPaths(), old.getStartOffsets(),
+          old.getLengths(), dedup(old.getLocations()));
+      _isShrinked = false;
+    }
+
+    private static String[] dedup(String[] locations) {
+      Set<String> dedup = new HashSet<String>();
+      Collections.addAll(dedup, locations);
+      return dedup.toArray(new String[dedup.size()]);
+    }
+
+    @Override
+    public void shrinkSplit(long length) {
+      _isShrinked = true;
+      shrinkedLength = length;
+    }
+
+    public boolean isShrinked() {
+      return _isShrinked;
+    }
+
+    public long getShrinkedLength() {
+      return shrinkedLength;
+    }
+
+    @Override
+    public void readFields(DataInput in) throws IOException {
+      super.readFields(in);
+      _isShrinked = in.readBoolean();
+      if (_isShrinked) {
+        shrinkedLength = in.readLong();
+      }
+    }
+
+    @Override
+    public void write(DataOutput out) throws IOException {
+      super.write(out);
+      out.writeBoolean(_isShrinked);
+      if (_isShrinked) {
+        out.writeLong(shrinkedLength);
+      }
+    }
+  }
+
+  /* This class should be replaced with org.apache.hadoop.mapred.lib.CombineFileRecordReader class, once
+   * https://issues.apache.org/jira/browse/MAPREDUCE-955 is fixed. This code should be removed - it is a copy
+   * of org.apache.hadoop.mapred.lib.CombineFileRecordReader
+   */
+  public static class CombineFileRecordReader<K, V> implements RecordReader<K, V> {
+
+    static final Class[] constructorSignature = new Class[] {
+        InputSplit.class,
+        Configuration.class,
+        Reporter.class,
+        Integer.class
+        };
+
+    protected CombineFileSplit split;
+    protected JobConf jc;
+    protected Reporter reporter;
+    protected Class<RecordReader<K, V>> rrClass;
+    protected Constructor<RecordReader<K, V>> rrConstructor;
+    protected FileSystem fs;
+
+    protected int idx;
+    protected long progress;
+    protected RecordReader<K, V> curReader;
+    protected boolean isShrinked;
+    protected long shrinkedLength;
+
+    public boolean next(K key, V value) throws IOException {
+
+      while ((curReader == null)
+          || !doNextWithExceptionHandler((K) ((CombineHiveKey) key).getKey(),
+              value)) {
+        if (!initNextRecordReader(key)) {
+          return false;
+        }
+      }
+      return true;
+    }
+
+    public K createKey() {
+      K newKey = curReader.createKey();
+      return (K)(new CombineHiveKey(newKey));
+    }
+
+    public V createValue() {
+      return curReader.createValue();
+    }
+
+    /**
+     * Return the amount of data processed.
+     */
+    public long getPos() throws IOException {
+      return progress;
+    }
+
+    public void close() throws IOException {
+      if (curReader != null) {
+        curReader.close();
+        curReader = null;
+      }
+    }
+
+    /**
+     * Return progress based on the amount of data processed so far.
+     */
+    public float getProgress() throws IOException {
+      return Math.min(1.0f, progress / (float) (split.getLength()));
+    }
+
+    /**
+     * A generic RecordReader that can hand out different recordReaders
+     * for each chunk in the CombineFileSplit.
+     */
+    public CombineFileRecordReader(JobConf job, CombineFileSplit split,
+        Reporter reporter,
+        Class<RecordReader<K, V>> rrClass)
+        throws IOException {
+      this.split = split;
+      this.jc = job;
+      this.rrClass = rrClass;
+      this.reporter = reporter;
+      this.idx = 0;
+      this.curReader = null;
+      this.progress = 0;
+
+      isShrinked = false;
+
+      assert (split instanceof InputSplitShim);
+      if (((InputSplitShim) split).isShrinked()) {
+        isShrinked = true;
+        shrinkedLength = ((InputSplitShim) split).getShrinkedLength();
+      }
+
+      try {
+        rrConstructor = rrClass.getDeclaredConstructor(constructorSignature);
+        rrConstructor.setAccessible(true);
+      } catch (Exception e) {
+        throw new RuntimeException(rrClass.getName() +
+            " does not have valid constructor", e);
+      }
+      initNextRecordReader(null);
+    }
+
+    /**
+     * do next and handle exception inside it.
+     * @param key
+     * @param value
+     * @return
+     * @throws IOException
+     */
+    private boolean doNextWithExceptionHandler(K key, V value) throws IOException {
+      try {
+        return curReader.next(key, value);
+      } catch (Exception e) {
+        return HiveIOExceptionHandlerUtil
+            .handleRecordReaderNextException(e, jc);
+      }
+    }
+
+    /**
+     * Get the record reader for the next chunk in this CombineFileSplit.
+     */
+    protected boolean initNextRecordReader(K key) throws IOException {
+
+      if (curReader != null) {
+        curReader.close();
+        curReader = null;
+        if (idx > 0) {
+          progress += split.getLength(idx - 1); // done processing so far
+        }
+      }
+
+      // if all chunks have been processed, nothing more to do.
+      if (idx == split.getNumPaths() || (isShrinked && progress > shrinkedLength)) {
+        return false;
+      }
+
+      // get a record reader for the idx-th chunk
+      try {
+        curReader = rrConstructor.newInstance(new Object[]
+            {split, jc, reporter, Integer.valueOf(idx)});
+
+        // change the key if need be
+        if (key != null) {
+          K newKey = curReader.createKey();
+          ((CombineHiveKey)key).setKey(newKey);
+        }
+
+        // setup some helper config variables.
+        jc.set("map.input.file", split.getPath(idx).toString());
+        jc.setLong("map.input.start", split.getOffset(idx));
+        jc.setLong("map.input.length", split.getLength(idx));
+      } catch (Exception e) {
+        curReader = HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(
+            e, jc);
+      }
+      idx++;
+      return true;
+    }
+  }
+
+  public abstract static class CombineFileInputFormatShim<K, V> extends
+      CombineFileInputFormat<K, V>
+      implements HadoopShims.CombineFileInputFormatShim<K, V> {
+
+    public Path[] getInputPathsShim(JobConf conf) {
+      try {
+        return FileInputFormat.getInputPaths(conf);
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+    }
+
+    @Override
+    public void createPool(JobConf conf, PathFilter... filters) {
+      super.createPool(conf, filters);
+    }
+
+    @Override
+    public InputSplitShim[] getSplits(JobConf job, int numSplits) throws IOException {
+      long minSize = job.getLong("mapred.min.split.size", 0);
+
+      // For backward compatibility, let the above parameter be used
+      if (job.getLong("mapred.min.split.size.per.node", 0) == 0) {
+        super.setMinSplitSizeNode(minSize);
+      }
+
+      if (job.getLong("mapred.min.split.size.per.rack", 0) == 0) {
+        super.setMinSplitSizeRack(minSize);
+      }
+
+      if (job.getLong("mapred.max.split.size", 0) == 0) {
+        super.setMaxSplitSize(minSize);
+      }
+
+      InputSplit[] splits = (InputSplit[]) super.getSplits(job, numSplits);
+
+      InputSplitShim[] isplits = new InputSplitShim[splits.length];
+      for (int pos = 0; pos < splits.length; pos++) {
+        isplits[pos] = new InputSplitShim((CombineFileSplit)splits[pos]);
+      }
+
+      return isplits;
+    }
+
+    public InputSplitShim getInputSplitShim() throws IOException {
+      return new InputSplitShim();
+    }
+
+    public RecordReader getRecordReader(JobConf job, HadoopShims.InputSplitShim split,
+        Reporter reporter,
+        Class<RecordReader<K, V>> rrClass)
+        throws IOException {
+      CombineFileSplit cfSplit = (CombineFileSplit) split;
+      return new CombineFileRecordReader(job, cfSplit, reporter, rrClass);
+    }
+
+  }
+
+  public String getInputFormatClassName() {
+    return "org.apache.hadoop.hive.ql.io.CombineHiveInputFormat";
+  }
+
+  String[] ret = new String[2];
+
+  @Override
+  public String[] getTaskJobIDs(TaskCompletionEvent t) {
+    TaskID tid = t.getTaskAttemptId().getTaskID();
+    ret[0] = tid.toString();
+    ret[1] = tid.getJobID().toString();
+    return ret;
+  }
+
+  public void setFloatConf(Configuration conf, String varName, float val) {
+    conf.setFloat(varName, val);
+  }
+
+  @Override
+  public int createHadoopArchive(Configuration conf, Path sourceDir, Path destDir,
+      String archiveName) throws Exception {
+
+    HadoopArchives har = new HadoopArchives(conf);
+    List<String> args = new ArrayList<String>();
+
+    args.add("-archiveName");
+    args.add(archiveName);
+    args.add("-p");
+    args.add(sourceDir.toString());
+    args.add(destDir.toString());
+
+    return ToolRunner.run(har, args.toArray(new String[0]));
+  }
+
+  /*
+   * This particular instance is for Hadoop 1.0 which creates an archive
+   * with only the relative path of the archived directory stored within
+   * the archive as compared to the full path in case of earlier versions.
+   * See this api in Hadoop20Shims for comparison.
+   */
+  public URI getHarUri(URI original, URI base, URI originalBase)
+    throws URISyntaxException {
+    URI relative = originalBase.relativize(original);
+    if (relative.isAbsolute()) {
+      throw new URISyntaxException("Couldn't create URI for location.",
+                                   "Relative: " + relative + " Base: "
+                                   + base + " OriginalBase: " + originalBase);
+    }
+
+    return base.resolve(relative);
+  }
+
+  public static class NullOutputCommitter extends OutputCommitter {
+    @Override
+    public void setupJob(JobContext jobContext) { }
+    @Override
+    public void cleanupJob(JobContext jobContext) { }
+
+    @Override
+    public void setupTask(TaskAttemptContext taskContext) { }
+    @Override
+    public boolean needsTaskCommit(TaskAttemptContext taskContext) {
+      return false;
+    }
+    @Override
+    public void commitTask(TaskAttemptContext taskContext) { }
+    @Override
+    public void abortTask(TaskAttemptContext taskContext) { }
+  }
+
+  public void prepareJobOutput(JobConf conf) {
+    conf.setOutputCommitter(NullOutputCommitter.class);
+
+    // option to bypass job setup and cleanup was introduced in hadoop-21 (MAPREDUCE-463)
+    // but can be backported. So we disable setup/cleanup in all versions >= 0.19
+    conf.setBoolean("mapred.committer.job.setup.cleanup.needed", false);
+
+    // option to bypass task cleanup task was introduced in hadoop-23 (MAPREDUCE-2206)
+    // but can be backported. So we disable setup/cleanup in all versions >= 0.19
+    conf.setBoolean("mapreduce.job.committer.task.cleanup.needed", false);
+  }
+
+  @Override
+  public UserGroupInformation getUGIForConf(Configuration conf) throws IOException {
+    return UserGroupInformation.getCurrentUser();
+  }
+
+  @Override
+  public boolean isSecureShimImpl() {
+    return true;
+  }
+
+  @Override
+  public String getShortUserName(UserGroupInformation ugi) {
+    return ugi.getShortUserName();
+  }
+
+  @Override
+  public String getTokenStrForm(String tokenSignature) throws IOException {
+    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
+    TokenSelector<? extends TokenIdentifier> tokenSelector = new DelegationTokenSelector();
+
+    Token<? extends TokenIdentifier> token = tokenSelector.selectToken(
+        tokenSignature == null ? new Text() : new Text(tokenSignature), ugi.getTokens());
+    return token != null ? token.encodeToUrlString() : null;
+  }
+
+  @Override
+  public void setTokenStr(UserGroupInformation ugi, String tokenStr, String tokenService) throws IOException {
+    Token<DelegationTokenIdentifier> delegationToken = new Token<DelegationTokenIdentifier>();
+    delegationToken.decodeFromUrlString(tokenStr);
+    delegationToken.setService(new Text(tokenService));
+    ugi.addToken(delegationToken);
+  }
+
+  @Override
+  public <T> T doAs(UserGroupInformation ugi, PrivilegedExceptionAction<T> pvea) throws IOException, InterruptedException {
+    return ugi.doAs(pvea);
+  }
+
+  @Override
+  public Path createDelegationTokenFile(Configuration conf) throws IOException {
+
+    //get delegation token for user
+    String uname = UserGroupInformation.getLoginUser().getShortUserName();
+    FileSystem fs = FileSystem.get(conf);
+    Token<?> fsToken = fs.getDelegationToken(uname);
+
+    File t = File.createTempFile("hive_hadoop_delegation_token", null);
+    Path tokenPath = new Path(t.toURI());
+
+    //write credential with token to file
+    Credentials cred = new Credentials();
+    cred.addToken(fsToken.getService(), fsToken);
+    cred.writeTokenStorageFile(tokenPath, conf);
+
+    return tokenPath;
+  }
+
+
+  @Override
+  public UserGroupInformation createProxyUser(String userName) throws IOException {
+    return UserGroupInformation.createProxyUser(
+        userName, UserGroupInformation.getLoginUser());
+  }
+
+  @Override
+  public boolean isSecurityEnabled() {
+    return UserGroupInformation.isSecurityEnabled();
+  }
+
+  @Override
+  public UserGroupInformation createRemoteUser(String userName, List<String> groupNames) {
+    return UserGroupInformation.createRemoteUser(userName);
+  }
+
+  @Override
+  public void closeAllForUGI(UserGroupInformation ugi) {
+    try {
+      FileSystem.closeAllForUGI(ugi);
+    } catch (IOException e) {
+      LOG.error("Could not clean up file-system handles for UGI: " + ugi, e);
+    }
+  }
+
+  @Override
+  public void loginUserFromKeytab(String principal, String keytabFile) throws IOException {
+    String hostPrincipal = SecurityUtil.getServerPrincipal(principal, "0.0.0.0");
+    UserGroupInformation.loginUserFromKeytab(hostPrincipal, keytabFile);
+  }
+
+  @Override
+  public String getTokenFileLocEnvName() {
+    return UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION;
+  }
+
+  @Override
+  public void reLoginUserFromKeytab() throws IOException{
+    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
+    //checkTGT calls ugi.relogin only after checking if it is close to tgt expiry
+    //hadoop relogin is actually done only every x minutes (x=10 in hadoop 1.x)
+    if(ugi.isFromKeytab()){
+      ugi.checkTGTAndReloginFromKeytab();
+    }
+  }
+
+  @Override
+  abstract public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception;
+
+  @Override
+  abstract public org.apache.hadoop.mapreduce.TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable);
+
+  @Override
+  abstract public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job);
+
+  @Override
+  abstract public boolean isLocalMode(Configuration conf);
+
+  @Override
+  abstract public void setJobLauncherRpcAddress(Configuration conf, String val);
+
+  @Override
+  abstract public String getJobLauncherHttpAddress(Configuration conf);
+
+  @Override
+  abstract public String getJobLauncherRpcAddress(Configuration conf);
+
+  @Override
+  abstract public short getDefaultReplication(FileSystem fs, Path path);
+
+  @Override
+  abstract public long getDefaultBlockSize(FileSystem fs, Path path);
+
+  @Override
+  abstract public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration conf)
+          throws IOException;
+
+  @Override
+  abstract public FileSystem createProxyFileSystem(FileSystem fs, URI uri);
+}
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DBTokenStore.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DBTokenStore.java
new file mode 100644
index 0000000..b507a09
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DBTokenStore.java
@@ -0,0 +1,153 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import java.io.IOException;
+import java.lang.reflect.InvocationTargetException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.commons.codec.binary.Base64;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
+import org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport;
+
+public class DBTokenStore implements DelegationTokenStore {
+
+
+  @Override
+  public int addMasterKey(String s) throws TokenStoreException {
+    return (Integer)invokeOnRawStore("addMasterKey", new Object[]{s},String.class);
+  }
+
+  @Override
+  public void updateMasterKey(int keySeq, String s) throws TokenStoreException {
+    invokeOnRawStore("updateMasterKey", new Object[] {Integer.valueOf(keySeq), s},
+        Integer.class, String.class);
+  }
+
+  @Override
+  public boolean removeMasterKey(int keySeq) {
+    return (Boolean)invokeOnRawStore("removeMasterKey", new Object[] {Integer.valueOf(keySeq)},
+      Integer.class);
+  }
+
+  @Override
+  public String[] getMasterKeys() throws TokenStoreException {
+    return (String[])invokeOnRawStore("getMasterKeys", new Object[0]);
+  }
+
+  @Override
+  public boolean addToken(DelegationTokenIdentifier tokenIdentifier,
+      DelegationTokenInformation token) throws TokenStoreException {
+
+    try {
+      String identifier = TokenStoreDelegationTokenSecretManager.encodeWritable(tokenIdentifier);
+      String tokenStr = Base64.encodeBase64URLSafeString(
+        HiveDelegationTokenSupport.encodeDelegationTokenInformation(token));
+      return (Boolean)invokeOnRawStore("addToken", new Object[] {identifier, tokenStr},
+        String.class, String.class);
+    } catch (IOException e) {
+      throw new TokenStoreException(e);
+    }
+  }
+
+  @Override
+  public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier)
+      throws TokenStoreException {
+    try {
+      String tokenStr = (String)invokeOnRawStore("getToken", new Object[] {
+          TokenStoreDelegationTokenSecretManager.encodeWritable(tokenIdentifier)}, String.class);
+      return (null == tokenStr) ? null : HiveDelegationTokenSupport.decodeDelegationTokenInformation(Base64.decodeBase64(tokenStr));
+    } catch (IOException e) {
+      throw new TokenStoreException(e);
+    }
+  }
+
+  @Override
+  public boolean removeToken(DelegationTokenIdentifier tokenIdentifier) throws TokenStoreException{
+    try {
+      return (Boolean)invokeOnRawStore("removeToken", new Object[] {
+        TokenStoreDelegationTokenSecretManager.encodeWritable(tokenIdentifier)}, String.class);
+    } catch (IOException e) {
+      throw new TokenStoreException(e);
+    }
+  }
+
+  @Override
+  public List<DelegationTokenIdentifier> getAllDelegationTokenIdentifiers() throws TokenStoreException{
+
+    List<String> tokenIdents = (List<String>)invokeOnRawStore("getAllTokenIdentifiers", new Object[0]);
+    List<DelegationTokenIdentifier> delTokenIdents = new ArrayList<DelegationTokenIdentifier>(tokenIdents.size());
+
+    for (String tokenIdent : tokenIdents) {
+      DelegationTokenIdentifier delToken = new DelegationTokenIdentifier();
+      try {
+        TokenStoreDelegationTokenSecretManager.decodeWritable(delToken, tokenIdent);
+      } catch (IOException e) {
+        throw new TokenStoreException(e);
+      }
+      delTokenIdents.add(delToken);
+    }
+    return delTokenIdents;
+  }
+
+  private Object hmsHandler;
+
+  @Override
+  public void setStore(Object hms) throws TokenStoreException {
+    hmsHandler = hms;
+  }
+
+  private Object invokeOnRawStore(String methName, Object[] params, Class<?> ... paramTypes)
+      throws TokenStoreException{
+
+    try {
+      Object rawStore = hmsHandler.getClass().getMethod("getMS").invoke(hmsHandler);
+      return rawStore.getClass().getMethod(methName, paramTypes).invoke(rawStore, params);
+    } catch (IllegalArgumentException e) {
+        throw new TokenStoreException(e);
+    } catch (SecurityException e) {
+        throw new TokenStoreException(e);
+    } catch (IllegalAccessException e) {
+        throw new TokenStoreException(e);
+    } catch (InvocationTargetException e) {
+        throw new TokenStoreException(e.getCause());
+    } catch (NoSuchMethodException e) {
+        throw new TokenStoreException(e);
+    }
+  }
+
+  @Override
+  public void setConf(Configuration conf) {
+    // No-op
+  }
+
+  @Override
+  public Configuration getConf() {
+    return null;
+  }
+
+  @Override
+  public void close() throws IOException {
+    // No-op.
+  }
+
+
+}
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenIdentifier.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenIdentifier.java
new file mode 100644
index 0000000..4ca3c0b
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenIdentifier.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier;
+
+/**
+ * A delegation token identifier that is specific to Hive.
+ */
+public class DelegationTokenIdentifier
+    extends AbstractDelegationTokenIdentifier {
+  public static final Text HIVE_DELEGATION_KIND = new Text("HIVE_DELEGATION_TOKEN");
+
+  /**
+   * Create an empty delegation token identifier for reading into.
+   */
+  public DelegationTokenIdentifier() {
+  }
+
+  /**
+   * Create a new delegation token identifier
+   * @param owner the effective username of the token owner
+   * @param renewer the username of the renewer
+   * @param realUser the real username of the token owner
+   */
+  public DelegationTokenIdentifier(Text owner, Text renewer, Text realUser) {
+    super(owner, renewer, realUser);
+  }
+
+  @Override
+  public Text getKind() {
+    return HIVE_DELEGATION_KIND;
+  }
+
+}
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenSecretManager.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenSecretManager.java
new file mode 100644
index 0000000..29114f0
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenSecretManager.java
@@ -0,0 +1,87 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;
+
+/**
+ * A Hive specific delegation token secret manager.
+ * The secret manager is responsible for generating and accepting the password
+ * for each token.
+ */
+public class DelegationTokenSecretManager
+    extends AbstractDelegationTokenSecretManager<DelegationTokenIdentifier> {
+
+  /**
+   * Create a secret manager
+   * @param delegationKeyUpdateInterval the number of seconds for rolling new
+   *        secret keys.
+   * @param delegationTokenMaxLifetime the maximum lifetime of the delegation
+   *        tokens
+   * @param delegationTokenRenewInterval how often the tokens must be renewed
+   * @param delegationTokenRemoverScanInterval how often the tokens are scanned
+   *        for expired tokens
+   */
+  public DelegationTokenSecretManager(long delegationKeyUpdateInterval,
+                                      long delegationTokenMaxLifetime,
+                                      long delegationTokenRenewInterval,
+                                      long delegationTokenRemoverScanInterval) {
+    super(delegationKeyUpdateInterval, delegationTokenMaxLifetime,
+          delegationTokenRenewInterval, delegationTokenRemoverScanInterval);
+  }
+
+  @Override
+  public DelegationTokenIdentifier createIdentifier() {
+    return new DelegationTokenIdentifier();
+  }
+
+  public synchronized void cancelDelegationToken(String tokenStrForm) throws IOException {
+    Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
+    t.decodeFromUrlString(tokenStrForm);
+    String user = UserGroupInformation.getCurrentUser().getUserName();
+    cancelToken(t, user);
+  }
+
+  public synchronized long renewDelegationToken(String tokenStrForm) throws IOException {
+    Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
+    t.decodeFromUrlString(tokenStrForm);
+    String user = UserGroupInformation.getCurrentUser().getUserName();
+    return renewToken(t, user);
+  }
+
+  public synchronized String getDelegationToken(String renewer) throws IOException {
+    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
+    Text owner = new Text(ugi.getUserName());
+    Text realUser = null;
+    if (ugi.getRealUser() != null) {
+      realUser = new Text(ugi.getRealUser().getUserName());
+    }
+    DelegationTokenIdentifier ident =
+      new DelegationTokenIdentifier(owner, new Text(renewer), realUser);
+    Token<DelegationTokenIdentifier> t = new Token<DelegationTokenIdentifier>(
+        ident, this);
+    return t.encodeToUrlString();
+  }
+}
+
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenSelector.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenSelector.java
new file mode 100644
index 0000000..f6e2420
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenSelector.java
@@ -0,0 +1,33 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector;
+
+/**
+ * A delegation token that is specialized for Hive
+ */
+
+public class DelegationTokenSelector
+    extends AbstractDelegationTokenSelector<DelegationTokenIdentifier>{
+
+  public DelegationTokenSelector() {
+    super(DelegationTokenIdentifier.HIVE_DELEGATION_KIND);
+  }
+}
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenStore.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenStore.java
new file mode 100644
index 0000000..f3c2e48
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/DelegationTokenStore.java
@@ -0,0 +1,113 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.thrift;
+
+import java.io.Closeable;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configurable;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
+
+/**
+ * Interface for pluggable token store that can be implemented with shared external
+ * storage for load balancing and high availability (for example using ZooKeeper).
+ * Internal, store specific errors are translated into {@link TokenStoreException}.
+ */
+public interface DelegationTokenStore extends Configurable, Closeable {
+
+  /**
+   * Exception for internal token store errors that typically cannot be handled by the caller.
+   */
+  public static class TokenStoreException extends RuntimeException {
+    private static final long serialVersionUID = -8693819817623074083L;
+
+    public TokenStoreException(Throwable cause) {
+      super(cause);
+    }
+
+    public TokenStoreException(String message, Throwable cause) {
+      super(message, cause);
+    }
+  }
+
+  /**
+   * Add new master key. The token store assigns and returns the sequence number.
+   * Caller needs to use the identifier to update the key (since it is embedded in the key).
+   *
+   * @param s
+   * @return sequence number for new key
+   */
+  int addMasterKey(String s) throws TokenStoreException;
+
+  /**
+   * Update master key (for expiration and setting store assigned sequence within key)
+   * @param keySeq
+   * @param s
+   * @throws TokenStoreException
+   */
+  void updateMasterKey(int keySeq, String s) throws TokenStoreException;
+
+  /**
+   * Remove key for given id.
+   * @param keySeq
+   * @return false if key no longer present, true otherwise.
+   */
+  boolean removeMasterKey(int keySeq);
+
+  /**
+   * Return all master keys.
+   * @return
+   * @throws TokenStoreException
+   */
+  String[] getMasterKeys() throws TokenStoreException;
+
+  /**
+   * Add token. If identifier is already present, token won't be added.
+   * @param tokenIdentifier
+   * @param token
+   * @return true if token was added, false for existing identifier
+   */
+  boolean addToken(DelegationTokenIdentifier tokenIdentifier,
+      DelegationTokenInformation token) throws TokenStoreException;
+
+  /**
+   * Get token. Returns null if the token does not exist.
+   * @param tokenIdentifier
+   * @return
+   */
+  DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier)
+      throws TokenStoreException;
+
+  /**
+   * Remove token. Return value can be used by caller to detect concurrency.
+   * @param tokenIdentifier
+   * @return true if token was removed, false if it was already removed.
+   * @throws TokenStoreException
+   */
+  boolean removeToken(DelegationTokenIdentifier tokenIdentifier) throws TokenStoreException;
+
+  /**
+   * List of all token identifiers in the store. This is used to remove expired tokens
+   * and a potential scalability improvement would be to partition by master key id
+   * @return
+   */
+  List<DelegationTokenIdentifier> getAllDelegationTokenIdentifiers() throws TokenStoreException;
+
+  void setStore(Object hmsHandler) throws TokenStoreException;
+
+}
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java
new file mode 100644
index 0000000..b3d016e
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java
@@ -0,0 +1,645 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.thrift;
+
+import static org.apache.hadoop.fs.CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.net.Socket;
+import java.security.PrivilegedAction;
+import java.security.PrivilegedExceptionAction;
+import java.util.Map;
+
+import javax.security.auth.callback.Callback;
+import javax.security.auth.callback.CallbackHandler;
+import javax.security.auth.callback.NameCallback;
+import javax.security.auth.callback.PasswordCallback;
+import javax.security.auth.callback.UnsupportedCallbackException;
+import javax.security.sasl.AuthorizeCallback;
+import javax.security.sasl.RealmCallback;
+import javax.security.sasl.RealmChoiceCallback;
+import javax.security.sasl.SaslException;
+import javax.security.sasl.SaslServer;
+
+import org.apache.commons.codec.binary.Base64;
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.Client;
+import org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport;
+import org.apache.hadoop.security.SaslRpcServer;
+import org.apache.hadoop.security.SaslRpcServer.AuthMethod;
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.UserGroupInformation.AuthenticationMethod;
+import org.apache.hadoop.security.authorize.AuthorizationException;
+import org.apache.hadoop.security.authorize.ProxyUsers;
+import org.apache.hadoop.security.token.SecretManager.InvalidToken;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.security.token.TokenIdentifier;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.thrift.TException;
+import org.apache.thrift.TProcessor;
+import org.apache.thrift.protocol.TProtocol;
+import org.apache.thrift.transport.TSaslClientTransport;
+import org.apache.thrift.transport.TSaslServerTransport;
+import org.apache.thrift.transport.TSocket;
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
+import org.apache.thrift.transport.TTransportFactory;
+
+ /**
+  * Functions that bridge Thrift's SASL transports to Hadoop's
+  * SASL callback handlers and authentication classes.
+  */
+ public class HadoopThriftAuthBridge20S extends HadoopThriftAuthBridge {
+   static final Log LOG = LogFactory.getLog(HadoopThriftAuthBridge.class);
+
+   @Override
+   public Client createClient() {
+     return new Client();
+   }
+
+   @Override
+   public Client createClientWithConf(String authType) {
+     Configuration conf = new Configuration();
+     conf.set(HADOOP_SECURITY_AUTHENTICATION, authType);
+     UserGroupInformation.setConfiguration(conf);
+     return new Client();
+   }
+
+   @Override
+   public Server createServer(String keytabFile, String principalConf) throws TTransportException {
+     return new Server(keytabFile, principalConf);
+   }
+
+   /**
+    * Read and return Hadoop SASL configuration which can be configured using
+    * "hadoop.rpc.protection"
+    * @param conf
+    * @return Hadoop SASL configuration
+    */
+   @Override
+   public Map<String, String> getHadoopSaslProperties(Configuration conf) {
+     // Initialize the SaslRpcServer to ensure QOP parameters are read from conf
+     SaslRpcServer.init(conf);
+     return SaslRpcServer.SASL_PROPS;
+   }
+
+   public static class Client extends HadoopThriftAuthBridge.Client {
+     /**
+      * Create a client-side SASL transport that wraps an underlying transport.
+      *
+      * @param method The authentication method to use. Currently only KERBEROS is
+      *               supported.
+      * @param serverPrincipal The Kerberos principal of the target server.
+      * @param underlyingTransport The underlying transport mechanism, usually a TSocket.
+      * @param saslProps the sasl properties to create the client with
+      */
+
+     @Override
+     public TTransport createClientTransport(
+       String principalConfig, String host,
+       String methodStr, String tokenStrForm, TTransport underlyingTransport,
+       Map<String, String> saslProps) throws IOException {
+       AuthMethod method = AuthMethod.valueOf(AuthMethod.class, methodStr);
+
+       TTransport saslTransport = null;
+       switch (method) {
+         case DIGEST:
+           Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
+           t.decodeFromUrlString(tokenStrForm);
+           saslTransport = new TSaslClientTransport(
+            method.getMechanismName(),
+            null,
+            null, SaslRpcServer.SASL_DEFAULT_REALM,
+            saslProps, new SaslClientCallbackHandler(t),
+            underlyingTransport);
+           return new TUGIAssumingTransport(saslTransport, UserGroupInformation.getCurrentUser());
+
+         case KERBEROS:
+           String serverPrincipal = SecurityUtil.getServerPrincipal(principalConfig, host);
+           String names[] = SaslRpcServer.splitKerberosName(serverPrincipal);
+           if (names.length != 3) {
+             throw new IOException(
+               "Kerberos principal name does NOT have the expected hostname part: "
+                 + serverPrincipal);
+           }
+           try {
+             saslTransport = new TSaslClientTransport(
+               method.getMechanismName(),
+               null,
+               names[0], names[1],
+               saslProps, null,
+               underlyingTransport);
+             return new TUGIAssumingTransport(saslTransport, UserGroupInformation.getCurrentUser());
+           } catch (SaslException se) {
+             throw new IOException("Could not instantiate SASL transport", se);
+           }
+
+         default:
+           throw new IOException("Unsupported authentication method: " + method);
+       }
+     }
+    private static class SaslClientCallbackHandler implements CallbackHandler {
+      private final String userName;
+      private final char[] userPassword;
+
+      public SaslClientCallbackHandler(Token<? extends TokenIdentifier> token) {
+        this.userName = encodeIdentifier(token.getIdentifier());
+        this.userPassword = encodePassword(token.getPassword());
+      }
+
+      public void handle(Callback[] callbacks)
+      throws UnsupportedCallbackException {
+        NameCallback nc = null;
+        PasswordCallback pc = null;
+        RealmCallback rc = null;
+        for (Callback callback : callbacks) {
+          if (callback instanceof RealmChoiceCallback) {
+            continue;
+          } else if (callback instanceof NameCallback) {
+            nc = (NameCallback) callback;
+          } else if (callback instanceof PasswordCallback) {
+            pc = (PasswordCallback) callback;
+          } else if (callback instanceof RealmCallback) {
+            rc = (RealmCallback) callback;
+          } else {
+            throw new UnsupportedCallbackException(callback,
+                "Unrecognized SASL client callback");
+          }
+        }
+        if (nc != null) {
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("SASL client callback: setting username: " + userName);
+          }
+          nc.setName(userName);
+        }
+        if (pc != null) {
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("SASL client callback: setting userPassword");
+          }
+          pc.setPassword(userPassword);
+        }
+        if (rc != null) {
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("SASL client callback: setting realm: "
+                + rc.getDefaultText());
+          }
+          rc.setText(rc.getDefaultText());
+        }
+      }
+
+      static String encodeIdentifier(byte[] identifier) {
+        return new String(Base64.encodeBase64(identifier));
+      }
+
+      static char[] encodePassword(byte[] password) {
+        return new String(Base64.encodeBase64(password)).toCharArray();
+       }
+     }
+       }
+
+   public static class Server extends HadoopThriftAuthBridge.Server {
+     final UserGroupInformation realUgi;
+     DelegationTokenSecretManager secretManager;
+     private final static long DELEGATION_TOKEN_GC_INTERVAL = 3600000; // 1 hour
+     //Delegation token related keys
+     public static final String  DELEGATION_KEY_UPDATE_INTERVAL_KEY =
+       "hive.cluster.delegation.key.update-interval";
+     public static final long    DELEGATION_KEY_UPDATE_INTERVAL_DEFAULT =
+       24*60*60*1000; // 1 day
+     public static final String  DELEGATION_TOKEN_RENEW_INTERVAL_KEY =
+       "hive.cluster.delegation.token.renew-interval";
+     public static final long    DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT =
+       24*60*60*1000;  // 1 day
+     public static final String  DELEGATION_TOKEN_MAX_LIFETIME_KEY =
+       "hive.cluster.delegation.token.max-lifetime";
+     public static final long    DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT =
+       7*24*60*60*1000; // 7 days
+     public static final String DELEGATION_TOKEN_STORE_CLS =
+       "hive.cluster.delegation.token.store.class";
+     public static final String DELEGATION_TOKEN_STORE_ZK_CONNECT_STR =
+         "hive.cluster.delegation.token.store.zookeeper.connectString";
+     public static final String DELEGATION_TOKEN_STORE_ZK_CONNECT_TIMEOUTMILLIS =
+         "hive.cluster.delegation.token.store.zookeeper.connectTimeoutMillis";
+     public static final String DELEGATION_TOKEN_STORE_ZK_ZNODE =
+         "hive.cluster.delegation.token.store.zookeeper.znode";
+     public static final String DELEGATION_TOKEN_STORE_ZK_ACL =
+             "hive.cluster.delegation.token.store.zookeeper.acl";
+     public static final String DELEGATION_TOKEN_STORE_ZK_ZNODE_DEFAULT =
+         "/hive/cluster/delegation";
+
+     public Server() throws TTransportException {
+       try {
+         realUgi = UserGroupInformation.getCurrentUser();
+       } catch (IOException ioe) {
+         throw new TTransportException(ioe);
+       }
+     }
+     /**
+      * Create a server with a kerberos keytab/principal.
+      */
+     protected Server(String keytabFile, String principalConf)
+       throws TTransportException {
+       if (keytabFile == null || keytabFile.isEmpty()) {
+         throw new TTransportException("No keytab specified");
+       }
+       if (principalConf == null || principalConf.isEmpty()) {
+         throw new TTransportException("No principal specified");
+       }
+
+       // Login from the keytab
+       String kerberosName;
+       try {
+         kerberosName =
+           SecurityUtil.getServerPrincipal(principalConf, "0.0.0.0");
+         UserGroupInformation.loginUserFromKeytab(
+             kerberosName, keytabFile);
+         realUgi = UserGroupInformation.getLoginUser();
+         assert realUgi.isFromKeytab();
+       } catch (IOException ioe) {
+         throw new TTransportException(ioe);
+       }
+     }
+
+     /**
+      * Create a TTransportFactory that, upon connection of a client socket,
+      * negotiates a Kerberized SASL transport. The resulting TTransportFactory
+      * can be passed as both the input and output transport factory when
+      * instantiating a TThreadPoolServer, for example.
+      *
+      * @param saslProps Map of SASL properties
+      */
+     @Override
+     public TTransportFactory createTransportFactory(Map<String, String> saslProps)
+             throws TTransportException {
+       // Parse out the kerberos principal, host, realm.
+       String kerberosName = realUgi.getUserName();
+       final String names[] = SaslRpcServer.splitKerberosName(kerberosName);
+       if (names.length != 3) {
+         throw new TTransportException("Kerberos principal should have 3 parts: " + kerberosName);
+       }
+
+       TSaslServerTransport.Factory transFactory = new TSaslServerTransport.Factory();
+       transFactory.addServerDefinition(
+         AuthMethod.KERBEROS.getMechanismName(),
+         names[0], names[1],  // two parts of kerberos principal
+         saslProps,
+         new SaslRpcServer.SaslGssCallbackHandler());
+       transFactory.addServerDefinition(AuthMethod.DIGEST.getMechanismName(),
+          null, SaslRpcServer.SASL_DEFAULT_REALM,
+          saslProps, new SaslDigestCallbackHandler(secretManager));
+
+       return new TUGIAssumingTransportFactory(transFactory, realUgi);
+     }
+
+     /**
+      * Wrap a TProcessor in such a way that, before processing any RPC, it
+      * assumes the UserGroupInformation of the user authenticated by
+      * the SASL transport.
+      */
+     @Override
+     public TProcessor wrapProcessor(TProcessor processor) {
+       return new TUGIAssumingProcessor(processor, secretManager, true);
+     }
+
+     /**
+      * Wrap a TProcessor to capture the client information like connecting userid, ip etc
+      */
+     @Override
+     public TProcessor wrapNonAssumingProcessor(TProcessor processor) {
+      return new TUGIAssumingProcessor(processor, secretManager, false);
+     }
+
+    protected DelegationTokenStore getTokenStore(Configuration conf)
+        throws IOException {
+       String tokenStoreClassName = conf.get(DELEGATION_TOKEN_STORE_CLS, "");
+       if (StringUtils.isBlank(tokenStoreClassName)) {
+         return new MemoryTokenStore();
+       }
+       try {
+        Class<? extends DelegationTokenStore> storeClass = Class
+            .forName(tokenStoreClassName).asSubclass(
+                DelegationTokenStore.class);
+        return ReflectionUtils.newInstance(storeClass, conf);
+       } catch (ClassNotFoundException e) {
+        throw new IOException("Error initializing delegation token store: " + tokenStoreClassName,
+            e);
+       }
+     }
+
+     @Override
+     public void startDelegationTokenSecretManager(Configuration conf, Object hms)
+     throws IOException{
+       long secretKeyInterval =
+         conf.getLong(DELEGATION_KEY_UPDATE_INTERVAL_KEY,
+                        DELEGATION_KEY_UPDATE_INTERVAL_DEFAULT);
+       long tokenMaxLifetime =
+           conf.getLong(DELEGATION_TOKEN_MAX_LIFETIME_KEY,
+                        DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT);
+       long tokenRenewInterval =
+           conf.getLong(DELEGATION_TOKEN_RENEW_INTERVAL_KEY,
+                        DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT);
+
+       DelegationTokenStore dts = getTokenStore(conf);
+       dts.setStore(hms);
+       secretManager = new TokenStoreDelegationTokenSecretManager(secretKeyInterval,
+             tokenMaxLifetime,
+             tokenRenewInterval,
+             DELEGATION_TOKEN_GC_INTERVAL, dts);
+       secretManager.startThreads();
+     }
+
+     @Override
+     public String getDelegationToken(final String owner, final String renewer)
+     throws IOException, InterruptedException {
+       if (!authenticationMethod.get().equals(AuthenticationMethod.KERBEROS)) {
+         throw new AuthorizationException(
+         "Delegation Token can be issued only with kerberos authentication. " +
+         "Current AuthenticationMethod: " + authenticationMethod.get()
+             );
+       }
+       //if the user asking the token is same as the 'owner' then don't do
+       //any proxy authorization checks. For cases like oozie, where it gets
+       //a delegation token for another user, we need to make sure oozie is
+       //authorized to get a delegation token.
+       //Do all checks on short names
+       UserGroupInformation currUser = UserGroupInformation.getCurrentUser();
+       UserGroupInformation ownerUgi = UserGroupInformation.createRemoteUser(owner);
+       if (!ownerUgi.getShortUserName().equals(currUser.getShortUserName())) {
+         //in the case of proxy users, the getCurrentUser will return the
+         //real user (for e.g. oozie) due to the doAs that happened just before the
+         //server started executing the method getDelegationToken in the MetaStore
+         ownerUgi = UserGroupInformation.createProxyUser(owner,
+           UserGroupInformation.getCurrentUser());
+         InetAddress remoteAddr = getRemoteAddress();
+         ProxyUsers.authorize(ownerUgi,remoteAddr.getHostAddress(), null);
+       }
+       return ownerUgi.doAs(new PrivilegedExceptionAction<String>() {
+         public String run() throws IOException {
+           return secretManager.getDelegationToken(renewer);
+         }
+       });
+     }
+
+     @Override
+     public long renewDelegationToken(String tokenStrForm) throws IOException {
+       if (!authenticationMethod.get().equals(AuthenticationMethod.KERBEROS)) {
+         throw new AuthorizationException(
+         "Delegation Token can be issued only with kerberos authentication. " +
+         "Current AuthenticationMethod: " + authenticationMethod.get()
+             );
+       }
+       return secretManager.renewDelegationToken(tokenStrForm);
+     }
+
+     @Override
+     public void cancelDelegationToken(String tokenStrForm) throws IOException {
+       secretManager.cancelDelegationToken(tokenStrForm);
+     }
+
+     final static ThreadLocal<InetAddress> remoteAddress =
+       new ThreadLocal<InetAddress>() {
+       @Override
+       protected synchronized InetAddress initialValue() {
+         return null;
+       }
+     };
+
+     @Override
+     public InetAddress getRemoteAddress() {
+       return remoteAddress.get();
+     }
+
+     final static ThreadLocal<AuthenticationMethod> authenticationMethod =
+       new ThreadLocal<AuthenticationMethod>() {
+       @Override
+       protected synchronized AuthenticationMethod initialValue() {
+         return AuthenticationMethod.TOKEN;
+       }
+     };
+
+     private static ThreadLocal<String> remoteUser = new ThreadLocal<String> () {
+       @Override
+       protected synchronized String initialValue() {
+         return null;
+       }
+     };
+
+     @Override
+     public String getRemoteUser() {
+       return remoteUser.get();
+     }
+
+    /** CallbackHandler for SASL DIGEST-MD5 mechanism */
+    // This code is pretty much completely based on Hadoop's
+    // SaslRpcServer.SaslDigestCallbackHandler - the only reason we could not
+    // use that Hadoop class as-is was because it needs a Server.Connection object
+    // which is relevant in hadoop rpc but not here in the metastore - so the
+    // code below does not deal with the Connection Server.object.
+    static class SaslDigestCallbackHandler implements CallbackHandler {
+      private final DelegationTokenSecretManager secretManager;
+
+      public SaslDigestCallbackHandler(
+          DelegationTokenSecretManager secretManager) {
+        this.secretManager = secretManager;
+      }
+
+      private char[] getPassword(DelegationTokenIdentifier tokenid) throws InvalidToken {
+        return encodePassword(secretManager.retrievePassword(tokenid));
+      }
+
+      private char[] encodePassword(byte[] password) {
+        return new String(Base64.encodeBase64(password)).toCharArray();
+      }
+      /** {@inheritDoc} */
+      @Override
+      public void handle(Callback[] callbacks) throws InvalidToken,
+      UnsupportedCallbackException {
+        NameCallback nc = null;
+        PasswordCallback pc = null;
+        AuthorizeCallback ac = null;
+        for (Callback callback : callbacks) {
+          if (callback instanceof AuthorizeCallback) {
+            ac = (AuthorizeCallback) callback;
+          } else if (callback instanceof NameCallback) {
+            nc = (NameCallback) callback;
+          } else if (callback instanceof PasswordCallback) {
+            pc = (PasswordCallback) callback;
+          } else if (callback instanceof RealmCallback) {
+            continue; // realm is ignored
+          } else {
+            throw new UnsupportedCallbackException(callback,
+            "Unrecognized SASL DIGEST-MD5 Callback");
+          }
+        }
+        if (pc != null) {
+          DelegationTokenIdentifier tokenIdentifier = SaslRpcServer.
+          getIdentifier(nc.getDefaultName(), secretManager);
+          char[] password = getPassword(tokenIdentifier);
+
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("SASL server DIGEST-MD5 callback: setting password "
+                + "for client: " + tokenIdentifier.getUser());
+          }
+          pc.setPassword(password);
+        }
+        if (ac != null) {
+          String authid = ac.getAuthenticationID();
+          String authzid = ac.getAuthorizationID();
+          if (authid.equals(authzid)) {
+            ac.setAuthorized(true);
+          } else {
+            ac.setAuthorized(false);
+          }
+          if (ac.isAuthorized()) {
+            if (LOG.isDebugEnabled()) {
+              String username =
+                SaslRpcServer.getIdentifier(authzid, secretManager).getUser().getUserName();
+              LOG.debug("SASL server DIGEST-MD5 callback: setting "
+                  + "canonicalized client ID: " + username);
+            }
+            ac.setAuthorizedID(authzid);
+          }
+        }
+      }
+     }
+
+     /**
+      * Processor that pulls the SaslServer object out of the transport, and
+      * assumes the remote user's UGI before calling through to the original
+      * processor.
+      *
+      * This is used on the server side to set the UGI for each specific call.
+      */
+     protected class TUGIAssumingProcessor implements TProcessor {
+       final TProcessor wrapped;
+       DelegationTokenSecretManager secretManager;
+       boolean useProxy;
+       TUGIAssumingProcessor(TProcessor wrapped, DelegationTokenSecretManager secretManager,
+           boolean useProxy) {
+         this.wrapped = wrapped;
+         this.secretManager = secretManager;
+         this.useProxy = useProxy;
+       }
+
+       public boolean process(final TProtocol inProt, final TProtocol outProt) throws TException {
+         TTransport trans = inProt.getTransport();
+         if (!(trans instanceof TSaslServerTransport)) {
+           throw new TException("Unexpected non-SASL transport " + trans.getClass());
+         }
+         TSaslServerTransport saslTrans = (TSaslServerTransport)trans;
+         SaslServer saslServer = saslTrans.getSaslServer();
+         String authId = saslServer.getAuthorizationID();
+         authenticationMethod.set(AuthenticationMethod.KERBEROS);
+         LOG.debug("AUTH ID ======>" + authId);
+         String endUser = authId;
+
+         if(saslServer.getMechanismName().equals("DIGEST-MD5")) {
+           try {
+             TokenIdentifier tokenId = SaslRpcServer.getIdentifier(authId,
+                 secretManager);
+             endUser = tokenId.getUser().getUserName();
+             authenticationMethod.set(AuthenticationMethod.TOKEN);
+           } catch (InvalidToken e) {
+             throw new TException(e.getMessage());
+           }
+         }
+         Socket socket = ((TSocket)(saslTrans.getUnderlyingTransport())).getSocket();
+         remoteAddress.set(socket.getInetAddress());
+         UserGroupInformation clientUgi = null;
+         try {
+           if (useProxy) {
+             clientUgi = UserGroupInformation.createProxyUser(
+               endUser, UserGroupInformation.getLoginUser());
+             remoteUser.set(clientUgi.getShortUserName());
+             return clientUgi.doAs(new PrivilegedExceptionAction<Boolean>() {
+                 public Boolean run() {
+                   try {
+                     return wrapped.process(inProt, outProt);
+                   } catch (TException te) {
+                     throw new RuntimeException(te);
+                   }
+                 }
+               });
+           } else {
+             // check for kerberos v5
+             if (saslServer.getMechanismName().equals("GSSAPI")) {
+               String shortName = ShimLoader.getHadoopShims().getKerberosShortName(endUser);
+               remoteUser.set(shortName);
+             } else {
+               remoteUser.set(endUser);
+             }
+             return wrapped.process(inProt, outProt);
+           }
+         } catch (RuntimeException rte) {
+           if (rte.getCause() instanceof TException) {
+             throw (TException)rte.getCause();
+           }
+           throw rte;
+         } catch (InterruptedException ie) {
+           throw new RuntimeException(ie); // unexpected!
+         } catch (IOException ioe) {
+           throw new RuntimeException(ioe); // unexpected!
+         }
+         finally {
+           if (clientUgi != null) {
+            try { FileSystem.closeAllForUGI(clientUgi); }
+              catch(IOException exception) {
+                LOG.error("Could not clean up file-system handles for UGI: " + clientUgi, exception);
+              }
+          }
+         }
+       }
+     }
+
+    /**
+      * A TransportFactory that wraps another one, but assumes a specified UGI
+      * before calling through.
+      *
+      * This is used on the server side to assume the server's Principal when accepting
+      * clients.
+      */
+     static class TUGIAssumingTransportFactory extends TTransportFactory {
+       private final UserGroupInformation ugi;
+       private final TTransportFactory wrapped;
+
+       public TUGIAssumingTransportFactory(TTransportFactory wrapped, UserGroupInformation ugi) {
+         assert wrapped != null;
+         assert ugi != null;
+
+         this.wrapped = wrapped;
+         this.ugi = ugi;
+       }
+
+       @Override
+       public TTransport getTransport(final TTransport trans) {
+         return ugi.doAs(new PrivilegedAction<TTransport>() {
+           public TTransport run() {
+             return wrapped.getTransport(trans);
+           }
+         });
+       }
+     }
+   }
+ }
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/MemoryTokenStore.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/MemoryTokenStore.java
new file mode 100644
index 0000000..9908aa4
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/MemoryTokenStore.java
@@ -0,0 +1,115 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
+
+/**
+ * Default in-memory token store implementation.
+ */
+public class MemoryTokenStore implements DelegationTokenStore {
+
+  private final Map<Integer, String> masterKeys
+      = new ConcurrentHashMap<Integer, String>();
+
+  private final ConcurrentHashMap<DelegationTokenIdentifier, DelegationTokenInformation> tokens
+      = new ConcurrentHashMap<DelegationTokenIdentifier, DelegationTokenInformation>();
+
+  private final AtomicInteger masterKeySeq = new AtomicInteger();
+  private Configuration conf;
+
+  @Override
+  public void setConf(Configuration conf) {
+    this.conf = conf;
+  }
+
+  @Override
+  public Configuration getConf() {
+    return this.conf;
+  }
+
+  @Override
+  public int addMasterKey(String s) {
+    int keySeq = masterKeySeq.getAndIncrement();
+    masterKeys.put(keySeq, s);
+    return keySeq;
+  }
+
+  @Override
+  public void updateMasterKey(int keySeq, String s) {
+    masterKeys.put(keySeq, s);
+  }
+
+  @Override
+  public boolean removeMasterKey(int keySeq) {
+    return masterKeys.remove(keySeq) != null;
+  }
+
+  @Override
+  public String[] getMasterKeys() {
+    return masterKeys.values().toArray(new String[0]);
+  }
+
+  @Override
+  public boolean addToken(DelegationTokenIdentifier tokenIdentifier,
+    DelegationTokenInformation token) {
+    DelegationTokenInformation tokenInfo = tokens.putIfAbsent(tokenIdentifier, token);
+    return (tokenInfo == null);
+  }
+
+  @Override
+  public boolean removeToken(DelegationTokenIdentifier tokenIdentifier) {
+    DelegationTokenInformation tokenInfo = tokens.remove(tokenIdentifier);
+    return tokenInfo != null;
+  }
+
+  @Override
+  public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {
+    return tokens.get(tokenIdentifier);
+  }
+
+  @Override
+  public List<DelegationTokenIdentifier> getAllDelegationTokenIdentifiers() {
+    List<DelegationTokenIdentifier> result = new ArrayList<DelegationTokenIdentifier>(
+        tokens.size());
+    for (DelegationTokenIdentifier id : tokens.keySet()) {
+        result.add(id);
+    }
+    return result;
+  }
+
+  @Override
+  public void close() throws IOException {
+    //no-op
+  }
+
+  @Override
+  public void setStore(Object hmsHandler) throws TokenStoreException {
+    // no-op
+  }
+
+}
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java
new file mode 100644
index 0000000..4ccf895
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java
@@ -0,0 +1,334 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.lang.reflect.Method;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.commons.codec.binary.Base64;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;
+import org.apache.hadoop.security.token.delegation.DelegationKey;
+import org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport;
+import org.apache.hadoop.util.Daemon;
+import org.apache.hadoop.util.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Extension of {@link DelegationTokenSecretManager} to support alternative to default in-memory
+ * token management for fail-over and clustering through plug-able token store (ZooKeeper etc.).
+ * Delegation tokens will be retrieved from the store on-demand and (unlike base class behavior) not
+ * cached in memory. This avoids complexities related to token expiration. The security token is
+ * needed only at the time the transport is opened (as opposed to per interface operation). The
+ * assumption therefore is low cost of interprocess token retrieval (for random read efficient store
+ * such as ZooKeeper) compared to overhead of synchronizing per-process in-memory token caches.
+ * The wrapper incorporates the token store abstraction within the limitations of current
+ * Hive/Hadoop dependency (.20S) with minimum code duplication.
+ * Eventually this should be supported by Hadoop security directly.
+ */
+public class TokenStoreDelegationTokenSecretManager extends DelegationTokenSecretManager {
+
+  private static final Logger LOGGER =
+      LoggerFactory.getLogger(TokenStoreDelegationTokenSecretManager.class.getName());
+
+  final private long keyUpdateInterval;
+  final private long tokenRemoverScanInterval;
+  private Thread tokenRemoverThread;
+
+  final private DelegationTokenStore tokenStore;
+
+  public TokenStoreDelegationTokenSecretManager(long delegationKeyUpdateInterval,
+      long delegationTokenMaxLifetime, long delegationTokenRenewInterval,
+      long delegationTokenRemoverScanInterval,
+      DelegationTokenStore sharedStore) {
+    super(delegationKeyUpdateInterval, delegationTokenMaxLifetime, delegationTokenRenewInterval,
+        delegationTokenRemoverScanInterval);
+    this.keyUpdateInterval = delegationKeyUpdateInterval;
+    this.tokenRemoverScanInterval = delegationTokenRemoverScanInterval;
+
+    this.tokenStore = sharedStore;
+  }
+
+  protected DelegationTokenIdentifier getTokenIdentifier(Token<DelegationTokenIdentifier> token)
+      throws IOException {
+    // turn bytes back into identifier for cache lookup
+    ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());
+    DataInputStream in = new DataInputStream(buf);
+    DelegationTokenIdentifier id = createIdentifier();
+    id.readFields(in);
+    return id;
+  }
+
+  protected Map<Integer, DelegationKey> reloadKeys() {
+    // read keys from token store
+    String[] allKeys = tokenStore.getMasterKeys();
+    Map<Integer, DelegationKey> keys
+        = new HashMap<Integer, DelegationKey>(allKeys.length);
+    for (String keyStr : allKeys) {
+      DelegationKey key = new DelegationKey();
+      try {
+        decodeWritable(key, keyStr);
+        keys.put(key.getKeyId(), key);
+      } catch (IOException ex) {
+        LOGGER.error("Failed to load master key.", ex);
+      }
+    }
+    synchronized (this) {
+        super.allKeys.clear();
+        super.allKeys.putAll(keys);
+    }
+    return keys;
+  }
+
+  @Override
+  public byte[] retrievePassword(DelegationTokenIdentifier identifier) throws InvalidToken {
+      DelegationTokenInformation info = this.tokenStore.getToken(identifier);
+      if (info == null) {
+          throw new InvalidToken("token expired or does not exist: " + identifier);
+      }
+      // must reuse super as info.getPassword is not accessible
+      synchronized (this) {
+        try {
+          super.currentTokens.put(identifier, info);
+          return super.retrievePassword(identifier);
+        } finally {
+          super.currentTokens.remove(identifier);
+        }
+      }
+  }
+
+  @Override
+  public DelegationTokenIdentifier cancelToken(Token<DelegationTokenIdentifier> token,
+      String canceller) throws IOException {
+    DelegationTokenIdentifier id = getTokenIdentifier(token);
+    LOGGER.info("Token cancelation requested for identifier: "+id);
+    this.tokenStore.removeToken(id);
+    return id;
+  }
+
+  /**
+   * Create the password and add it to shared store.
+   */
+  @Override
+  protected byte[] createPassword(DelegationTokenIdentifier id) {
+    byte[] password;
+    DelegationTokenInformation info;
+    synchronized (this) {
+      password = super.createPassword(id);
+      // add new token to shared store
+      // need to persist expiration along with password
+      info = super.currentTokens.remove(id);
+      if (info == null) {
+        throw new IllegalStateException("Failed to retrieve token after creation");
+      }
+    }
+    this.tokenStore.addToken(id, info);
+    return password;
+  }
+
+  @Override
+  public long renewToken(Token<DelegationTokenIdentifier> token,
+      String renewer) throws InvalidToken, IOException {
+    // since renewal is KERBEROS authenticated token may not be cached
+    final DelegationTokenIdentifier id = getTokenIdentifier(token);
+    DelegationTokenInformation tokenInfo = this.tokenStore.getToken(id);
+    if (tokenInfo == null) {
+        throw new InvalidToken("token does not exist: " + id); // no token found
+    }
+    // ensure associated master key is available
+    if (!super.allKeys.containsKey(id.getMasterKeyId())) {
+      LOGGER.info("Unknown master key (id={}), (re)loading keys from token store.",
+        id.getMasterKeyId());
+      reloadKeys();
+    }
+    // reuse super renewal logic
+    synchronized (this) {
+      super.currentTokens.put(id,  tokenInfo);
+      try {
+        return super.renewToken(token, renewer);
+      } finally {
+        super.currentTokens.remove(id);
+      }
+    }
+  }
+
+  public static String encodeWritable(Writable key) throws IOException {
+    ByteArrayOutputStream bos = new ByteArrayOutputStream();
+    DataOutputStream dos = new DataOutputStream(bos);
+    key.write(dos);
+    dos.flush();
+    return Base64.encodeBase64URLSafeString(bos.toByteArray());
+  }
+
+  public static void decodeWritable(Writable w, String idStr) throws IOException {
+    DataInputStream in = new DataInputStream(new ByteArrayInputStream(Base64.decodeBase64(idStr)));
+    w.readFields(in);
+  }
+
+  /**
+   * Synchronize master key updates / sequence generation for multiple nodes.
+   * NOTE: {@Link AbstractDelegationTokenSecretManager} keeps currentKey private, so we need
+   * to utilize this "hook" to manipulate the key through the object reference.
+   * This .20S workaround should cease to exist when Hadoop supports token store.
+   */
+  @Override
+  protected void logUpdateMasterKey(DelegationKey key) throws IOException {
+    int keySeq = this.tokenStore.addMasterKey(encodeWritable(key));
+    // update key with assigned identifier
+    DelegationKey keyWithSeq = new DelegationKey(keySeq, key.getExpiryDate(), key.getKey());
+    String keyStr = encodeWritable(keyWithSeq);
+    this.tokenStore.updateMasterKey(keySeq, keyStr);
+    decodeWritable(key, keyStr);
+    LOGGER.info("New master key with key id={}", key.getKeyId());
+    super.logUpdateMasterKey(key);
+  }
+
+  @Override
+  public synchronized void startThreads() throws IOException {
+    try {
+      // updateCurrentKey needs to be called to initialize the master key
+      // (there should be a null check added in the future in rollMasterKey)
+      // updateCurrentKey();
+      Method m = AbstractDelegationTokenSecretManager.class.getDeclaredMethod("updateCurrentKey");
+      m.setAccessible(true);
+      m.invoke(this);
+    } catch (Exception e) {
+      throw new IOException("Failed to initialize master key", e);
+    }
+    running = true;
+    tokenRemoverThread = new Daemon(new ExpiredTokenRemover());
+    tokenRemoverThread.start();
+  }
+
+  @Override
+  public synchronized void stopThreads() {
+    if (LOGGER.isDebugEnabled()) {
+      LOGGER.debug("Stopping expired delegation token remover thread");
+    }
+    running = false;
+    if (tokenRemoverThread != null) {
+      tokenRemoverThread.interrupt();
+    }
+  }
+
+  /**
+   * Remove expired tokens. Replaces logic in {@link AbstractDelegationTokenSecretManager}
+   * that cannot be reused due to private method access. Logic here can more efficiently
+   * deal with external token store by only loading into memory the minimum data needed.
+   */
+  protected void removeExpiredTokens() {
+    long now = System.currentTimeMillis();
+    Iterator<DelegationTokenIdentifier> i = tokenStore.getAllDelegationTokenIdentifiers()
+        .iterator();
+    while (i.hasNext()) {
+      DelegationTokenIdentifier id = i.next();
+      if (now > id.getMaxDate()) {
+        this.tokenStore.removeToken(id); // no need to look at token info
+      } else {
+        // get token info to check renew date
+        DelegationTokenInformation tokenInfo = tokenStore.getToken(id);
+        if (tokenInfo != null) {
+          if (now > tokenInfo.getRenewDate()) {
+            this.tokenStore.removeToken(id);
+          }
+        }
+      }
+    }
+  }
+
+  /**
+   * Extension of rollMasterKey to remove expired keys from store.
+   * @throws IOException
+   */
+  protected void rollMasterKeyExt() throws IOException {
+    Map<Integer, DelegationKey> keys = reloadKeys();
+    int currentKeyId = super.currentId;
+    HiveDelegationTokenSupport.rollMasterKey(TokenStoreDelegationTokenSecretManager.this);
+    List<DelegationKey> keysAfterRoll = Arrays.asList(getAllKeys());
+    for (DelegationKey key : keysAfterRoll) {
+        keys.remove(key.getKeyId());
+        if (key.getKeyId() == currentKeyId) {
+          tokenStore.updateMasterKey(currentKeyId, encodeWritable(key));
+        }
+    }
+    for (DelegationKey expiredKey : keys.values()) {
+      LOGGER.info("Removing expired key id={}", expiredKey.getKeyId());
+      tokenStore.removeMasterKey(expiredKey.getKeyId());
+    }
+  }
+
+
+  /**
+   * Cloned from {@link AbstractDelegationTokenSecretManager} to deal with private access
+   * restriction (there would not be an need to clone the remove thread if the remove logic was
+   * protected/extensible).
+   */
+  protected class ExpiredTokenRemover extends Thread {
+    private long lastMasterKeyUpdate;
+    private long lastTokenCacheCleanup;
+
+    @Override
+    public void run() {
+      LOGGER.info("Starting expired delegation token remover thread, "
+          + "tokenRemoverScanInterval=" + tokenRemoverScanInterval
+          / (60 * 1000) + " min(s)");
+      try {
+        while (running) {
+          long now = System.currentTimeMillis();
+          if (lastMasterKeyUpdate + keyUpdateInterval < now) {
+            try {
+              rollMasterKeyExt();
+              lastMasterKeyUpdate = now;
+            } catch (IOException e) {
+              LOGGER.error("Master key updating failed. "
+                  + StringUtils.stringifyException(e));
+            }
+          }
+          if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {
+            removeExpiredTokens();
+            lastTokenCacheCleanup = now;
+          }
+          try {
+            Thread.sleep(5000); // 5 seconds
+          } catch (InterruptedException ie) {
+            LOGGER
+            .error("InterruptedExcpetion recieved for ExpiredTokenRemover thread "
+                + ie);
+          }
+        }
+      } catch (Throwable t) {
+        LOGGER.error("ExpiredTokenRemover thread received unexpected exception. "
+            + t, t);
+        Runtime.getRuntime().exit(-1);
+      }
+    }
+  }
+
+}
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java
new file mode 100644
index 0000000..8683496
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java
@@ -0,0 +1,468 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
+import org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport;
+import org.apache.zookeeper.CreateMode;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.WatchedEvent;
+import org.apache.zookeeper.Watcher;
+import org.apache.zookeeper.ZooDefs;
+import org.apache.zookeeper.ZooDefs.Ids;
+import org.apache.zookeeper.ZooKeeper;
+import org.apache.zookeeper.ZooKeeper.States;
+import org.apache.zookeeper.data.ACL;
+import org.apache.zookeeper.data.Id;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * ZooKeeper token store implementation.
+ */
+public class ZooKeeperTokenStore implements DelegationTokenStore {
+
+  private static final Logger LOGGER =
+      LoggerFactory.getLogger(ZooKeeperTokenStore.class.getName());
+
+  protected static final String ZK_SEQ_FORMAT = "%010d";
+  private static final String NODE_KEYS = "/keys";
+  private static final String NODE_TOKENS = "/tokens";
+
+  private String rootNode = "";
+  private volatile ZooKeeper zkSession;
+  private String zkConnectString;
+  private final int zkSessionTimeout = 3000;
+  private long connectTimeoutMillis = -1;
+  private List<ACL> newNodeAcl = Ids.OPEN_ACL_UNSAFE;
+
+  private class ZooKeeperWatcher implements Watcher {
+    public void process(org.apache.zookeeper.WatchedEvent event) {
+      LOGGER.info(event.toString());
+      if (event.getState() == Watcher.Event.KeeperState.Expired) {
+        LOGGER.warn("ZooKeeper session expired, discarding connection");
+        try {
+          zkSession.close();
+        } catch (Throwable e) {
+          LOGGER.warn("Failed to close connection on expired session", e);
+        }
+      }
+    }
+
+  }
+
+  /**
+   * Default constructor for dynamic instantiation w/ Configurable
+   * (ReflectionUtils does not support Configuration constructor injection).
+   */
+  protected ZooKeeperTokenStore() {
+  }
+
+  public ZooKeeperTokenStore(String hostPort) {
+    this.zkConnectString = hostPort;
+    init();
+  }
+
+  private ZooKeeper getSession() {
+    if (zkSession == null || zkSession.getState() == States.CLOSED) {
+        synchronized (this) {
+          if (zkSession == null || zkSession.getState() == States.CLOSED) {
+            try {
+              zkSession = createConnectedClient(this.zkConnectString, this.zkSessionTimeout,
+                this.connectTimeoutMillis, new ZooKeeperWatcher());
+            } catch (IOException ex) {
+              throw new TokenStoreException("Token store error.", ex);
+            }
+          }
+        }
+    }
+    return zkSession;
+  }
+
+  /**
+   * Create a ZooKeeper session that is in connected state.
+   *
+   * @param connectString ZooKeeper connect String
+   * @param sessionTimeout ZooKeeper session timeout
+   * @param connectTimeout milliseconds to wait for connection, 0 or negative value means no wait
+   * @param watchers
+   * @return
+   * @throws InterruptedException
+   * @throws IOException
+   */
+  public static ZooKeeper createConnectedClient(String connectString,
+      int sessionTimeout, long connectTimeout, final Watcher... watchers)
+      throws IOException {
+    final CountDownLatch connected = new CountDownLatch(1);
+    Watcher connectWatcher = new Watcher() {
+      @Override
+      public void process(WatchedEvent event) {
+        switch (event.getState()) {
+        case SyncConnected:
+          connected.countDown();
+          break;
+        }
+        for (Watcher w : watchers) {
+          w.process(event);
+        }
+      }
+    };
+    ZooKeeper zk = new ZooKeeper(connectString, sessionTimeout, connectWatcher);
+    if (connectTimeout > 0) {
+      try {
+        if (!connected.await(connectTimeout, TimeUnit.MILLISECONDS)) {
+          zk.close();
+          throw new IOException("Timeout waiting for connection after "
+              + connectTimeout + "ms");
+        }
+      } catch (InterruptedException e) {
+        throw new IOException("Error waiting for connection.", e);
+      }
+    }
+    return zk;
+  }
+
+  /**
+   * Create a path if it does not already exist ("mkdir -p")
+   * @param zk ZooKeeper session
+   * @param path string with '/' separator
+   * @param acl list of ACL entries
+   * @return
+   * @throws KeeperException
+   * @throws InterruptedException
+   */
+  public static String ensurePath(ZooKeeper zk, String path, List<ACL> acl) throws KeeperException,
+      InterruptedException {
+    String[] pathComps = StringUtils.splitByWholeSeparator(path, "/");
+    String currentPath = "";
+    for (String pathComp : pathComps) {
+      currentPath += "/" + pathComp;
+      try {
+        String node = zk.create(currentPath, new byte[0], acl,
+            CreateMode.PERSISTENT);
+        LOGGER.info("Created path: " + node);
+      } catch (KeeperException.NodeExistsException e) {
+      }
+    }
+    return currentPath;
+  }
+
+  /**
+   * Parse ACL permission string, from ZooKeeperMain private method
+   * @param permString
+   * @return
+   */
+  public static int getPermFromString(String permString) {
+      int perm = 0;
+      for (int i = 0; i < permString.length(); i++) {
+          switch (permString.charAt(i)) {
+          case 'r':
+              perm |= ZooDefs.Perms.READ;
+              break;
+          case 'w':
+              perm |= ZooDefs.Perms.WRITE;
+              break;
+          case 'c':
+              perm |= ZooDefs.Perms.CREATE;
+              break;
+          case 'd':
+              perm |= ZooDefs.Perms.DELETE;
+              break;
+          case 'a':
+              perm |= ZooDefs.Perms.ADMIN;
+              break;
+          default:
+              LOGGER.error("Unknown perm type: " + permString.charAt(i));
+          }
+      }
+      return perm;
+  }
+
+  /**
+   * Parse comma separated list of ACL entries to secure generated nodes, e.g.
+   * <code>sasl:hive/host1@MY.DOMAIN:cdrwa,sasl:hive/host2@MY.DOMAIN:cdrwa</code>
+   * @param aclString
+   * @return ACL list
+   */
+  public static List<ACL> parseACLs(String aclString) {
+    String[] aclComps = StringUtils.splitByWholeSeparator(aclString, ",");
+    List<ACL> acl = new ArrayList<ACL>(aclComps.length);
+    for (String a : aclComps) {
+      if (StringUtils.isBlank(a)) {
+         continue;
+      }
+      a = a.trim();
+      // from ZooKeeperMain private method
+      int firstColon = a.indexOf(':');
+      int lastColon = a.lastIndexOf(':');
+      if (firstColon == -1 || lastColon == -1 || firstColon == lastColon) {
+         LOGGER.error(a + " does not have the form scheme:id:perm");
+         continue;
+      }
+      ACL newAcl = new ACL();
+      newAcl.setId(new Id(a.substring(0, firstColon), a.substring(
+          firstColon + 1, lastColon)));
+      newAcl.setPerms(getPermFromString(a.substring(lastColon + 1)));
+      acl.add(newAcl);
+    }
+    return acl;
+  }
+
+  private void init() {
+    if (this.zkConnectString == null) {
+      throw new IllegalStateException("Not initialized");
+    }
+
+    if (this.zkSession != null) {
+      try {
+        this.zkSession.close();
+      } catch (InterruptedException ex) {
+        LOGGER.warn("Failed to close existing session.", ex);
+      }
+    }
+    ZooKeeper zk = getSession();
+
+    try {
+        ensurePath(zk, rootNode + NODE_KEYS, newNodeAcl);
+        ensurePath(zk, rootNode + NODE_TOKENS, newNodeAcl);
+      } catch (Exception e) {
+        throw new TokenStoreException("Failed to validate token path.", e);
+      }
+  }
+
+  @Override
+  public void setConf(Configuration conf) {
+    if (conf == null) {
+       throw new IllegalArgumentException("conf is null");
+    }
+    this.zkConnectString = conf.get(
+      HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_CONNECT_STR, null);
+    this.connectTimeoutMillis = conf.getLong(
+      HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_CONNECT_TIMEOUTMILLIS, -1);
+    this.rootNode = conf.get(
+      HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ZNODE,
+      HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ZNODE_DEFAULT);
+    String csv = conf.get(HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ACL, null);
+    if (StringUtils.isNotBlank(csv)) {
+       this.newNodeAcl = parseACLs(csv);
+    }
+    init();
+  }
+
+  @Override
+  public Configuration getConf() {
+    return null; // not required
+  }
+
+  private Map<Integer, byte[]> getAllKeys() throws KeeperException,
+      InterruptedException {
+
+    String masterKeyNode = rootNode + NODE_KEYS;
+    ZooKeeper zk = getSession();
+    List<String> nodes = zk.getChildren(masterKeyNode, false);
+    Map<Integer, byte[]> result = new HashMap<Integer, byte[]>();
+    for (String node : nodes) {
+      byte[] data = zk.getData(masterKeyNode + "/" + node, false, null);
+      if (data != null) {
+        result.put(getSeq(node), data);
+      }
+    }
+    return result;
+  }
+
+  private int getSeq(String path) {
+    String[] pathComps = path.split("/");
+    return Integer.parseInt(pathComps[pathComps.length-1]);
+  }
+
+  @Override
+  public int addMasterKey(String s) {
+    try {
+      ZooKeeper zk = getSession();
+      String newNode = zk.create(rootNode + NODE_KEYS + "/", s.getBytes(), newNodeAcl,
+          CreateMode.PERSISTENT_SEQUENTIAL);
+      LOGGER.info("Added key {}", newNode);
+      return getSeq(newNode);
+    } catch (KeeperException ex) {
+      throw new TokenStoreException(ex);
+    } catch (InterruptedException ex) {
+      throw new TokenStoreException(ex);
+    }
+  }
+
+  @Override
+  public void updateMasterKey(int keySeq, String s) {
+    try {
+      ZooKeeper zk = getSession();
+      zk.setData(rootNode + NODE_KEYS + "/" + String.format(ZK_SEQ_FORMAT, keySeq), s.getBytes(),
+          -1);
+    } catch (KeeperException ex) {
+      throw new TokenStoreException(ex);
+    } catch (InterruptedException ex) {
+      throw new TokenStoreException(ex);
+    }
+  }
+
+  @Override
+  public boolean removeMasterKey(int keySeq) {
+    try {
+      ZooKeeper zk = getSession();
+      zk.delete(rootNode + NODE_KEYS + "/" + String.format(ZK_SEQ_FORMAT, keySeq), -1);
+      return true;
+    } catch (KeeperException.NoNodeException ex) {
+      return false;
+    } catch (KeeperException ex) {
+      throw new TokenStoreException(ex);
+    } catch (InterruptedException ex) {
+      throw new TokenStoreException(ex);
+    }
+  }
+
+  @Override
+  public String[] getMasterKeys() {
+    try {
+      Map<Integer, byte[]> allKeys = getAllKeys();
+      String[] result = new String[allKeys.size()];
+      int resultIdx = 0;
+      for (byte[] keyBytes : allKeys.values()) {
+          result[resultIdx++] = new String(keyBytes);
+      }
+      return result;
+    } catch (KeeperException ex) {
+      throw new TokenStoreException(ex);
+    } catch (InterruptedException ex) {
+      throw new TokenStoreException(ex);
+    }
+  }
+
+
+  private String getTokenPath(DelegationTokenIdentifier tokenIdentifier) {
+    try {
+      return rootNode + NODE_TOKENS + "/"
+          + TokenStoreDelegationTokenSecretManager.encodeWritable(tokenIdentifier);
+    } catch (IOException ex) {
+      throw new TokenStoreException("Failed to encode token identifier", ex);
+    }
+  }
+
+  @Override
+  public boolean addToken(DelegationTokenIdentifier tokenIdentifier,
+      DelegationTokenInformation token) {
+    try {
+      ZooKeeper zk = getSession();
+      byte[] tokenBytes = HiveDelegationTokenSupport.encodeDelegationTokenInformation(token);
+      String newNode = zk.create(getTokenPath(tokenIdentifier),
+          tokenBytes, newNodeAcl, CreateMode.PERSISTENT);
+      LOGGER.info("Added token: {}", newNode);
+      return true;
+    } catch (KeeperException.NodeExistsException ex) {
+      return false;
+    } catch (KeeperException ex) {
+      throw new TokenStoreException(ex);
+    } catch (InterruptedException ex) {
+      throw new TokenStoreException(ex);
+    }
+  }
+
+  @Override
+  public boolean removeToken(DelegationTokenIdentifier tokenIdentifier) {
+    try {
+      ZooKeeper zk = getSession();
+      zk.delete(getTokenPath(tokenIdentifier), -1);
+      return true;
+    } catch (KeeperException.NoNodeException ex) {
+      return false;
+    } catch (KeeperException ex) {
+      throw new TokenStoreException(ex);
+    } catch (InterruptedException ex) {
+      throw new TokenStoreException(ex);
+    }
+  }
+
+  @Override
+  public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {
+    try {
+      ZooKeeper zk = getSession();
+      byte[] tokenBytes = zk.getData(getTokenPath(tokenIdentifier), false, null);
+      try {
+        return HiveDelegationTokenSupport.decodeDelegationTokenInformation(tokenBytes);
+      } catch (Exception ex) {
+        throw new TokenStoreException("Failed to decode token", ex);
+      }
+    } catch (KeeperException.NoNodeException ex) {
+      return null;
+    } catch (KeeperException ex) {
+      throw new TokenStoreException(ex);
+    } catch (InterruptedException ex) {
+      throw new TokenStoreException(ex);
+    }
+  }
+
+  @Override
+  public List<DelegationTokenIdentifier> getAllDelegationTokenIdentifiers() {
+    String containerNode = rootNode + NODE_TOKENS;
+    final List<String> nodes;
+    try  {
+      nodes = getSession().getChildren(containerNode, false);
+    } catch (KeeperException ex) {
+      throw new TokenStoreException(ex);
+    } catch (InterruptedException ex) {
+      throw new TokenStoreException(ex);
+    }
+    List<DelegationTokenIdentifier> result = new java.util.ArrayList<DelegationTokenIdentifier>(
+        nodes.size());
+    for (String node : nodes) {
+      DelegationTokenIdentifier id = new DelegationTokenIdentifier();
+      try {
+        TokenStoreDelegationTokenSecretManager.decodeWritable(id, node);
+        result.add(id);
+      } catch (Exception e) {
+        LOGGER.warn("Failed to decode token '{}'", node);
+      }
+    }
+    return result;
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (this.zkSession != null) {
+      try {
+        this.zkSession.close();
+      } catch (InterruptedException ex) {
+        LOGGER.warn("Failed to close existing session.", ex);
+      }
+    }
+  }
+
+  @Override
+  public void setStore(Object hmsHandler) throws TokenStoreException {
+    // no-op.
+  }
+
+}
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/client/TUGIAssumingTransport.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/client/TUGIAssumingTransport.java
new file mode 100644
index 0000000..fe18706
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/client/TUGIAssumingTransport.java
@@ -0,0 +1,74 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift.client;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+
+import org.apache.hadoop.hive.thrift.TFilterTransport;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
+
+/**
+  * The Thrift SASL transports call Sasl.createSaslServer and Sasl.createSaslClient
+  * inside open(). So, we need to assume the correct UGI when the transport is opened
+  * so that the SASL mechanisms have access to the right principal. This transport
+  * wraps the Sasl transports to set up the right UGI context for open().
+  *
+  * This is used on the client side, where the API explicitly opens a transport to
+  * the server.
+  */
+ public class TUGIAssumingTransport extends TFilterTransport {
+   protected UserGroupInformation ugi;
+
+   public TUGIAssumingTransport(TTransport wrapped, UserGroupInformation ugi) {
+     super(wrapped);
+     this.ugi = ugi;
+   }
+
+   @Override
+   public void open() throws TTransportException {
+     try {
+       ugi.doAs(new PrivilegedExceptionAction<Void>() {
+         public Void run() {
+           try {
+             wrapped.open();
+           } catch (TTransportException tte) {
+             // Wrap the transport exception in an RTE, since UGI.doAs() then goes
+             // and unwraps this for us out of the doAs block. We then unwrap one
+             // more time in our catch clause to get back the TTE. (ugh)
+             throw new RuntimeException(tte);
+           }
+           return null;
+         }
+       });
+     } catch (IOException ioe) {
+       throw new RuntimeException("Received an ioe we never threw!", ioe);
+     } catch (InterruptedException ie) {
+       throw new RuntimeException("Received an ie we never threw!", ie);
+     } catch (RuntimeException rte) {
+       if (rte.getCause() instanceof TTransportException) {
+         throw (TTransportException)rte.getCause();
+       } else {
+         throw rte;
+       }
+     }
+   }
+ }
diff --git a/src/shims/common-secure/src/main/java/org/apache/hadoop/security/token/delegation/HiveDelegationTokenSupport.java b/src/shims/common-secure/src/main/java/org/apache/hadoop/security/token/delegation/HiveDelegationTokenSupport.java
new file mode 100644
index 0000000..6b39a14
--- /dev/null
+++ b/src/shims/common-secure/src/main/java/org/apache/hadoop/security/token/delegation/HiveDelegationTokenSupport.java
@@ -0,0 +1,68 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.security.token.delegation;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
+
+/**
+ * Workaround for serialization of {@link DelegationTokenInformation} through package access.
+ * Future version of Hadoop should add this to DelegationTokenInformation itself.
+ */
+public final class HiveDelegationTokenSupport {
+
+  private HiveDelegationTokenSupport() {}
+
+  public static byte[] encodeDelegationTokenInformation(DelegationTokenInformation token) {
+    try {
+      ByteArrayOutputStream bos = new ByteArrayOutputStream();
+      DataOutputStream out = new DataOutputStream(bos);
+      WritableUtils.writeVInt(out, token.password.length);
+      out.write(token.password);
+      out.writeLong(token.renewDate);
+      out.flush();
+      return bos.toByteArray();
+    } catch (IOException ex) {
+      throw new RuntimeException("Failed to encode token.", ex);
+    }
+  }
+
+  public static DelegationTokenInformation decodeDelegationTokenInformation(byte[] tokenBytes)
+      throws IOException {
+    DataInputStream in = new DataInputStream(new ByteArrayInputStream(tokenBytes));
+    DelegationTokenInformation token = new DelegationTokenInformation(0, null);
+    int len = WritableUtils.readVInt(in);
+    token.password = new byte[len];
+    in.readFully(token.password);
+    token.renewDate = in.readLong();
+    return token;
+  }
+
+  public static void rollMasterKey(
+      AbstractDelegationTokenSecretManager<? extends AbstractDelegationTokenIdentifier> mgr)
+      throws IOException {
+    mgr.rollMasterKey();
+  }
+
+}
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/fs/ProxyFileSystem.java b/src/shims/common/src/main/java/org/apache/hadoop/fs/ProxyFileSystem.java
new file mode 100644
index 0000000..cb1e2b7
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/fs/ProxyFileSystem.java
@@ -0,0 +1,291 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs;
+
+import java.io.IOException;
+import java.net.URI;
+import java.net.URISyntaxException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.util.Shell;
+
+/****************************************************************
+ * A FileSystem that can serve a given scheme/authority using some
+ * other file system. In that sense, it serves as a proxy for the
+ * real/underlying file system
+ *****************************************************************/
+
+public class ProxyFileSystem extends FilterFileSystem {
+
+  protected String myScheme;
+  protected String myAuthority;
+  protected URI myUri;
+
+  protected String realScheme;
+  protected String realAuthority;
+  protected URI realUri;
+
+
+
+  protected Path swizzleParamPath(Path p) {
+    String pathUriString = p.toUri().toString();
+    URI newPathUri = URI.create(pathUriString);
+    return new Path (realScheme, realAuthority, newPathUri.getPath());
+  }
+
+  private Path swizzleReturnPath(Path p) {
+    String pathUriString = p.toUri().toString();
+    URI newPathUri = URI.create(pathUriString);
+    return new Path (myScheme, myAuthority, newPathUri.getPath());
+  }
+
+  protected FileStatus swizzleFileStatus(FileStatus orig, boolean isParam) {
+    FileStatus ret =
+      new FileStatus(orig.getLen(), orig.isDir(), orig.getReplication(),
+                     orig.getBlockSize(), orig.getModificationTime(),
+                     orig.getAccessTime(), orig.getPermission(),
+                     orig.getOwner(), orig.getGroup(),
+                     isParam ? swizzleParamPath(orig.getPath()) :
+                     swizzleReturnPath(orig.getPath()));
+    return ret;
+  }
+
+  public ProxyFileSystem() {
+    throw new RuntimeException ("Unsupported constructor");
+  }
+
+  public ProxyFileSystem(FileSystem fs) {
+    throw new RuntimeException ("Unsupported constructor");
+  }
+
+  /**
+   *
+   * @param p
+   * @return
+   * @throws IOException
+   */
+  public Path resolvePath(final Path p) throws IOException {
+    // Return the fully-qualified path of path f resolving the path
+    // through any symlinks or mount point
+    checkPath(p);
+    return getFileStatus(p).getPath();
+  }
+
+  /**
+   * Create a proxy file system for fs.
+   *
+   * @param fs FileSystem to create proxy for
+   * @param myUri URI to use as proxy. Only the scheme and authority from
+   *              this are used right now
+   */
+  public ProxyFileSystem(FileSystem fs, URI myUri) {
+    super(fs);
+
+    URI realUri = fs.getUri();
+    this.realScheme = realUri.getScheme();
+    this.realAuthority=realUri.getAuthority();
+    this.realUri = realUri;
+
+    this.myScheme = myUri.getScheme();
+    this.myAuthority=myUri.getAuthority();
+    this.myUri = myUri;
+  }
+
+  @Override
+  public void initialize(URI name, Configuration conf) throws IOException {
+    try {
+      URI realUri = new URI (realScheme, realAuthority,
+                            name.getPath(), name.getQuery(), name.getFragment());
+      super.initialize(realUri, conf);
+    } catch (URISyntaxException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  @Override
+  public URI getUri() {
+    return myUri;
+  }
+
+  @Override
+  public String getName() {
+    return getUri().toString();
+  }
+
+  @Override
+  public Path makeQualified(Path path) {
+    return swizzleReturnPath(super.makeQualified(swizzleParamPath(path)));
+  }
+
+
+  @Override
+  protected void checkPath(final Path path) {
+    super.checkPath(swizzleParamPath(path));
+  }
+
+  @Override
+  public BlockLocation[] getFileBlockLocations(FileStatus file, long start,
+    long len) throws IOException {
+    return super.getFileBlockLocations(swizzleFileStatus(file, true),
+                                       start, len);
+  }
+
+  @Override
+  public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+    return super.open(swizzleParamPath(f), bufferSize);
+  }
+
+  @Override
+  public FSDataOutputStream append(Path f, int bufferSize,
+      Progressable progress) throws IOException {
+    return super.append(swizzleParamPath(f), bufferSize, progress);
+  }
+
+  @Override
+  public FSDataOutputStream create(Path f, FsPermission permission,
+      boolean overwrite, int bufferSize, short replication, long blockSize,
+      Progressable progress) throws IOException {
+    return super.create(swizzleParamPath(f), permission,
+        overwrite, bufferSize, replication, blockSize, progress);
+  }
+
+  @Override
+  public boolean setReplication(Path src, short replication) throws IOException {
+    return super.setReplication(swizzleParamPath(src), replication);
+  }
+
+  @Override
+  public boolean rename(Path src, Path dst) throws IOException {
+    return super.rename(swizzleParamPath(src), swizzleParamPath(dst));
+  }
+
+  @Override
+  public boolean delete(Path f, boolean recursive) throws IOException {
+    return super.delete(swizzleParamPath(f), recursive);
+  }
+
+  @Override
+  public boolean deleteOnExit(Path f) throws IOException {
+    return super.deleteOnExit(swizzleParamPath(f));
+  }
+
+  @Override
+  public FileStatus[] listStatus(Path f) throws IOException {
+    FileStatus[] orig = super.listStatus(swizzleParamPath(f));
+    FileStatus[] ret = new FileStatus [orig.length];
+    for (int i=0; i<orig.length; i++) {
+      ret[i] = swizzleFileStatus(orig[i], false);
+    }
+    return ret;
+  }
+
+  @Override
+  public Path getHomeDirectory() {
+    return swizzleReturnPath(super.getHomeDirectory());
+  }
+
+  @Override
+  public void setWorkingDirectory(Path newDir) {
+    super.setWorkingDirectory(swizzleParamPath(newDir));
+  }
+
+  @Override
+  public Path getWorkingDirectory() {
+    return swizzleReturnPath(super.getWorkingDirectory());
+  }
+
+  @Override
+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {
+    return super.mkdirs(swizzleParamPath(f), permission);
+  }
+
+  @Override
+  public void copyFromLocalFile(boolean delSrc, Path src, Path dst)
+    throws IOException {
+    super.copyFromLocalFile(delSrc, swizzleParamPath(src), swizzleParamPath(dst));
+  }
+
+  @Override
+  public void copyFromLocalFile(boolean delSrc, boolean overwrite,
+                                Path[] srcs, Path dst)
+    throws IOException {
+    super.copyFromLocalFile(delSrc, overwrite, srcs, swizzleParamPath(dst));
+  }
+
+  @Override
+  public void copyFromLocalFile(boolean delSrc, boolean overwrite,
+                                Path src, Path dst)
+    throws IOException {
+    super.copyFromLocalFile(delSrc, overwrite, src, swizzleParamPath(dst));
+  }
+
+  @Override
+  public void copyToLocalFile(boolean delSrc, Path src, Path dst)
+    throws IOException {
+    super.copyToLocalFile(delSrc, swizzleParamPath(src), dst);
+  }
+
+  @Override
+  public Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile)
+    throws IOException {
+    return super.startLocalOutput(swizzleParamPath(fsOutputFile), tmpLocalFile);
+  }
+
+  @Override
+  public void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile)
+    throws IOException {
+    super.completeLocalOutput(swizzleParamPath(fsOutputFile), tmpLocalFile);
+  }
+
+  @Override
+  public ContentSummary getContentSummary(Path f) throws IOException {
+    return super.getContentSummary(swizzleParamPath(f));
+  }
+
+  @Override
+  public FileStatus getFileStatus(Path f) throws IOException {
+    return swizzleFileStatus(super.getFileStatus(swizzleParamPath(f)), false);
+  }
+
+  @Override
+  public FileChecksum getFileChecksum(Path f) throws IOException {
+    return super.getFileChecksum(swizzleParamPath(f));
+  }
+
+  @Override
+  public void setOwner(Path p, String username, String groupname
+      ) throws IOException {
+    super.setOwner(swizzleParamPath(p), username, groupname);
+  }
+
+  @Override
+  public void setTimes(Path p, long mtime, long atime
+      ) throws IOException {
+    super.setTimes(swizzleParamPath(p), mtime, atime);
+  }
+
+  @Override
+  public void setPermission(Path p, FsPermission permission
+      ) throws IOException {
+    super.setPermission(swizzleParamPath(p), permission);
+  }
+}
+
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java b/src/shims/common/src/main/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java
new file mode 100644
index 0000000..228a972
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java
@@ -0,0 +1,72 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs;
+
+import java.io.IOException;
+import java.net.URI;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.util.Shell;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.hive.shims.HadoopShims;
+
+/****************************************************************
+ * A Proxy for LocalFileSystem
+ *
+ * Serves uri's corresponding to 'pfile:///' namespace with using
+ * a LocalFileSystem
+ *****************************************************************/
+
+public class ProxyLocalFileSystem extends FilterFileSystem {
+
+  protected LocalFileSystem localFs;
+
+  public ProxyLocalFileSystem() {
+    localFs = new LocalFileSystem();
+  }
+
+  public ProxyLocalFileSystem(FileSystem fs) {
+    throw new RuntimeException ("Unsupported Constructor");
+  }
+
+  @Override
+  public void initialize(URI name, Configuration conf) throws IOException {
+    // create a proxy for the local filesystem
+    // the scheme/authority serving as the proxy is derived
+    // from the supplied URI
+    String scheme = name.getScheme();
+    String nameUriString = name.toString();
+    if (Shell.WINDOWS) {
+      // Replace the encoded backward slash with forward slash
+      // Remove the windows drive letter
+      nameUriString = nameUriString.replaceAll("%5C", "/")
+          .replaceFirst("/[c-zC-Z]:", "/")
+          .replaceFirst("^[c-zC-Z]:", "");
+      name = URI.create(nameUriString);
+    }
+
+    String authority = name.getAuthority() != null ? name.getAuthority() : "";
+    String proxyUriString = nameUriString + "://" + authority + "/";
+
+    fs = ShimLoader.getHadoopShims().createProxyFileSystem(
+        localFs, URI.create(proxyUriString));
+
+    fs.initialize(name, conf);
+  }
+}
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandler.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandler.java
new file mode 100644
index 0000000..fba32fb
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandler.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.io;
+
+import java.io.IOException;
+
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.RecordReader;
+
+/**
+ * HiveIOExceptionHandler defines an interface that all io exception handler in
+ * Hive should implement. Different IO exception handlers can implement
+ * different logics based on the exception input into it.
+ */
+public interface HiveIOExceptionHandler {
+
+  /**
+   * process exceptions raised when creating a record reader.
+   *
+   * @param e
+   * @return RecordReader
+   */
+  public RecordReader<?, ?> handleRecordReaderCreationException(Exception e)
+      throws IOException;
+
+  /**
+   * process exceptions thrown when calling rr's next
+   *
+   * @param e
+   * @param result
+   * @throws IOException
+   */
+  public void handleRecorReaderNextException(Exception e,
+      HiveIOExceptionNextHandleResult result) throws IOException;
+
+}
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java
new file mode 100644
index 0000000..a58f1f2
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java
@@ -0,0 +1,124 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.io;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.util.ReflectionUtils;
+
+/**
+ * An exception handler chain that process the input exception by going through
+ * all exceptions defined in this chain one by one until either one exception
+ * handler returns true or it reaches the end of the chain. If it reaches the
+ * end of the chain, and still no exception handler returns true, throw the
+ * exception to the caller.
+ */
+public class HiveIOExceptionHandlerChain {
+
+  public static String HIVE_IO_EXCEPTION_HANDLE_CHAIN = "hive.io.exception.handlers";
+
+  @SuppressWarnings("unchecked")
+  public static HiveIOExceptionHandlerChain getHiveIOExceptionHandlerChain(
+      JobConf conf) {
+    HiveIOExceptionHandlerChain chain = new HiveIOExceptionHandlerChain();
+    String exceptionHandlerStr = conf.get(HIVE_IO_EXCEPTION_HANDLE_CHAIN);
+    List<HiveIOExceptionHandler> handlerChain = new ArrayList<HiveIOExceptionHandler>();
+    if (exceptionHandlerStr != null && !exceptionHandlerStr.trim().equals("")) {
+      String[] handlerArr = exceptionHandlerStr.split(",");
+      if (handlerArr != null && handlerArr.length > 0) {
+        for (String handlerStr : handlerArr) {
+          if (!handlerStr.trim().equals("")) {
+            try {
+              Class<? extends HiveIOExceptionHandler> handlerCls =
+                (Class<? extends HiveIOExceptionHandler>) Class.forName(handlerStr);
+              HiveIOExceptionHandler handler = ReflectionUtils.newInstance(handlerCls, null);
+              handlerChain.add(handler);
+            } catch (Exception e) {
+            }
+          }
+        }
+      }
+    }
+
+    chain.setHandlerChain(handlerChain);
+    return chain;
+  }
+
+  private List<HiveIOExceptionHandler> handlerChain;
+
+  /**
+   * @return the exception handler chain defined
+   */
+  protected List<HiveIOExceptionHandler> getHandlerChain() {
+    return handlerChain;
+  }
+
+  /**
+   * set the exception handler chain
+   * @param handlerChain
+   */
+  protected void setHandlerChain(List<HiveIOExceptionHandler> handlerChain) {
+    this.handlerChain = handlerChain;
+  }
+
+  public RecordReader<?,?>  handleRecordReaderCreationException(Exception e) throws IOException {
+    RecordReader<?, ?> ret = null;
+
+    if (handlerChain != null && handlerChain.size() > 0) {
+      for (HiveIOExceptionHandler handler : handlerChain) {
+        ret = handler.handleRecordReaderCreationException(e);
+        if (ret != null) {
+          return ret;
+        }
+      }
+    }
+
+    //re-throw the exception as an IOException
+    throw new IOException(e);
+  }
+
+  /**
+   * This is to handle exception when doing next operations. Here we use a
+   * HiveIOExceptionNextHandleResult to store the results of each handler. If
+   * the exception is handled by one handler, the handler should set
+   * HiveIOExceptionNextHandleResult to be handled, and also set the handle
+   * result. The handle result is used to return the reader's next to determine
+   * if need to open a new file for read or not.
+   */
+  public boolean handleRecordReaderNextException(Exception e)
+      throws IOException {
+    HiveIOExceptionNextHandleResult result = new HiveIOExceptionNextHandleResult();
+    if (handlerChain != null && handlerChain.size() > 0) {
+      for (HiveIOExceptionHandler handler : handlerChain) {
+        handler.handleRecorReaderNextException(e, result);
+        if (result.getHandled()) {
+          return result.getHandleResult();
+        }
+      }
+    }
+
+    //re-throw the exception as an IOException
+    throw new IOException(e);
+  }
+
+}
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java
new file mode 100644
index 0000000..d972edb
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java
@@ -0,0 +1,82 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.io;
+
+import java.io.IOException;
+
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+
+public class HiveIOExceptionHandlerUtil {
+
+  private static ThreadLocal<HiveIOExceptionHandlerChain> handlerChainInstance =
+    new ThreadLocal<HiveIOExceptionHandlerChain>();
+
+  private static HiveIOExceptionHandlerChain get(JobConf job) {
+    HiveIOExceptionHandlerChain cache = HiveIOExceptionHandlerUtil.handlerChainInstance
+        .get();
+    if (cache == null) {
+      HiveIOExceptionHandlerChain toSet = HiveIOExceptionHandlerChain
+          .getHiveIOExceptionHandlerChain(job);
+      handlerChainInstance.set(toSet);
+      cache = HiveIOExceptionHandlerUtil.handlerChainInstance.get();
+    }
+    return cache;
+  }
+
+  /**
+   * Handle exception thrown when creating record reader. In case that there is
+   * an exception raised when construction the record reader and one handler can
+   * handle this exception, it should return an record reader, which is either a
+   * dummy empty record reader or a specific record reader that do some magic.
+   *
+   * @param e
+   * @param job
+   * @return RecordReader
+   * @throws IOException
+   */
+  public static RecordReader handleRecordReaderCreationException(Exception e,
+      JobConf job) throws IOException {
+    HiveIOExceptionHandlerChain ioExpectionHandlerChain = get(job);
+    if (ioExpectionHandlerChain != null) {
+      return ioExpectionHandlerChain.handleRecordReaderCreationException(e);
+    }
+    throw new IOException(e);
+  }
+
+  /**
+   * Handle exception thrown when calling record reader's next. If this
+   * exception is handled by one handler, will just return true. Otherwise,
+   * either re-throw this exception in one handler or at the end of the handler
+   * chain.
+   *
+   * @param e
+   * @param job
+   * @return true on success
+   * @throws IOException
+   */
+  public static boolean handleRecordReaderNextException(Exception e, JobConf job)
+      throws IOException {
+    HiveIOExceptionHandlerChain ioExpectionHandlerChain = get(job);
+    if (ioExpectionHandlerChain != null) {
+      return ioExpectionHandlerChain.handleRecordReaderNextException(e);
+    }
+    throw new IOException(e);
+  }
+
+}
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionNextHandleResult.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionNextHandleResult.java
new file mode 100644
index 0000000..6143c8f
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionNextHandleResult.java
@@ -0,0 +1,55 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.io;
+
+/**
+ * A container to store handling results for exceptions produced in record
+ * reader's next(). It basically contains 2 fields, one is to store if it is
+ * handled or not, another field to store the result.
+ */
+public class HiveIOExceptionNextHandleResult {
+
+  // this exception has been handled
+  private boolean handled;
+
+  //the handling results
+  private boolean handleResult;
+
+  public boolean getHandled() {
+    return handled;
+  }
+
+  public void setHandled(boolean handled) {
+    this.handled = handled;
+  }
+
+  public boolean getHandleResult() {
+    return handleResult;
+  }
+
+  public void setHandleResult(boolean handleResult) {
+    this.handleResult = handleResult;
+  }
+
+  public void clear() {
+    handled = false;
+    handleResult = false;
+  }
+
+}
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/CombineHiveKey.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/CombineHiveKey.java
new file mode 100644
index 0000000..bd0934e
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/CombineHiveKey.java
@@ -0,0 +1,54 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.shims;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.WritableComparable;
+
+public class CombineHiveKey implements WritableComparable {
+  Object key;
+
+  public CombineHiveKey(Object key) {
+    this.key = key;
+  }
+
+  public Object getKey() {
+    return key;
+  }
+
+  public void setKey(Object key) {
+    this.key = key;
+  }
+
+  public void write(DataOutput out) throws IOException {
+    throw new IOException("Method not supported");
+  }
+
+  public void readFields(DataInput in) throws IOException {
+    throw new IOException("Method not supported");
+  }
+
+  public int compareTo(Object w) {
+    assert false;
+    return 0;
+  }
+}
\ No newline at end of file
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
new file mode 100644
index 0000000..ec4eef8
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
@@ -0,0 +1,555 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.shims;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.net.InetSocketAddress;
+import java.net.MalformedURLException;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+
+import javax.security.auth.login.LoginException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobProfile;
+import org.apache.hadoop.mapred.JobStatus;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.hadoop.mapred.TaskCompletionEvent;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.JobID;
+import org.apache.hadoop.mapreduce.OutputFormat;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.TaskID;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.util.Progressable;
+
+/**
+ * In order to be compatible with multiple versions of Hadoop, all parts
+ * of the Hadoop interface that are not cross-version compatible are
+ * encapsulated in an implementation of this class. Users should use
+ * the ShimLoader class as a factory to obtain an implementation of
+ * HadoopShims corresponding to the version of Hadoop currently on the
+ * classpath.
+ */
+public interface HadoopShims {
+
+  static final Log LOG = LogFactory.getLog(HadoopShims.class);
+
+  /**
+   * Return true if the current version of Hadoop uses the JobShell for
+   * command line interpretation.
+   */
+  boolean usesJobShell();
+
+  /**
+   * Constructs and Returns TaskAttempt Log Url
+   * or null if the TaskLogServlet is not available
+   *
+   *  @return TaskAttempt Log Url
+   */
+  String getTaskAttemptLogUrl(JobConf conf,
+    String taskTrackerHttpAddress,
+    String taskAttemptId)
+    throws MalformedURLException;
+
+  /**
+   * Return true if the job has not switched to RUNNING state yet
+   * and is still in PREP state
+   */
+  boolean isJobPreparing(RunningJob job) throws IOException;
+
+  /**
+   * Calls fs.deleteOnExit(path) if such a function exists.
+   *
+   * @return true if the call was successful
+   */
+  boolean fileSystemDeleteOnExit(FileSystem fs, Path path) throws IOException;
+
+  /**
+   * Calls fmt.validateInput(conf) if such a function exists.
+   */
+  void inputFormatValidateInput(InputFormat fmt, JobConf conf) throws IOException;
+
+  /**
+   * If JobClient.getCommandLineConfig exists, sets the given
+   * property/value pair in that Configuration object.
+   *
+   * This applies for Hadoop 0.17 through 0.19
+   */
+  void setTmpFiles(String prop, String files);
+
+  /**
+   * return the last access time of the given file.
+   * @param file
+   * @return last access time. -1 if not supported.
+   */
+  long getAccessTime(FileStatus file);
+
+  /**
+   * return the Kerberos short name
+   * @param full Kerberos name
+   * @return short Kerberos name
+   * @throws IOException
+   */
+  String getKerberosShortName(String kerberosName) throws IOException;
+
+  /**
+   * Returns a shim to wrap MiniMrCluster
+   */
+  public MiniMrShim getMiniMrCluster(Configuration conf, int numberOfTaskTrackers,
+                                     String nameNode, int numDir) throws IOException;
+
+  /**
+   * Shim for MiniMrCluster
+   */
+  public interface MiniMrShim {
+    public int getJobTrackerPort() throws UnsupportedOperationException;
+    public void shutdown() throws IOException;
+    public void setupConfiguration(Configuration conf);
+  }
+
+  /**
+   * Returns a shim to wrap MiniDFSCluster. This is necessary since this class
+   * was moved from org.apache.hadoop.dfs to org.apache.hadoop.hdfs
+   */
+  MiniDFSShim getMiniDfs(Configuration conf,
+      int numDataNodes,
+      boolean format,
+      String[] racks) throws IOException;
+
+  /**
+   * Shim around the functions in MiniDFSCluster that Hive uses.
+   */
+  public interface MiniDFSShim {
+    FileSystem getFileSystem() throws IOException;
+
+    void shutdown() throws IOException;
+  }
+
+  /**
+   * We define this function here to make the code compatible between
+   * hadoop 0.17 and hadoop 0.20.
+   *
+   * Hive binary that compiled Text.compareTo(Text) with hadoop 0.20 won't
+   * work with hadoop 0.17 because in hadoop 0.20, Text.compareTo(Text) is
+   * implemented in org.apache.hadoop.io.BinaryComparable, and Java compiler
+   * references that class, which is not available in hadoop 0.17.
+   */
+  int compareText(Text a, Text b);
+
+  CombineFileInputFormatShim getCombineFileInputFormat();
+
+  String getInputFormatClassName();
+
+  /**
+   * Wrapper for Configuration.setFloat, which was not introduced
+   * until 0.20.
+   */
+  void setFloatConf(Configuration conf, String varName, float val);
+
+  /**
+   * getTaskJobIDs returns an array of String with two elements. The first
+   * element is a string representing the task id and the second is a string
+   * representing the job id. This is necessary as TaskID and TaskAttemptID
+   * are not supported in Haddop 0.17
+   */
+  String[] getTaskJobIDs(TaskCompletionEvent t);
+
+  int createHadoopArchive(Configuration conf, Path parentDir, Path destDir,
+      String archiveName) throws Exception;
+
+  public URI getHarUri(URI original, URI base, URI originalBase)
+        throws URISyntaxException;
+  /**
+   * Hive uses side effect files exclusively for it's output. It also manages
+   * the setup/cleanup/commit of output from the hive client. As a result it does
+   * not need support for the same inside the MR framework
+   *
+   * This routine sets the appropriate options related to bypass setup/cleanup/commit
+   * support in the MR framework, but does not set the OutputFormat class.
+   */
+  void prepareJobOutput(JobConf conf);
+
+  /**
+   * Used by TaskLogProcessor to Remove HTML quoting from a string
+   * @param item the string to unquote
+   * @return the unquoted string
+   *
+   */
+  public String unquoteHtmlChars(String item);
+
+
+
+  public void closeAllForUGI(UserGroupInformation ugi);
+
+  /**
+   * Get the UGI that the given job configuration will run as.
+   *
+   * In secure versions of Hadoop, this simply returns the current
+   * access control context's user, ignoring the configuration.
+   */
+  public UserGroupInformation getUGIForConf(Configuration conf) throws LoginException, IOException;
+
+  /**
+   * Used by metastore server to perform requested rpc in client context.
+   * @param <T>
+   * @param ugi
+   * @param pvea
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  public <T> T doAs(UserGroupInformation ugi, PrivilegedExceptionAction<T> pvea) throws
+    IOException, InterruptedException;
+
+  /**
+   * Once a delegation token is stored in a file, the location is specified
+   * for a child process that runs hadoop operations, using an environment
+   * variable .
+   * @return Return the name of environment variable used by hadoop to find
+   *  location of token file
+   */
+  public String getTokenFileLocEnvName();
+
+
+  /**
+   * Get delegation token from filesystem and write the token along with
+   * metastore tokens into a file
+   * @param conf
+   * @return Path of the file with token credential
+   * @throws IOException
+   */
+  public Path createDelegationTokenFile(final Configuration conf) throws IOException;
+
+
+  /**
+   * Used by metastore server to creates UGI object for a remote user.
+   * @param userName remote User Name
+   * @param groupNames group names associated with remote user name
+   * @return UGI created for the remote user.
+   */
+
+  public UserGroupInformation createRemoteUser(String userName, List<String> groupNames);
+  /**
+   * Get the short name corresponding to the subject in the passed UGI
+   *
+   * In secure versions of Hadoop, this returns the short name (after
+   * undergoing the translation in the kerberos name rule mapping).
+   * In unsecure versions of Hadoop, this returns the name of the subject
+   */
+  public String getShortUserName(UserGroupInformation ugi);
+
+  /**
+   * Return true if the Shim is based on Hadoop Security APIs.
+   */
+  public boolean isSecureShimImpl();
+
+  /**
+   * Return true if the hadoop configuration has security enabled
+   * @return
+   */
+  public boolean isSecurityEnabled();
+
+  /**
+   * Get the string form of the token given a token signature.
+   * The signature is used as the value of the "service" field in the token for lookup.
+   * Ref: AbstractDelegationTokenSelector in Hadoop. If there exists such a token
+   * in the token cache (credential store) of the job, the lookup returns that.
+   * This is relevant only when running against a "secure" hadoop release
+   * The method gets hold of the tokens if they are set up by hadoop - this should
+   * happen on the map/reduce tasks if the client added the tokens into hadoop's
+   * credential store in the front end during job submission. The method will
+   * select the hive delegation token among the set of tokens and return the string
+   * form of it
+   * @param tokenSignature
+   * @return the string form of the token found
+   * @throws IOException
+   */
+  String getTokenStrForm(String tokenSignature) throws IOException;
+
+  /**
+   * Add a delegation token to the given ugi
+   * @param ugi
+   * @param tokenStr
+   * @param tokenService
+   * @throws IOException
+   */
+  void setTokenStr(UserGroupInformation ugi, String tokenStr, String tokenService)
+    throws IOException;
+
+
+  enum JobTrackerState { INITIALIZING, RUNNING };
+
+  /**
+   * Convert the ClusterStatus to its Thrift equivalent: JobTrackerState.
+   * See MAPREDUCE-2455 for why this is a part of the shim.
+   * @param clusterStatus
+   * @return the matching JobTrackerState
+   * @throws Exception if no equivalent JobTrackerState exists
+   */
+  public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception;
+
+  public TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable);
+
+  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id);
+
+  public JobContext newJobContext(Job job);
+
+  /**
+   * Check wether MR is configured to run in local-mode
+   * @param conf
+   * @return
+   */
+  public boolean isLocalMode(Configuration conf);
+
+  /**
+   * All retrieval of jobtracker/resource manager rpc address
+   * in the configuration should be done through this shim
+   * @param conf
+   * @return
+   */
+  public String getJobLauncherRpcAddress(Configuration conf);
+
+  /**
+   * All updates to jobtracker/resource manager rpc address
+   * in the configuration should be done through this shim
+   * @param conf
+   * @return
+   */
+  public void setJobLauncherRpcAddress(Configuration conf, String val);
+
+  /**
+   * All references to jobtracker/resource manager http address
+   * in the configuration should be done through this shim
+   * @param conf
+   * @return
+   */
+  public String getJobLauncherHttpAddress(Configuration conf);
+
+
+  /**
+   *  Perform kerberos login using the given principal and keytab
+   * @throws IOException
+   */
+  public void loginUserFromKeytab(String principal, String keytabFile) throws IOException;
+
+  /**
+   * Perform kerberos re-login using the given principal and keytab, to renew
+   * the credentials
+   * @throws IOException
+   */
+  public void reLoginUserFromKeytab() throws IOException;
+
+  /**
+   * Move the directory/file to trash. In case of the symlinks or mount points, the file is
+   * moved to the trashbin in the actual volume of the path p being deleted
+   * @param fs
+   * @param path
+   * @param conf
+   * @return false if the item is already in the trash or trash is disabled
+   * @throws IOException
+   */
+  public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration conf)
+          throws IOException;
+
+  /**
+   * Get the default block size for the path. FileSystem alone is not sufficient to
+   * determine the same, as in case of CSMT the underlying file system determines that.
+   * @param fs
+   * @param path
+   * @return
+   */
+  public long getDefaultBlockSize(FileSystem fs, Path path);
+
+  /**
+   * Get the default replication for a path. In case of CSMT the given path will be used to
+   * locate the actual filesystem.
+   * @param fs
+   * @param path
+   * @return
+   */
+  public short getDefaultReplication(FileSystem fs, Path path);
+
+  /**
+   * Create the proxy ugi for the given userid
+   * @param userName
+   * @return
+   */
+  UserGroupInformation createProxyUser(String userName) throws IOException;
+
+  /**
+   * The method sets to set the partition file has a different signature between
+   * hadoop versions.
+   * @param jobConf
+   * @param partition
+   */
+  void setTotalOrderPartitionFile(JobConf jobConf, Path partition);
+  /**
+   * InputSplitShim.
+   *
+   */
+  public interface InputSplitShim extends InputSplit {
+    JobConf getJob();
+
+    long getLength();
+
+    /** Returns an array containing the startoffsets of the files in the split. */
+    long[] getStartOffsets();
+
+    /** Returns an array containing the lengths of the files in the split. */
+    long[] getLengths();
+
+    /** Returns the start offset of the i<sup>th</sup> Path. */
+    long getOffset(int i);
+
+    /** Returns the length of the i<sup>th</sup> Path. */
+    long getLength(int i);
+
+    /** Returns the number of Paths in the split. */
+    int getNumPaths();
+
+    /** Returns the i<sup>th</sup> Path. */
+    Path getPath(int i);
+
+    /** Returns all the Paths in the split. */
+    Path[] getPaths();
+
+    /** Returns all the Paths where this input-split resides. */
+    String[] getLocations() throws IOException;
+
+    void shrinkSplit(long length);
+
+    String toString();
+
+    void readFields(DataInput in) throws IOException;
+
+    void write(DataOutput out) throws IOException;
+  }
+
+  /**
+   * CombineFileInputFormatShim.
+   *
+   * @param <K>
+   * @param <V>
+   */
+  interface CombineFileInputFormatShim<K, V> {
+    Path[] getInputPathsShim(JobConf conf);
+
+    void createPool(JobConf conf, PathFilter... filters);
+
+    InputSplitShim[] getSplits(JobConf job, int numSplits) throws IOException;
+
+    InputSplitShim getInputSplitShim() throws IOException;
+
+    RecordReader getRecordReader(JobConf job, InputSplitShim split, Reporter reporter,
+        Class<RecordReader<K, V>> rrClass) throws IOException;
+  }
+
+  public HCatHadoopShims getHCatShim();
+  public interface HCatHadoopShims {
+
+    enum PropertyName {CACHE_ARCHIVES, CACHE_FILES, CACHE_SYMLINK}
+
+    public TaskID createTaskID();
+
+    public TaskAttemptID createTaskAttemptID();
+
+    public org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(Configuration conf,
+                                                                                   TaskAttemptID taskId);
+
+    public org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(JobConf conf,
+                                                                                org.apache.hadoop.mapred.TaskAttemptID taskId, Progressable progressable);
+
+    public JobContext createJobContext(Configuration conf, JobID jobId);
+
+    public org.apache.hadoop.mapred.JobContext createJobContext(JobConf conf, JobID jobId, Progressable progressable);
+
+    public void commitJob(OutputFormat outputFormat, Job job) throws IOException;
+
+    public void abortJob(OutputFormat outputFormat, Job job) throws IOException;
+
+    /* Referring to job tracker in 0.20 and resource manager in 0.23 */
+    public InetSocketAddress getResourceManagerAddress(Configuration conf);
+
+    public String getPropertyName(PropertyName name);
+
+    /**
+     * Checks if file is in HDFS filesystem.
+     *
+     * @param fs
+     * @param path
+     * @return true if the file is in HDFS, false if the file is in other file systems.
+     */
+    public boolean isFileInHDFS(FileSystem fs, Path path) throws IOException;
+  }
+  /**
+   * Provides a Hadoop JobTracker shim.
+   * @param conf not {@code null}
+   */
+  public WebHCatJTShim getWebHCatShim(Configuration conf, UserGroupInformation ugi) throws IOException;
+  public interface WebHCatJTShim {
+    /**
+     * Grab a handle to a job that is already known to the JobTracker.
+     *
+     * @return Profile of the job, or null if not found.
+     */
+    public JobProfile getJobProfile(org.apache.hadoop.mapred.JobID jobid) throws IOException;
+    /**
+     * Grab a handle to a job that is already known to the JobTracker.
+     *
+     * @return Status of the job, or null if not found.
+     */
+    public JobStatus getJobStatus(org.apache.hadoop.mapred.JobID jobid) throws IOException;
+    /**
+     * Kill a job.
+     */
+    public void killJob(org.apache.hadoop.mapred.JobID jobid) throws IOException;
+    /**
+     * Get all the jobs submitted.
+     */
+    public JobStatus[] getAllJobs() throws IOException;
+    /**
+     * Close the connection to the Job Tracker.
+     */
+    public void close();
+  }
+
+  /**
+   * Create a proxy file system that can serve a given scheme/authority using some
+   * other file system.
+   */
+  public FileSystem createProxyFileSystem(FileSystem fs, URI uri);
+}
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HiveEventCounter.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HiveEventCounter.java
new file mode 100644
index 0000000..224b135
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HiveEventCounter.java
@@ -0,0 +1,102 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.shims;
+
+import org.apache.log4j.Appender;
+import org.apache.log4j.AppenderSkeleton;
+import org.apache.log4j.Layout;
+import org.apache.log4j.spi.ErrorHandler;
+import org.apache.log4j.spi.Filter;
+import org.apache.log4j.spi.LoggingEvent;
+import org.apache.log4j.spi.OptionHandler;
+
+public class HiveEventCounter implements Appender, OptionHandler {
+
+  AppenderSkeleton hadoopEventCounter;
+
+  public HiveEventCounter() {
+    hadoopEventCounter = ShimLoader.getEventCounter();
+  }
+
+  @Override
+  public void close() {
+    hadoopEventCounter.close();
+  }
+
+  @Override
+  public boolean requiresLayout() {
+    return hadoopEventCounter.requiresLayout();
+  }
+
+  @Override
+  public void addFilter(Filter filter) {
+    hadoopEventCounter.addFilter(filter);
+  }
+
+  @Override
+  public void clearFilters() {
+    hadoopEventCounter.clearFilters();
+  }
+
+  @Override
+  public void doAppend(LoggingEvent event) {
+    hadoopEventCounter.doAppend(event);
+  }
+
+  @Override
+  public ErrorHandler getErrorHandler() {
+    return hadoopEventCounter.getErrorHandler();
+  }
+
+  @Override
+  public Filter getFilter() {
+    return hadoopEventCounter.getFilter();
+  }
+
+  @Override
+  public Layout getLayout() {
+    return hadoopEventCounter.getLayout();
+  }
+
+  @Override
+  public String getName() {
+    return hadoopEventCounter.getName();
+  }
+
+  @Override
+  public void setErrorHandler(ErrorHandler handler) {
+    hadoopEventCounter.setErrorHandler(handler);
+  }
+
+  @Override
+  public void setLayout(Layout layout) {
+    hadoopEventCounter.setLayout(layout);
+  }
+
+  @Override
+  public void setName(String name) {
+    hadoopEventCounter.setName(name);
+  }
+
+  @Override
+  public void activateOptions() {
+    hadoopEventCounter.activateOptions();
+  }
+
+}
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HiveHarFileSystem.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HiveHarFileSystem.java
new file mode 100644
index 0000000..323ebbb
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HiveHarFileSystem.java
@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.shims;
+
+import java.io.IOException;
+
+import org.apache.hadoop.fs.BlockLocation;
+import org.apache.hadoop.fs.ContentSummary;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.HarFileSystem;
+import org.apache.hadoop.fs.Path;
+
+/**
+ * HiveHarFileSystem - fixes issues with Hadoop's HarFileSystem
+ *
+ */
+public class HiveHarFileSystem extends HarFileSystem {
+
+  @Override
+  public BlockLocation[] getFileBlockLocations(FileStatus file, long start,
+      long len) throws IOException {
+
+    // In some places (e.g. FileInputFormat) this BlockLocation is used to
+    // figure out sizes/offsets and so a completely blank one will not work.
+    String [] hosts = {"DUMMY_HOST"};
+    return new BlockLocation[]{new BlockLocation(null, hosts, 0, file.getLen())};
+  }
+
+  @Override
+  public ContentSummary getContentSummary(Path f) throws IOException {
+    // HarFileSystem has a bug where this method does not work properly
+    // if the underlying FS is HDFS. See MAPREDUCE-1877 for more
+    // information. This method is from FileSystem.
+    FileStatus status = getFileStatus(f);
+    if (!status.isDir()) {
+      // f is a file
+      return new ContentSummary(status.getLen(), 1, 0);
+    }
+    // f is a directory
+    long[] summary = {0, 0, 1};
+    for(FileStatus s : listStatus(f)) {
+      ContentSummary c = s.isDir() ? getContentSummary(s.getPath()) :
+                                     new ContentSummary(s.getLen(), 1, 0);
+      summary[0] += c.getLength();
+      summary[1] += c.getFileCount();
+      summary[2] += c.getDirectoryCount();
+    }
+    return new ContentSummary(summary[0], summary[1], summary[2]);
+  }
+}
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/JettyShims.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/JettyShims.java
new file mode 100644
index 0000000..5ecd664
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/JettyShims.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.shims;
+
+import java.io.IOException;
+
+/**
+ * Since Hadoop ships with different versions of Jetty in different versions,
+ * Hive uses a shim layer to access the parts of the API that have changed.
+ * Users should obtain an instance of this class using the ShimLoader factory.
+ */
+public interface JettyShims {
+
+  Server startServer(String listen, int port) throws IOException;
+
+  /**
+   * Server.
+   *
+   */
+  interface Server {
+    void addWar(String war, String mount);
+
+    void start() throws Exception;
+
+    void join() throws InterruptedException;
+
+    void stop() throws Exception;
+  }
+}
\ No newline at end of file
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
new file mode 100644
index 0000000..bf9c84f
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
@@ -0,0 +1,174 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.shims;
+
+import java.lang.IllegalArgumentException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge;
+import org.apache.hadoop.util.VersionInfo;
+import org.apache.log4j.AppenderSkeleton;
+
+/**
+ * ShimLoader.
+ *
+ */
+public abstract class ShimLoader {
+  private static HadoopShims hadoopShims;
+  private static JettyShims jettyShims;
+  private static AppenderSkeleton eventCounter;
+
+  /**
+   * The names of the classes for shimming Hadoop for each major version.
+   */
+  private static final HashMap<String, String> HADOOP_SHIM_CLASSES =
+      new HashMap<String, String>();
+
+  static {
+    HADOOP_SHIM_CLASSES.put("0.20", "org.apache.hadoop.hive.shims.Hadoop20Shims");
+    HADOOP_SHIM_CLASSES.put("0.20S", "org.apache.hadoop.hive.shims.Hadoop20SShims");
+    HADOOP_SHIM_CLASSES.put("0.23", "org.apache.hadoop.hive.shims.Hadoop23Shims");
+  }
+
+  /**
+   * The names of the classes for shimming Jetty for each major version of
+   * Hadoop.
+   */
+  private static final HashMap<String, String> JETTY_SHIM_CLASSES =
+      new HashMap<String, String>();
+
+  static {
+    JETTY_SHIM_CLASSES.put("0.20", "org.apache.hadoop.hive.shims.Jetty20Shims");
+    JETTY_SHIM_CLASSES.put("0.20S", "org.apache.hadoop.hive.shims.Jetty20SShims");
+    JETTY_SHIM_CLASSES.put("0.23", "org.apache.hadoop.hive.shims.Jetty23Shims");
+  }
+
+  /**
+   * The names of the classes for shimming Hadoop's event counter
+   */
+  private static final HashMap<String, String> EVENT_COUNTER_SHIM_CLASSES =
+      new HashMap<String, String>();
+
+  static {
+    EVENT_COUNTER_SHIM_CLASSES.put("0.20", "org.apache.hadoop.metrics.jvm.EventCounter");
+    EVENT_COUNTER_SHIM_CLASSES.put("0.20S", "org.apache.hadoop.log.metrics.EventCounter");
+    EVENT_COUNTER_SHIM_CLASSES.put("0.23", "org.apache.hadoop.log.metrics.EventCounter");
+  }
+
+  /**
+   * Factory method to get an instance of HadoopShims based on the
+   * version of Hadoop on the classpath.
+   */
+  public static synchronized HadoopShims getHadoopShims() {
+    if (hadoopShims == null) {
+      hadoopShims = loadShims(HADOOP_SHIM_CLASSES, HadoopShims.class);
+    }
+    return hadoopShims;
+  }
+
+  /**
+   * Factory method to get an instance of JettyShims based on the version
+   * of Hadoop on the classpath.
+   */
+  public static synchronized JettyShims getJettyShims() {
+    if (jettyShims == null) {
+      jettyShims = loadShims(JETTY_SHIM_CLASSES, JettyShims.class);
+    }
+    return jettyShims;
+  }
+
+  public static synchronized AppenderSkeleton getEventCounter() {
+    if (eventCounter == null) {
+      eventCounter = loadShims(EVENT_COUNTER_SHIM_CLASSES, AppenderSkeleton.class);
+    }
+    return eventCounter;
+  }
+
+  public static synchronized HadoopThriftAuthBridge getHadoopThriftAuthBridge() {
+      if (getHadoopShims().isSecureShimImpl()) {
+          return createShim("org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S",
+                            HadoopThriftAuthBridge.class);
+        } else {
+          return new HadoopThriftAuthBridge();
+        }
+      }
+
+  private static <T> T loadShims(Map<String, String> classMap, Class<T> xface) {
+    String vers = getMajorVersion();
+    String className = classMap.get(vers);
+    return createShim(className, xface);
+  }
+
+    private static <T> T createShim(String className, Class<T> xface) {
+    try {
+      Class<?> clazz = Class.forName(className);
+      return xface.cast(clazz.newInstance());
+    } catch (Exception e) {
+      throw new RuntimeException("Could not load shims in class " +
+          className, e);
+    }
+  }
+
+  /**
+   * Return the "major" version of Hadoop currently on the classpath.
+   * For releases in the 0.x series this is simply the first two
+   * components of the version, e.g. "0.20" or "0.23". Releases in
+   * the 1.x and 2.x series are mapped to the appropriate
+   * 0.x release series, e.g. 1.x is mapped to "0.20S" and 2.x
+   * is mapped to "0.23".
+   */
+  public static String getMajorVersion() {
+    String vers = VersionInfo.getVersion();
+
+    String[] parts = vers.split("\\.");
+    if (parts.length < 2) {
+      throw new RuntimeException("Illegal Hadoop Version: " + vers +
+          " (expected A.B.* format)");
+    }
+
+    // Special handling for Hadoop 1.x and 2.x
+    switch (Integer.parseInt(parts[0])) {
+    case 0:
+      break;
+    case 1:
+      return "0.20S";
+    case 2:
+      return "0.23";
+    default:
+      throw new IllegalArgumentException("Unrecognized Hadoop major version number: " + vers);
+    }
+
+    String majorVersion = parts[0] + "." + parts[1];
+
+    // If we are running a security release, we won't have UnixUserGroupInformation
+    // (removed by HADOOP-6299 when switching to JAAS for Login)
+    try {
+      Class.forName("org.apache.hadoop.security.UnixUserGroupInformation");
+    } catch (ClassNotFoundException cnf) {
+      if ("0.20".equals(majorVersion)) {
+        majorVersion += "S";
+      }
+    }
+    return majorVersion;
+  }
+
+  private ShimLoader() {
+    // prevent instantiation
+  }
+}
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java
new file mode 100644
index 0000000..03f4e51
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java
@@ -0,0 +1,101 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+ package org.apache.hadoop.hive.thrift;
+
+ import java.io.IOException;
+import java.net.InetAddress;
+import java.util.Map;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.thrift.TProcessor;
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
+import org.apache.thrift.transport.TTransportFactory;
+
+ /**
+  * This class is only overridden by the secure hadoop shim. It allows
+  * the Thrift SASL support to bridge to Hadoop's UserGroupInformation
+  * & DelegationToken infrastructure.
+  */
+ public class HadoopThriftAuthBridge {
+   public Client createClient() {
+     throw new UnsupportedOperationException(
+       "The current version of Hadoop does not support Authentication");
+   }
+
+   public Client createClientWithConf(String authType) {
+     throw new UnsupportedOperationException(
+       "The current version of Hadoop does not support Authentication");
+   }
+
+   public Server createServer(String keytabFile, String principalConf)
+     throws TTransportException {
+     throw new UnsupportedOperationException(
+       "The current version of Hadoop does not support Authentication");
+   }
+
+
+  /**
+   * Read and return Hadoop SASL configuration which can be configured using
+   * "hadoop.rpc.protection"
+   *
+   * @param conf
+   * @return Hadoop SASL configuration
+   */
+   public Map<String, String> getHadoopSaslProperties(Configuration conf) {
+     throw new UnsupportedOperationException(
+       "The current version of Hadoop does not support Authentication");
+   }
+
+   public static abstract class Client {
+   /**
+    *
+    * @param principalConfig In the case of Kerberos authentication this will
+    * be the kerberos principal name, for DIGEST-MD5 (delegation token) based
+    * authentication this will be null
+    * @param host The metastore server host name
+    * @param methodStr "KERBEROS" or "DIGEST"
+    * @param tokenStrForm This is url encoded string form of
+    * org.apache.hadoop.security.token.
+    * @param underlyingTransport the underlying transport
+    * @return the transport
+    * @throws IOException
+    */
+     public abstract TTransport createClientTransport(
+             String principalConfig, String host,
+             String methodStr, String tokenStrForm, TTransport underlyingTransport,
+             Map<String, String> saslProps)
+             throws IOException;
+   }
+
+   public static abstract class Server {
+     public abstract TTransportFactory createTransportFactory(Map<String, String> saslProps) throws TTransportException;
+     public abstract TProcessor wrapProcessor(TProcessor processor);
+     public abstract TProcessor wrapNonAssumingProcessor(TProcessor processor);
+     public abstract InetAddress getRemoteAddress();
+     public abstract void startDelegationTokenSecretManager(Configuration conf,
+       Object hmsHandler) throws IOException;
+     public abstract String getRemoteUser();
+     public abstract String getDelegationToken(String owner, String renewer)
+     throws IOException, InterruptedException;
+     public abstract long renewDelegationToken(String tokenStrForm) throws IOException;
+     public abstract void cancelDelegationToken(String tokenStrForm) throws IOException;
+   }
+ }
+
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TFilterTransport.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TFilterTransport.java
new file mode 100644
index 0000000..b990ea5
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TFilterTransport.java
@@ -0,0 +1,99 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
+
+/**
+  * Transport that simply wraps another transport.
+  * This is the equivalent of FilterInputStream for Thrift transports.
+  */
+ public class TFilterTransport extends TTransport {
+   protected final TTransport wrapped;
+
+   public TFilterTransport(TTransport wrapped) {
+     this.wrapped = wrapped;
+   }
+
+   @Override
+   public void open() throws TTransportException {
+     wrapped.open();
+   }
+
+   @Override
+   public boolean isOpen() {
+     return wrapped.isOpen();
+   }
+
+   @Override
+   public boolean peek() {
+     return wrapped.peek();
+   }
+
+   @Override
+   public void close() {
+     wrapped.close();
+   }
+
+   @Override
+   public int read(byte[] buf, int off, int len) throws TTransportException {
+     return wrapped.read(buf, off, len);
+   }
+
+   @Override
+   public int readAll(byte[] buf, int off, int len) throws TTransportException {
+     return wrapped.readAll(buf, off, len);
+   }
+
+   @Override
+   public void write(byte[] buf) throws TTransportException {
+     wrapped.write(buf);
+   }
+
+   @Override
+   public void write(byte[] buf, int off, int len) throws TTransportException {
+     wrapped.write(buf, off, len);
+   }
+
+   @Override
+   public void flush() throws TTransportException {
+     wrapped.flush();
+   }
+
+   @Override
+   public byte[] getBuffer() {
+     return wrapped.getBuffer();
+   }
+
+   @Override
+   public int getBufferPosition() {
+     return wrapped.getBufferPosition();
+   }
+
+   @Override
+   public int getBytesRemainingInBuffer() {
+     return wrapped.getBytesRemainingInBuffer();
+   }
+
+   @Override
+   public void consumeBuffer(int len) {
+     wrapped.consumeBuffer(len);
+   }
+ }
\ No newline at end of file
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java
new file mode 100644
index 0000000..75f7297
--- /dev/null
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java
@@ -0,0 +1,96 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.thrift;
+
+import java.net.Socket;
+import java.util.concurrent.ConcurrentMap;
+
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.thrift.transport.TSocket;
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportFactory;
+
+import com.google.common.collect.MapMaker;
+
+/** TUGIContainingTransport associates ugi information with connection (transport).
+ *  Wraps underlying <code>TSocket</code> transport and annotates it with ugi.
+*/
+
+public class TUGIContainingTransport extends TFilterTransport {
+
+  private UserGroupInformation ugi;
+
+  public TUGIContainingTransport(TTransport wrapped) {
+    super(wrapped);
+  }
+
+  public UserGroupInformation getClientUGI(){
+    return ugi;
+  }
+
+  public void setClientUGI(UserGroupInformation ugi){
+    this.ugi = ugi;
+  }
+
+  /**
+   * If the underlying TTransport is an instance of TSocket, it returns the Socket object
+   * which it contains.  Otherwise it returns null.
+   */
+  public Socket getSocket() {
+    if (wrapped instanceof TSocket) {
+      return (((TSocket)wrapped).getSocket());
+    }
+
+    return null;
+  }
+
+  /** Factory to create TUGIContainingTransport.
+   */
+
+  public static class Factory extends TTransportFactory {
+
+    // Need a concurrent weakhashmap. WeakKeys() so that when underlying transport gets out of
+    // scope, it still can be GC'ed. Since value of map has a ref to key, need weekValues as well.
+    private static final ConcurrentMap<TTransport, TUGIContainingTransport> transMap =
+        new MapMaker().weakKeys().weakValues().makeMap();
+
+    /**
+     * Get a new <code>TUGIContainingTransport</code> instance, or reuse the
+     * existing one if a <code>TUGIContainingTransport</code> has already been
+     * created before using the given <code>TTransport</code> as an underlying
+     * transport. This ensures that a given underlying transport instance
+     * receives the same <code>TUGIContainingTransport</code>.
+     */
+    @Override
+    public TUGIContainingTransport getTransport(TTransport trans) {
+
+      // UGI information is not available at connection setup time, it will be set later
+      // via set_ugi() rpc.
+      TUGIContainingTransport tugiTrans = transMap.get(trans);
+      if (tugiTrans == null) {
+        tugiTrans = new TUGIContainingTransport(trans);
+        TUGIContainingTransport prev = transMap.putIfAbsent(trans, tugiTrans);
+        if (prev != null) {
+          return prev;
+        }
+      }
+      return tugiTrans;
+    }
+  }
+}
diff --git a/src/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java b/src/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
deleted file mode 100644
index 7bf5293..0000000
--- a/src/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
+++ /dev/null
@@ -1,762 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.shims;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.lang.reflect.Constructor;
-import java.net.MalformedURLException;
-import java.net.URI;
-import java.net.URISyntaxException;
-import java.net.URL;
-import java.security.PrivilegedActionException;
-import java.security.PrivilegedExceptionAction;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import javax.security.auth.Subject;
-import javax.security.auth.login.LoginException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.fs.ProxyFileSystem;
-import org.apache.hadoop.fs.Trash;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.InputFormat;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.JobContext;
-import org.apache.hadoop.mapred.JobStatus;
-import org.apache.hadoop.mapred.MiniMRCluster;
-import org.apache.hadoop.mapred.OutputCommitter;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.RunningJob;
-import org.apache.hadoop.mapred.TaskAttemptContext;
-import org.apache.hadoop.mapred.TaskCompletionEvent;
-import org.apache.hadoop.mapred.TaskID;
-import org.apache.hadoop.mapred.TaskLogServlet;
-import org.apache.hadoop.mapred.lib.CombineFileInputFormat;
-import org.apache.hadoop.mapred.lib.CombineFileSplit;
-import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobID;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hadoop.security.SecurityUtil;
-import org.apache.hadoop.security.UnixUserGroupInformation;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.HadoopArchives;
-import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.util.VersionInfo;
-
-/**
- * Implemention of shims against Hadoop 0.20.0.
- */
-public class Hadoop20Shims implements HadoopShims {
-
-  public boolean usesJobShell() {
-    return false;
-  }
-
-  public boolean fileSystemDeleteOnExit(FileSystem fs, Path path)
-      throws IOException {
-
-    return fs.deleteOnExit(path);
-  }
-
-  public void inputFormatValidateInput(InputFormat fmt, JobConf conf)
-      throws IOException {
-    // gone in 0.18+
-  }
-
-  public boolean isJobPreparing(RunningJob job) throws IOException {
-    return job.getJobState() == JobStatus.PREP;
-  }
-  /**
-   * Workaround for hadoop-17 - jobclient only looks at commandlineconfig.
-   */
-  public void setTmpFiles(String prop, String files) {
-    // gone in 20+
-  }
-
-   public String getKerberosShortName(String kerberosName) throws IOException {
-    // raise an exception
-    throw new IOException("Authentication is not supported with 0.20");
-  }
-
-
-  /**
-   * Returns a shim to wrap MiniMrCluster
-   */
-  public MiniMrShim getMiniMrCluster(Configuration conf, int numberOfTaskTrackers,
-                                     String nameNode, int numDir) throws IOException {
-    return new MiniMrShim(conf, numberOfTaskTrackers, nameNode, numDir);
-  }
-
-  /**
-   * Shim for MiniMrCluster
-   */
-  public class MiniMrShim implements HadoopShims.MiniMrShim {
-
-    private final MiniMRCluster mr;
-
-    public MiniMrShim(Configuration conf, int numberOfTaskTrackers,
-        String nameNode, int numDir) throws IOException {
-      this.mr = new MiniMRCluster(numberOfTaskTrackers, nameNode, numDir);
-    }
-
-    @Override
-    public int getJobTrackerPort() throws UnsupportedOperationException {
-      return mr.getJobTrackerPort();
-    }
-
-    @Override
-    public void shutdown() throws IOException {
-      mr.shutdown();
-    }
-
-    @Override
-    public void setupConfiguration(Configuration conf) {
-      setJobLauncherRpcAddress(conf, "localhost:" + mr.getJobTrackerPort());
-    }
-  }
-
-  public HadoopShims.MiniDFSShim getMiniDfs(Configuration conf,
-      int numDataNodes,
-      boolean format,
-      String[] racks) throws IOException {
-    return new MiniDFSShim(new MiniDFSCluster(conf, numDataNodes, format, racks));
-  }
-
-  /**
-   * MiniDFSShim.
-   *
-   */
-  public class MiniDFSShim implements HadoopShims.MiniDFSShim {
-    private final MiniDFSCluster cluster;
-
-    public MiniDFSShim(MiniDFSCluster cluster) {
-      this.cluster = cluster;
-    }
-
-    public FileSystem getFileSystem() throws IOException {
-      return cluster.getFileSystem();
-    }
-
-    public void shutdown() {
-      cluster.shutdown();
-    }
-  }
-
-  /**
-   * We define this function here to make the code compatible between
-   * hadoop 0.17 and hadoop 0.20.
-   *
-   * Hive binary that compiled Text.compareTo(Text) with hadoop 0.20 won't
-   * work with hadoop 0.17 because in hadoop 0.20, Text.compareTo(Text) is
-   * implemented in org.apache.hadoop.io.BinaryComparable, and Java compiler
-   * references that class, which is not available in hadoop 0.17.
-   */
-  public int compareText(Text a, Text b) {
-    return a.compareTo(b);
-  }
-
-  @Override
-  public long getAccessTime(FileStatus file) {
-    return file.getAccessTime();
-  }
-
-  public HadoopShims.CombineFileInputFormatShim getCombineFileInputFormat() {
-    return new CombineFileInputFormatShim() {
-      @Override
-      public RecordReader getRecordReader(InputSplit split,
-          JobConf job, Reporter reporter) throws IOException {
-        throw new IOException("CombineFileInputFormat.getRecordReader not needed.");
-      }
-    };
-  }
-
-  public void setTotalOrderPartitionFile(JobConf jobConf, Path partitionFile){
-    TotalOrderPartitioner.setPartitionFile(jobConf, partitionFile);
-  }
-
-  public static class InputSplitShim extends CombineFileSplit implements HadoopShims.InputSplitShim {
-    long shrinkedLength;
-    boolean _isShrinked;
-    public InputSplitShim() {
-      super();
-      _isShrinked = false;
-    }
-
-    public InputSplitShim(CombineFileSplit old) throws IOException {
-      super(old.getJob(), old.getPaths(), old.getStartOffsets(),
-          old.getLengths(), dedup(old.getLocations()));
-      _isShrinked = false;
-    }
-
-    private static String[] dedup(String[] locations) {
-      Set<String> dedup = new HashSet<String>();
-      Collections.addAll(dedup, locations);
-      return dedup.toArray(new String[dedup.size()]);
-    }
-
-    @Override
-    public void shrinkSplit(long length) {
-      _isShrinked = true;
-      shrinkedLength = length;
-    }
-
-    public boolean isShrinked() {
-      return _isShrinked;
-    }
-
-    public long getShrinkedLength() {
-      return shrinkedLength;
-    }
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-      super.readFields(in);
-      _isShrinked = in.readBoolean();
-      if (_isShrinked) {
-        shrinkedLength = in.readLong();
-      }
-    }
-
-    @Override
-    public void write(DataOutput out) throws IOException {
-      super.write(out);
-      out.writeBoolean(_isShrinked);
-      if (_isShrinked) {
-        out.writeLong(shrinkedLength);
-      }
-    }
-  }
-
-  /* This class should be replaced with org.apache.hadoop.mapred.lib.CombineFileRecordReader class, once
-   * https://issues.apache.org/jira/browse/MAPREDUCE-955 is fixed. This code should be removed - it is a copy
-   * of org.apache.hadoop.mapred.lib.CombineFileRecordReader
-   */
-  public static class CombineFileRecordReader<K, V> implements RecordReader<K, V> {
-
-    static final Class[] constructorSignature = new Class[] {
-        InputSplit.class,
-        Configuration.class,
-        Reporter.class,
-        Integer.class
-        };
-
-    protected CombineFileSplit split;
-    protected JobConf jc;
-    protected Reporter reporter;
-    protected Class<RecordReader<K, V>> rrClass;
-    protected Constructor<RecordReader<K, V>> rrConstructor;
-    protected FileSystem fs;
-
-    protected int idx;
-    protected long progress;
-    protected RecordReader<K, V> curReader;
-    protected boolean isShrinked;
-    protected long shrinkedLength;
-
-    public boolean next(K key, V value) throws IOException {
-
-      while ((curReader == null)
-          || !doNextWithExceptionHandler((K) ((CombineHiveKey) key).getKey(),
-              value)) {
-        if (!initNextRecordReader(key)) {
-          return false;
-        }
-      }
-      return true;
-    }
-
-    public K createKey() {
-      K newKey = curReader.createKey();
-      return (K)(new CombineHiveKey(newKey));
-    }
-
-    public V createValue() {
-      return curReader.createValue();
-    }
-
-    /**
-     * Return the amount of data processed.
-     */
-    public long getPos() throws IOException {
-      return progress;
-    }
-
-    public void close() throws IOException {
-      if (curReader != null) {
-        curReader.close();
-        curReader = null;
-      }
-    }
-
-    /**
-     * Return progress based on the amount of data processed so far.
-     */
-    public float getProgress() throws IOException {
-      long subprogress = 0;    // bytes processed in current split
-      if (null != curReader) {
-        // idx is always one past the current subsplit's true index.
-        subprogress = (long)(curReader.getProgress() * split.getLength(idx - 1));
-      }
-      return Math.min(1.0f, (progress + subprogress) / (float) (split.getLength()));
-    }
-
-    /**
-     * A generic RecordReader that can hand out different recordReaders
-     * for each chunk in the CombineFileSplit.
-     */
-    public CombineFileRecordReader(JobConf job, CombineFileSplit split,
-        Reporter reporter,
-        Class<RecordReader<K, V>> rrClass)
-        throws IOException {
-      this.split = split;
-      this.jc = job;
-      this.rrClass = rrClass;
-      this.reporter = reporter;
-      this.idx = 0;
-      this.curReader = null;
-      this.progress = 0;
-
-      isShrinked = false;
-
-      assert (split instanceof InputSplitShim);
-      if (((InputSplitShim) split).isShrinked()) {
-        isShrinked = true;
-        shrinkedLength = ((InputSplitShim) split).getShrinkedLength();
-      }
-
-      try {
-        rrConstructor = rrClass.getDeclaredConstructor(constructorSignature);
-        rrConstructor.setAccessible(true);
-      } catch (Exception e) {
-        throw new RuntimeException(rrClass.getName() +
-            " does not have valid constructor", e);
-      }
-      initNextRecordReader(null);
-    }
-
-    /**
-     * do next and handle exception inside it.
-     * @param key
-     * @param value
-     * @return
-     * @throws IOException
-     */
-    private boolean doNextWithExceptionHandler(K key, V value) throws IOException {
-      try {
-        return curReader.next(key, value);
-      } catch (Exception e) {
-        return HiveIOExceptionHandlerUtil.handleRecordReaderNextException(e, jc);
-      }
-    }
-
-    /**
-     * Get the record reader for the next chunk in this CombineFileSplit.
-     */
-    protected boolean initNextRecordReader(K key) throws IOException {
-
-      if (curReader != null) {
-        curReader.close();
-        curReader = null;
-        if (idx > 0) {
-          progress += split.getLength(idx - 1); // done processing so far
-        }
-      }
-
-      // if all chunks have been processed or reached the length, nothing more to do.
-      if (idx == split.getNumPaths() || (isShrinked && progress > shrinkedLength)) {
-        return false;
-      }
-
-      // get a record reader for the idx-th chunk
-      try {
-        curReader = rrConstructor.newInstance(new Object[]
-            {split, jc, reporter, Integer.valueOf(idx)});
-
-        // change the key if need be
-        if (key != null) {
-          K newKey = curReader.createKey();
-          ((CombineHiveKey)key).setKey(newKey);
-        }
-
-        // setup some helper config variables.
-        jc.set("map.input.file", split.getPath(idx).toString());
-        jc.setLong("map.input.start", split.getOffset(idx));
-        jc.setLong("map.input.length", split.getLength(idx));
-      } catch (Exception e) {
-        curReader=HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(e, jc);
-      }
-      idx++;
-      return true;
-    }
-  }
-
-  public abstract static class CombineFileInputFormatShim<K, V> extends
-      CombineFileInputFormat<K, V>
-      implements HadoopShims.CombineFileInputFormatShim<K, V> {
-
-    public Path[] getInputPathsShim(JobConf conf) {
-      try {
-        return FileInputFormat.getInputPaths(conf);
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void createPool(JobConf conf, PathFilter... filters) {
-      super.createPool(conf, filters);
-    }
-
-    @Override
-    public InputSplitShim[] getSplits(JobConf job, int numSplits) throws IOException {
-      long minSize = job.getLong("mapred.min.split.size", 0);
-
-      // For backward compatibility, let the above parameter be used
-      if (job.getLong("mapred.min.split.size.per.node", 0) == 0) {
-        super.setMinSplitSizeNode(minSize);
-      }
-
-      if (job.getLong("mapred.min.split.size.per.rack", 0) == 0) {
-        super.setMinSplitSizeRack(minSize);
-      }
-
-      if (job.getLong("mapred.max.split.size", 0) == 0) {
-        super.setMaxSplitSize(minSize);
-      }
-
-      CombineFileSplit[] splits = (CombineFileSplit[]) super.getSplits(job, numSplits);
-
-      InputSplitShim[] isplits = new InputSplitShim[splits.length];
-      for (int pos = 0; pos < splits.length; pos++) {
-        isplits[pos] = new InputSplitShim(splits[pos]);
-      }
-
-      return isplits;
-    }
-
-    public InputSplitShim getInputSplitShim() throws IOException {
-      return new InputSplitShim();
-    }
-
-    public RecordReader getRecordReader(JobConf job, HadoopShims.InputSplitShim split,
-        Reporter reporter,
-        Class<RecordReader<K, V>> rrClass)
-        throws IOException {
-      CombineFileSplit cfSplit = (CombineFileSplit) split;
-      return new CombineFileRecordReader(job, cfSplit, reporter, rrClass);
-    }
-
-  }
-
-  public String getInputFormatClassName() {
-    return "org.apache.hadoop.hive.ql.io.CombineHiveInputFormat";
-  }
-
-  String[] ret = new String[2];
-
-  @Override
-  public String[] getTaskJobIDs(TaskCompletionEvent t) {
-    TaskID tid = t.getTaskAttemptId().getTaskID();
-    ret[0] = tid.toString();
-    ret[1] = tid.getJobID().toString();
-    return ret;
-  }
-
-  public void setFloatConf(Configuration conf, String varName, float val) {
-    conf.setFloat(varName, val);
-  }
-
-  @Override
-  public int createHadoopArchive(Configuration conf, Path sourceDir, Path destDir,
-      String archiveName) throws Exception {
-
-    HadoopArchives har = new HadoopArchives(conf);
-    List<String> args = new ArrayList<String>();
-
-    args.add("-archiveName");
-    args.add(archiveName);
-    args.add(sourceDir.toString());
-    args.add(destDir.toString());
-
-    return ToolRunner.run(har, args.toArray(new String[0]));
-  }
-
-  /*
-   *(non-Javadoc)
-   * @see org.apache.hadoop.hive.shims.HadoopShims#getHarUri(java.net.URI, java.net.URI, java.net.URI)
-   * This particular instance is for Hadoop 20 which creates an archive
-   * with the entire directory path from which one created the archive as
-   * compared against the one used by Hadoop 1.0 (within HadoopShimsSecure)
-   * where a relative path is stored within the archive.
-   */
-  public URI getHarUri (URI original, URI base, URI originalBase)
-    throws URISyntaxException {
-    URI relative = null;
-
-    String dirInArchive = original.getPath();
-    if (dirInArchive.length() > 1 && dirInArchive.charAt(0) == '/') {
-      dirInArchive = dirInArchive.substring(1);
-    }
-
-    relative = new URI(null, null, dirInArchive, null);
-
-    return base.resolve(relative);
-  }
-
-  public static class NullOutputCommitter extends OutputCommitter {
-    @Override
-    public void setupJob(JobContext jobContext) { }
-    @Override
-    public void cleanupJob(JobContext jobContext) { }
-
-    @Override
-    public void setupTask(TaskAttemptContext taskContext) { }
-    @Override
-    public boolean needsTaskCommit(TaskAttemptContext taskContext) {
-      return false;
-    }
-    @Override
-    public void commitTask(TaskAttemptContext taskContext) { }
-    @Override
-    public void abortTask(TaskAttemptContext taskContext) { }
-  }
-
-  public void prepareJobOutput(JobConf conf) {
-    conf.setOutputCommitter(Hadoop20Shims.NullOutputCommitter.class);
-
-    // option to bypass job setup and cleanup was introduced in hadoop-21 (MAPREDUCE-463)
-    // but can be backported. So we disable setup/cleanup in all versions >= 0.19
-    conf.setBoolean("mapred.committer.job.setup.cleanup.needed", false);
-
-    // option to bypass task cleanup task was introduced in hadoop-23 (MAPREDUCE-2206)
-    // but can be backported. So we disable setup/cleanup in all versions >= 0.19
-    conf.setBoolean("mapreduce.job.committer.task.cleanup.needed", false);
-  }
-
-  @Override
-  public UserGroupInformation getUGIForConf(Configuration conf) throws LoginException {
-    UserGroupInformation ugi =
-      UnixUserGroupInformation.readFromConf(conf, UnixUserGroupInformation.UGI_PROPERTY_NAME);
-    if(ugi == null) {
-      ugi = UserGroupInformation.login(conf);
-    }
-    return ugi;
-  }
-
-  @Override
-  public boolean isSecureShimImpl() {
-    return false;
-  }
-
-  @Override
-  public String getShortUserName(UserGroupInformation ugi) {
-    return ugi.getUserName();
-  }
-
-  @Override
-  public String getTokenStrForm(String tokenSignature) throws IOException {
-    throw new UnsupportedOperationException("Tokens are not supported in current hadoop version");
-  }
-
-  @Override
-  public void setTokenStr(UserGroupInformation ugi, String tokenStr, String tokenService)
-    throws IOException {
-    throw new UnsupportedOperationException("Tokens are not supported in current hadoop version");
-  }
-
-  @Override
-  public <T> T doAs(UserGroupInformation ugi, PrivilegedExceptionAction<T> pvea) throws
-    IOException, InterruptedException {
-    try {
-      return Subject.doAs(SecurityUtil.getSubject(ugi),pvea);
-    } catch (PrivilegedActionException e) {
-      throw new IOException(e);
-    }
-  }
-
-  @Override
-  public Path createDelegationTokenFile(Configuration conf) throws IOException {
-    throw new UnsupportedOperationException("Tokens are not supported in current hadoop version");
-  }
-
-  @Override
-  public UserGroupInformation createRemoteUser(String userName, List<String> groupNames) {
-    return new UnixUserGroupInformation(userName, groupNames.toArray(new String[0]));
-  }
-
-  @Override
-  public void loginUserFromKeytab(String principal, String keytabFile) throws IOException {
-    throwKerberosUnsupportedError();
-  }
-
-  @Override
-  public void reLoginUserFromKeytab() throws IOException{
-    throwKerberosUnsupportedError();
-  }
-
-  private void throwKerberosUnsupportedError() throws UnsupportedOperationException{
-    throw new UnsupportedOperationException("Kerberos login is not supported" +
-        " in this hadoop version (" + VersionInfo.getVersion() + ")");
-  }
-
-  @Override
-  public UserGroupInformation createProxyUser(String userName) throws IOException {
-    return createRemoteUser(userName, null);
-  }
-
-  @Override
-  public boolean isSecurityEnabled() {
-    return false;
-  }
-
-  @Override
-  public String getTaskAttemptLogUrl(JobConf conf,
-    String taskTrackerHttpAddress, String taskAttemptId)
-    throws MalformedURLException {
-    URL taskTrackerHttpURL = new URL(taskTrackerHttpAddress);
-    return TaskLogServlet.getTaskLogUrl(
-      taskTrackerHttpURL.getHost(),
-      Integer.toString(taskTrackerHttpURL.getPort()),
-      taskAttemptId);
-  }
-
-  @Override
-  public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception {
-    JobTrackerState state;
-    switch (clusterStatus.getJobTrackerState()) {
-    case INITIALIZING:
-      return JobTrackerState.INITIALIZING;
-    case RUNNING:
-      return JobTrackerState.RUNNING;
-    default:
-      String errorMsg = "Unrecognized JobTracker state: " + clusterStatus.getJobTrackerState();
-      throw new Exception(errorMsg);
-    }
-  }
-
-  @Override
-  public String unquoteHtmlChars(String item) {
-    return item;
-  }
-
-
-  @Override
-  public org.apache.hadoop.mapreduce.TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable) {
-    return new org.apache.hadoop.mapreduce.TaskAttemptContext(conf, new TaskAttemptID()) {
-      @Override
-      public void progress() {
-        progressable.progress();
-      }
-    };
-  }
-
-  @Override
-  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id) {
-    return new TaskAttemptID(jobId.getJtIdentifier(), jobId.getId(), isMap, taskId, id);
-  }
-
-  @Override
-  public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job) {
-    return new org.apache.hadoop.mapreduce.JobContext(job.getConfiguration(), job.getJobID());
-  }
-
-  @Override
-  public void closeAllForUGI(UserGroupInformation ugi) {
-    // No such functionality in ancient hadoop
-    return;
-  }
-
-  @Override
-  public boolean isLocalMode(Configuration conf) {
-    return "local".equals(getJobLauncherRpcAddress(conf));
-  }
-
-  @Override
-  public String getJobLauncherRpcAddress(Configuration conf) {
-    return conf.get("mapred.job.tracker");
-  }
-
-  @Override
-  public void setJobLauncherRpcAddress(Configuration conf, String val) {
-    conf.set("mapred.job.tracker", val);
-  }
-
-  @Override
-  public String getJobLauncherHttpAddress(Configuration conf) {
-    return conf.get("mapred.job.tracker.http.address");
-  }
-
-  @Override
-  public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration conf)
-          throws IOException {
-    // older versions of Hadoop don't have a Trash constructor based on the
-    // Path or FileSystem. So need to achieve this by creating a dummy conf.
-    // this needs to be filtered out based on version
-
-    Configuration dupConf = new Configuration(conf);
-    FileSystem.setDefaultUri(dupConf, fs.getUri());
-    Trash trash = new Trash(dupConf);
-    return trash.moveToTrash(path);
-  }
-
-  @Override
-  public long getDefaultBlockSize(FileSystem fs, Path path) {
-    return fs.getDefaultBlockSize();
-  }
-
-  @Override
-  public short getDefaultReplication(FileSystem fs, Path path) {
-    return fs.getDefaultReplication();
-  }
-
-  @Override
-  public String getTokenFileLocEnvName() {
-    throw new UnsupportedOperationException(
-        "Kerberos not supported in current hadoop version");
-  }
-  @Override
-  public HCatHadoopShims getHCatShim() {
-      throw new UnsupportedOperationException("HCatalog does not support Hadoop 0.20.x");
-  }
-  @Override
-  public WebHCatJTShim getWebHCatShim(Configuration conf, UserGroupInformation ugi) throws IOException {
-      throw new UnsupportedOperationException("WebHCat does not support Hadoop 0.20.x");
-  }
-  @Override
-  public FileSystem createProxyFileSystem(FileSystem fs, URI uri) {
-    return new ProxyFileSystem(fs, uri);
-  }
-}
diff --git a/src/shims/src/0.20/java/org/apache/hadoop/hive/shims/Jetty20Shims.java b/src/shims/src/0.20/java/org/apache/hadoop/hive/shims/Jetty20Shims.java
deleted file mode 100644
index 13c6b31..0000000
--- a/src/shims/src/0.20/java/org/apache/hadoop/hive/shims/Jetty20Shims.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.shims;
-
-import java.io.IOException;
-
-import org.mortbay.jetty.bio.SocketConnector;
-import org.mortbay.jetty.handler.RequestLogHandler;
-import org.mortbay.jetty.webapp.WebAppContext;
-
-/**
- * Jetty20Shims.
- *
- */
-public class Jetty20Shims implements JettyShims {
-  public Server startServer(String listen, int port) throws IOException {
-    Server s = new Server();
-    s.setupListenerHostPort(listen, port);
-    return s;
-  }
-
-  private static class Server extends org.mortbay.jetty.Server implements JettyShims.Server {
-    public void addWar(String war, String contextPath) {
-      WebAppContext wac = new WebAppContext();
-      wac.setContextPath(contextPath);
-      wac.setWar(war);
-      RequestLogHandler rlh = new RequestLogHandler();
-      rlh.setHandler(wac);
-      this.addHandler(rlh);
-    }
-
-    public void setupListenerHostPort(String listen, int port)
-        throws IOException {
-
-      SocketConnector connector = new SocketConnector();
-      connector.setPort(port);
-      connector.setHost(listen);
-      this.addConnector(connector);
-    }
-  }
-}
diff --git a/src/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java b/src/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
deleted file mode 100644
index d159142..0000000
--- a/src/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
+++ /dev/null
@@ -1,350 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.shims;
-
-import java.io.IOException;
-import java.net.InetSocketAddress;
-import java.net.MalformedURLException;
-import java.net.URL;
-import java.net.URI;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.filecache.DistributedCache;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.ProxyFileSystem;
-import org.apache.hadoop.fs.Trash;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.mapred.JobTracker;
-import org.apache.hadoop.mapred.MiniMRCluster;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.TaskLogServlet;
-import org.apache.hadoop.mapred.WebHCatJTShim20S;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.JobID;
-import org.apache.hadoop.mapreduce.JobStatus;
-import org.apache.hadoop.mapreduce.OutputFormat;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hadoop.mapreduce.TaskID;
-import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.security.KerberosName;
-
-/**
- * Implemention of shims against Hadoop 0.20 with Security.
- */
-public class Hadoop20SShims extends HadoopShimsSecure {
-
-  @Override
-  public String getTaskAttemptLogUrl(JobConf conf,
-    String taskTrackerHttpAddress, String taskAttemptId)
-    throws MalformedURLException {
-    URL taskTrackerHttpURL = new URL(taskTrackerHttpAddress);
-    return TaskLogServlet.getTaskLogUrl(
-      taskTrackerHttpURL.getHost(),
-      Integer.toString(taskTrackerHttpURL.getPort()),
-      taskAttemptId);
-  }
-
-  @Override
-  public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception {
-    JobTrackerState state;
-    switch (clusterStatus.getJobTrackerState()) {
-    case INITIALIZING:
-      return JobTrackerState.INITIALIZING;
-    case RUNNING:
-      return JobTrackerState.RUNNING;
-    default:
-      String errorMsg = "Unrecognized JobTracker state: " + clusterStatus.getJobTrackerState();
-      throw new Exception(errorMsg);
-    }
-  }
-
-  @Override
-  public org.apache.hadoop.mapreduce.TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable) {
-    return new org.apache.hadoop.mapreduce.TaskAttemptContext(conf, new TaskAttemptID()) {
-      @Override
-      public void progress() {
-        progressable.progress();
-      }
-    };
-  }
-
-  public String getKerberosShortName(String kerberosLongName) throws IOException {
-    KerberosName kerberosName = new KerberosName(kerberosLongName);
-    return kerberosName.getShortName();
-  }
-
-  @Override
-  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id) {
-    return new TaskAttemptID(jobId.getJtIdentifier(), jobId.getId(), isMap, taskId, id);
-  }
-
-  @Override
-  public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job) {
-    return new org.apache.hadoop.mapreduce.JobContext(job.getConfiguration(), job.getJobID());
-  }
-
-  @Override
-  public boolean isLocalMode(Configuration conf) {
-    return "local".equals(getJobLauncherRpcAddress(conf));
-  }
-
-  @Override
-  public String getJobLauncherRpcAddress(Configuration conf) {
-    return conf.get("mapred.job.tracker");
-  }
-
-  @Override
-  public void setJobLauncherRpcAddress(Configuration conf, String val) {
-    conf.set("mapred.job.tracker", val);
-  }
-
-  @Override
-  public String getJobLauncherHttpAddress(Configuration conf) {
-    return conf.get("mapred.job.tracker.http.address");
-  }
-
-  @Override
-  public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration conf)
-          throws IOException {
-    // older versions of Hadoop don't have a Trash constructor based on the
-    // Path or FileSystem. So need to achieve this by creating a dummy conf.
-    // this needs to be filtered out based on version
-
-    Configuration dupConf = new Configuration(conf);
-    FileSystem.setDefaultUri(dupConf, fs.getUri());
-    Trash trash = new Trash(dupConf);
-    return trash.moveToTrash(path);
-  }
-  @Override
-  public long getDefaultBlockSize(FileSystem fs, Path path) {
-    return fs.getDefaultBlockSize();
-  }
-
-  @Override
-  public short getDefaultReplication(FileSystem fs, Path path) {
-    return fs.getDefaultReplication();
-  }
-
-  @Override
-  public void setTotalOrderPartitionFile(JobConf jobConf, Path partitionFile){
-    TotalOrderPartitioner.setPartitionFile(jobConf, partitionFile);
-  }
-
-  /**
-   * Returns a shim to wrap MiniMrCluster
-   */
-  public MiniMrShim getMiniMrCluster(Configuration conf, int numberOfTaskTrackers,
-                                     String nameNode, int numDir) throws IOException {
-    return new MiniMrShim(conf, numberOfTaskTrackers, nameNode, numDir);
-  }
-
-  /**
-   * Shim for MiniMrCluster
-   */
-  public class MiniMrShim implements HadoopShims.MiniMrShim {
-
-    private final MiniMRCluster mr;
-
-    public MiniMrShim(Configuration conf, int numberOfTaskTrackers,
-        String nameNode, int numDir) throws IOException {
-      this.mr = new MiniMRCluster(numberOfTaskTrackers, nameNode, numDir);
-    }
-
-    @Override
-    public int getJobTrackerPort() throws UnsupportedOperationException {
-      return mr.getJobTrackerPort();
-    }
-
-    @Override
-    public void shutdown() throws IOException {
-      mr.shutdown();
-    }
-
-    @Override
-    public void setupConfiguration(Configuration conf) {
-      setJobLauncherRpcAddress(conf, "localhost:" + mr.getJobTrackerPort());
-    }
-  }
-
-  // Don't move this code to the parent class. There's a binary
-  // incompatibility between hadoop 1 and 2 wrt MiniDFSCluster and we
-  // need to have two different shim classes even though they are
-  // exactly the same.
-  public HadoopShims.MiniDFSShim getMiniDfs(Configuration conf,
-      int numDataNodes,
-      boolean format,
-      String[] racks) throws IOException {
-    return new MiniDFSShim(new MiniDFSCluster(conf, numDataNodes, format, racks));
-  }
-
-  /**
-   * MiniDFSShim.
-   *
-   */
-  public class MiniDFSShim implements HadoopShims.MiniDFSShim {
-    private final MiniDFSCluster cluster;
-
-    public MiniDFSShim(MiniDFSCluster cluster) {
-      this.cluster = cluster;
-    }
-
-    public FileSystem getFileSystem() throws IOException {
-      return cluster.getFileSystem();
-    }
-
-    public void shutdown() {
-      cluster.shutdown();
-    }
-  }
-  private volatile HCatHadoopShims hcatShimInstance;
-  @Override
-  public HCatHadoopShims getHCatShim() {
-    if(hcatShimInstance == null) {
-      hcatShimInstance = new HCatHadoopShims20S();
-    }
-    return hcatShimInstance;
-  }
-  private final class HCatHadoopShims20S implements HCatHadoopShims {
-    @Override
-    public TaskID createTaskID() {
-      return new TaskID();
-    }
-
-    @Override
-    public TaskAttemptID createTaskAttemptID() {
-      return new TaskAttemptID();
-    }
-
-    @Override
-    public TaskAttemptContext createTaskAttemptContext(Configuration conf, TaskAttemptID taskId) {
-      return new TaskAttemptContext(conf, taskId);
-    }
-
-    @Override
-    public org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf conf,
-                 org.apache.hadoop.mapred.TaskAttemptID taskId, Progressable progressable) {
-      org.apache.hadoop.mapred.TaskAttemptContext newContext = null;
-      try {
-        java.lang.reflect.Constructor construct = org.apache.hadoop.mapred.TaskAttemptContext.class.getDeclaredConstructor(
-                org.apache.hadoop.mapred.JobConf.class, org.apache.hadoop.mapred.TaskAttemptID.class,
-                Progressable.class);
-        construct.setAccessible(true);
-        newContext = (org.apache.hadoop.mapred.TaskAttemptContext)construct.newInstance(conf, taskId, progressable);
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-      return newContext;
-    }
-
-    @Override
-    public JobContext createJobContext(Configuration conf,
-                                       JobID jobId) {
-      return new JobContext(conf, jobId);
-    }
-
-    @Override
-    public org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf conf,
-                                   org.apache.hadoop.mapreduce.JobID jobId, Progressable progressable) {
-      org.apache.hadoop.mapred.JobContext newContext = null;
-      try {
-        java.lang.reflect.Constructor construct = org.apache.hadoop.mapred.JobContext.class.getDeclaredConstructor(
-                org.apache.hadoop.mapred.JobConf.class, org.apache.hadoop.mapreduce.JobID.class,
-                Progressable.class);
-        construct.setAccessible(true);
-        newContext = (org.apache.hadoop.mapred.JobContext)construct.newInstance(conf, jobId, progressable);
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-      return newContext;
-    }
-
-    @Override
-    public void commitJob(OutputFormat outputFormat, Job job) throws IOException {
-      if( job.getConfiguration().get("mapred.job.tracker", "").equalsIgnoreCase("local") ) {
-        try {
-          //In local mode, mapreduce will not call OutputCommitter.cleanupJob.
-          //Calling it from here so that the partition publish happens.
-          //This call needs to be removed after MAPREDUCE-1447 is fixed.
-          outputFormat.getOutputCommitter(createTaskAttemptContext(
-                  job.getConfiguration(), createTaskAttemptID())).commitJob(job);
-        } catch (IOException e) {
-          throw new IOException("Failed to cleanup job",e);
-        } catch (InterruptedException e) {
-          throw new IOException("Failed to cleanup job",e);
-        }
-      }
-    }
-
-    @Override
-    public void abortJob(OutputFormat outputFormat, Job job) throws IOException {
-      if (job.getConfiguration().get("mapred.job.tracker", "")
-              .equalsIgnoreCase("local")) {
-        try {
-          // This call needs to be removed after MAPREDUCE-1447 is fixed.
-          outputFormat.getOutputCommitter(createTaskAttemptContext(
-                  job.getConfiguration(), new TaskAttemptID())).abortJob(job, JobStatus.State.FAILED);
-        } catch (IOException e) {
-          throw new IOException("Failed to abort job", e);
-        } catch (InterruptedException e) {
-          throw new IOException("Failed to abort job", e);
-        }
-      }
-    }
-
-    @Override
-    public InetSocketAddress getResourceManagerAddress(Configuration conf)
-    {
-      return JobTracker.getAddress(conf);
-    }
-
-    @Override
-    public String getPropertyName(PropertyName name) {
-      switch (name) {
-        case CACHE_ARCHIVES:
-          return DistributedCache.CACHE_ARCHIVES;
-        case CACHE_FILES:
-          return DistributedCache.CACHE_FILES;
-        case CACHE_SYMLINK:
-          return DistributedCache.CACHE_SYMLINK;
-      }
-
-      return "";
-    }
-
-    @Override
-    public boolean isFileInHDFS(FileSystem fs, Path path) throws IOException {
-      // In hadoop 1.x.x the file system URI is sufficient to determine the uri of the file
-      return "hdfs".equals(fs.getUri().getScheme());
-    }
-  }
-  @Override
-  public WebHCatJTShim getWebHCatShim(Configuration conf, UserGroupInformation ugi) throws IOException {
-    return new WebHCatJTShim20S(conf, ugi);//this has state, so can't be cached
-  }
-
-  @Override
-  public FileSystem createProxyFileSystem(FileSystem fs, URI uri) {
-    return new ProxyFileSystem(fs, uri);
-  }
-}
diff --git a/src/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Jetty20SShims.java b/src/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Jetty20SShims.java
deleted file mode 100644
index 75659ff..0000000
--- a/src/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Jetty20SShims.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.shims;
-
-
-import org.mortbay.jetty.bio.SocketConnector;
-import org.mortbay.jetty.handler.RequestLogHandler;
-import org.mortbay.jetty.webapp.WebAppContext;
-
-import java.io.IOException;
-
-public class Jetty20SShims implements JettyShims {
-  public Server startServer(String listen, int port) throws IOException {
-    Server s = new Server();
-    s.setupListenerHostPort(listen, port);
-    return s;
-  }
-
-  private static class Server extends org.mortbay.jetty.Server implements JettyShims.Server {
-    public void addWar(String war, String contextPath) {
-      WebAppContext wac = new WebAppContext();
-      wac.setContextPath(contextPath);
-      wac.setWar(war);
-      RequestLogHandler rlh = new RequestLogHandler();
-      rlh.setHandler(wac);
-      this.addHandler(rlh);
-   }
-
-    public void setupListenerHostPort(String listen, int port)
-      throws IOException {
-
-      SocketConnector connector  = new SocketConnector();
-      connector.setPort(port);
-      connector.setHost(listen);
-      this.addConnector(connector);
-    }
-  }
-}
diff --git a/src/shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java b/src/shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java
deleted file mode 100644
index 4c9bd73..0000000
--- a/src/shims/src/0.20S/java/org/apache/hadoop/mapred/WebHCatJTShim20S.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.shims.HadoopShims.WebHCatJTShim;
-import org.apache.hadoop.ipc.RPC;
-import org.apache.hadoop.net.NetUtils;
-import org.apache.hadoop.security.UserGroupInformation;
-
-import java.io.IOException;
-import java.net.InetSocketAddress;
-
-/**
- * This is in org.apache.hadoop.mapred package because it relies on
- * JobSubmissionProtocol which is package private
- */
-public class WebHCatJTShim20S implements WebHCatJTShim {
-    private JobSubmissionProtocol cnx;
-
-    /**
-     * Create a connection to the Job Tracker.
-     */
-    public WebHCatJTShim20S(Configuration conf, UserGroupInformation ugi)
-            throws IOException {
-      cnx = (JobSubmissionProtocol)
-              RPC.getProxy(JobSubmissionProtocol.class,
-                      JobSubmissionProtocol.versionID,
-                      getAddress(conf),
-                      ugi,
-                      conf,
-                      NetUtils.getSocketFactory(conf,
-                              JobSubmissionProtocol.class));
-    }
-
-    /**
-     * Grab a handle to a job that is already known to the JobTracker.
-     *
-     * @return Profile of the job, or null if not found.
-     */
-    public JobProfile getJobProfile(org.apache.hadoop.mapred.JobID jobid)
-            throws IOException {
-      return cnx.getJobProfile(jobid);
-    }
-
-    /**
-     * Grab a handle to a job that is already known to the JobTracker.
-     *
-     * @return Status of the job, or null if not found.
-     */
-    public org.apache.hadoop.mapred.JobStatus getJobStatus(org.apache.hadoop.mapred.JobID jobid)
-            throws IOException {
-      return cnx.getJobStatus(jobid);
-    }
-
-
-    /**
-     * Kill a job.
-     */
-    public void killJob(org.apache.hadoop.mapred.JobID jobid)
-            throws IOException {
-      cnx.killJob(jobid);
-    }
-
-    /**
-     * Get all the jobs submitted.
-     */
-    public org.apache.hadoop.mapred.JobStatus[] getAllJobs()
-            throws IOException {
-      return cnx.getAllJobs();
-    }
-
-    /**
-     * Close the connection to the Job Tracker.
-     */
-    public void close() {
-      RPC.stopProxy(cnx);
-    }
-    private InetSocketAddress getAddress(Configuration conf) {
-      String jobTrackerStr = conf.get("mapred.job.tracker", "localhost:8012");
-      return NetUtils.createSocketAddr(jobTrackerStr);
-    }
-  }
-
diff --git a/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java b/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
deleted file mode 100644
index dc5facd..0000000
--- a/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
+++ /dev/null
@@ -1,390 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.shims;
-
-import java.io.IOException;
-import java.lang.Integer;
-import java.net.InetSocketAddress;
-import java.net.MalformedURLException;
-import java.net.URL;
-import java.util.Map;
-import java.net.URI;
-import java.io.FileNotFoundException;
-
-import org.apache.commons.lang.StringUtils;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.ProxyFileSystem;
-import org.apache.hadoop.fs.LocatedFileStatus;
-import org.apache.hadoop.fs.RemoteIterator;
-import org.apache.hadoop.fs.Trash;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.mapred.MiniMRCluster;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.WebHCatJTShim23;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.JobID;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.OutputFormat;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hadoop.mapreduce.TaskID;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.task.JobContextImpl;
-import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
-import org.apache.hadoop.mapreduce.util.HostUtil;
-import org.apache.hadoop.net.NetUtils;
-import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.security.authentication.util.KerberosName;
-
-/**
- * Implemention of shims against Hadoop 0.23.0.
- */
-public class Hadoop23Shims extends HadoopShimsSecure {
-
-  @Override
-  public String getTaskAttemptLogUrl(JobConf conf,
-    String taskTrackerHttpAddress, String taskAttemptId)
-    throws MalformedURLException {
-    if (conf.get("mapreduce.framework.name") != null
-      && conf.get("mapreduce.framework.name").equals("yarn")) {
-      // if the cluster is running in MR2 mode, return null
-      LOG.warn("Can't fetch tasklog: TaskLogServlet is not supported in MR2 mode.");
-      return null;
-    } else {
-      // if the cluster is running in MR1 mode, using HostUtil to construct TaskLogURL
-      URL taskTrackerHttpURL = new URL(taskTrackerHttpAddress);
-      return HostUtil.getTaskLogUrl(taskTrackerHttpURL.getHost(),
-        Integer.toString(taskTrackerHttpURL.getPort()),
-        taskAttemptId);
-    }
-  }
-
-  @Override
-  public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception {
-    switch (clusterStatus.getJobTrackerStatus()) {
-    case INITIALIZING:
-      return JobTrackerState.INITIALIZING;
-    case RUNNING:
-      return JobTrackerState.RUNNING;
-    default:
-      String errorMsg = "Unrecognized JobTracker state: " + clusterStatus.getJobTrackerStatus();
-      throw new Exception(errorMsg);
-    }
-  }
-
-  @Override
-  public org.apache.hadoop.mapreduce.TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable) {
-    return new TaskAttemptContextImpl(conf, new TaskAttemptID()) {
-      @Override
-      public void progress() {
-        progressable.progress();
-      }
-    };
-  }
-
-  @Override
-  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id) {
-    return new TaskAttemptID(jobId.getJtIdentifier(), jobId.getId(), isMap ?  TaskType.MAP : TaskType.REDUCE, taskId, id);
-  }
-
-  @Override
-  public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job) {
-    return new JobContextImpl(job.getConfiguration(), job.getJobID());
-  }
-
-  @Override
-  public boolean isLocalMode(Configuration conf) {
-    return "local".equals(conf.get("mapreduce.framework.name"));
-  }
-
-  @Override
-  public String getJobLauncherRpcAddress(Configuration conf) {
-    return conf.get("yarn.resourcemanager.address");
-  }
-
-  @Override
-  public void setJobLauncherRpcAddress(Configuration conf, String val) {
-    if (val.equals("local")) {
-      // LocalClientProtocolProvider expects both parameters to be 'local'.
-      conf.set("mapreduce.framework.name", val);
-      conf.set("mapreduce.jobtracker.address", val);
-    }
-    else {
-      conf.set("mapreduce.framework.name", "yarn");
-      conf.set("yarn.resourcemanager.address", val);
-    }
-  }
-
-  public String getKerberosShortName(String kerberosLongName) throws IOException {
-    KerberosName kerberosName = new KerberosName(kerberosLongName);
-    return kerberosName.getShortName();
-  }
-
-  @Override
-  public String getJobLauncherHttpAddress(Configuration conf) {
-    return conf.get("yarn.resourcemanager.webapp.address");
-  }
-
-  @Override
-  public long getDefaultBlockSize(FileSystem fs, Path path) {
-    return fs.getDefaultBlockSize(path);
-  }
-
-  @Override
-  public short getDefaultReplication(FileSystem fs, Path path) {
-    return fs.getDefaultReplication(path);
-  }
-
-  @Override
-  public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration conf)
-          throws IOException {
-    return Trash.moveToAppropriateTrash(fs, path, conf);
-  }
-
-  @Override
-  public void setTotalOrderPartitionFile(JobConf jobConf, Path partitionFile){
-    TotalOrderPartitioner.setPartitionFile(jobConf, partitionFile);
-  }
-
-  /**
-   * Returns a shim to wrap MiniMrCluster
-   */
-  public MiniMrShim getMiniMrCluster(Configuration conf, int numberOfTaskTrackers,
-                                     String nameNode, int numDir) throws IOException {
-    return new MiniMrShim(conf, numberOfTaskTrackers, nameNode, numDir);
-  }
-
-  /**
-   * Shim for MiniMrCluster
-   */
-  public class MiniMrShim implements HadoopShims.MiniMrShim {
-
-    private final MiniMRCluster mr;
-    private final Configuration conf;
-
-    public MiniMrShim(Configuration conf, int numberOfTaskTrackers,
-                      String nameNode, int numDir) throws IOException {
-      this.conf = conf;
-
-      JobConf jConf = new JobConf(conf);
-      jConf.set("yarn.scheduler.capacity.root.queues", "default");
-      jConf.set("yarn.scheduler.capacity.root.default.capacity", "100");
-
-      mr = new MiniMRCluster(numberOfTaskTrackers, nameNode, numDir, null, null, jConf);
-    }
-
-    @Override
-    public int getJobTrackerPort() throws UnsupportedOperationException {
-      String address = conf.get("yarn.resourcemanager.address");
-      address = StringUtils.substringAfterLast(address, ":");
-
-      if (StringUtils.isBlank(address)) {
-        throw new IllegalArgumentException("Invalid YARN resource manager port.");
-      }
-
-      return Integer.parseInt(address);
-    }
-
-    @Override
-    public void shutdown() throws IOException {
-      mr.shutdown();
-    }
-
-    @Override
-    public void setupConfiguration(Configuration conf) {
-      JobConf jConf = mr.createJobConf();
-      for (Map.Entry<String, String> pair: jConf) {
-        conf.set(pair.getKey(), pair.getValue());
-      }
-    }
-  }
-
-  // Don't move this code to the parent class. There's a binary
-  // incompatibility between hadoop 1 and 2 wrt MiniDFSCluster and we
-  // need to have two different shim classes even though they are
-  // exactly the same.
-  public HadoopShims.MiniDFSShim getMiniDfs(Configuration conf,
-      int numDataNodes,
-      boolean format,
-      String[] racks) throws IOException {
-    return new MiniDFSShim(new MiniDFSCluster(conf, numDataNodes, format, racks));
-  }
-
-  /**
-   * MiniDFSShim.
-   *
-   */
-  public class MiniDFSShim implements HadoopShims.MiniDFSShim {
-    private final MiniDFSCluster cluster;
-
-    public MiniDFSShim(MiniDFSCluster cluster) {
-      this.cluster = cluster;
-    }
-
-    public FileSystem getFileSystem() throws IOException {
-      return cluster.getFileSystem();
-    }
-
-    public void shutdown() {
-      cluster.shutdown();
-    }
-  }
-  private volatile HCatHadoopShims hcatShimInstance;
-  @Override
-  public HCatHadoopShims getHCatShim() {
-    if(hcatShimInstance == null) {
-      hcatShimInstance = new HCatHadoopShims23();
-    }
-    return hcatShimInstance;
-  }
-  private final class HCatHadoopShims23 implements HCatHadoopShims {
-    @Override
-    public TaskID createTaskID() {
-      return new TaskID("", 0, TaskType.MAP, 0);
-    }
-
-    @Override
-    public TaskAttemptID createTaskAttemptID() {
-      return new TaskAttemptID("", 0, TaskType.MAP, 0, 0);
-    }
-
-    @Override
-    public org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(Configuration conf,
-                                                                                   org.apache.hadoop.mapreduce.TaskAttemptID taskId) {
-      return new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(
-              conf instanceof JobConf? new JobConf(conf) : conf,
-              taskId);
-    }
-
-    @Override
-    public org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapred.JobConf conf,
-                                                                                org.apache.hadoop.mapred.TaskAttemptID taskId, Progressable progressable) {
-      org.apache.hadoop.mapred.TaskAttemptContext newContext = null;
-      try {
-        java.lang.reflect.Constructor construct = org.apache.hadoop.mapred.TaskAttemptContextImpl.class.getDeclaredConstructor(
-                org.apache.hadoop.mapred.JobConf.class, org.apache.hadoop.mapred.TaskAttemptID.class,
-                Reporter.class);
-        construct.setAccessible(true);
-        newContext = (org.apache.hadoop.mapred.TaskAttemptContext) construct.newInstance(
-                new JobConf(conf), taskId, (Reporter) progressable);
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-      return newContext;
-    }
-
-    @Override
-    public JobContext createJobContext(Configuration conf,
-                                       JobID jobId) {
-      return new JobContextImpl(conf instanceof JobConf? new JobConf(conf) : conf,
-              jobId);
-    }
-
-    @Override
-    public org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf conf,
-                                                                org.apache.hadoop.mapreduce.JobID jobId, Progressable progressable) {
-      return new org.apache.hadoop.mapred.JobContextImpl(
-              new JobConf(conf), jobId, (org.apache.hadoop.mapred.Reporter) progressable);
-    }
-
-    @Override
-    public void commitJob(OutputFormat outputFormat, Job job) throws IOException {
-      // Do nothing as this was fixed by MAPREDUCE-1447.
-    }
-
-    @Override
-    public void abortJob(OutputFormat outputFormat, Job job) throws IOException {
-      // Do nothing as this was fixed by MAPREDUCE-1447.
-    }
-
-    @Override
-    public InetSocketAddress getResourceManagerAddress(Configuration conf) {
-      String addr = conf.get("yarn.resourcemanager.address", "localhost:8032");
-
-      return NetUtils.createSocketAddr(addr);
-    }
-
-    @Override
-    public String getPropertyName(PropertyName name) {
-      switch (name) {
-        case CACHE_ARCHIVES:
-          return MRJobConfig.CACHE_ARCHIVES;
-        case CACHE_FILES:
-          return MRJobConfig.CACHE_FILES;
-        case CACHE_SYMLINK:
-          return MRJobConfig.CACHE_SYMLINK;
-      }
-
-      return "";
-    }
-
-    @Override
-    public boolean isFileInHDFS(FileSystem fs, Path path) throws IOException {
-      // In case of viewfs we need to lookup where the actual file is to know the filesystem in use.
-      // resolvePath is a sure shot way of knowing which file system the file is.
-      return "hdfs".equals(fs.resolvePath(path).toUri().getScheme());
-    }
-  }
-  @Override
-  public WebHCatJTShim getWebHCatShim(Configuration conf, UserGroupInformation ugi) throws IOException {
-    return new WebHCatJTShim23(conf, ugi);//this has state, so can't be cached
-  }
-
-  class ProxyFileSystem23 extends ProxyFileSystem {
-    public ProxyFileSystem23(FileSystem fs) {
-      super(fs);
-    }
-    public ProxyFileSystem23(FileSystem fs, URI uri) {
-      super(fs, uri);
-    }
-
-    @Override
-    public RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path f)
-      throws FileNotFoundException, IOException {
-      return new RemoteIterator<LocatedFileStatus>() {
-        private RemoteIterator<LocatedFileStatus> stats =
-            ProxyFileSystem23.super.listLocatedStatus(
-                ProxyFileSystem23.super.swizzleParamPath(f));
-
-        @Override
-        public boolean hasNext() throws IOException {
-          return stats.hasNext();
-        }
-
-        @Override
-        public LocatedFileStatus next() throws IOException {
-          LocatedFileStatus result = stats.next();
-          return new LocatedFileStatus(
-              ProxyFileSystem23.super.swizzleFileStatus(result, false),
-              result.getBlockLocations());
-        }
-      };
-    }
-  }
-
-  @Override
-  public FileSystem createProxyFileSystem(FileSystem fs, URI uri) {
-    return new ProxyFileSystem23(fs, uri);
-  }
-}
diff --git a/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Jetty23Shims.java b/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Jetty23Shims.java
deleted file mode 100644
index 9328749..0000000
--- a/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Jetty23Shims.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.shims;
-
-import java.io.IOException;
-
-import org.mortbay.jetty.bio.SocketConnector;
-import org.mortbay.jetty.handler.RequestLogHandler;
-import org.mortbay.jetty.webapp.WebAppContext;
-
-/**
- * Jetty23Shims.
- *
- */
-public class Jetty23Shims implements JettyShims {
-  public Server startServer(String listen, int port) throws IOException {
-    Server s = new Server();
-    s.setupListenerHostPort(listen, port);
-    return s;
-  }
-
-  private static class Server extends org.mortbay.jetty.Server implements JettyShims.Server {
-    public void addWar(String war, String contextPath) {
-      WebAppContext wac = new WebAppContext();
-      wac.setContextPath(contextPath);
-      wac.setWar(war);
-      RequestLogHandler rlh = new RequestLogHandler();
-      rlh.setHandler(wac);
-      this.addHandler(rlh);
-    }
-
-    public void setupListenerHostPort(String listen, int port)
-        throws IOException {
-
-      SocketConnector connector = new SocketConnector();
-      connector.setPort(port);
-      connector.setHost(listen);
-      this.addConnector(connector);
-    }
-  }
-}
diff --git a/src/shims/src/0.23/java/org/apache/hadoop/mapred/WebHCatJTShim23.java b/src/shims/src/0.23/java/org/apache/hadoop/mapred/WebHCatJTShim23.java
deleted file mode 100644
index abb3911..0000000
--- a/src/shims/src/0.23/java/org/apache/hadoop/mapred/WebHCatJTShim23.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.hive.shims.HadoopShims.WebHCatJTShim;
-
-import java.io.IOException;
-
-public class WebHCatJTShim23 implements WebHCatJTShim {
-  private JobClient jc;
-
-  /**
-   * Create a connection to the Job Tracker.
-   */
-  public WebHCatJTShim23(Configuration conf, final UserGroupInformation ugi)
-          throws IOException {
-
-    jc = new JobClient(conf);
-  }
-
-  /**
-   * Grab a handle to a job that is already known to the JobTracker.
-   *
-   * @return Profile of the job, or null if not found.
-   */
-  public JobProfile getJobProfile(JobID jobid)
-          throws IOException {
-    RunningJob rj = jc.getJob(jobid);
-    JobStatus jobStatus = rj.getJobStatus();
-    JobProfile jobProfile = new JobProfile(jobStatus.getUsername(), jobStatus.getJobID(),
-            jobStatus.getJobFile(), jobStatus.getTrackingUrl(), jobStatus.getJobName());
-    return jobProfile;
-  }
-
-  /**
-   * Grab a handle to a job that is already known to the JobTracker.
-   *
-   * @return Status of the job, or null if not found.
-   */
-  public JobStatus getJobStatus(JobID jobid)
-          throws IOException {
-    RunningJob rj = jc.getJob(jobid);
-    JobStatus jobStatus = rj.getJobStatus();
-    return jobStatus;
-  }
-
-
-  /**
-   * Kill a job.
-   */
-  public void killJob(JobID jobid)
-          throws IOException {
-    RunningJob rj = jc.getJob(jobid);
-    rj.killJob();
-  }
-
-  /**
-   * Get all the jobs submitted.
-   */
-  public JobStatus[] getAllJobs()
-          throws IOException {
-    return jc.getAllJobs();
-  }
-
-  /**
-   * Close the connection to the Job Tracker.
-   */
-  public void close() {
-    try {
-      jc.close();
-    } catch (IOException e) {
-    }
-  }
-}
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java b/src/shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
deleted file mode 100644
index fd274a6..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
+++ /dev/null
@@ -1,628 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.shims;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.File;
-import java.io.IOException;
-import java.lang.reflect.Constructor;
-import java.net.URI;
-import java.net.URISyntaxException;
-import java.security.PrivilegedExceptionAction;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil;
-import org.apache.hadoop.hive.thrift.DelegationTokenIdentifier;
-import org.apache.hadoop.hive.thrift.DelegationTokenSelector;
-import org.apache.hadoop.http.HtmlQuoting;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.InputFormat;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.JobContext;
-import org.apache.hadoop.mapred.JobStatus;
-import org.apache.hadoop.mapred.OutputCommitter;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.RunningJob;
-import org.apache.hadoop.mapred.TaskAttemptContext;
-import org.apache.hadoop.mapred.TaskCompletionEvent;
-import org.apache.hadoop.mapred.TaskID;
-import org.apache.hadoop.mapred.lib.CombineFileInputFormat;
-import org.apache.hadoop.mapred.lib.CombineFileSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.security.Credentials;
-import org.apache.hadoop.security.SecurityUtil;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.security.token.Token;
-import org.apache.hadoop.security.token.TokenIdentifier;
-import org.apache.hadoop.security.token.TokenSelector;
-import org.apache.hadoop.tools.HadoopArchives;
-import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.util.ToolRunner;
-
-/**
- * Base implemention for shims against secure Hadoop 0.20.3/0.23.
- */
-public abstract class HadoopShimsSecure implements HadoopShims {
-
-  static final Log LOG = LogFactory.getLog(HadoopShimsSecure.class);
-
-  public boolean usesJobShell() {
-    return false;
-  }
-
-  public boolean fileSystemDeleteOnExit(FileSystem fs, Path path)
-      throws IOException {
-
-    return fs.deleteOnExit(path);
-  }
-
-  public void inputFormatValidateInput(InputFormat fmt, JobConf conf)
-      throws IOException {
-    // gone in 0.18+
-  }
-
-  @Override
-  public String unquoteHtmlChars(String item) {
-    return HtmlQuoting.unquoteHtmlChars(item);
-  }
-
-  public boolean isJobPreparing(RunningJob job) throws IOException {
-    return job.getJobState() == JobStatus.PREP;
-  }
-  /**
-   * Workaround for hadoop-17 - jobclient only looks at commandlineconfig.
-   */
-  public void setTmpFiles(String prop, String files) {
-    // gone in 20+
-  }
-
-  /**
-   * We define this function here to make the code compatible between
-   * hadoop 0.17 and hadoop 0.20.
-   *
-   * Hive binary that compiled Text.compareTo(Text) with hadoop 0.20 won't
-   * work with hadoop 0.17 because in hadoop 0.20, Text.compareTo(Text) is
-   * implemented in org.apache.hadoop.io.BinaryComparable, and Java compiler
-   * references that class, which is not available in hadoop 0.17.
-   */
-  public int compareText(Text a, Text b) {
-    return a.compareTo(b);
-  }
-
-  @Override
-  public long getAccessTime(FileStatus file) {
-    return file.getAccessTime();
-  }
-
-  public HadoopShims.CombineFileInputFormatShim getCombineFileInputFormat() {
-    return new CombineFileInputFormatShim() {
-      @Override
-      public RecordReader getRecordReader(InputSplit split,
-          JobConf job, Reporter reporter) throws IOException {
-        throw new IOException("CombineFileInputFormat.getRecordReader not needed.");
-      }
-    };
-  }
-
-  public static class InputSplitShim extends CombineFileSplit implements HadoopShims.InputSplitShim {
-    long shrinkedLength;
-    boolean _isShrinked;
-    public InputSplitShim() {
-      super();
-      _isShrinked = false;
-    }
-
-    public InputSplitShim(CombineFileSplit old) throws IOException {
-      super(old.getJob(), old.getPaths(), old.getStartOffsets(),
-          old.getLengths(), dedup(old.getLocations()));
-      _isShrinked = false;
-    }
-
-    private static String[] dedup(String[] locations) {
-      Set<String> dedup = new HashSet<String>();
-      Collections.addAll(dedup, locations);
-      return dedup.toArray(new String[dedup.size()]);
-    }
-
-    @Override
-    public void shrinkSplit(long length) {
-      _isShrinked = true;
-      shrinkedLength = length;
-    }
-
-    public boolean isShrinked() {
-      return _isShrinked;
-    }
-
-    public long getShrinkedLength() {
-      return shrinkedLength;
-    }
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-      super.readFields(in);
-      _isShrinked = in.readBoolean();
-      if (_isShrinked) {
-        shrinkedLength = in.readLong();
-      }
-    }
-
-    @Override
-    public void write(DataOutput out) throws IOException {
-      super.write(out);
-      out.writeBoolean(_isShrinked);
-      if (_isShrinked) {
-        out.writeLong(shrinkedLength);
-      }
-    }
-  }
-
-  /* This class should be replaced with org.apache.hadoop.mapred.lib.CombineFileRecordReader class, once
-   * https://issues.apache.org/jira/browse/MAPREDUCE-955 is fixed. This code should be removed - it is a copy
-   * of org.apache.hadoop.mapred.lib.CombineFileRecordReader
-   */
-  public static class CombineFileRecordReader<K, V> implements RecordReader<K, V> {
-
-    static final Class[] constructorSignature = new Class[] {
-        InputSplit.class,
-        Configuration.class,
-        Reporter.class,
-        Integer.class
-        };
-
-    protected CombineFileSplit split;
-    protected JobConf jc;
-    protected Reporter reporter;
-    protected Class<RecordReader<K, V>> rrClass;
-    protected Constructor<RecordReader<K, V>> rrConstructor;
-    protected FileSystem fs;
-
-    protected int idx;
-    protected long progress;
-    protected RecordReader<K, V> curReader;
-    protected boolean isShrinked;
-    protected long shrinkedLength;
-
-    public boolean next(K key, V value) throws IOException {
-
-      while ((curReader == null)
-          || !doNextWithExceptionHandler((K) ((CombineHiveKey) key).getKey(),
-              value)) {
-        if (!initNextRecordReader(key)) {
-          return false;
-        }
-      }
-      return true;
-    }
-
-    public K createKey() {
-      K newKey = curReader.createKey();
-      return (K)(new CombineHiveKey(newKey));
-    }
-
-    public V createValue() {
-      return curReader.createValue();
-    }
-
-    /**
-     * Return the amount of data processed.
-     */
-    public long getPos() throws IOException {
-      return progress;
-    }
-
-    public void close() throws IOException {
-      if (curReader != null) {
-        curReader.close();
-        curReader = null;
-      }
-    }
-
-    /**
-     * Return progress based on the amount of data processed so far.
-     */
-    public float getProgress() throws IOException {
-      return Math.min(1.0f, progress / (float) (split.getLength()));
-    }
-
-    /**
-     * A generic RecordReader that can hand out different recordReaders
-     * for each chunk in the CombineFileSplit.
-     */
-    public CombineFileRecordReader(JobConf job, CombineFileSplit split,
-        Reporter reporter,
-        Class<RecordReader<K, V>> rrClass)
-        throws IOException {
-      this.split = split;
-      this.jc = job;
-      this.rrClass = rrClass;
-      this.reporter = reporter;
-      this.idx = 0;
-      this.curReader = null;
-      this.progress = 0;
-
-      isShrinked = false;
-
-      assert (split instanceof InputSplitShim);
-      if (((InputSplitShim) split).isShrinked()) {
-        isShrinked = true;
-        shrinkedLength = ((InputSplitShim) split).getShrinkedLength();
-      }
-
-      try {
-        rrConstructor = rrClass.getDeclaredConstructor(constructorSignature);
-        rrConstructor.setAccessible(true);
-      } catch (Exception e) {
-        throw new RuntimeException(rrClass.getName() +
-            " does not have valid constructor", e);
-      }
-      initNextRecordReader(null);
-    }
-
-    /**
-     * do next and handle exception inside it.
-     * @param key
-     * @param value
-     * @return
-     * @throws IOException
-     */
-    private boolean doNextWithExceptionHandler(K key, V value) throws IOException {
-      try {
-        return curReader.next(key, value);
-      } catch (Exception e) {
-        return HiveIOExceptionHandlerUtil
-            .handleRecordReaderNextException(e, jc);
-      }
-    }
-
-    /**
-     * Get the record reader for the next chunk in this CombineFileSplit.
-     */
-    protected boolean initNextRecordReader(K key) throws IOException {
-
-      if (curReader != null) {
-        curReader.close();
-        curReader = null;
-        if (idx > 0) {
-          progress += split.getLength(idx - 1); // done processing so far
-        }
-      }
-
-      // if all chunks have been processed, nothing more to do.
-      if (idx == split.getNumPaths() || (isShrinked && progress > shrinkedLength)) {
-        return false;
-      }
-
-      // get a record reader for the idx-th chunk
-      try {
-        curReader = rrConstructor.newInstance(new Object[]
-            {split, jc, reporter, Integer.valueOf(idx)});
-
-        // change the key if need be
-        if (key != null) {
-          K newKey = curReader.createKey();
-          ((CombineHiveKey)key).setKey(newKey);
-        }
-
-        // setup some helper config variables.
-        jc.set("map.input.file", split.getPath(idx).toString());
-        jc.setLong("map.input.start", split.getOffset(idx));
-        jc.setLong("map.input.length", split.getLength(idx));
-      } catch (Exception e) {
-        curReader = HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(
-            e, jc);
-      }
-      idx++;
-      return true;
-    }
-  }
-
-  public abstract static class CombineFileInputFormatShim<K, V> extends
-      CombineFileInputFormat<K, V>
-      implements HadoopShims.CombineFileInputFormatShim<K, V> {
-
-    public Path[] getInputPathsShim(JobConf conf) {
-      try {
-        return FileInputFormat.getInputPaths(conf);
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void createPool(JobConf conf, PathFilter... filters) {
-      super.createPool(conf, filters);
-    }
-
-    @Override
-    public InputSplitShim[] getSplits(JobConf job, int numSplits) throws IOException {
-      long minSize = job.getLong("mapred.min.split.size", 0);
-
-      // For backward compatibility, let the above parameter be used
-      if (job.getLong("mapred.min.split.size.per.node", 0) == 0) {
-        super.setMinSplitSizeNode(minSize);
-      }
-
-      if (job.getLong("mapred.min.split.size.per.rack", 0) == 0) {
-        super.setMinSplitSizeRack(minSize);
-      }
-
-      if (job.getLong("mapred.max.split.size", 0) == 0) {
-        super.setMaxSplitSize(minSize);
-      }
-
-      InputSplit[] splits = (InputSplit[]) super.getSplits(job, numSplits);
-
-      InputSplitShim[] isplits = new InputSplitShim[splits.length];
-      for (int pos = 0; pos < splits.length; pos++) {
-        isplits[pos] = new InputSplitShim((CombineFileSplit)splits[pos]);
-      }
-
-      return isplits;
-    }
-
-    public InputSplitShim getInputSplitShim() throws IOException {
-      return new InputSplitShim();
-    }
-
-    public RecordReader getRecordReader(JobConf job, HadoopShims.InputSplitShim split,
-        Reporter reporter,
-        Class<RecordReader<K, V>> rrClass)
-        throws IOException {
-      CombineFileSplit cfSplit = (CombineFileSplit) split;
-      return new CombineFileRecordReader(job, cfSplit, reporter, rrClass);
-    }
-
-  }
-
-  public String getInputFormatClassName() {
-    return "org.apache.hadoop.hive.ql.io.CombineHiveInputFormat";
-  }
-
-  String[] ret = new String[2];
-
-  @Override
-  public String[] getTaskJobIDs(TaskCompletionEvent t) {
-    TaskID tid = t.getTaskAttemptId().getTaskID();
-    ret[0] = tid.toString();
-    ret[1] = tid.getJobID().toString();
-    return ret;
-  }
-
-  public void setFloatConf(Configuration conf, String varName, float val) {
-    conf.setFloat(varName, val);
-  }
-
-  @Override
-  public int createHadoopArchive(Configuration conf, Path sourceDir, Path destDir,
-      String archiveName) throws Exception {
-
-    HadoopArchives har = new HadoopArchives(conf);
-    List<String> args = new ArrayList<String>();
-
-    args.add("-archiveName");
-    args.add(archiveName);
-    args.add("-p");
-    args.add(sourceDir.toString());
-    args.add(destDir.toString());
-
-    return ToolRunner.run(har, args.toArray(new String[0]));
-  }
-
-  /*
-   * This particular instance is for Hadoop 1.0 which creates an archive
-   * with only the relative path of the archived directory stored within
-   * the archive as compared to the full path in case of earlier versions.
-   * See this api in Hadoop20Shims for comparison.
-   */
-  public URI getHarUri(URI original, URI base, URI originalBase)
-    throws URISyntaxException {
-    URI relative = originalBase.relativize(original);
-    if (relative.isAbsolute()) {
-      throw new URISyntaxException("Couldn't create URI for location.",
-                                   "Relative: " + relative + " Base: "
-                                   + base + " OriginalBase: " + originalBase);
-    }
-
-    return base.resolve(relative);
-  }
-
-  public static class NullOutputCommitter extends OutputCommitter {
-    @Override
-    public void setupJob(JobContext jobContext) { }
-    @Override
-    public void cleanupJob(JobContext jobContext) { }
-
-    @Override
-    public void setupTask(TaskAttemptContext taskContext) { }
-    @Override
-    public boolean needsTaskCommit(TaskAttemptContext taskContext) {
-      return false;
-    }
-    @Override
-    public void commitTask(TaskAttemptContext taskContext) { }
-    @Override
-    public void abortTask(TaskAttemptContext taskContext) { }
-  }
-
-  public void prepareJobOutput(JobConf conf) {
-    conf.setOutputCommitter(NullOutputCommitter.class);
-
-    // option to bypass job setup and cleanup was introduced in hadoop-21 (MAPREDUCE-463)
-    // but can be backported. So we disable setup/cleanup in all versions >= 0.19
-    conf.setBoolean("mapred.committer.job.setup.cleanup.needed", false);
-
-    // option to bypass task cleanup task was introduced in hadoop-23 (MAPREDUCE-2206)
-    // but can be backported. So we disable setup/cleanup in all versions >= 0.19
-    conf.setBoolean("mapreduce.job.committer.task.cleanup.needed", false);
-  }
-
-  @Override
-  public UserGroupInformation getUGIForConf(Configuration conf) throws IOException {
-    return UserGroupInformation.getCurrentUser();
-  }
-
-  @Override
-  public boolean isSecureShimImpl() {
-    return true;
-  }
-
-  @Override
-  public String getShortUserName(UserGroupInformation ugi) {
-    return ugi.getShortUserName();
-  }
-
-  @Override
-  public String getTokenStrForm(String tokenSignature) throws IOException {
-    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
-    TokenSelector<? extends TokenIdentifier> tokenSelector = new DelegationTokenSelector();
-
-    Token<? extends TokenIdentifier> token = tokenSelector.selectToken(
-        tokenSignature == null ? new Text() : new Text(tokenSignature), ugi.getTokens());
-    return token != null ? token.encodeToUrlString() : null;
-  }
-
-  @Override
-  public void setTokenStr(UserGroupInformation ugi, String tokenStr, String tokenService) throws IOException {
-    Token<DelegationTokenIdentifier> delegationToken = new Token<DelegationTokenIdentifier>();
-    delegationToken.decodeFromUrlString(tokenStr);
-    delegationToken.setService(new Text(tokenService));
-    ugi.addToken(delegationToken);
-  }
-
-  @Override
-  public <T> T doAs(UserGroupInformation ugi, PrivilegedExceptionAction<T> pvea) throws IOException, InterruptedException {
-    return ugi.doAs(pvea);
-  }
-
-  @Override
-  public Path createDelegationTokenFile(Configuration conf) throws IOException {
-
-    //get delegation token for user
-    String uname = UserGroupInformation.getLoginUser().getShortUserName();
-    FileSystem fs = FileSystem.get(conf);
-    Token<?> fsToken = fs.getDelegationToken(uname);
-
-    File t = File.createTempFile("hive_hadoop_delegation_token", null);
-    Path tokenPath = new Path(t.toURI());
-
-    //write credential with token to file
-    Credentials cred = new Credentials();
-    cred.addToken(fsToken.getService(), fsToken);
-    cred.writeTokenStorageFile(tokenPath, conf);
-
-    return tokenPath;
-  }
-
-
-  @Override
-  public UserGroupInformation createProxyUser(String userName) throws IOException {
-    return UserGroupInformation.createProxyUser(
-        userName, UserGroupInformation.getLoginUser());
-  }
-
-  @Override
-  public boolean isSecurityEnabled() {
-    return UserGroupInformation.isSecurityEnabled();
-  }
-
-  @Override
-  public UserGroupInformation createRemoteUser(String userName, List<String> groupNames) {
-    return UserGroupInformation.createRemoteUser(userName);
-  }
-
-  @Override
-  public void closeAllForUGI(UserGroupInformation ugi) {
-    try {
-      FileSystem.closeAllForUGI(ugi);
-    } catch (IOException e) {
-      LOG.error("Could not clean up file-system handles for UGI: " + ugi, e);
-    }
-  }
-
-  @Override
-  public void loginUserFromKeytab(String principal, String keytabFile) throws IOException {
-    String hostPrincipal = SecurityUtil.getServerPrincipal(principal, "0.0.0.0");
-    UserGroupInformation.loginUserFromKeytab(hostPrincipal, keytabFile);
-  }
-
-  @Override
-  public String getTokenFileLocEnvName() {
-    return UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION;
-  }
-
-  @Override
-  public void reLoginUserFromKeytab() throws IOException{
-    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-    //checkTGT calls ugi.relogin only after checking if it is close to tgt expiry
-    //hadoop relogin is actually done only every x minutes (x=10 in hadoop 1.x)
-    if(ugi.isFromKeytab()){
-      ugi.checkTGTAndReloginFromKeytab();
-    }
-  }
-
-  @Override
-  abstract public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception;
-
-  @Override
-  abstract public org.apache.hadoop.mapreduce.TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable);
-
-  @Override
-  abstract public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job);
-
-  @Override
-  abstract public boolean isLocalMode(Configuration conf);
-
-  @Override
-  abstract public void setJobLauncherRpcAddress(Configuration conf, String val);
-
-  @Override
-  abstract public String getJobLauncherHttpAddress(Configuration conf);
-
-  @Override
-  abstract public String getJobLauncherRpcAddress(Configuration conf);
-
-  @Override
-  abstract public short getDefaultReplication(FileSystem fs, Path path);
-
-  @Override
-  abstract public long getDefaultBlockSize(FileSystem fs, Path path);
-
-  @Override
-  abstract public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration conf)
-          throws IOException;
-
-  @Override
-  abstract public FileSystem createProxyFileSystem(FileSystem fs, URI uri);
-}
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DBTokenStore.java b/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DBTokenStore.java
deleted file mode 100644
index b507a09..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DBTokenStore.java
+++ /dev/null
@@ -1,153 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import java.io.IOException;
-import java.lang.reflect.InvocationTargetException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.commons.codec.binary.Base64;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
-import org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport;
-
-public class DBTokenStore implements DelegationTokenStore {
-
-
-  @Override
-  public int addMasterKey(String s) throws TokenStoreException {
-    return (Integer)invokeOnRawStore("addMasterKey", new Object[]{s},String.class);
-  }
-
-  @Override
-  public void updateMasterKey(int keySeq, String s) throws TokenStoreException {
-    invokeOnRawStore("updateMasterKey", new Object[] {Integer.valueOf(keySeq), s},
-        Integer.class, String.class);
-  }
-
-  @Override
-  public boolean removeMasterKey(int keySeq) {
-    return (Boolean)invokeOnRawStore("removeMasterKey", new Object[] {Integer.valueOf(keySeq)},
-      Integer.class);
-  }
-
-  @Override
-  public String[] getMasterKeys() throws TokenStoreException {
-    return (String[])invokeOnRawStore("getMasterKeys", new Object[0]);
-  }
-
-  @Override
-  public boolean addToken(DelegationTokenIdentifier tokenIdentifier,
-      DelegationTokenInformation token) throws TokenStoreException {
-
-    try {
-      String identifier = TokenStoreDelegationTokenSecretManager.encodeWritable(tokenIdentifier);
-      String tokenStr = Base64.encodeBase64URLSafeString(
-        HiveDelegationTokenSupport.encodeDelegationTokenInformation(token));
-      return (Boolean)invokeOnRawStore("addToken", new Object[] {identifier, tokenStr},
-        String.class, String.class);
-    } catch (IOException e) {
-      throw new TokenStoreException(e);
-    }
-  }
-
-  @Override
-  public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier)
-      throws TokenStoreException {
-    try {
-      String tokenStr = (String)invokeOnRawStore("getToken", new Object[] {
-          TokenStoreDelegationTokenSecretManager.encodeWritable(tokenIdentifier)}, String.class);
-      return (null == tokenStr) ? null : HiveDelegationTokenSupport.decodeDelegationTokenInformation(Base64.decodeBase64(tokenStr));
-    } catch (IOException e) {
-      throw new TokenStoreException(e);
-    }
-  }
-
-  @Override
-  public boolean removeToken(DelegationTokenIdentifier tokenIdentifier) throws TokenStoreException{
-    try {
-      return (Boolean)invokeOnRawStore("removeToken", new Object[] {
-        TokenStoreDelegationTokenSecretManager.encodeWritable(tokenIdentifier)}, String.class);
-    } catch (IOException e) {
-      throw new TokenStoreException(e);
-    }
-  }
-
-  @Override
-  public List<DelegationTokenIdentifier> getAllDelegationTokenIdentifiers() throws TokenStoreException{
-
-    List<String> tokenIdents = (List<String>)invokeOnRawStore("getAllTokenIdentifiers", new Object[0]);
-    List<DelegationTokenIdentifier> delTokenIdents = new ArrayList<DelegationTokenIdentifier>(tokenIdents.size());
-
-    for (String tokenIdent : tokenIdents) {
-      DelegationTokenIdentifier delToken = new DelegationTokenIdentifier();
-      try {
-        TokenStoreDelegationTokenSecretManager.decodeWritable(delToken, tokenIdent);
-      } catch (IOException e) {
-        throw new TokenStoreException(e);
-      }
-      delTokenIdents.add(delToken);
-    }
-    return delTokenIdents;
-  }
-
-  private Object hmsHandler;
-
-  @Override
-  public void setStore(Object hms) throws TokenStoreException {
-    hmsHandler = hms;
-  }
-
-  private Object invokeOnRawStore(String methName, Object[] params, Class<?> ... paramTypes)
-      throws TokenStoreException{
-
-    try {
-      Object rawStore = hmsHandler.getClass().getMethod("getMS").invoke(hmsHandler);
-      return rawStore.getClass().getMethod(methName, paramTypes).invoke(rawStore, params);
-    } catch (IllegalArgumentException e) {
-        throw new TokenStoreException(e);
-    } catch (SecurityException e) {
-        throw new TokenStoreException(e);
-    } catch (IllegalAccessException e) {
-        throw new TokenStoreException(e);
-    } catch (InvocationTargetException e) {
-        throw new TokenStoreException(e.getCause());
-    } catch (NoSuchMethodException e) {
-        throw new TokenStoreException(e);
-    }
-  }
-
-  @Override
-  public void setConf(Configuration conf) {
-    // No-op
-  }
-
-  @Override
-  public Configuration getConf() {
-    return null;
-  }
-
-  @Override
-  public void close() throws IOException {
-    // No-op.
-  }
-
-
-}
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenIdentifier.java b/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenIdentifier.java
deleted file mode 100644
index 4ca3c0b..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenIdentifier.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier;
-
-/**
- * A delegation token identifier that is specific to Hive.
- */
-public class DelegationTokenIdentifier
-    extends AbstractDelegationTokenIdentifier {
-  public static final Text HIVE_DELEGATION_KIND = new Text("HIVE_DELEGATION_TOKEN");
-
-  /**
-   * Create an empty delegation token identifier for reading into.
-   */
-  public DelegationTokenIdentifier() {
-  }
-
-  /**
-   * Create a new delegation token identifier
-   * @param owner the effective username of the token owner
-   * @param renewer the username of the renewer
-   * @param realUser the real username of the token owner
-   */
-  public DelegationTokenIdentifier(Text owner, Text renewer, Text realUser) {
-    super(owner, renewer, realUser);
-  }
-
-  @Override
-  public Text getKind() {
-    return HIVE_DELEGATION_KIND;
-  }
-
-}
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenSecretManager.java b/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenSecretManager.java
deleted file mode 100644
index 29114f0..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenSecretManager.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import java.io.IOException;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.security.token.Token;
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;
-
-/**
- * A Hive specific delegation token secret manager.
- * The secret manager is responsible for generating and accepting the password
- * for each token.
- */
-public class DelegationTokenSecretManager
-    extends AbstractDelegationTokenSecretManager<DelegationTokenIdentifier> {
-
-  /**
-   * Create a secret manager
-   * @param delegationKeyUpdateInterval the number of seconds for rolling new
-   *        secret keys.
-   * @param delegationTokenMaxLifetime the maximum lifetime of the delegation
-   *        tokens
-   * @param delegationTokenRenewInterval how often the tokens must be renewed
-   * @param delegationTokenRemoverScanInterval how often the tokens are scanned
-   *        for expired tokens
-   */
-  public DelegationTokenSecretManager(long delegationKeyUpdateInterval,
-                                      long delegationTokenMaxLifetime,
-                                      long delegationTokenRenewInterval,
-                                      long delegationTokenRemoverScanInterval) {
-    super(delegationKeyUpdateInterval, delegationTokenMaxLifetime,
-          delegationTokenRenewInterval, delegationTokenRemoverScanInterval);
-  }
-
-  @Override
-  public DelegationTokenIdentifier createIdentifier() {
-    return new DelegationTokenIdentifier();
-  }
-
-  public synchronized void cancelDelegationToken(String tokenStrForm) throws IOException {
-    Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
-    t.decodeFromUrlString(tokenStrForm);
-    String user = UserGroupInformation.getCurrentUser().getUserName();
-    cancelToken(t, user);
-  }
-
-  public synchronized long renewDelegationToken(String tokenStrForm) throws IOException {
-    Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
-    t.decodeFromUrlString(tokenStrForm);
-    String user = UserGroupInformation.getCurrentUser().getUserName();
-    return renewToken(t, user);
-  }
-
-  public synchronized String getDelegationToken(String renewer) throws IOException {
-    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
-    Text owner = new Text(ugi.getUserName());
-    Text realUser = null;
-    if (ugi.getRealUser() != null) {
-      realUser = new Text(ugi.getRealUser().getUserName());
-    }
-    DelegationTokenIdentifier ident =
-      new DelegationTokenIdentifier(owner, new Text(renewer), realUser);
-    Token<DelegationTokenIdentifier> t = new Token<DelegationTokenIdentifier>(
-        ident, this);
-    return t.encodeToUrlString();
-  }
-}
-
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenSelector.java b/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenSelector.java
deleted file mode 100644
index f6e2420..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenSelector.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector;
-
-/**
- * A delegation token that is specialized for Hive
- */
-
-public class DelegationTokenSelector
-    extends AbstractDelegationTokenSelector<DelegationTokenIdentifier>{
-
-  public DelegationTokenSelector() {
-    super(DelegationTokenIdentifier.HIVE_DELEGATION_KIND);
-  }
-}
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenStore.java b/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenStore.java
deleted file mode 100644
index f3c2e48..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/DelegationTokenStore.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.thrift;
-
-import java.io.Closeable;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configurable;
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
-
-/**
- * Interface for pluggable token store that can be implemented with shared external
- * storage for load balancing and high availability (for example using ZooKeeper).
- * Internal, store specific errors are translated into {@link TokenStoreException}.
- */
-public interface DelegationTokenStore extends Configurable, Closeable {
-
-  /**
-   * Exception for internal token store errors that typically cannot be handled by the caller.
-   */
-  public static class TokenStoreException extends RuntimeException {
-    private static final long serialVersionUID = -8693819817623074083L;
-
-    public TokenStoreException(Throwable cause) {
-      super(cause);
-    }
-
-    public TokenStoreException(String message, Throwable cause) {
-      super(message, cause);
-    }
-  }
-
-  /**
-   * Add new master key. The token store assigns and returns the sequence number.
-   * Caller needs to use the identifier to update the key (since it is embedded in the key).
-   *
-   * @param s
-   * @return sequence number for new key
-   */
-  int addMasterKey(String s) throws TokenStoreException;
-
-  /**
-   * Update master key (for expiration and setting store assigned sequence within key)
-   * @param keySeq
-   * @param s
-   * @throws TokenStoreException
-   */
-  void updateMasterKey(int keySeq, String s) throws TokenStoreException;
-
-  /**
-   * Remove key for given id.
-   * @param keySeq
-   * @return false if key no longer present, true otherwise.
-   */
-  boolean removeMasterKey(int keySeq);
-
-  /**
-   * Return all master keys.
-   * @return
-   * @throws TokenStoreException
-   */
-  String[] getMasterKeys() throws TokenStoreException;
-
-  /**
-   * Add token. If identifier is already present, token won't be added.
-   * @param tokenIdentifier
-   * @param token
-   * @return true if token was added, false for existing identifier
-   */
-  boolean addToken(DelegationTokenIdentifier tokenIdentifier,
-      DelegationTokenInformation token) throws TokenStoreException;
-
-  /**
-   * Get token. Returns null if the token does not exist.
-   * @param tokenIdentifier
-   * @return
-   */
-  DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier)
-      throws TokenStoreException;
-
-  /**
-   * Remove token. Return value can be used by caller to detect concurrency.
-   * @param tokenIdentifier
-   * @return true if token was removed, false if it was already removed.
-   * @throws TokenStoreException
-   */
-  boolean removeToken(DelegationTokenIdentifier tokenIdentifier) throws TokenStoreException;
-
-  /**
-   * List of all token identifiers in the store. This is used to remove expired tokens
-   * and a potential scalability improvement would be to partition by master key id
-   * @return
-   */
-  List<DelegationTokenIdentifier> getAllDelegationTokenIdentifiers() throws TokenStoreException;
-
-  void setStore(Object hmsHandler) throws TokenStoreException;
-
-}
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java b/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java
deleted file mode 100644
index b3d016e..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java
+++ /dev/null
@@ -1,645 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.thrift;
-
-import static org.apache.hadoop.fs.CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION;
-
-import java.io.IOException;
-import java.net.InetAddress;
-import java.net.Socket;
-import java.security.PrivilegedAction;
-import java.security.PrivilegedExceptionAction;
-import java.util.Map;
-
-import javax.security.auth.callback.Callback;
-import javax.security.auth.callback.CallbackHandler;
-import javax.security.auth.callback.NameCallback;
-import javax.security.auth.callback.PasswordCallback;
-import javax.security.auth.callback.UnsupportedCallbackException;
-import javax.security.sasl.AuthorizeCallback;
-import javax.security.sasl.RealmCallback;
-import javax.security.sasl.RealmChoiceCallback;
-import javax.security.sasl.SaslException;
-import javax.security.sasl.SaslServer;
-
-import org.apache.commons.codec.binary.Base64;
-import org.apache.commons.lang.StringUtils;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.Client;
-import org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport;
-import org.apache.hadoop.security.SaslRpcServer;
-import org.apache.hadoop.security.SaslRpcServer.AuthMethod;
-import org.apache.hadoop.security.SecurityUtil;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.security.UserGroupInformation.AuthenticationMethod;
-import org.apache.hadoop.security.authorize.AuthorizationException;
-import org.apache.hadoop.security.authorize.ProxyUsers;
-import org.apache.hadoop.security.token.SecretManager.InvalidToken;
-import org.apache.hadoop.security.token.Token;
-import org.apache.hadoop.security.token.TokenIdentifier;
-import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.thrift.TException;
-import org.apache.thrift.TProcessor;
-import org.apache.thrift.protocol.TProtocol;
-import org.apache.thrift.transport.TSaslClientTransport;
-import org.apache.thrift.transport.TSaslServerTransport;
-import org.apache.thrift.transport.TSocket;
-import org.apache.thrift.transport.TTransport;
-import org.apache.thrift.transport.TTransportException;
-import org.apache.thrift.transport.TTransportFactory;
-
- /**
-  * Functions that bridge Thrift's SASL transports to Hadoop's
-  * SASL callback handlers and authentication classes.
-  */
- public class HadoopThriftAuthBridge20S extends HadoopThriftAuthBridge {
-   static final Log LOG = LogFactory.getLog(HadoopThriftAuthBridge.class);
-
-   @Override
-   public Client createClient() {
-     return new Client();
-   }
-
-   @Override
-   public Client createClientWithConf(String authType) {
-     Configuration conf = new Configuration();
-     conf.set(HADOOP_SECURITY_AUTHENTICATION, authType);
-     UserGroupInformation.setConfiguration(conf);
-     return new Client();
-   }
-
-   @Override
-   public Server createServer(String keytabFile, String principalConf) throws TTransportException {
-     return new Server(keytabFile, principalConf);
-   }
-
-   /**
-    * Read and return Hadoop SASL configuration which can be configured using
-    * "hadoop.rpc.protection"
-    * @param conf
-    * @return Hadoop SASL configuration
-    */
-   @Override
-   public Map<String, String> getHadoopSaslProperties(Configuration conf) {
-     // Initialize the SaslRpcServer to ensure QOP parameters are read from conf
-     SaslRpcServer.init(conf);
-     return SaslRpcServer.SASL_PROPS;
-   }
-
-   public static class Client extends HadoopThriftAuthBridge.Client {
-     /**
-      * Create a client-side SASL transport that wraps an underlying transport.
-      *
-      * @param method The authentication method to use. Currently only KERBEROS is
-      *               supported.
-      * @param serverPrincipal The Kerberos principal of the target server.
-      * @param underlyingTransport The underlying transport mechanism, usually a TSocket.
-      * @param saslProps the sasl properties to create the client with
-      */
-
-     @Override
-     public TTransport createClientTransport(
-       String principalConfig, String host,
-       String methodStr, String tokenStrForm, TTransport underlyingTransport,
-       Map<String, String> saslProps) throws IOException {
-       AuthMethod method = AuthMethod.valueOf(AuthMethod.class, methodStr);
-
-       TTransport saslTransport = null;
-       switch (method) {
-         case DIGEST:
-           Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
-           t.decodeFromUrlString(tokenStrForm);
-           saslTransport = new TSaslClientTransport(
-            method.getMechanismName(),
-            null,
-            null, SaslRpcServer.SASL_DEFAULT_REALM,
-            saslProps, new SaslClientCallbackHandler(t),
-            underlyingTransport);
-           return new TUGIAssumingTransport(saslTransport, UserGroupInformation.getCurrentUser());
-
-         case KERBEROS:
-           String serverPrincipal = SecurityUtil.getServerPrincipal(principalConfig, host);
-           String names[] = SaslRpcServer.splitKerberosName(serverPrincipal);
-           if (names.length != 3) {
-             throw new IOException(
-               "Kerberos principal name does NOT have the expected hostname part: "
-                 + serverPrincipal);
-           }
-           try {
-             saslTransport = new TSaslClientTransport(
-               method.getMechanismName(),
-               null,
-               names[0], names[1],
-               saslProps, null,
-               underlyingTransport);
-             return new TUGIAssumingTransport(saslTransport, UserGroupInformation.getCurrentUser());
-           } catch (SaslException se) {
-             throw new IOException("Could not instantiate SASL transport", se);
-           }
-
-         default:
-           throw new IOException("Unsupported authentication method: " + method);
-       }
-     }
-    private static class SaslClientCallbackHandler implements CallbackHandler {
-      private final String userName;
-      private final char[] userPassword;
-
-      public SaslClientCallbackHandler(Token<? extends TokenIdentifier> token) {
-        this.userName = encodeIdentifier(token.getIdentifier());
-        this.userPassword = encodePassword(token.getPassword());
-      }
-
-      public void handle(Callback[] callbacks)
-      throws UnsupportedCallbackException {
-        NameCallback nc = null;
-        PasswordCallback pc = null;
-        RealmCallback rc = null;
-        for (Callback callback : callbacks) {
-          if (callback instanceof RealmChoiceCallback) {
-            continue;
-          } else if (callback instanceof NameCallback) {
-            nc = (NameCallback) callback;
-          } else if (callback instanceof PasswordCallback) {
-            pc = (PasswordCallback) callback;
-          } else if (callback instanceof RealmCallback) {
-            rc = (RealmCallback) callback;
-          } else {
-            throw new UnsupportedCallbackException(callback,
-                "Unrecognized SASL client callback");
-          }
-        }
-        if (nc != null) {
-          if (LOG.isDebugEnabled()) {
-            LOG.debug("SASL client callback: setting username: " + userName);
-          }
-          nc.setName(userName);
-        }
-        if (pc != null) {
-          if (LOG.isDebugEnabled()) {
-            LOG.debug("SASL client callback: setting userPassword");
-          }
-          pc.setPassword(userPassword);
-        }
-        if (rc != null) {
-          if (LOG.isDebugEnabled()) {
-            LOG.debug("SASL client callback: setting realm: "
-                + rc.getDefaultText());
-          }
-          rc.setText(rc.getDefaultText());
-        }
-      }
-
-      static String encodeIdentifier(byte[] identifier) {
-        return new String(Base64.encodeBase64(identifier));
-      }
-
-      static char[] encodePassword(byte[] password) {
-        return new String(Base64.encodeBase64(password)).toCharArray();
-       }
-     }
-       }
-
-   public static class Server extends HadoopThriftAuthBridge.Server {
-     final UserGroupInformation realUgi;
-     DelegationTokenSecretManager secretManager;
-     private final static long DELEGATION_TOKEN_GC_INTERVAL = 3600000; // 1 hour
-     //Delegation token related keys
-     public static final String  DELEGATION_KEY_UPDATE_INTERVAL_KEY =
-       "hive.cluster.delegation.key.update-interval";
-     public static final long    DELEGATION_KEY_UPDATE_INTERVAL_DEFAULT =
-       24*60*60*1000; // 1 day
-     public static final String  DELEGATION_TOKEN_RENEW_INTERVAL_KEY =
-       "hive.cluster.delegation.token.renew-interval";
-     public static final long    DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT =
-       24*60*60*1000;  // 1 day
-     public static final String  DELEGATION_TOKEN_MAX_LIFETIME_KEY =
-       "hive.cluster.delegation.token.max-lifetime";
-     public static final long    DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT =
-       7*24*60*60*1000; // 7 days
-     public static final String DELEGATION_TOKEN_STORE_CLS =
-       "hive.cluster.delegation.token.store.class";
-     public static final String DELEGATION_TOKEN_STORE_ZK_CONNECT_STR =
-         "hive.cluster.delegation.token.store.zookeeper.connectString";
-     public static final String DELEGATION_TOKEN_STORE_ZK_CONNECT_TIMEOUTMILLIS =
-         "hive.cluster.delegation.token.store.zookeeper.connectTimeoutMillis";
-     public static final String DELEGATION_TOKEN_STORE_ZK_ZNODE =
-         "hive.cluster.delegation.token.store.zookeeper.znode";
-     public static final String DELEGATION_TOKEN_STORE_ZK_ACL =
-             "hive.cluster.delegation.token.store.zookeeper.acl";
-     public static final String DELEGATION_TOKEN_STORE_ZK_ZNODE_DEFAULT =
-         "/hive/cluster/delegation";
-
-     public Server() throws TTransportException {
-       try {
-         realUgi = UserGroupInformation.getCurrentUser();
-       } catch (IOException ioe) {
-         throw new TTransportException(ioe);
-       }
-     }
-     /**
-      * Create a server with a kerberos keytab/principal.
-      */
-     protected Server(String keytabFile, String principalConf)
-       throws TTransportException {
-       if (keytabFile == null || keytabFile.isEmpty()) {
-         throw new TTransportException("No keytab specified");
-       }
-       if (principalConf == null || principalConf.isEmpty()) {
-         throw new TTransportException("No principal specified");
-       }
-
-       // Login from the keytab
-       String kerberosName;
-       try {
-         kerberosName =
-           SecurityUtil.getServerPrincipal(principalConf, "0.0.0.0");
-         UserGroupInformation.loginUserFromKeytab(
-             kerberosName, keytabFile);
-         realUgi = UserGroupInformation.getLoginUser();
-         assert realUgi.isFromKeytab();
-       } catch (IOException ioe) {
-         throw new TTransportException(ioe);
-       }
-     }
-
-     /**
-      * Create a TTransportFactory that, upon connection of a client socket,
-      * negotiates a Kerberized SASL transport. The resulting TTransportFactory
-      * can be passed as both the input and output transport factory when
-      * instantiating a TThreadPoolServer, for example.
-      *
-      * @param saslProps Map of SASL properties
-      */
-     @Override
-     public TTransportFactory createTransportFactory(Map<String, String> saslProps)
-             throws TTransportException {
-       // Parse out the kerberos principal, host, realm.
-       String kerberosName = realUgi.getUserName();
-       final String names[] = SaslRpcServer.splitKerberosName(kerberosName);
-       if (names.length != 3) {
-         throw new TTransportException("Kerberos principal should have 3 parts: " + kerberosName);
-       }
-
-       TSaslServerTransport.Factory transFactory = new TSaslServerTransport.Factory();
-       transFactory.addServerDefinition(
-         AuthMethod.KERBEROS.getMechanismName(),
-         names[0], names[1],  // two parts of kerberos principal
-         saslProps,
-         new SaslRpcServer.SaslGssCallbackHandler());
-       transFactory.addServerDefinition(AuthMethod.DIGEST.getMechanismName(),
-          null, SaslRpcServer.SASL_DEFAULT_REALM,
-          saslProps, new SaslDigestCallbackHandler(secretManager));
-
-       return new TUGIAssumingTransportFactory(transFactory, realUgi);
-     }
-
-     /**
-      * Wrap a TProcessor in such a way that, before processing any RPC, it
-      * assumes the UserGroupInformation of the user authenticated by
-      * the SASL transport.
-      */
-     @Override
-     public TProcessor wrapProcessor(TProcessor processor) {
-       return new TUGIAssumingProcessor(processor, secretManager, true);
-     }
-
-     /**
-      * Wrap a TProcessor to capture the client information like connecting userid, ip etc
-      */
-     @Override
-     public TProcessor wrapNonAssumingProcessor(TProcessor processor) {
-      return new TUGIAssumingProcessor(processor, secretManager, false);
-     }
-
-    protected DelegationTokenStore getTokenStore(Configuration conf)
-        throws IOException {
-       String tokenStoreClassName = conf.get(DELEGATION_TOKEN_STORE_CLS, "");
-       if (StringUtils.isBlank(tokenStoreClassName)) {
-         return new MemoryTokenStore();
-       }
-       try {
-        Class<? extends DelegationTokenStore> storeClass = Class
-            .forName(tokenStoreClassName).asSubclass(
-                DelegationTokenStore.class);
-        return ReflectionUtils.newInstance(storeClass, conf);
-       } catch (ClassNotFoundException e) {
-        throw new IOException("Error initializing delegation token store: " + tokenStoreClassName,
-            e);
-       }
-     }
-
-     @Override
-     public void startDelegationTokenSecretManager(Configuration conf, Object hms)
-     throws IOException{
-       long secretKeyInterval =
-         conf.getLong(DELEGATION_KEY_UPDATE_INTERVAL_KEY,
-                        DELEGATION_KEY_UPDATE_INTERVAL_DEFAULT);
-       long tokenMaxLifetime =
-           conf.getLong(DELEGATION_TOKEN_MAX_LIFETIME_KEY,
-                        DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT);
-       long tokenRenewInterval =
-           conf.getLong(DELEGATION_TOKEN_RENEW_INTERVAL_KEY,
-                        DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT);
-
-       DelegationTokenStore dts = getTokenStore(conf);
-       dts.setStore(hms);
-       secretManager = new TokenStoreDelegationTokenSecretManager(secretKeyInterval,
-             tokenMaxLifetime,
-             tokenRenewInterval,
-             DELEGATION_TOKEN_GC_INTERVAL, dts);
-       secretManager.startThreads();
-     }
-
-     @Override
-     public String getDelegationToken(final String owner, final String renewer)
-     throws IOException, InterruptedException {
-       if (!authenticationMethod.get().equals(AuthenticationMethod.KERBEROS)) {
-         throw new AuthorizationException(
-         "Delegation Token can be issued only with kerberos authentication. " +
-         "Current AuthenticationMethod: " + authenticationMethod.get()
-             );
-       }
-       //if the user asking the token is same as the 'owner' then don't do
-       //any proxy authorization checks. For cases like oozie, where it gets
-       //a delegation token for another user, we need to make sure oozie is
-       //authorized to get a delegation token.
-       //Do all checks on short names
-       UserGroupInformation currUser = UserGroupInformation.getCurrentUser();
-       UserGroupInformation ownerUgi = UserGroupInformation.createRemoteUser(owner);
-       if (!ownerUgi.getShortUserName().equals(currUser.getShortUserName())) {
-         //in the case of proxy users, the getCurrentUser will return the
-         //real user (for e.g. oozie) due to the doAs that happened just before the
-         //server started executing the method getDelegationToken in the MetaStore
-         ownerUgi = UserGroupInformation.createProxyUser(owner,
-           UserGroupInformation.getCurrentUser());
-         InetAddress remoteAddr = getRemoteAddress();
-         ProxyUsers.authorize(ownerUgi,remoteAddr.getHostAddress(), null);
-       }
-       return ownerUgi.doAs(new PrivilegedExceptionAction<String>() {
-         public String run() throws IOException {
-           return secretManager.getDelegationToken(renewer);
-         }
-       });
-     }
-
-     @Override
-     public long renewDelegationToken(String tokenStrForm) throws IOException {
-       if (!authenticationMethod.get().equals(AuthenticationMethod.KERBEROS)) {
-         throw new AuthorizationException(
-         "Delegation Token can be issued only with kerberos authentication. " +
-         "Current AuthenticationMethod: " + authenticationMethod.get()
-             );
-       }
-       return secretManager.renewDelegationToken(tokenStrForm);
-     }
-
-     @Override
-     public void cancelDelegationToken(String tokenStrForm) throws IOException {
-       secretManager.cancelDelegationToken(tokenStrForm);
-     }
-
-     final static ThreadLocal<InetAddress> remoteAddress =
-       new ThreadLocal<InetAddress>() {
-       @Override
-       protected synchronized InetAddress initialValue() {
-         return null;
-       }
-     };
-
-     @Override
-     public InetAddress getRemoteAddress() {
-       return remoteAddress.get();
-     }
-
-     final static ThreadLocal<AuthenticationMethod> authenticationMethod =
-       new ThreadLocal<AuthenticationMethod>() {
-       @Override
-       protected synchronized AuthenticationMethod initialValue() {
-         return AuthenticationMethod.TOKEN;
-       }
-     };
-
-     private static ThreadLocal<String> remoteUser = new ThreadLocal<String> () {
-       @Override
-       protected synchronized String initialValue() {
-         return null;
-       }
-     };
-
-     @Override
-     public String getRemoteUser() {
-       return remoteUser.get();
-     }
-
-    /** CallbackHandler for SASL DIGEST-MD5 mechanism */
-    // This code is pretty much completely based on Hadoop's
-    // SaslRpcServer.SaslDigestCallbackHandler - the only reason we could not
-    // use that Hadoop class as-is was because it needs a Server.Connection object
-    // which is relevant in hadoop rpc but not here in the metastore - so the
-    // code below does not deal with the Connection Server.object.
-    static class SaslDigestCallbackHandler implements CallbackHandler {
-      private final DelegationTokenSecretManager secretManager;
-
-      public SaslDigestCallbackHandler(
-          DelegationTokenSecretManager secretManager) {
-        this.secretManager = secretManager;
-      }
-
-      private char[] getPassword(DelegationTokenIdentifier tokenid) throws InvalidToken {
-        return encodePassword(secretManager.retrievePassword(tokenid));
-      }
-
-      private char[] encodePassword(byte[] password) {
-        return new String(Base64.encodeBase64(password)).toCharArray();
-      }
-      /** {@inheritDoc} */
-      @Override
-      public void handle(Callback[] callbacks) throws InvalidToken,
-      UnsupportedCallbackException {
-        NameCallback nc = null;
-        PasswordCallback pc = null;
-        AuthorizeCallback ac = null;
-        for (Callback callback : callbacks) {
-          if (callback instanceof AuthorizeCallback) {
-            ac = (AuthorizeCallback) callback;
-          } else if (callback instanceof NameCallback) {
-            nc = (NameCallback) callback;
-          } else if (callback instanceof PasswordCallback) {
-            pc = (PasswordCallback) callback;
-          } else if (callback instanceof RealmCallback) {
-            continue; // realm is ignored
-          } else {
-            throw new UnsupportedCallbackException(callback,
-            "Unrecognized SASL DIGEST-MD5 Callback");
-          }
-        }
-        if (pc != null) {
-          DelegationTokenIdentifier tokenIdentifier = SaslRpcServer.
-          getIdentifier(nc.getDefaultName(), secretManager);
-          char[] password = getPassword(tokenIdentifier);
-
-          if (LOG.isDebugEnabled()) {
-            LOG.debug("SASL server DIGEST-MD5 callback: setting password "
-                + "for client: " + tokenIdentifier.getUser());
-          }
-          pc.setPassword(password);
-        }
-        if (ac != null) {
-          String authid = ac.getAuthenticationID();
-          String authzid = ac.getAuthorizationID();
-          if (authid.equals(authzid)) {
-            ac.setAuthorized(true);
-          } else {
-            ac.setAuthorized(false);
-          }
-          if (ac.isAuthorized()) {
-            if (LOG.isDebugEnabled()) {
-              String username =
-                SaslRpcServer.getIdentifier(authzid, secretManager).getUser().getUserName();
-              LOG.debug("SASL server DIGEST-MD5 callback: setting "
-                  + "canonicalized client ID: " + username);
-            }
-            ac.setAuthorizedID(authzid);
-          }
-        }
-      }
-     }
-
-     /**
-      * Processor that pulls the SaslServer object out of the transport, and
-      * assumes the remote user's UGI before calling through to the original
-      * processor.
-      *
-      * This is used on the server side to set the UGI for each specific call.
-      */
-     protected class TUGIAssumingProcessor implements TProcessor {
-       final TProcessor wrapped;
-       DelegationTokenSecretManager secretManager;
-       boolean useProxy;
-       TUGIAssumingProcessor(TProcessor wrapped, DelegationTokenSecretManager secretManager,
-           boolean useProxy) {
-         this.wrapped = wrapped;
-         this.secretManager = secretManager;
-         this.useProxy = useProxy;
-       }
-
-       public boolean process(final TProtocol inProt, final TProtocol outProt) throws TException {
-         TTransport trans = inProt.getTransport();
-         if (!(trans instanceof TSaslServerTransport)) {
-           throw new TException("Unexpected non-SASL transport " + trans.getClass());
-         }
-         TSaslServerTransport saslTrans = (TSaslServerTransport)trans;
-         SaslServer saslServer = saslTrans.getSaslServer();
-         String authId = saslServer.getAuthorizationID();
-         authenticationMethod.set(AuthenticationMethod.KERBEROS);
-         LOG.debug("AUTH ID ======>" + authId);
-         String endUser = authId;
-
-         if(saslServer.getMechanismName().equals("DIGEST-MD5")) {
-           try {
-             TokenIdentifier tokenId = SaslRpcServer.getIdentifier(authId,
-                 secretManager);
-             endUser = tokenId.getUser().getUserName();
-             authenticationMethod.set(AuthenticationMethod.TOKEN);
-           } catch (InvalidToken e) {
-             throw new TException(e.getMessage());
-           }
-         }
-         Socket socket = ((TSocket)(saslTrans.getUnderlyingTransport())).getSocket();
-         remoteAddress.set(socket.getInetAddress());
-         UserGroupInformation clientUgi = null;
-         try {
-           if (useProxy) {
-             clientUgi = UserGroupInformation.createProxyUser(
-               endUser, UserGroupInformation.getLoginUser());
-             remoteUser.set(clientUgi.getShortUserName());
-             return clientUgi.doAs(new PrivilegedExceptionAction<Boolean>() {
-                 public Boolean run() {
-                   try {
-                     return wrapped.process(inProt, outProt);
-                   } catch (TException te) {
-                     throw new RuntimeException(te);
-                   }
-                 }
-               });
-           } else {
-             // check for kerberos v5
-             if (saslServer.getMechanismName().equals("GSSAPI")) {
-               String shortName = ShimLoader.getHadoopShims().getKerberosShortName(endUser);
-               remoteUser.set(shortName);
-             } else {
-               remoteUser.set(endUser);
-             }
-             return wrapped.process(inProt, outProt);
-           }
-         } catch (RuntimeException rte) {
-           if (rte.getCause() instanceof TException) {
-             throw (TException)rte.getCause();
-           }
-           throw rte;
-         } catch (InterruptedException ie) {
-           throw new RuntimeException(ie); // unexpected!
-         } catch (IOException ioe) {
-           throw new RuntimeException(ioe); // unexpected!
-         }
-         finally {
-           if (clientUgi != null) {
-            try { FileSystem.closeAllForUGI(clientUgi); }
-              catch(IOException exception) {
-                LOG.error("Could not clean up file-system handles for UGI: " + clientUgi, exception);
-              }
-          }
-         }
-       }
-     }
-
-    /**
-      * A TransportFactory that wraps another one, but assumes a specified UGI
-      * before calling through.
-      *
-      * This is used on the server side to assume the server's Principal when accepting
-      * clients.
-      */
-     static class TUGIAssumingTransportFactory extends TTransportFactory {
-       private final UserGroupInformation ugi;
-       private final TTransportFactory wrapped;
-
-       public TUGIAssumingTransportFactory(TTransportFactory wrapped, UserGroupInformation ugi) {
-         assert wrapped != null;
-         assert ugi != null;
-
-         this.wrapped = wrapped;
-         this.ugi = ugi;
-       }
-
-       @Override
-       public TTransport getTransport(final TTransport trans) {
-         return ugi.doAs(new PrivilegedAction<TTransport>() {
-           public TTransport run() {
-             return wrapped.getTransport(trans);
-           }
-         });
-       }
-     }
-   }
- }
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/MemoryTokenStore.java b/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/MemoryTokenStore.java
deleted file mode 100644
index 9908aa4..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/MemoryTokenStore.java
+++ /dev/null
@@ -1,115 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
-
-/**
- * Default in-memory token store implementation.
- */
-public class MemoryTokenStore implements DelegationTokenStore {
-
-  private final Map<Integer, String> masterKeys
-      = new ConcurrentHashMap<Integer, String>();
-
-  private final ConcurrentHashMap<DelegationTokenIdentifier, DelegationTokenInformation> tokens
-      = new ConcurrentHashMap<DelegationTokenIdentifier, DelegationTokenInformation>();
-
-  private final AtomicInteger masterKeySeq = new AtomicInteger();
-  private Configuration conf;
-
-  @Override
-  public void setConf(Configuration conf) {
-    this.conf = conf;
-  }
-
-  @Override
-  public Configuration getConf() {
-    return this.conf;
-  }
-
-  @Override
-  public int addMasterKey(String s) {
-    int keySeq = masterKeySeq.getAndIncrement();
-    masterKeys.put(keySeq, s);
-    return keySeq;
-  }
-
-  @Override
-  public void updateMasterKey(int keySeq, String s) {
-    masterKeys.put(keySeq, s);
-  }
-
-  @Override
-  public boolean removeMasterKey(int keySeq) {
-    return masterKeys.remove(keySeq) != null;
-  }
-
-  @Override
-  public String[] getMasterKeys() {
-    return masterKeys.values().toArray(new String[0]);
-  }
-
-  @Override
-  public boolean addToken(DelegationTokenIdentifier tokenIdentifier,
-    DelegationTokenInformation token) {
-    DelegationTokenInformation tokenInfo = tokens.putIfAbsent(tokenIdentifier, token);
-    return (tokenInfo == null);
-  }
-
-  @Override
-  public boolean removeToken(DelegationTokenIdentifier tokenIdentifier) {
-    DelegationTokenInformation tokenInfo = tokens.remove(tokenIdentifier);
-    return tokenInfo != null;
-  }
-
-  @Override
-  public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {
-    return tokens.get(tokenIdentifier);
-  }
-
-  @Override
-  public List<DelegationTokenIdentifier> getAllDelegationTokenIdentifiers() {
-    List<DelegationTokenIdentifier> result = new ArrayList<DelegationTokenIdentifier>(
-        tokens.size());
-    for (DelegationTokenIdentifier id : tokens.keySet()) {
-        result.add(id);
-    }
-    return result;
-  }
-
-  @Override
-  public void close() throws IOException {
-    //no-op
-  }
-
-  @Override
-  public void setStore(Object hmsHandler) throws TokenStoreException {
-    // no-op
-  }
-
-}
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java b/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java
deleted file mode 100644
index 4ccf895..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java
+++ /dev/null
@@ -1,334 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.lang.reflect.Method;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.commons.codec.binary.Base64;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.security.token.Token;
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;
-import org.apache.hadoop.security.token.delegation.DelegationKey;
-import org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport;
-import org.apache.hadoop.util.Daemon;
-import org.apache.hadoop.util.StringUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Extension of {@link DelegationTokenSecretManager} to support alternative to default in-memory
- * token management for fail-over and clustering through plug-able token store (ZooKeeper etc.).
- * Delegation tokens will be retrieved from the store on-demand and (unlike base class behavior) not
- * cached in memory. This avoids complexities related to token expiration. The security token is
- * needed only at the time the transport is opened (as opposed to per interface operation). The
- * assumption therefore is low cost of interprocess token retrieval (for random read efficient store
- * such as ZooKeeper) compared to overhead of synchronizing per-process in-memory token caches.
- * The wrapper incorporates the token store abstraction within the limitations of current
- * Hive/Hadoop dependency (.20S) with minimum code duplication.
- * Eventually this should be supported by Hadoop security directly.
- */
-public class TokenStoreDelegationTokenSecretManager extends DelegationTokenSecretManager {
-
-  private static final Logger LOGGER =
-      LoggerFactory.getLogger(TokenStoreDelegationTokenSecretManager.class.getName());
-
-  final private long keyUpdateInterval;
-  final private long tokenRemoverScanInterval;
-  private Thread tokenRemoverThread;
-
-  final private DelegationTokenStore tokenStore;
-
-  public TokenStoreDelegationTokenSecretManager(long delegationKeyUpdateInterval,
-      long delegationTokenMaxLifetime, long delegationTokenRenewInterval,
-      long delegationTokenRemoverScanInterval,
-      DelegationTokenStore sharedStore) {
-    super(delegationKeyUpdateInterval, delegationTokenMaxLifetime, delegationTokenRenewInterval,
-        delegationTokenRemoverScanInterval);
-    this.keyUpdateInterval = delegationKeyUpdateInterval;
-    this.tokenRemoverScanInterval = delegationTokenRemoverScanInterval;
-
-    this.tokenStore = sharedStore;
-  }
-
-  protected DelegationTokenIdentifier getTokenIdentifier(Token<DelegationTokenIdentifier> token)
-      throws IOException {
-    // turn bytes back into identifier for cache lookup
-    ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());
-    DataInputStream in = new DataInputStream(buf);
-    DelegationTokenIdentifier id = createIdentifier();
-    id.readFields(in);
-    return id;
-  }
-
-  protected Map<Integer, DelegationKey> reloadKeys() {
-    // read keys from token store
-    String[] allKeys = tokenStore.getMasterKeys();
-    Map<Integer, DelegationKey> keys
-        = new HashMap<Integer, DelegationKey>(allKeys.length);
-    for (String keyStr : allKeys) {
-      DelegationKey key = new DelegationKey();
-      try {
-        decodeWritable(key, keyStr);
-        keys.put(key.getKeyId(), key);
-      } catch (IOException ex) {
-        LOGGER.error("Failed to load master key.", ex);
-      }
-    }
-    synchronized (this) {
-        super.allKeys.clear();
-        super.allKeys.putAll(keys);
-    }
-    return keys;
-  }
-
-  @Override
-  public byte[] retrievePassword(DelegationTokenIdentifier identifier) throws InvalidToken {
-      DelegationTokenInformation info = this.tokenStore.getToken(identifier);
-      if (info == null) {
-          throw new InvalidToken("token expired or does not exist: " + identifier);
-      }
-      // must reuse super as info.getPassword is not accessible
-      synchronized (this) {
-        try {
-          super.currentTokens.put(identifier, info);
-          return super.retrievePassword(identifier);
-        } finally {
-          super.currentTokens.remove(identifier);
-        }
-      }
-  }
-
-  @Override
-  public DelegationTokenIdentifier cancelToken(Token<DelegationTokenIdentifier> token,
-      String canceller) throws IOException {
-    DelegationTokenIdentifier id = getTokenIdentifier(token);
-    LOGGER.info("Token cancelation requested for identifier: "+id);
-    this.tokenStore.removeToken(id);
-    return id;
-  }
-
-  /**
-   * Create the password and add it to shared store.
-   */
-  @Override
-  protected byte[] createPassword(DelegationTokenIdentifier id) {
-    byte[] password;
-    DelegationTokenInformation info;
-    synchronized (this) {
-      password = super.createPassword(id);
-      // add new token to shared store
-      // need to persist expiration along with password
-      info = super.currentTokens.remove(id);
-      if (info == null) {
-        throw new IllegalStateException("Failed to retrieve token after creation");
-      }
-    }
-    this.tokenStore.addToken(id, info);
-    return password;
-  }
-
-  @Override
-  public long renewToken(Token<DelegationTokenIdentifier> token,
-      String renewer) throws InvalidToken, IOException {
-    // since renewal is KERBEROS authenticated token may not be cached
-    final DelegationTokenIdentifier id = getTokenIdentifier(token);
-    DelegationTokenInformation tokenInfo = this.tokenStore.getToken(id);
-    if (tokenInfo == null) {
-        throw new InvalidToken("token does not exist: " + id); // no token found
-    }
-    // ensure associated master key is available
-    if (!super.allKeys.containsKey(id.getMasterKeyId())) {
-      LOGGER.info("Unknown master key (id={}), (re)loading keys from token store.",
-        id.getMasterKeyId());
-      reloadKeys();
-    }
-    // reuse super renewal logic
-    synchronized (this) {
-      super.currentTokens.put(id,  tokenInfo);
-      try {
-        return super.renewToken(token, renewer);
-      } finally {
-        super.currentTokens.remove(id);
-      }
-    }
-  }
-
-  public static String encodeWritable(Writable key) throws IOException {
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    DataOutputStream dos = new DataOutputStream(bos);
-    key.write(dos);
-    dos.flush();
-    return Base64.encodeBase64URLSafeString(bos.toByteArray());
-  }
-
-  public static void decodeWritable(Writable w, String idStr) throws IOException {
-    DataInputStream in = new DataInputStream(new ByteArrayInputStream(Base64.decodeBase64(idStr)));
-    w.readFields(in);
-  }
-
-  /**
-   * Synchronize master key updates / sequence generation for multiple nodes.
-   * NOTE: {@Link AbstractDelegationTokenSecretManager} keeps currentKey private, so we need
-   * to utilize this "hook" to manipulate the key through the object reference.
-   * This .20S workaround should cease to exist when Hadoop supports token store.
-   */
-  @Override
-  protected void logUpdateMasterKey(DelegationKey key) throws IOException {
-    int keySeq = this.tokenStore.addMasterKey(encodeWritable(key));
-    // update key with assigned identifier
-    DelegationKey keyWithSeq = new DelegationKey(keySeq, key.getExpiryDate(), key.getKey());
-    String keyStr = encodeWritable(keyWithSeq);
-    this.tokenStore.updateMasterKey(keySeq, keyStr);
-    decodeWritable(key, keyStr);
-    LOGGER.info("New master key with key id={}", key.getKeyId());
-    super.logUpdateMasterKey(key);
-  }
-
-  @Override
-  public synchronized void startThreads() throws IOException {
-    try {
-      // updateCurrentKey needs to be called to initialize the master key
-      // (there should be a null check added in the future in rollMasterKey)
-      // updateCurrentKey();
-      Method m = AbstractDelegationTokenSecretManager.class.getDeclaredMethod("updateCurrentKey");
-      m.setAccessible(true);
-      m.invoke(this);
-    } catch (Exception e) {
-      throw new IOException("Failed to initialize master key", e);
-    }
-    running = true;
-    tokenRemoverThread = new Daemon(new ExpiredTokenRemover());
-    tokenRemoverThread.start();
-  }
-
-  @Override
-  public synchronized void stopThreads() {
-    if (LOGGER.isDebugEnabled()) {
-      LOGGER.debug("Stopping expired delegation token remover thread");
-    }
-    running = false;
-    if (tokenRemoverThread != null) {
-      tokenRemoverThread.interrupt();
-    }
-  }
-
-  /**
-   * Remove expired tokens. Replaces logic in {@link AbstractDelegationTokenSecretManager}
-   * that cannot be reused due to private method access. Logic here can more efficiently
-   * deal with external token store by only loading into memory the minimum data needed.
-   */
-  protected void removeExpiredTokens() {
-    long now = System.currentTimeMillis();
-    Iterator<DelegationTokenIdentifier> i = tokenStore.getAllDelegationTokenIdentifiers()
-        .iterator();
-    while (i.hasNext()) {
-      DelegationTokenIdentifier id = i.next();
-      if (now > id.getMaxDate()) {
-        this.tokenStore.removeToken(id); // no need to look at token info
-      } else {
-        // get token info to check renew date
-        DelegationTokenInformation tokenInfo = tokenStore.getToken(id);
-        if (tokenInfo != null) {
-          if (now > tokenInfo.getRenewDate()) {
-            this.tokenStore.removeToken(id);
-          }
-        }
-      }
-    }
-  }
-
-  /**
-   * Extension of rollMasterKey to remove expired keys from store.
-   * @throws IOException
-   */
-  protected void rollMasterKeyExt() throws IOException {
-    Map<Integer, DelegationKey> keys = reloadKeys();
-    int currentKeyId = super.currentId;
-    HiveDelegationTokenSupport.rollMasterKey(TokenStoreDelegationTokenSecretManager.this);
-    List<DelegationKey> keysAfterRoll = Arrays.asList(getAllKeys());
-    for (DelegationKey key : keysAfterRoll) {
-        keys.remove(key.getKeyId());
-        if (key.getKeyId() == currentKeyId) {
-          tokenStore.updateMasterKey(currentKeyId, encodeWritable(key));
-        }
-    }
-    for (DelegationKey expiredKey : keys.values()) {
-      LOGGER.info("Removing expired key id={}", expiredKey.getKeyId());
-      tokenStore.removeMasterKey(expiredKey.getKeyId());
-    }
-  }
-
-
-  /**
-   * Cloned from {@link AbstractDelegationTokenSecretManager} to deal with private access
-   * restriction (there would not be an need to clone the remove thread if the remove logic was
-   * protected/extensible).
-   */
-  protected class ExpiredTokenRemover extends Thread {
-    private long lastMasterKeyUpdate;
-    private long lastTokenCacheCleanup;
-
-    @Override
-    public void run() {
-      LOGGER.info("Starting expired delegation token remover thread, "
-          + "tokenRemoverScanInterval=" + tokenRemoverScanInterval
-          / (60 * 1000) + " min(s)");
-      try {
-        while (running) {
-          long now = System.currentTimeMillis();
-          if (lastMasterKeyUpdate + keyUpdateInterval < now) {
-            try {
-              rollMasterKeyExt();
-              lastMasterKeyUpdate = now;
-            } catch (IOException e) {
-              LOGGER.error("Master key updating failed. "
-                  + StringUtils.stringifyException(e));
-            }
-          }
-          if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {
-            removeExpiredTokens();
-            lastTokenCacheCleanup = now;
-          }
-          try {
-            Thread.sleep(5000); // 5 seconds
-          } catch (InterruptedException ie) {
-            LOGGER
-            .error("InterruptedExcpetion recieved for ExpiredTokenRemover thread "
-                + ie);
-          }
-        }
-      } catch (Throwable t) {
-        LOGGER.error("ExpiredTokenRemover thread received unexpected exception. "
-            + t, t);
-        Runtime.getRuntime().exit(-1);
-      }
-    }
-  }
-
-}
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java b/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java
deleted file mode 100644
index 8683496..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java
+++ /dev/null
@@ -1,468 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.commons.lang.StringUtils;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
-import org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport;
-import org.apache.zookeeper.CreateMode;
-import org.apache.zookeeper.KeeperException;
-import org.apache.zookeeper.WatchedEvent;
-import org.apache.zookeeper.Watcher;
-import org.apache.zookeeper.ZooDefs;
-import org.apache.zookeeper.ZooDefs.Ids;
-import org.apache.zookeeper.ZooKeeper;
-import org.apache.zookeeper.ZooKeeper.States;
-import org.apache.zookeeper.data.ACL;
-import org.apache.zookeeper.data.Id;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * ZooKeeper token store implementation.
- */
-public class ZooKeeperTokenStore implements DelegationTokenStore {
-
-  private static final Logger LOGGER =
-      LoggerFactory.getLogger(ZooKeeperTokenStore.class.getName());
-
-  protected static final String ZK_SEQ_FORMAT = "%010d";
-  private static final String NODE_KEYS = "/keys";
-  private static final String NODE_TOKENS = "/tokens";
-
-  private String rootNode = "";
-  private volatile ZooKeeper zkSession;
-  private String zkConnectString;
-  private final int zkSessionTimeout = 3000;
-  private long connectTimeoutMillis = -1;
-  private List<ACL> newNodeAcl = Ids.OPEN_ACL_UNSAFE;
-
-  private class ZooKeeperWatcher implements Watcher {
-    public void process(org.apache.zookeeper.WatchedEvent event) {
-      LOGGER.info(event.toString());
-      if (event.getState() == Watcher.Event.KeeperState.Expired) {
-        LOGGER.warn("ZooKeeper session expired, discarding connection");
-        try {
-          zkSession.close();
-        } catch (Throwable e) {
-          LOGGER.warn("Failed to close connection on expired session", e);
-        }
-      }
-    }
-
-  }
-
-  /**
-   * Default constructor for dynamic instantiation w/ Configurable
-   * (ReflectionUtils does not support Configuration constructor injection).
-   */
-  protected ZooKeeperTokenStore() {
-  }
-
-  public ZooKeeperTokenStore(String hostPort) {
-    this.zkConnectString = hostPort;
-    init();
-  }
-
-  private ZooKeeper getSession() {
-    if (zkSession == null || zkSession.getState() == States.CLOSED) {
-        synchronized (this) {
-          if (zkSession == null || zkSession.getState() == States.CLOSED) {
-            try {
-              zkSession = createConnectedClient(this.zkConnectString, this.zkSessionTimeout,
-                this.connectTimeoutMillis, new ZooKeeperWatcher());
-            } catch (IOException ex) {
-              throw new TokenStoreException("Token store error.", ex);
-            }
-          }
-        }
-    }
-    return zkSession;
-  }
-
-  /**
-   * Create a ZooKeeper session that is in connected state.
-   *
-   * @param connectString ZooKeeper connect String
-   * @param sessionTimeout ZooKeeper session timeout
-   * @param connectTimeout milliseconds to wait for connection, 0 or negative value means no wait
-   * @param watchers
-   * @return
-   * @throws InterruptedException
-   * @throws IOException
-   */
-  public static ZooKeeper createConnectedClient(String connectString,
-      int sessionTimeout, long connectTimeout, final Watcher... watchers)
-      throws IOException {
-    final CountDownLatch connected = new CountDownLatch(1);
-    Watcher connectWatcher = new Watcher() {
-      @Override
-      public void process(WatchedEvent event) {
-        switch (event.getState()) {
-        case SyncConnected:
-          connected.countDown();
-          break;
-        }
-        for (Watcher w : watchers) {
-          w.process(event);
-        }
-      }
-    };
-    ZooKeeper zk = new ZooKeeper(connectString, sessionTimeout, connectWatcher);
-    if (connectTimeout > 0) {
-      try {
-        if (!connected.await(connectTimeout, TimeUnit.MILLISECONDS)) {
-          zk.close();
-          throw new IOException("Timeout waiting for connection after "
-              + connectTimeout + "ms");
-        }
-      } catch (InterruptedException e) {
-        throw new IOException("Error waiting for connection.", e);
-      }
-    }
-    return zk;
-  }
-
-  /**
-   * Create a path if it does not already exist ("mkdir -p")
-   * @param zk ZooKeeper session
-   * @param path string with '/' separator
-   * @param acl list of ACL entries
-   * @return
-   * @throws KeeperException
-   * @throws InterruptedException
-   */
-  public static String ensurePath(ZooKeeper zk, String path, List<ACL> acl) throws KeeperException,
-      InterruptedException {
-    String[] pathComps = StringUtils.splitByWholeSeparator(path, "/");
-    String currentPath = "";
-    for (String pathComp : pathComps) {
-      currentPath += "/" + pathComp;
-      try {
-        String node = zk.create(currentPath, new byte[0], acl,
-            CreateMode.PERSISTENT);
-        LOGGER.info("Created path: " + node);
-      } catch (KeeperException.NodeExistsException e) {
-      }
-    }
-    return currentPath;
-  }
-
-  /**
-   * Parse ACL permission string, from ZooKeeperMain private method
-   * @param permString
-   * @return
-   */
-  public static int getPermFromString(String permString) {
-      int perm = 0;
-      for (int i = 0; i < permString.length(); i++) {
-          switch (permString.charAt(i)) {
-          case 'r':
-              perm |= ZooDefs.Perms.READ;
-              break;
-          case 'w':
-              perm |= ZooDefs.Perms.WRITE;
-              break;
-          case 'c':
-              perm |= ZooDefs.Perms.CREATE;
-              break;
-          case 'd':
-              perm |= ZooDefs.Perms.DELETE;
-              break;
-          case 'a':
-              perm |= ZooDefs.Perms.ADMIN;
-              break;
-          default:
-              LOGGER.error("Unknown perm type: " + permString.charAt(i));
-          }
-      }
-      return perm;
-  }
-
-  /**
-   * Parse comma separated list of ACL entries to secure generated nodes, e.g.
-   * <code>sasl:hive/host1@MY.DOMAIN:cdrwa,sasl:hive/host2@MY.DOMAIN:cdrwa</code>
-   * @param aclString
-   * @return ACL list
-   */
-  public static List<ACL> parseACLs(String aclString) {
-    String[] aclComps = StringUtils.splitByWholeSeparator(aclString, ",");
-    List<ACL> acl = new ArrayList<ACL>(aclComps.length);
-    for (String a : aclComps) {
-      if (StringUtils.isBlank(a)) {
-         continue;
-      }
-      a = a.trim();
-      // from ZooKeeperMain private method
-      int firstColon = a.indexOf(':');
-      int lastColon = a.lastIndexOf(':');
-      if (firstColon == -1 || lastColon == -1 || firstColon == lastColon) {
-         LOGGER.error(a + " does not have the form scheme:id:perm");
-         continue;
-      }
-      ACL newAcl = new ACL();
-      newAcl.setId(new Id(a.substring(0, firstColon), a.substring(
-          firstColon + 1, lastColon)));
-      newAcl.setPerms(getPermFromString(a.substring(lastColon + 1)));
-      acl.add(newAcl);
-    }
-    return acl;
-  }
-
-  private void init() {
-    if (this.zkConnectString == null) {
-      throw new IllegalStateException("Not initialized");
-    }
-
-    if (this.zkSession != null) {
-      try {
-        this.zkSession.close();
-      } catch (InterruptedException ex) {
-        LOGGER.warn("Failed to close existing session.", ex);
-      }
-    }
-    ZooKeeper zk = getSession();
-
-    try {
-        ensurePath(zk, rootNode + NODE_KEYS, newNodeAcl);
-        ensurePath(zk, rootNode + NODE_TOKENS, newNodeAcl);
-      } catch (Exception e) {
-        throw new TokenStoreException("Failed to validate token path.", e);
-      }
-  }
-
-  @Override
-  public void setConf(Configuration conf) {
-    if (conf == null) {
-       throw new IllegalArgumentException("conf is null");
-    }
-    this.zkConnectString = conf.get(
-      HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_CONNECT_STR, null);
-    this.connectTimeoutMillis = conf.getLong(
-      HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_CONNECT_TIMEOUTMILLIS, -1);
-    this.rootNode = conf.get(
-      HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ZNODE,
-      HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ZNODE_DEFAULT);
-    String csv = conf.get(HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ACL, null);
-    if (StringUtils.isNotBlank(csv)) {
-       this.newNodeAcl = parseACLs(csv);
-    }
-    init();
-  }
-
-  @Override
-  public Configuration getConf() {
-    return null; // not required
-  }
-
-  private Map<Integer, byte[]> getAllKeys() throws KeeperException,
-      InterruptedException {
-
-    String masterKeyNode = rootNode + NODE_KEYS;
-    ZooKeeper zk = getSession();
-    List<String> nodes = zk.getChildren(masterKeyNode, false);
-    Map<Integer, byte[]> result = new HashMap<Integer, byte[]>();
-    for (String node : nodes) {
-      byte[] data = zk.getData(masterKeyNode + "/" + node, false, null);
-      if (data != null) {
-        result.put(getSeq(node), data);
-      }
-    }
-    return result;
-  }
-
-  private int getSeq(String path) {
-    String[] pathComps = path.split("/");
-    return Integer.parseInt(pathComps[pathComps.length-1]);
-  }
-
-  @Override
-  public int addMasterKey(String s) {
-    try {
-      ZooKeeper zk = getSession();
-      String newNode = zk.create(rootNode + NODE_KEYS + "/", s.getBytes(), newNodeAcl,
-          CreateMode.PERSISTENT_SEQUENTIAL);
-      LOGGER.info("Added key {}", newNode);
-      return getSeq(newNode);
-    } catch (KeeperException ex) {
-      throw new TokenStoreException(ex);
-    } catch (InterruptedException ex) {
-      throw new TokenStoreException(ex);
-    }
-  }
-
-  @Override
-  public void updateMasterKey(int keySeq, String s) {
-    try {
-      ZooKeeper zk = getSession();
-      zk.setData(rootNode + NODE_KEYS + "/" + String.format(ZK_SEQ_FORMAT, keySeq), s.getBytes(),
-          -1);
-    } catch (KeeperException ex) {
-      throw new TokenStoreException(ex);
-    } catch (InterruptedException ex) {
-      throw new TokenStoreException(ex);
-    }
-  }
-
-  @Override
-  public boolean removeMasterKey(int keySeq) {
-    try {
-      ZooKeeper zk = getSession();
-      zk.delete(rootNode + NODE_KEYS + "/" + String.format(ZK_SEQ_FORMAT, keySeq), -1);
-      return true;
-    } catch (KeeperException.NoNodeException ex) {
-      return false;
-    } catch (KeeperException ex) {
-      throw new TokenStoreException(ex);
-    } catch (InterruptedException ex) {
-      throw new TokenStoreException(ex);
-    }
-  }
-
-  @Override
-  public String[] getMasterKeys() {
-    try {
-      Map<Integer, byte[]> allKeys = getAllKeys();
-      String[] result = new String[allKeys.size()];
-      int resultIdx = 0;
-      for (byte[] keyBytes : allKeys.values()) {
-          result[resultIdx++] = new String(keyBytes);
-      }
-      return result;
-    } catch (KeeperException ex) {
-      throw new TokenStoreException(ex);
-    } catch (InterruptedException ex) {
-      throw new TokenStoreException(ex);
-    }
-  }
-
-
-  private String getTokenPath(DelegationTokenIdentifier tokenIdentifier) {
-    try {
-      return rootNode + NODE_TOKENS + "/"
-          + TokenStoreDelegationTokenSecretManager.encodeWritable(tokenIdentifier);
-    } catch (IOException ex) {
-      throw new TokenStoreException("Failed to encode token identifier", ex);
-    }
-  }
-
-  @Override
-  public boolean addToken(DelegationTokenIdentifier tokenIdentifier,
-      DelegationTokenInformation token) {
-    try {
-      ZooKeeper zk = getSession();
-      byte[] tokenBytes = HiveDelegationTokenSupport.encodeDelegationTokenInformation(token);
-      String newNode = zk.create(getTokenPath(tokenIdentifier),
-          tokenBytes, newNodeAcl, CreateMode.PERSISTENT);
-      LOGGER.info("Added token: {}", newNode);
-      return true;
-    } catch (KeeperException.NodeExistsException ex) {
-      return false;
-    } catch (KeeperException ex) {
-      throw new TokenStoreException(ex);
-    } catch (InterruptedException ex) {
-      throw new TokenStoreException(ex);
-    }
-  }
-
-  @Override
-  public boolean removeToken(DelegationTokenIdentifier tokenIdentifier) {
-    try {
-      ZooKeeper zk = getSession();
-      zk.delete(getTokenPath(tokenIdentifier), -1);
-      return true;
-    } catch (KeeperException.NoNodeException ex) {
-      return false;
-    } catch (KeeperException ex) {
-      throw new TokenStoreException(ex);
-    } catch (InterruptedException ex) {
-      throw new TokenStoreException(ex);
-    }
-  }
-
-  @Override
-  public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {
-    try {
-      ZooKeeper zk = getSession();
-      byte[] tokenBytes = zk.getData(getTokenPath(tokenIdentifier), false, null);
-      try {
-        return HiveDelegationTokenSupport.decodeDelegationTokenInformation(tokenBytes);
-      } catch (Exception ex) {
-        throw new TokenStoreException("Failed to decode token", ex);
-      }
-    } catch (KeeperException.NoNodeException ex) {
-      return null;
-    } catch (KeeperException ex) {
-      throw new TokenStoreException(ex);
-    } catch (InterruptedException ex) {
-      throw new TokenStoreException(ex);
-    }
-  }
-
-  @Override
-  public List<DelegationTokenIdentifier> getAllDelegationTokenIdentifiers() {
-    String containerNode = rootNode + NODE_TOKENS;
-    final List<String> nodes;
-    try  {
-      nodes = getSession().getChildren(containerNode, false);
-    } catch (KeeperException ex) {
-      throw new TokenStoreException(ex);
-    } catch (InterruptedException ex) {
-      throw new TokenStoreException(ex);
-    }
-    List<DelegationTokenIdentifier> result = new java.util.ArrayList<DelegationTokenIdentifier>(
-        nodes.size());
-    for (String node : nodes) {
-      DelegationTokenIdentifier id = new DelegationTokenIdentifier();
-      try {
-        TokenStoreDelegationTokenSecretManager.decodeWritable(id, node);
-        result.add(id);
-      } catch (Exception e) {
-        LOGGER.warn("Failed to decode token '{}'", node);
-      }
-    }
-    return result;
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (this.zkSession != null) {
-      try {
-        this.zkSession.close();
-      } catch (InterruptedException ex) {
-        LOGGER.warn("Failed to close existing session.", ex);
-      }
-    }
-  }
-
-  @Override
-  public void setStore(Object hmsHandler) throws TokenStoreException {
-    // no-op.
-  }
-
-}
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/client/TUGIAssumingTransport.java b/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/client/TUGIAssumingTransport.java
deleted file mode 100644
index fe18706..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/client/TUGIAssumingTransport.java
+++ /dev/null
@@ -1,74 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift.client;
-
-import java.io.IOException;
-import java.security.PrivilegedExceptionAction;
-
-import org.apache.hadoop.hive.thrift.TFilterTransport;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.thrift.transport.TTransport;
-import org.apache.thrift.transport.TTransportException;
-
-/**
-  * The Thrift SASL transports call Sasl.createSaslServer and Sasl.createSaslClient
-  * inside open(). So, we need to assume the correct UGI when the transport is opened
-  * so that the SASL mechanisms have access to the right principal. This transport
-  * wraps the Sasl transports to set up the right UGI context for open().
-  *
-  * This is used on the client side, where the API explicitly opens a transport to
-  * the server.
-  */
- public class TUGIAssumingTransport extends TFilterTransport {
-   protected UserGroupInformation ugi;
-
-   public TUGIAssumingTransport(TTransport wrapped, UserGroupInformation ugi) {
-     super(wrapped);
-     this.ugi = ugi;
-   }
-
-   @Override
-   public void open() throws TTransportException {
-     try {
-       ugi.doAs(new PrivilegedExceptionAction<Void>() {
-         public Void run() {
-           try {
-             wrapped.open();
-           } catch (TTransportException tte) {
-             // Wrap the transport exception in an RTE, since UGI.doAs() then goes
-             // and unwraps this for us out of the doAs block. We then unwrap one
-             // more time in our catch clause to get back the TTE. (ugh)
-             throw new RuntimeException(tte);
-           }
-           return null;
-         }
-       });
-     } catch (IOException ioe) {
-       throw new RuntimeException("Received an ioe we never threw!", ioe);
-     } catch (InterruptedException ie) {
-       throw new RuntimeException("Received an ie we never threw!", ie);
-     } catch (RuntimeException rte) {
-       if (rte.getCause() instanceof TTransportException) {
-         throw (TTransportException)rte.getCause();
-       } else {
-         throw rte;
-       }
-     }
-   }
- }
diff --git a/src/shims/src/common-secure/java/org/apache/hadoop/security/token/delegation/HiveDelegationTokenSupport.java b/src/shims/src/common-secure/java/org/apache/hadoop/security/token/delegation/HiveDelegationTokenSupport.java
deleted file mode 100644
index 6b39a14..0000000
--- a/src/shims/src/common-secure/java/org/apache/hadoop/security/token/delegation/HiveDelegationTokenSupport.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.security.token.delegation;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
-
-/**
- * Workaround for serialization of {@link DelegationTokenInformation} through package access.
- * Future version of Hadoop should add this to DelegationTokenInformation itself.
- */
-public final class HiveDelegationTokenSupport {
-
-  private HiveDelegationTokenSupport() {}
-
-  public static byte[] encodeDelegationTokenInformation(DelegationTokenInformation token) {
-    try {
-      ByteArrayOutputStream bos = new ByteArrayOutputStream();
-      DataOutputStream out = new DataOutputStream(bos);
-      WritableUtils.writeVInt(out, token.password.length);
-      out.write(token.password);
-      out.writeLong(token.renewDate);
-      out.flush();
-      return bos.toByteArray();
-    } catch (IOException ex) {
-      throw new RuntimeException("Failed to encode token.", ex);
-    }
-  }
-
-  public static DelegationTokenInformation decodeDelegationTokenInformation(byte[] tokenBytes)
-      throws IOException {
-    DataInputStream in = new DataInputStream(new ByteArrayInputStream(tokenBytes));
-    DelegationTokenInformation token = new DelegationTokenInformation(0, null);
-    int len = WritableUtils.readVInt(in);
-    token.password = new byte[len];
-    in.readFully(token.password);
-    token.renewDate = in.readLong();
-    return token;
-  }
-
-  public static void rollMasterKey(
-      AbstractDelegationTokenSecretManager<? extends AbstractDelegationTokenIdentifier> mgr)
-      throws IOException {
-    mgr.rollMasterKey();
-  }
-
-}
diff --git a/src/shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestDBTokenStore.java b/src/shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestDBTokenStore.java
deleted file mode 100644
index a60ad70..0000000
--- a/src/shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestDBTokenStore.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import java.io.IOException;
-import java.util.List;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
-import org.apache.hadoop.hive.thrift.DelegationTokenStore.TokenStoreException;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
-import org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport;
-import org.junit.Assert;
-
-public class TestDBTokenStore extends TestCase{
-
-  public void testDBTokenStore() throws TokenStoreException, MetaException, IOException {
-
-    DelegationTokenStore ts = new DBTokenStore();
-    ts.setStore(new HMSHandler("Test handler"));
-    assertEquals(0, ts.getMasterKeys().length);
-    assertEquals(false,ts.removeMasterKey(-1));
-    try{
-      ts.updateMasterKey(-1, "non-existent-key");
-      fail("Updated non-existent key.");
-    } catch (TokenStoreException e) {
-      assertTrue(e.getCause() instanceof NoSuchObjectException);
-    }
-    int keySeq = ts.addMasterKey("key1Data");
-    int keySeq2 = ts.addMasterKey("key2Data");
-    int keySeq2same = ts.addMasterKey("key2Data");
-    assertEquals("keys sequential", keySeq + 1, keySeq2);
-    assertEquals("keys sequential", keySeq + 2, keySeq2same);
-    assertEquals("expected number of keys", 3, ts.getMasterKeys().length);
-    assertTrue(ts.removeMasterKey(keySeq));
-    assertTrue(ts.removeMasterKey(keySeq2same));
-    assertEquals("expected number of keys", 1, ts.getMasterKeys().length);
-    assertEquals("key2Data",ts.getMasterKeys()[0]);
-    ts.updateMasterKey(keySeq2, "updatedData");
-    assertEquals("updatedData",ts.getMasterKeys()[0]);
-    assertTrue(ts.removeMasterKey(keySeq2));
-
-    // tokens
-    assertEquals(0, ts.getAllDelegationTokenIdentifiers().size());
-    DelegationTokenIdentifier tokenId = new DelegationTokenIdentifier(
-        new Text("owner"), new Text("renewer"), new Text("realUser"));
-    assertNull(ts.getToken(tokenId));
-    assertFalse(ts.removeToken(tokenId));
-    DelegationTokenInformation tokenInfo = new DelegationTokenInformation(
-        99, "password".getBytes());
-    assertTrue(ts.addToken(tokenId, tokenInfo));
-    assertFalse(ts.addToken(tokenId, tokenInfo));
-    DelegationTokenInformation tokenInfoRead = ts.getToken(tokenId);
-    assertEquals(tokenInfo.getRenewDate(), tokenInfoRead.getRenewDate());
-    assertNotSame(tokenInfo, tokenInfoRead);
-    Assert.assertArrayEquals(HiveDelegationTokenSupport
-        .encodeDelegationTokenInformation(tokenInfo),
-        HiveDelegationTokenSupport
-            .encodeDelegationTokenInformation(tokenInfoRead));
-
-    List<DelegationTokenIdentifier> allIds = ts
-        .getAllDelegationTokenIdentifiers();
-    assertEquals(1, allIds.size());
-    Assert.assertEquals(TokenStoreDelegationTokenSecretManager
-        .encodeWritable(tokenId),
-        TokenStoreDelegationTokenSecretManager.encodeWritable(allIds
-            .get(0)));
-
-    assertTrue(ts.removeToken(tokenId));
-    assertEquals(0, ts.getAllDelegationTokenIdentifiers().size());
-    assertNull(ts.getToken(tokenId));
-    ts.close();
-  }
-}
diff --git a/src/shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java b/src/shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java
deleted file mode 100644
index 7ac7ebc..0000000
--- a/src/shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java
+++ /dev/null
@@ -1,418 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import java.io.ByteArrayInputStream;
-import java.io.DataInputStream;
-import java.io.IOException;
-import java.net.InetAddress;
-import java.net.NetworkInterface;
-import java.net.ServerSocket;
-import java.security.PrivilegedExceptionAction;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Enumeration;
-import java.util.List;
-import java.util.Map;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.HiveMetaStore;
-import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.security.SaslRpcServer;
-import org.apache.hadoop.security.SaslRpcServer.AuthMethod;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.security.UserGroupInformation.AuthenticationMethod;
-import org.apache.hadoop.security.authorize.AuthorizationException;
-import org.apache.hadoop.security.authorize.ProxyUsers;
-import org.apache.hadoop.security.token.SecretManager.InvalidToken;
-import org.apache.hadoop.security.token.Token;
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
-import org.apache.hadoop.security.token.delegation.DelegationKey;
-import org.apache.hadoop.util.StringUtils;
-import org.apache.thrift.transport.TSaslServerTransport;
-import org.apache.thrift.transport.TTransportException;
-import org.apache.thrift.transport.TTransportFactory;
-
-public class TestHadoop20SAuthBridge extends TestCase {
-
-  /**
-   * set to true when metastore token manager has intitialized token manager
-   * through call to HadoopThriftAuthBridge20S.Server.startDelegationTokenSecretManager
-   */
-  static volatile boolean isMetastoreTokenManagerInited;
-
-  private static class MyHadoopThriftAuthBridge20S extends HadoopThriftAuthBridge20S {
-    @Override
-    public Server createServer(String keytabFile, String principalConf)
-    throws TTransportException {
-      //Create a Server that doesn't interpret any Kerberos stuff
-      return new Server();
-    }
-
-    static class Server extends HadoopThriftAuthBridge20S.Server {
-      public Server() throws TTransportException {
-        super();
-      }
-      @Override
-      public TTransportFactory createTransportFactory(Map<String, String> saslProps)
-      throws TTransportException {
-        TSaslServerTransport.Factory transFactory =
-          new TSaslServerTransport.Factory();
-        transFactory.addServerDefinition(AuthMethod.DIGEST.getMechanismName(),
-            null, SaslRpcServer.SASL_DEFAULT_REALM,
-            saslProps,
-            new SaslDigestCallbackHandler(secretManager));
-
-        return new TUGIAssumingTransportFactory(transFactory, realUgi);
-      }
-      static DelegationTokenStore TOKEN_STORE = new MemoryTokenStore();
-
-      @Override
-      protected DelegationTokenStore getTokenStore(Configuration conf) throws IOException {
-        return TOKEN_STORE;
-      }
-
-      @Override
-      public void startDelegationTokenSecretManager(Configuration conf, Object hms)
-      throws IOException{
-        super.startDelegationTokenSecretManager(conf, hms);
-        isMetastoreTokenManagerInited = true;
-      }
-
-    }
-  }
-
-
-  private HiveConf conf;
-
-  private void configureSuperUserIPAddresses(Configuration conf,
-      String superUserShortName) throws IOException {
-    List<String> ipList = new ArrayList<String>();
-    Enumeration<NetworkInterface> netInterfaceList = NetworkInterface
-        .getNetworkInterfaces();
-    while (netInterfaceList.hasMoreElements()) {
-      NetworkInterface inf = netInterfaceList.nextElement();
-      Enumeration<InetAddress> addrList = inf.getInetAddresses();
-      while (addrList.hasMoreElements()) {
-        InetAddress addr = addrList.nextElement();
-        ipList.add(addr.getHostAddress());
-      }
-    }
-    StringBuilder builder = new StringBuilder();
-    for (String ip : ipList) {
-      builder.append(ip);
-      builder.append(',');
-    }
-    builder.append("127.0.1.1,");
-    builder.append(InetAddress.getLocalHost().getCanonicalHostName());
-    conf.setStrings(ProxyUsers.getProxySuperuserIpConfKey(superUserShortName),
-        builder.toString());
-  }
-
-  public void setup() throws Exception {
-    isMetastoreTokenManagerInited = false;
-    int port = findFreePort();
-    System.setProperty(HiveConf.ConfVars.METASTORE_USE_THRIFT_SASL.varname,
-        "true");
-    System.setProperty(HiveConf.ConfVars.METASTOREURIS.varname,
-        "thrift://localhost:" + port);
-    System.setProperty(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, new Path(
-        System.getProperty("test.build.data", "/tmp")).toString());
-    conf = new HiveConf(TestHadoop20SAuthBridge.class);
-    MetaStoreUtils.startMetaStore(port, new MyHadoopThriftAuthBridge20S());
-  }
-
-  /**
-   * Test delegation token store/load from shared store.
-   * @throws Exception
-   */
-  public void testDelegationTokenSharedStore() throws Exception {
-    UserGroupInformation clientUgi = UserGroupInformation.getCurrentUser();
-
-    TokenStoreDelegationTokenSecretManager tokenManager =
-        new TokenStoreDelegationTokenSecretManager(0, 60*60*1000, 60*60*1000, 0,
-            MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE);
-    // initializes current key
-    tokenManager.startThreads();
-    tokenManager.stopThreads();
-
-    String tokenStrForm = tokenManager.getDelegationToken(clientUgi.getShortUserName());
-    Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
-    t.decodeFromUrlString(tokenStrForm);
-
-    //check whether the username in the token is what we expect
-    DelegationTokenIdentifier d = new DelegationTokenIdentifier();
-    d.readFields(new DataInputStream(new ByteArrayInputStream(
-        t.getIdentifier())));
-    assertTrue("Usernames don't match",
-        clientUgi.getShortUserName().equals(d.getUser().getShortUserName()));
-
-    DelegationTokenInformation tokenInfo = MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE
-        .getToken(d);
-    assertNotNull("token not in store", tokenInfo);
-    assertFalse("duplicate token add",
-        MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.addToken(d, tokenInfo));
-
-    // check keys are copied from token store when token is loaded
-    TokenStoreDelegationTokenSecretManager anotherManager =
-        new TokenStoreDelegationTokenSecretManager(0, 0, 0, 0,
-            MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE);
-   assertEquals("master keys empty on init", 0,
-        anotherManager.getAllKeys().length);
-    assertNotNull("token loaded",
-        anotherManager.retrievePassword(d));
-    anotherManager.renewToken(t, clientUgi.getShortUserName());
-    assertEquals("master keys not loaded from store",
-        MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.getMasterKeys().length,
-        anotherManager.getAllKeys().length);
-
-    // cancel the delegation token
-    tokenManager.cancelDelegationToken(tokenStrForm);
-    assertNull("token not removed from store after cancel",
-        MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.getToken(d));
-    assertFalse("token removed (again)",
-        MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.removeToken(d));
-    try {
-      anotherManager.retrievePassword(d);
-      fail("InvalidToken expected after cancel");
-    } catch (InvalidToken ex) {
-      // expected
-    }
-
-    // token expiration
-    MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.addToken(d,
-        new DelegationTokenInformation(0, t.getPassword()));
-    assertNotNull(MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.getToken(d));
-    anotherManager.removeExpiredTokens();
-    assertNull("Expired token not removed",
-        MyHadoopThriftAuthBridge20S.Server.TOKEN_STORE.getToken(d));
-
-    // key expiration - create an already expired key
-    anotherManager.startThreads(); // generates initial key
-    anotherManager.stopThreads();
-    DelegationKey expiredKey = new DelegationKey(-1, 0, anotherManager.getAllKeys()[0].getKey());
-    anotherManager.logUpdateMasterKey(expiredKey); // updates key with sequence number
-    assertTrue("expired key not in allKeys",
-        anotherManager.reloadKeys().containsKey(expiredKey.getKeyId()));
-    anotherManager.rollMasterKeyExt();
-    assertFalse("Expired key not removed",
-        anotherManager.reloadKeys().containsKey(expiredKey.getKeyId()));
-  }
-
-  public void testSaslWithHiveMetaStore() throws Exception {
-    setup();
-    UserGroupInformation clientUgi = UserGroupInformation.getCurrentUser();
-    obtainTokenAndAddIntoUGI(clientUgi, null);
-    obtainTokenAndAddIntoUGI(clientUgi, "tokenForFooTablePartition");
-  }
-
-  public void testMetastoreProxyUser() throws Exception {
-    setup();
-
-    final String proxyUserName = "proxyUser";
-    //set the configuration up such that proxyUser can act on
-    //behalf of all users belonging to the group foo_bar_group (
-    //a dummy group)
-    String[] groupNames =
-      new String[] { "foo_bar_group" };
-    setGroupsInConf(groupNames, proxyUserName);
-
-    final UserGroupInformation delegationTokenUser =
-      UserGroupInformation.getCurrentUser();
-
-    final UserGroupInformation proxyUserUgi =
-      UserGroupInformation.createRemoteUser(proxyUserName);
-    String tokenStrForm = proxyUserUgi.doAs(new PrivilegedExceptionAction<String>() {
-      public String run() throws Exception {
-        try {
-          //Since the user running the test won't belong to a non-existent group
-          //foo_bar_group, the call to getDelegationTokenStr will fail
-          return getDelegationTokenStr(delegationTokenUser, proxyUserUgi);
-        } catch (AuthorizationException ae) {
-          return null;
-        }
-      }
-    });
-    assertTrue("Expected the getDelegationToken call to fail",
-        tokenStrForm == null);
-
-    //set the configuration up such that proxyUser can act on
-    //behalf of all users belonging to the real group(s) that the
-    //user running the test belongs to
-    setGroupsInConf(UserGroupInformation.getCurrentUser().getGroupNames(),
-        proxyUserName);
-    tokenStrForm = proxyUserUgi.doAs(new PrivilegedExceptionAction<String>() {
-      public String run() throws Exception {
-        try {
-          //Since the user running the test belongs to the group
-          //obtained above the call to getDelegationTokenStr will succeed
-          return getDelegationTokenStr(delegationTokenUser, proxyUserUgi);
-        } catch (AuthorizationException ae) {
-          return null;
-        }
-      }
-    });
-    assertTrue("Expected the getDelegationToken call to not fail",
-        tokenStrForm != null);
-    Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
-    t.decodeFromUrlString(tokenStrForm);
-    //check whether the username in the token is what we expect
-    DelegationTokenIdentifier d = new DelegationTokenIdentifier();
-    d.readFields(new DataInputStream(new ByteArrayInputStream(
-        t.getIdentifier())));
-    assertTrue("Usernames don't match",
-        delegationTokenUser.getShortUserName().equals(d.getUser().getShortUserName()));
-
-  }
-
-  private void setGroupsInConf(String[] groupNames, String proxyUserName)
-  throws IOException {
-   conf.set(
-      ProxyUsers.getProxySuperuserGroupConfKey(proxyUserName),
-      StringUtils.join(",", Arrays.asList(groupNames)));
-    configureSuperUserIPAddresses(conf, proxyUserName);
-    ProxyUsers.refreshSuperUserGroupsConfiguration(conf);
-  }
-
-  private String getDelegationTokenStr(UserGroupInformation ownerUgi,
-      UserGroupInformation realUgi) throws Exception {
-    //obtain a token by directly invoking the metastore operation(without going
-    //through the thrift interface). Obtaining a token makes the secret manager
-    //aware of the user and that it gave the token to the user
-    //also set the authentication method explicitly to KERBEROS. Since the
-    //metastore checks whether the authentication method is KERBEROS or not
-    //for getDelegationToken, and the testcases don't use
-    //kerberos, this needs to be done
-
-    waitForMetastoreTokenInit();
-
-    HadoopThriftAuthBridge20S.Server.authenticationMethod
-                             .set(AuthenticationMethod.KERBEROS);
-    HadoopThriftAuthBridge20S.Server.remoteAddress.set(InetAddress.getLocalHost());
-    return
-        HiveMetaStore.getDelegationToken(ownerUgi.getShortUserName(),
-            realUgi.getShortUserName());
-  }
-
-  /**
-   * Wait for metastore to have initialized token manager
-   * This does not have to be done in other metastore test cases as they
-   * use metastore client which will retry few times on failure
-   * @throws InterruptedException
-   */
-  private void waitForMetastoreTokenInit() throws InterruptedException {
-    int waitAttempts = 30;
-    while(waitAttempts > 0 && !isMetastoreTokenManagerInited){
-      Thread.sleep(1000);
-      waitAttempts--;
-    }
-  }
-
-  private void obtainTokenAndAddIntoUGI(UserGroupInformation clientUgi,
-      String tokenSig) throws Exception {
-    String tokenStrForm = getDelegationTokenStr(clientUgi, clientUgi);
-    Token<DelegationTokenIdentifier> t= new Token<DelegationTokenIdentifier>();
-    t.decodeFromUrlString(tokenStrForm);
-
-    //check whether the username in the token is what we expect
-    DelegationTokenIdentifier d = new DelegationTokenIdentifier();
-    d.readFields(new DataInputStream(new ByteArrayInputStream(
-        t.getIdentifier())));
-    assertTrue("Usernames don't match",
-        clientUgi.getShortUserName().equals(d.getUser().getShortUserName()));
-
-    if (tokenSig != null) {
-      conf.set("hive.metastore.token.signature", tokenSig);
-      t.setService(new Text(tokenSig));
-    }
-    //add the token to the clientUgi for securely talking to the metastore
-    clientUgi.addToken(t);
-    //Create the metastore client as the clientUgi. Doing so this
-    //way will give the client access to the token that was added earlier
-    //in the clientUgi
-    HiveMetaStoreClient hiveClient =
-      clientUgi.doAs(new PrivilegedExceptionAction<HiveMetaStoreClient>() {
-        public HiveMetaStoreClient run() throws Exception {
-          HiveMetaStoreClient hiveClient =
-            new HiveMetaStoreClient(conf);
-          return hiveClient;
-        }
-      });
-
-    assertTrue("Couldn't connect to metastore", hiveClient != null);
-
-    //try out some metastore operations
-    createDBAndVerifyExistence(hiveClient);
-
-    //check that getDelegationToken fails since we are not authenticating
-    //over kerberos
-    boolean pass = false;
-    try {
-      hiveClient.getDelegationToken(clientUgi.getUserName());
-    } catch (MetaException ex) {
-      pass = true;
-    }
-    assertTrue("Expected the getDelegationToken call to fail", pass == true);
-    hiveClient.close();
-
-    //Now cancel the delegation token
-    HiveMetaStore.cancelDelegationToken(tokenStrForm);
-
-    //now metastore connection should fail
-    hiveClient =
-      clientUgi.doAs(new PrivilegedExceptionAction<HiveMetaStoreClient>() {
-        public HiveMetaStoreClient run() {
-          try {
-            HiveMetaStoreClient hiveClient =
-              new HiveMetaStoreClient(conf);
-            return hiveClient;
-          } catch (MetaException e) {
-            return null;
-          }
-        }
-      });
-    assertTrue("Expected metastore operations to fail", hiveClient == null);
-  }
-
-  private void createDBAndVerifyExistence(HiveMetaStoreClient client)
-  throws Exception {
-    String dbName = "simpdb";
-    Database db = new Database();
-    db.setName(dbName);
-    client.createDatabase(db);
-    Database db1 = client.getDatabase(dbName);
-    client.dropDatabase(dbName);
-    assertTrue("Databases do not match", db1.getName().equals(db.getName()));
-  }
-
-  private int findFreePort() throws IOException {
-    ServerSocket socket= new ServerSocket(0);
-    int port = socket.getLocalPort();
-    socket.close();
-    return port;
-  }
-}
diff --git a/src/shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java b/src/shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java
deleted file mode 100644
index 83a80b4..0000000
--- a/src/shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java
+++ /dev/null
@@ -1,181 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.List;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
-import org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport;
-import org.apache.zookeeper.KeeperException;
-import org.apache.zookeeper.ZooKeeper;
-import org.apache.zookeeper.data.ACL;
-import org.apache.zookeeper.data.Stat;
-import org.junit.Assert;
-
-public class TestZooKeeperTokenStore extends TestCase {
-
-  private MiniZooKeeperCluster zkCluster = null;
-  private ZooKeeper zkClient = null;
-  private int zkPort = -1;
-  private ZooKeeperTokenStore ts;
-  // connect timeout large enough for slower test environments
-  private final int connectTimeoutMillis = 30000;
-
-  @Override
-  protected void setUp() throws Exception {
-    File zkDataDir = new File(System.getProperty("test.tmp.dir"));
-    if (this.zkCluster != null) {
-      throw new IOException("Cluster already running");
-    }
-    this.zkCluster = new MiniZooKeeperCluster();
-    this.zkPort = this.zkCluster.startup(zkDataDir);
-
-    this.zkClient = ZooKeeperTokenStore.createConnectedClient("localhost:" + zkPort, 3000,
-        connectTimeoutMillis);
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    this.zkClient.close();
-    if (ts != null) {
-      ts.close();
-    }
-    this.zkCluster.shutdown();
-    this.zkCluster = null;
-  }
-
-  private Configuration createConf(String zkPath) {
-    Configuration conf = new Configuration();
-    conf.set(
-        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_CONNECT_STR,
-        "localhost:" + this.zkPort);
-    conf.set(
-        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ZNODE,
-        zkPath);
-    conf.setLong(
-        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_CONNECT_TIMEOUTMILLIS,
-        connectTimeoutMillis);
-    return conf;
-  }
-
-  public void testTokenStorage() throws Exception {
-    String ZK_PATH = "/zktokenstore-testTokenStorage";
-    ts = new ZooKeeperTokenStore();
-    ts.setConf(createConf(ZK_PATH));
-
-    int keySeq = ts.addMasterKey("key1Data");
-    byte[] keyBytes = zkClient.getData(
-        ZK_PATH
-            + "/keys/"
-            + String.format(ZooKeeperTokenStore.ZK_SEQ_FORMAT,
-                keySeq), false, null);
-    assertNotNull(keyBytes);
-    assertEquals(new String(keyBytes), "key1Data");
-
-    int keySeq2 = ts.addMasterKey("key2Data");
-    assertEquals("keys sequential", keySeq + 1, keySeq2);
-    assertEquals("expected number keys", 2, ts.getMasterKeys().length);
-
-    ts.removeMasterKey(keySeq);
-    assertEquals("expected number keys", 1, ts.getMasterKeys().length);
-
-    // tokens
-    DelegationTokenIdentifier tokenId = new DelegationTokenIdentifier(
-        new Text("owner"), new Text("renewer"), new Text("realUser"));
-    DelegationTokenInformation tokenInfo = new DelegationTokenInformation(
-        99, "password".getBytes());
-    ts.addToken(tokenId, tokenInfo);
-    DelegationTokenInformation tokenInfoRead = ts.getToken(tokenId);
-    assertEquals(tokenInfo.getRenewDate(), tokenInfoRead.getRenewDate());
-    assertNotSame(tokenInfo, tokenInfoRead);
-    Assert.assertArrayEquals(HiveDelegationTokenSupport
-        .encodeDelegationTokenInformation(tokenInfo),
-        HiveDelegationTokenSupport
-            .encodeDelegationTokenInformation(tokenInfoRead));
-
-    List<DelegationTokenIdentifier> allIds = ts
-        .getAllDelegationTokenIdentifiers();
-    assertEquals(1, allIds.size());
-    Assert.assertEquals(TokenStoreDelegationTokenSecretManager
-        .encodeWritable(tokenId),
-        TokenStoreDelegationTokenSecretManager.encodeWritable(allIds
-            .get(0)));
-
-    assertTrue(ts.removeToken(tokenId));
-    assertEquals(0, ts.getAllDelegationTokenIdentifiers().size());
-  }
-
-  public void testAclNoAuth() throws Exception {
-    String ZK_PATH = "/zktokenstore-testAclNoAuth";
-    Configuration conf = createConf(ZK_PATH);
-    conf.set(
-        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ACL,
-        "ip:127.0.0.1:r");
-
-    ts = new ZooKeeperTokenStore();
-    try {
-      ts.setConf(conf);
-      fail("expected ACL exception");
-    } catch (DelegationTokenStore.TokenStoreException e) {
-      assertEquals(e.getCause().getClass(),
-          KeeperException.NoAuthException.class);
-    }
-  }
-
-  public void testAclInvalid() throws Exception {
-    String ZK_PATH = "/zktokenstore-testAclInvalid";
-    String aclString = "sasl:hive/host@TEST.DOMAIN:cdrwa, fail-parse-ignored";
-    Configuration conf = createConf(ZK_PATH);
-    conf.set(
-        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ACL,
-        aclString);
-
-    List<ACL> aclList = ZooKeeperTokenStore.parseACLs(aclString);
-    assertEquals(1, aclList.size());
-
-    ts = new ZooKeeperTokenStore();
-    try {
-      ts.setConf(conf);
-      fail("expected ACL exception");
-    } catch (DelegationTokenStore.TokenStoreException e) {
-      assertEquals(e.getCause().getClass(),
-          KeeperException.InvalidACLException.class);
-    }
-  }
-
-  public void testAclPositive() throws Exception {
-    String ZK_PATH = "/zktokenstore-testAcl";
-    Configuration conf = createConf(ZK_PATH);
-    conf.set(
-        HadoopThriftAuthBridge20S.Server.DELEGATION_TOKEN_STORE_ZK_ACL,
-        "world:anyone:cdrwa,ip:127.0.0.1:cdrwa");
-    ts = new ZooKeeperTokenStore();
-    ts.setConf(conf);
-    List<ACL> acl = zkClient.getACL(ZK_PATH, new Stat());
-    assertEquals(2, acl.size());
-  }
-
-}
diff --git a/src/shims/src/common/java/org/apache/hadoop/fs/ProxyFileSystem.java b/src/shims/src/common/java/org/apache/hadoop/fs/ProxyFileSystem.java
deleted file mode 100644
index cb1e2b7..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/fs/ProxyFileSystem.java
+++ /dev/null
@@ -1,291 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.fs;
-
-import java.io.IOException;
-import java.net.URI;
-import java.net.URISyntaxException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.util.Shell;
-
-/****************************************************************
- * A FileSystem that can serve a given scheme/authority using some
- * other file system. In that sense, it serves as a proxy for the
- * real/underlying file system
- *****************************************************************/
-
-public class ProxyFileSystem extends FilterFileSystem {
-
-  protected String myScheme;
-  protected String myAuthority;
-  protected URI myUri;
-
-  protected String realScheme;
-  protected String realAuthority;
-  protected URI realUri;
-
-
-
-  protected Path swizzleParamPath(Path p) {
-    String pathUriString = p.toUri().toString();
-    URI newPathUri = URI.create(pathUriString);
-    return new Path (realScheme, realAuthority, newPathUri.getPath());
-  }
-
-  private Path swizzleReturnPath(Path p) {
-    String pathUriString = p.toUri().toString();
-    URI newPathUri = URI.create(pathUriString);
-    return new Path (myScheme, myAuthority, newPathUri.getPath());
-  }
-
-  protected FileStatus swizzleFileStatus(FileStatus orig, boolean isParam) {
-    FileStatus ret =
-      new FileStatus(orig.getLen(), orig.isDir(), orig.getReplication(),
-                     orig.getBlockSize(), orig.getModificationTime(),
-                     orig.getAccessTime(), orig.getPermission(),
-                     orig.getOwner(), orig.getGroup(),
-                     isParam ? swizzleParamPath(orig.getPath()) :
-                     swizzleReturnPath(orig.getPath()));
-    return ret;
-  }
-
-  public ProxyFileSystem() {
-    throw new RuntimeException ("Unsupported constructor");
-  }
-
-  public ProxyFileSystem(FileSystem fs) {
-    throw new RuntimeException ("Unsupported constructor");
-  }
-
-  /**
-   *
-   * @param p
-   * @return
-   * @throws IOException
-   */
-  public Path resolvePath(final Path p) throws IOException {
-    // Return the fully-qualified path of path f resolving the path
-    // through any symlinks or mount point
-    checkPath(p);
-    return getFileStatus(p).getPath();
-  }
-
-  /**
-   * Create a proxy file system for fs.
-   *
-   * @param fs FileSystem to create proxy for
-   * @param myUri URI to use as proxy. Only the scheme and authority from
-   *              this are used right now
-   */
-  public ProxyFileSystem(FileSystem fs, URI myUri) {
-    super(fs);
-
-    URI realUri = fs.getUri();
-    this.realScheme = realUri.getScheme();
-    this.realAuthority=realUri.getAuthority();
-    this.realUri = realUri;
-
-    this.myScheme = myUri.getScheme();
-    this.myAuthority=myUri.getAuthority();
-    this.myUri = myUri;
-  }
-
-  @Override
-  public void initialize(URI name, Configuration conf) throws IOException {
-    try {
-      URI realUri = new URI (realScheme, realAuthority,
-                            name.getPath(), name.getQuery(), name.getFragment());
-      super.initialize(realUri, conf);
-    } catch (URISyntaxException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  @Override
-  public URI getUri() {
-    return myUri;
-  }
-
-  @Override
-  public String getName() {
-    return getUri().toString();
-  }
-
-  @Override
-  public Path makeQualified(Path path) {
-    return swizzleReturnPath(super.makeQualified(swizzleParamPath(path)));
-  }
-
-
-  @Override
-  protected void checkPath(final Path path) {
-    super.checkPath(swizzleParamPath(path));
-  }
-
-  @Override
-  public BlockLocation[] getFileBlockLocations(FileStatus file, long start,
-    long len) throws IOException {
-    return super.getFileBlockLocations(swizzleFileStatus(file, true),
-                                       start, len);
-  }
-
-  @Override
-  public FSDataInputStream open(Path f, int bufferSize) throws IOException {
-    return super.open(swizzleParamPath(f), bufferSize);
-  }
-
-  @Override
-  public FSDataOutputStream append(Path f, int bufferSize,
-      Progressable progress) throws IOException {
-    return super.append(swizzleParamPath(f), bufferSize, progress);
-  }
-
-  @Override
-  public FSDataOutputStream create(Path f, FsPermission permission,
-      boolean overwrite, int bufferSize, short replication, long blockSize,
-      Progressable progress) throws IOException {
-    return super.create(swizzleParamPath(f), permission,
-        overwrite, bufferSize, replication, blockSize, progress);
-  }
-
-  @Override
-  public boolean setReplication(Path src, short replication) throws IOException {
-    return super.setReplication(swizzleParamPath(src), replication);
-  }
-
-  @Override
-  public boolean rename(Path src, Path dst) throws IOException {
-    return super.rename(swizzleParamPath(src), swizzleParamPath(dst));
-  }
-
-  @Override
-  public boolean delete(Path f, boolean recursive) throws IOException {
-    return super.delete(swizzleParamPath(f), recursive);
-  }
-
-  @Override
-  public boolean deleteOnExit(Path f) throws IOException {
-    return super.deleteOnExit(swizzleParamPath(f));
-  }
-
-  @Override
-  public FileStatus[] listStatus(Path f) throws IOException {
-    FileStatus[] orig = super.listStatus(swizzleParamPath(f));
-    FileStatus[] ret = new FileStatus [orig.length];
-    for (int i=0; i<orig.length; i++) {
-      ret[i] = swizzleFileStatus(orig[i], false);
-    }
-    return ret;
-  }
-
-  @Override
-  public Path getHomeDirectory() {
-    return swizzleReturnPath(super.getHomeDirectory());
-  }
-
-  @Override
-  public void setWorkingDirectory(Path newDir) {
-    super.setWorkingDirectory(swizzleParamPath(newDir));
-  }
-
-  @Override
-  public Path getWorkingDirectory() {
-    return swizzleReturnPath(super.getWorkingDirectory());
-  }
-
-  @Override
-  public boolean mkdirs(Path f, FsPermission permission) throws IOException {
-    return super.mkdirs(swizzleParamPath(f), permission);
-  }
-
-  @Override
-  public void copyFromLocalFile(boolean delSrc, Path src, Path dst)
-    throws IOException {
-    super.copyFromLocalFile(delSrc, swizzleParamPath(src), swizzleParamPath(dst));
-  }
-
-  @Override
-  public void copyFromLocalFile(boolean delSrc, boolean overwrite,
-                                Path[] srcs, Path dst)
-    throws IOException {
-    super.copyFromLocalFile(delSrc, overwrite, srcs, swizzleParamPath(dst));
-  }
-
-  @Override
-  public void copyFromLocalFile(boolean delSrc, boolean overwrite,
-                                Path src, Path dst)
-    throws IOException {
-    super.copyFromLocalFile(delSrc, overwrite, src, swizzleParamPath(dst));
-  }
-
-  @Override
-  public void copyToLocalFile(boolean delSrc, Path src, Path dst)
-    throws IOException {
-    super.copyToLocalFile(delSrc, swizzleParamPath(src), dst);
-  }
-
-  @Override
-  public Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile)
-    throws IOException {
-    return super.startLocalOutput(swizzleParamPath(fsOutputFile), tmpLocalFile);
-  }
-
-  @Override
-  public void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile)
-    throws IOException {
-    super.completeLocalOutput(swizzleParamPath(fsOutputFile), tmpLocalFile);
-  }
-
-  @Override
-  public ContentSummary getContentSummary(Path f) throws IOException {
-    return super.getContentSummary(swizzleParamPath(f));
-  }
-
-  @Override
-  public FileStatus getFileStatus(Path f) throws IOException {
-    return swizzleFileStatus(super.getFileStatus(swizzleParamPath(f)), false);
-  }
-
-  @Override
-  public FileChecksum getFileChecksum(Path f) throws IOException {
-    return super.getFileChecksum(swizzleParamPath(f));
-  }
-
-  @Override
-  public void setOwner(Path p, String username, String groupname
-      ) throws IOException {
-    super.setOwner(swizzleParamPath(p), username, groupname);
-  }
-
-  @Override
-  public void setTimes(Path p, long mtime, long atime
-      ) throws IOException {
-    super.setTimes(swizzleParamPath(p), mtime, atime);
-  }
-
-  @Override
-  public void setPermission(Path p, FsPermission permission
-      ) throws IOException {
-    super.setPermission(swizzleParamPath(p), permission);
-  }
-}
-
diff --git a/src/shims/src/common/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java b/src/shims/src/common/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java
deleted file mode 100644
index 228a972..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.fs;
-
-import java.io.IOException;
-import java.net.URI;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.util.Shell;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.hive.shims.HadoopShims;
-
-/****************************************************************
- * A Proxy for LocalFileSystem
- *
- * Serves uri's corresponding to 'pfile:///' namespace with using
- * a LocalFileSystem
- *****************************************************************/
-
-public class ProxyLocalFileSystem extends FilterFileSystem {
-
-  protected LocalFileSystem localFs;
-
-  public ProxyLocalFileSystem() {
-    localFs = new LocalFileSystem();
-  }
-
-  public ProxyLocalFileSystem(FileSystem fs) {
-    throw new RuntimeException ("Unsupported Constructor");
-  }
-
-  @Override
-  public void initialize(URI name, Configuration conf) throws IOException {
-    // create a proxy for the local filesystem
-    // the scheme/authority serving as the proxy is derived
-    // from the supplied URI
-    String scheme = name.getScheme();
-    String nameUriString = name.toString();
-    if (Shell.WINDOWS) {
-      // Replace the encoded backward slash with forward slash
-      // Remove the windows drive letter
-      nameUriString = nameUriString.replaceAll("%5C", "/")
-          .replaceFirst("/[c-zC-Z]:", "/")
-          .replaceFirst("^[c-zC-Z]:", "");
-      name = URI.create(nameUriString);
-    }
-
-    String authority = name.getAuthority() != null ? name.getAuthority() : "";
-    String proxyUriString = nameUriString + "://" + authority + "/";
-
-    fs = ShimLoader.getHadoopShims().createProxyFileSystem(
-        localFs, URI.create(proxyUriString));
-
-    fs.initialize(name, conf);
-  }
-}
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandler.java b/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandler.java
deleted file mode 100644
index fba32fb..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandler.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.io;
-
-import java.io.IOException;
-
-import org.apache.hadoop.mapred.InputFormat;
-import org.apache.hadoop.mapred.RecordReader;
-
-/**
- * HiveIOExceptionHandler defines an interface that all io exception handler in
- * Hive should implement. Different IO exception handlers can implement
- * different logics based on the exception input into it.
- */
-public interface HiveIOExceptionHandler {
-
-  /**
-   * process exceptions raised when creating a record reader.
-   *
-   * @param e
-   * @return RecordReader
-   */
-  public RecordReader<?, ?> handleRecordReaderCreationException(Exception e)
-      throws IOException;
-
-  /**
-   * process exceptions thrown when calling rr's next
-   *
-   * @param e
-   * @param result
-   * @throws IOException
-   */
-  public void handleRecorReaderNextException(Exception e,
-      HiveIOExceptionNextHandleResult result) throws IOException;
-
-}
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java b/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java
deleted file mode 100644
index a58f1f2..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.io;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.mapred.InputFormat;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.util.ReflectionUtils;
-
-/**
- * An exception handler chain that process the input exception by going through
- * all exceptions defined in this chain one by one until either one exception
- * handler returns true or it reaches the end of the chain. If it reaches the
- * end of the chain, and still no exception handler returns true, throw the
- * exception to the caller.
- */
-public class HiveIOExceptionHandlerChain {
-
-  public static String HIVE_IO_EXCEPTION_HANDLE_CHAIN = "hive.io.exception.handlers";
-
-  @SuppressWarnings("unchecked")
-  public static HiveIOExceptionHandlerChain getHiveIOExceptionHandlerChain(
-      JobConf conf) {
-    HiveIOExceptionHandlerChain chain = new HiveIOExceptionHandlerChain();
-    String exceptionHandlerStr = conf.get(HIVE_IO_EXCEPTION_HANDLE_CHAIN);
-    List<HiveIOExceptionHandler> handlerChain = new ArrayList<HiveIOExceptionHandler>();
-    if (exceptionHandlerStr != null && !exceptionHandlerStr.trim().equals("")) {
-      String[] handlerArr = exceptionHandlerStr.split(",");
-      if (handlerArr != null && handlerArr.length > 0) {
-        for (String handlerStr : handlerArr) {
-          if (!handlerStr.trim().equals("")) {
-            try {
-              Class<? extends HiveIOExceptionHandler> handlerCls =
-                (Class<? extends HiveIOExceptionHandler>) Class.forName(handlerStr);
-              HiveIOExceptionHandler handler = ReflectionUtils.newInstance(handlerCls, null);
-              handlerChain.add(handler);
-            } catch (Exception e) {
-            }
-          }
-        }
-      }
-    }
-
-    chain.setHandlerChain(handlerChain);
-    return chain;
-  }
-
-  private List<HiveIOExceptionHandler> handlerChain;
-
-  /**
-   * @return the exception handler chain defined
-   */
-  protected List<HiveIOExceptionHandler> getHandlerChain() {
-    return handlerChain;
-  }
-
-  /**
-   * set the exception handler chain
-   * @param handlerChain
-   */
-  protected void setHandlerChain(List<HiveIOExceptionHandler> handlerChain) {
-    this.handlerChain = handlerChain;
-  }
-
-  public RecordReader<?,?>  handleRecordReaderCreationException(Exception e) throws IOException {
-    RecordReader<?, ?> ret = null;
-
-    if (handlerChain != null && handlerChain.size() > 0) {
-      for (HiveIOExceptionHandler handler : handlerChain) {
-        ret = handler.handleRecordReaderCreationException(e);
-        if (ret != null) {
-          return ret;
-        }
-      }
-    }
-
-    //re-throw the exception as an IOException
-    throw new IOException(e);
-  }
-
-  /**
-   * This is to handle exception when doing next operations. Here we use a
-   * HiveIOExceptionNextHandleResult to store the results of each handler. If
-   * the exception is handled by one handler, the handler should set
-   * HiveIOExceptionNextHandleResult to be handled, and also set the handle
-   * result. The handle result is used to return the reader's next to determine
-   * if need to open a new file for read or not.
-   */
-  public boolean handleRecordReaderNextException(Exception e)
-      throws IOException {
-    HiveIOExceptionNextHandleResult result = new HiveIOExceptionNextHandleResult();
-    if (handlerChain != null && handlerChain.size() > 0) {
-      for (HiveIOExceptionHandler handler : handlerChain) {
-        handler.handleRecorReaderNextException(e, result);
-        if (result.getHandled()) {
-          return result.getHandleResult();
-        }
-      }
-    }
-
-    //re-throw the exception as an IOException
-    throw new IOException(e);
-  }
-
-}
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java b/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java
deleted file mode 100644
index d972edb..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.io;
-
-import java.io.IOException;
-
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordReader;
-
-public class HiveIOExceptionHandlerUtil {
-
-  private static ThreadLocal<HiveIOExceptionHandlerChain> handlerChainInstance =
-    new ThreadLocal<HiveIOExceptionHandlerChain>();
-
-  private static HiveIOExceptionHandlerChain get(JobConf job) {
-    HiveIOExceptionHandlerChain cache = HiveIOExceptionHandlerUtil.handlerChainInstance
-        .get();
-    if (cache == null) {
-      HiveIOExceptionHandlerChain toSet = HiveIOExceptionHandlerChain
-          .getHiveIOExceptionHandlerChain(job);
-      handlerChainInstance.set(toSet);
-      cache = HiveIOExceptionHandlerUtil.handlerChainInstance.get();
-    }
-    return cache;
-  }
-
-  /**
-   * Handle exception thrown when creating record reader. In case that there is
-   * an exception raised when construction the record reader and one handler can
-   * handle this exception, it should return an record reader, which is either a
-   * dummy empty record reader or a specific record reader that do some magic.
-   *
-   * @param e
-   * @param job
-   * @return RecordReader
-   * @throws IOException
-   */
-  public static RecordReader handleRecordReaderCreationException(Exception e,
-      JobConf job) throws IOException {
-    HiveIOExceptionHandlerChain ioExpectionHandlerChain = get(job);
-    if (ioExpectionHandlerChain != null) {
-      return ioExpectionHandlerChain.handleRecordReaderCreationException(e);
-    }
-    throw new IOException(e);
-  }
-
-  /**
-   * Handle exception thrown when calling record reader's next. If this
-   * exception is handled by one handler, will just return true. Otherwise,
-   * either re-throw this exception in one handler or at the end of the handler
-   * chain.
-   *
-   * @param e
-   * @param job
-   * @return true on success
-   * @throws IOException
-   */
-  public static boolean handleRecordReaderNextException(Exception e, JobConf job)
-      throws IOException {
-    HiveIOExceptionHandlerChain ioExpectionHandlerChain = get(job);
-    if (ioExpectionHandlerChain != null) {
-      return ioExpectionHandlerChain.handleRecordReaderNextException(e);
-    }
-    throw new IOException(e);
-  }
-
-}
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionNextHandleResult.java b/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionNextHandleResult.java
deleted file mode 100644
index 6143c8f..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionNextHandleResult.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.io;
-
-/**
- * A container to store handling results for exceptions produced in record
- * reader's next(). It basically contains 2 fields, one is to store if it is
- * handled or not, another field to store the result.
- */
-public class HiveIOExceptionNextHandleResult {
-
-  // this exception has been handled
-  private boolean handled;
-
-  //the handling results
-  private boolean handleResult;
-
-  public boolean getHandled() {
-    return handled;
-  }
-
-  public void setHandled(boolean handled) {
-    this.handled = handled;
-  }
-
-  public boolean getHandleResult() {
-    return handleResult;
-  }
-
-  public void setHandleResult(boolean handleResult) {
-    this.handleResult = handleResult;
-  }
-
-  public void clear() {
-    handled = false;
-    handleResult = false;
-  }
-
-}
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/shims/CombineHiveKey.java b/src/shims/src/common/java/org/apache/hadoop/hive/shims/CombineHiveKey.java
deleted file mode 100644
index bd0934e..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/shims/CombineHiveKey.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.shims;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.WritableComparable;
-
-public class CombineHiveKey implements WritableComparable {
-  Object key;
-
-  public CombineHiveKey(Object key) {
-    this.key = key;
-  }
-
-  public Object getKey() {
-    return key;
-  }
-
-  public void setKey(Object key) {
-    this.key = key;
-  }
-
-  public void write(DataOutput out) throws IOException {
-    throw new IOException("Method not supported");
-  }
-
-  public void readFields(DataInput in) throws IOException {
-    throw new IOException("Method not supported");
-  }
-
-  public int compareTo(Object w) {
-    assert false;
-    return 0;
-  }
-}
\ No newline at end of file
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java b/src/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
deleted file mode 100644
index ec4eef8..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
+++ /dev/null
@@ -1,555 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.shims;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.net.InetSocketAddress;
-import java.net.MalformedURLException;
-import java.net.URI;
-import java.net.URISyntaxException;
-import java.security.PrivilegedExceptionAction;
-import java.util.List;
-
-import javax.security.auth.login.LoginException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.InputFormat;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.JobProfile;
-import org.apache.hadoop.mapred.JobStatus;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.RunningJob;
-import org.apache.hadoop.mapred.TaskCompletionEvent;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.JobID;
-import org.apache.hadoop.mapreduce.OutputFormat;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hadoop.mapreduce.TaskID;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.util.Progressable;
-
-/**
- * In order to be compatible with multiple versions of Hadoop, all parts
- * of the Hadoop interface that are not cross-version compatible are
- * encapsulated in an implementation of this class. Users should use
- * the ShimLoader class as a factory to obtain an implementation of
- * HadoopShims corresponding to the version of Hadoop currently on the
- * classpath.
- */
-public interface HadoopShims {
-
-  static final Log LOG = LogFactory.getLog(HadoopShims.class);
-
-  /**
-   * Return true if the current version of Hadoop uses the JobShell for
-   * command line interpretation.
-   */
-  boolean usesJobShell();
-
-  /**
-   * Constructs and Returns TaskAttempt Log Url
-   * or null if the TaskLogServlet is not available
-   *
-   *  @return TaskAttempt Log Url
-   */
-  String getTaskAttemptLogUrl(JobConf conf,
-    String taskTrackerHttpAddress,
-    String taskAttemptId)
-    throws MalformedURLException;
-
-  /**
-   * Return true if the job has not switched to RUNNING state yet
-   * and is still in PREP state
-   */
-  boolean isJobPreparing(RunningJob job) throws IOException;
-
-  /**
-   * Calls fs.deleteOnExit(path) if such a function exists.
-   *
-   * @return true if the call was successful
-   */
-  boolean fileSystemDeleteOnExit(FileSystem fs, Path path) throws IOException;
-
-  /**
-   * Calls fmt.validateInput(conf) if such a function exists.
-   */
-  void inputFormatValidateInput(InputFormat fmt, JobConf conf) throws IOException;
-
-  /**
-   * If JobClient.getCommandLineConfig exists, sets the given
-   * property/value pair in that Configuration object.
-   *
-   * This applies for Hadoop 0.17 through 0.19
-   */
-  void setTmpFiles(String prop, String files);
-
-  /**
-   * return the last access time of the given file.
-   * @param file
-   * @return last access time. -1 if not supported.
-   */
-  long getAccessTime(FileStatus file);
-
-  /**
-   * return the Kerberos short name
-   * @param full Kerberos name
-   * @return short Kerberos name
-   * @throws IOException
-   */
-  String getKerberosShortName(String kerberosName) throws IOException;
-
-  /**
-   * Returns a shim to wrap MiniMrCluster
-   */
-  public MiniMrShim getMiniMrCluster(Configuration conf, int numberOfTaskTrackers,
-                                     String nameNode, int numDir) throws IOException;
-
-  /**
-   * Shim for MiniMrCluster
-   */
-  public interface MiniMrShim {
-    public int getJobTrackerPort() throws UnsupportedOperationException;
-    public void shutdown() throws IOException;
-    public void setupConfiguration(Configuration conf);
-  }
-
-  /**
-   * Returns a shim to wrap MiniDFSCluster. This is necessary since this class
-   * was moved from org.apache.hadoop.dfs to org.apache.hadoop.hdfs
-   */
-  MiniDFSShim getMiniDfs(Configuration conf,
-      int numDataNodes,
-      boolean format,
-      String[] racks) throws IOException;
-
-  /**
-   * Shim around the functions in MiniDFSCluster that Hive uses.
-   */
-  public interface MiniDFSShim {
-    FileSystem getFileSystem() throws IOException;
-
-    void shutdown() throws IOException;
-  }
-
-  /**
-   * We define this function here to make the code compatible between
-   * hadoop 0.17 and hadoop 0.20.
-   *
-   * Hive binary that compiled Text.compareTo(Text) with hadoop 0.20 won't
-   * work with hadoop 0.17 because in hadoop 0.20, Text.compareTo(Text) is
-   * implemented in org.apache.hadoop.io.BinaryComparable, and Java compiler
-   * references that class, which is not available in hadoop 0.17.
-   */
-  int compareText(Text a, Text b);
-
-  CombineFileInputFormatShim getCombineFileInputFormat();
-
-  String getInputFormatClassName();
-
-  /**
-   * Wrapper for Configuration.setFloat, which was not introduced
-   * until 0.20.
-   */
-  void setFloatConf(Configuration conf, String varName, float val);
-
-  /**
-   * getTaskJobIDs returns an array of String with two elements. The first
-   * element is a string representing the task id and the second is a string
-   * representing the job id. This is necessary as TaskID and TaskAttemptID
-   * are not supported in Haddop 0.17
-   */
-  String[] getTaskJobIDs(TaskCompletionEvent t);
-
-  int createHadoopArchive(Configuration conf, Path parentDir, Path destDir,
-      String archiveName) throws Exception;
-
-  public URI getHarUri(URI original, URI base, URI originalBase)
-        throws URISyntaxException;
-  /**
-   * Hive uses side effect files exclusively for it's output. It also manages
-   * the setup/cleanup/commit of output from the hive client. As a result it does
-   * not need support for the same inside the MR framework
-   *
-   * This routine sets the appropriate options related to bypass setup/cleanup/commit
-   * support in the MR framework, but does not set the OutputFormat class.
-   */
-  void prepareJobOutput(JobConf conf);
-
-  /**
-   * Used by TaskLogProcessor to Remove HTML quoting from a string
-   * @param item the string to unquote
-   * @return the unquoted string
-   *
-   */
-  public String unquoteHtmlChars(String item);
-
-
-
-  public void closeAllForUGI(UserGroupInformation ugi);
-
-  /**
-   * Get the UGI that the given job configuration will run as.
-   *
-   * In secure versions of Hadoop, this simply returns the current
-   * access control context's user, ignoring the configuration.
-   */
-  public UserGroupInformation getUGIForConf(Configuration conf) throws LoginException, IOException;
-
-  /**
-   * Used by metastore server to perform requested rpc in client context.
-   * @param <T>
-   * @param ugi
-   * @param pvea
-   * @throws IOException
-   * @throws InterruptedException
-   */
-  public <T> T doAs(UserGroupInformation ugi, PrivilegedExceptionAction<T> pvea) throws
-    IOException, InterruptedException;
-
-  /**
-   * Once a delegation token is stored in a file, the location is specified
-   * for a child process that runs hadoop operations, using an environment
-   * variable .
-   * @return Return the name of environment variable used by hadoop to find
-   *  location of token file
-   */
-  public String getTokenFileLocEnvName();
-
-
-  /**
-   * Get delegation token from filesystem and write the token along with
-   * metastore tokens into a file
-   * @param conf
-   * @return Path of the file with token credential
-   * @throws IOException
-   */
-  public Path createDelegationTokenFile(final Configuration conf) throws IOException;
-
-
-  /**
-   * Used by metastore server to creates UGI object for a remote user.
-   * @param userName remote User Name
-   * @param groupNames group names associated with remote user name
-   * @return UGI created for the remote user.
-   */
-
-  public UserGroupInformation createRemoteUser(String userName, List<String> groupNames);
-  /**
-   * Get the short name corresponding to the subject in the passed UGI
-   *
-   * In secure versions of Hadoop, this returns the short name (after
-   * undergoing the translation in the kerberos name rule mapping).
-   * In unsecure versions of Hadoop, this returns the name of the subject
-   */
-  public String getShortUserName(UserGroupInformation ugi);
-
-  /**
-   * Return true if the Shim is based on Hadoop Security APIs.
-   */
-  public boolean isSecureShimImpl();
-
-  /**
-   * Return true if the hadoop configuration has security enabled
-   * @return
-   */
-  public boolean isSecurityEnabled();
-
-  /**
-   * Get the string form of the token given a token signature.
-   * The signature is used as the value of the "service" field in the token for lookup.
-   * Ref: AbstractDelegationTokenSelector in Hadoop. If there exists such a token
-   * in the token cache (credential store) of the job, the lookup returns that.
-   * This is relevant only when running against a "secure" hadoop release
-   * The method gets hold of the tokens if they are set up by hadoop - this should
-   * happen on the map/reduce tasks if the client added the tokens into hadoop's
-   * credential store in the front end during job submission. The method will
-   * select the hive delegation token among the set of tokens and return the string
-   * form of it
-   * @param tokenSignature
-   * @return the string form of the token found
-   * @throws IOException
-   */
-  String getTokenStrForm(String tokenSignature) throws IOException;
-
-  /**
-   * Add a delegation token to the given ugi
-   * @param ugi
-   * @param tokenStr
-   * @param tokenService
-   * @throws IOException
-   */
-  void setTokenStr(UserGroupInformation ugi, String tokenStr, String tokenService)
-    throws IOException;
-
-
-  enum JobTrackerState { INITIALIZING, RUNNING };
-
-  /**
-   * Convert the ClusterStatus to its Thrift equivalent: JobTrackerState.
-   * See MAPREDUCE-2455 for why this is a part of the shim.
-   * @param clusterStatus
-   * @return the matching JobTrackerState
-   * @throws Exception if no equivalent JobTrackerState exists
-   */
-  public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception;
-
-  public TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable);
-
-  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id);
-
-  public JobContext newJobContext(Job job);
-
-  /**
-   * Check wether MR is configured to run in local-mode
-   * @param conf
-   * @return
-   */
-  public boolean isLocalMode(Configuration conf);
-
-  /**
-   * All retrieval of jobtracker/resource manager rpc address
-   * in the configuration should be done through this shim
-   * @param conf
-   * @return
-   */
-  public String getJobLauncherRpcAddress(Configuration conf);
-
-  /**
-   * All updates to jobtracker/resource manager rpc address
-   * in the configuration should be done through this shim
-   * @param conf
-   * @return
-   */
-  public void setJobLauncherRpcAddress(Configuration conf, String val);
-
-  /**
-   * All references to jobtracker/resource manager http address
-   * in the configuration should be done through this shim
-   * @param conf
-   * @return
-   */
-  public String getJobLauncherHttpAddress(Configuration conf);
-
-
-  /**
-   *  Perform kerberos login using the given principal and keytab
-   * @throws IOException
-   */
-  public void loginUserFromKeytab(String principal, String keytabFile) throws IOException;
-
-  /**
-   * Perform kerberos re-login using the given principal and keytab, to renew
-   * the credentials
-   * @throws IOException
-   */
-  public void reLoginUserFromKeytab() throws IOException;
-
-  /**
-   * Move the directory/file to trash. In case of the symlinks or mount points, the file is
-   * moved to the trashbin in the actual volume of the path p being deleted
-   * @param fs
-   * @param path
-   * @param conf
-   * @return false if the item is already in the trash or trash is disabled
-   * @throws IOException
-   */
-  public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration conf)
-          throws IOException;
-
-  /**
-   * Get the default block size for the path. FileSystem alone is not sufficient to
-   * determine the same, as in case of CSMT the underlying file system determines that.
-   * @param fs
-   * @param path
-   * @return
-   */
-  public long getDefaultBlockSize(FileSystem fs, Path path);
-
-  /**
-   * Get the default replication for a path. In case of CSMT the given path will be used to
-   * locate the actual filesystem.
-   * @param fs
-   * @param path
-   * @return
-   */
-  public short getDefaultReplication(FileSystem fs, Path path);
-
-  /**
-   * Create the proxy ugi for the given userid
-   * @param userName
-   * @return
-   */
-  UserGroupInformation createProxyUser(String userName) throws IOException;
-
-  /**
-   * The method sets to set the partition file has a different signature between
-   * hadoop versions.
-   * @param jobConf
-   * @param partition
-   */
-  void setTotalOrderPartitionFile(JobConf jobConf, Path partition);
-  /**
-   * InputSplitShim.
-   *
-   */
-  public interface InputSplitShim extends InputSplit {
-    JobConf getJob();
-
-    long getLength();
-
-    /** Returns an array containing the startoffsets of the files in the split. */
-    long[] getStartOffsets();
-
-    /** Returns an array containing the lengths of the files in the split. */
-    long[] getLengths();
-
-    /** Returns the start offset of the i<sup>th</sup> Path. */
-    long getOffset(int i);
-
-    /** Returns the length of the i<sup>th</sup> Path. */
-    long getLength(int i);
-
-    /** Returns the number of Paths in the split. */
-    int getNumPaths();
-
-    /** Returns the i<sup>th</sup> Path. */
-    Path getPath(int i);
-
-    /** Returns all the Paths in the split. */
-    Path[] getPaths();
-
-    /** Returns all the Paths where this input-split resides. */
-    String[] getLocations() throws IOException;
-
-    void shrinkSplit(long length);
-
-    String toString();
-
-    void readFields(DataInput in) throws IOException;
-
-    void write(DataOutput out) throws IOException;
-  }
-
-  /**
-   * CombineFileInputFormatShim.
-   *
-   * @param <K>
-   * @param <V>
-   */
-  interface CombineFileInputFormatShim<K, V> {
-    Path[] getInputPathsShim(JobConf conf);
-
-    void createPool(JobConf conf, PathFilter... filters);
-
-    InputSplitShim[] getSplits(JobConf job, int numSplits) throws IOException;
-
-    InputSplitShim getInputSplitShim() throws IOException;
-
-    RecordReader getRecordReader(JobConf job, InputSplitShim split, Reporter reporter,
-        Class<RecordReader<K, V>> rrClass) throws IOException;
-  }
-
-  public HCatHadoopShims getHCatShim();
-  public interface HCatHadoopShims {
-
-    enum PropertyName {CACHE_ARCHIVES, CACHE_FILES, CACHE_SYMLINK}
-
-    public TaskID createTaskID();
-
-    public TaskAttemptID createTaskAttemptID();
-
-    public org.apache.hadoop.mapreduce.TaskAttemptContext createTaskAttemptContext(Configuration conf,
-                                                                                   TaskAttemptID taskId);
-
-    public org.apache.hadoop.mapred.TaskAttemptContext createTaskAttemptContext(JobConf conf,
-                                                                                org.apache.hadoop.mapred.TaskAttemptID taskId, Progressable progressable);
-
-    public JobContext createJobContext(Configuration conf, JobID jobId);
-
-    public org.apache.hadoop.mapred.JobContext createJobContext(JobConf conf, JobID jobId, Progressable progressable);
-
-    public void commitJob(OutputFormat outputFormat, Job job) throws IOException;
-
-    public void abortJob(OutputFormat outputFormat, Job job) throws IOException;
-
-    /* Referring to job tracker in 0.20 and resource manager in 0.23 */
-    public InetSocketAddress getResourceManagerAddress(Configuration conf);
-
-    public String getPropertyName(PropertyName name);
-
-    /**
-     * Checks if file is in HDFS filesystem.
-     *
-     * @param fs
-     * @param path
-     * @return true if the file is in HDFS, false if the file is in other file systems.
-     */
-    public boolean isFileInHDFS(FileSystem fs, Path path) throws IOException;
-  }
-  /**
-   * Provides a Hadoop JobTracker shim.
-   * @param conf not {@code null}
-   */
-  public WebHCatJTShim getWebHCatShim(Configuration conf, UserGroupInformation ugi) throws IOException;
-  public interface WebHCatJTShim {
-    /**
-     * Grab a handle to a job that is already known to the JobTracker.
-     *
-     * @return Profile of the job, or null if not found.
-     */
-    public JobProfile getJobProfile(org.apache.hadoop.mapred.JobID jobid) throws IOException;
-    /**
-     * Grab a handle to a job that is already known to the JobTracker.
-     *
-     * @return Status of the job, or null if not found.
-     */
-    public JobStatus getJobStatus(org.apache.hadoop.mapred.JobID jobid) throws IOException;
-    /**
-     * Kill a job.
-     */
-    public void killJob(org.apache.hadoop.mapred.JobID jobid) throws IOException;
-    /**
-     * Get all the jobs submitted.
-     */
-    public JobStatus[] getAllJobs() throws IOException;
-    /**
-     * Close the connection to the Job Tracker.
-     */
-    public void close();
-  }
-
-  /**
-   * Create a proxy file system that can serve a given scheme/authority using some
-   * other file system.
-   */
-  public FileSystem createProxyFileSystem(FileSystem fs, URI uri);
-}
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/shims/HiveEventCounter.java b/src/shims/src/common/java/org/apache/hadoop/hive/shims/HiveEventCounter.java
deleted file mode 100644
index 224b135..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/shims/HiveEventCounter.java
+++ /dev/null
@@ -1,102 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.shims;
-
-import org.apache.log4j.Appender;
-import org.apache.log4j.AppenderSkeleton;
-import org.apache.log4j.Layout;
-import org.apache.log4j.spi.ErrorHandler;
-import org.apache.log4j.spi.Filter;
-import org.apache.log4j.spi.LoggingEvent;
-import org.apache.log4j.spi.OptionHandler;
-
-public class HiveEventCounter implements Appender, OptionHandler {
-
-  AppenderSkeleton hadoopEventCounter;
-
-  public HiveEventCounter() {
-    hadoopEventCounter = ShimLoader.getEventCounter();
-  }
-
-  @Override
-  public void close() {
-    hadoopEventCounter.close();
-  }
-
-  @Override
-  public boolean requiresLayout() {
-    return hadoopEventCounter.requiresLayout();
-  }
-
-  @Override
-  public void addFilter(Filter filter) {
-    hadoopEventCounter.addFilter(filter);
-  }
-
-  @Override
-  public void clearFilters() {
-    hadoopEventCounter.clearFilters();
-  }
-
-  @Override
-  public void doAppend(LoggingEvent event) {
-    hadoopEventCounter.doAppend(event);
-  }
-
-  @Override
-  public ErrorHandler getErrorHandler() {
-    return hadoopEventCounter.getErrorHandler();
-  }
-
-  @Override
-  public Filter getFilter() {
-    return hadoopEventCounter.getFilter();
-  }
-
-  @Override
-  public Layout getLayout() {
-    return hadoopEventCounter.getLayout();
-  }
-
-  @Override
-  public String getName() {
-    return hadoopEventCounter.getName();
-  }
-
-  @Override
-  public void setErrorHandler(ErrorHandler handler) {
-    hadoopEventCounter.setErrorHandler(handler);
-  }
-
-  @Override
-  public void setLayout(Layout layout) {
-    hadoopEventCounter.setLayout(layout);
-  }
-
-  @Override
-  public void setName(String name) {
-    hadoopEventCounter.setName(name);
-  }
-
-  @Override
-  public void activateOptions() {
-    hadoopEventCounter.activateOptions();
-  }
-
-}
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/shims/HiveHarFileSystem.java b/src/shims/src/common/java/org/apache/hadoop/hive/shims/HiveHarFileSystem.java
deleted file mode 100644
index 323ebbb..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/shims/HiveHarFileSystem.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.shims;
-
-import java.io.IOException;
-
-import org.apache.hadoop.fs.BlockLocation;
-import org.apache.hadoop.fs.ContentSummary;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.HarFileSystem;
-import org.apache.hadoop.fs.Path;
-
-/**
- * HiveHarFileSystem - fixes issues with Hadoop's HarFileSystem
- *
- */
-public class HiveHarFileSystem extends HarFileSystem {
-
-  @Override
-  public BlockLocation[] getFileBlockLocations(FileStatus file, long start,
-      long len) throws IOException {
-
-    // In some places (e.g. FileInputFormat) this BlockLocation is used to
-    // figure out sizes/offsets and so a completely blank one will not work.
-    String [] hosts = {"DUMMY_HOST"};
-    return new BlockLocation[]{new BlockLocation(null, hosts, 0, file.getLen())};
-  }
-
-  @Override
-  public ContentSummary getContentSummary(Path f) throws IOException {
-    // HarFileSystem has a bug where this method does not work properly
-    // if the underlying FS is HDFS. See MAPREDUCE-1877 for more
-    // information. This method is from FileSystem.
-    FileStatus status = getFileStatus(f);
-    if (!status.isDir()) {
-      // f is a file
-      return new ContentSummary(status.getLen(), 1, 0);
-    }
-    // f is a directory
-    long[] summary = {0, 0, 1};
-    for(FileStatus s : listStatus(f)) {
-      ContentSummary c = s.isDir() ? getContentSummary(s.getPath()) :
-                                     new ContentSummary(s.getLen(), 1, 0);
-      summary[0] += c.getLength();
-      summary[1] += c.getFileCount();
-      summary[2] += c.getDirectoryCount();
-    }
-    return new ContentSummary(summary[0], summary[1], summary[2]);
-  }
-}
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/shims/JettyShims.java b/src/shims/src/common/java/org/apache/hadoop/hive/shims/JettyShims.java
deleted file mode 100644
index 5ecd664..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/shims/JettyShims.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.shims;
-
-import java.io.IOException;
-
-/**
- * Since Hadoop ships with different versions of Jetty in different versions,
- * Hive uses a shim layer to access the parts of the API that have changed.
- * Users should obtain an instance of this class using the ShimLoader factory.
- */
-public interface JettyShims {
-
-  Server startServer(String listen, int port) throws IOException;
-
-  /**
-   * Server.
-   *
-   */
-  interface Server {
-    void addWar(String war, String mount);
-
-    void start() throws Exception;
-
-    void join() throws InterruptedException;
-
-    void stop() throws Exception;
-  }
-}
\ No newline at end of file
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/shims/ShimLoader.java b/src/shims/src/common/java/org/apache/hadoop/hive/shims/ShimLoader.java
deleted file mode 100644
index bf9c84f..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/shims/ShimLoader.java
+++ /dev/null
@@ -1,174 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.shims;
-
-import java.lang.IllegalArgumentException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge;
-import org.apache.hadoop.util.VersionInfo;
-import org.apache.log4j.AppenderSkeleton;
-
-/**
- * ShimLoader.
- *
- */
-public abstract class ShimLoader {
-  private static HadoopShims hadoopShims;
-  private static JettyShims jettyShims;
-  private static AppenderSkeleton eventCounter;
-
-  /**
-   * The names of the classes for shimming Hadoop for each major version.
-   */
-  private static final HashMap<String, String> HADOOP_SHIM_CLASSES =
-      new HashMap<String, String>();
-
-  static {
-    HADOOP_SHIM_CLASSES.put("0.20", "org.apache.hadoop.hive.shims.Hadoop20Shims");
-    HADOOP_SHIM_CLASSES.put("0.20S", "org.apache.hadoop.hive.shims.Hadoop20SShims");
-    HADOOP_SHIM_CLASSES.put("0.23", "org.apache.hadoop.hive.shims.Hadoop23Shims");
-  }
-
-  /**
-   * The names of the classes for shimming Jetty for each major version of
-   * Hadoop.
-   */
-  private static final HashMap<String, String> JETTY_SHIM_CLASSES =
-      new HashMap<String, String>();
-
-  static {
-    JETTY_SHIM_CLASSES.put("0.20", "org.apache.hadoop.hive.shims.Jetty20Shims");
-    JETTY_SHIM_CLASSES.put("0.20S", "org.apache.hadoop.hive.shims.Jetty20SShims");
-    JETTY_SHIM_CLASSES.put("0.23", "org.apache.hadoop.hive.shims.Jetty23Shims");
-  }
-
-  /**
-   * The names of the classes for shimming Hadoop's event counter
-   */
-  private static final HashMap<String, String> EVENT_COUNTER_SHIM_CLASSES =
-      new HashMap<String, String>();
-
-  static {
-    EVENT_COUNTER_SHIM_CLASSES.put("0.20", "org.apache.hadoop.metrics.jvm.EventCounter");
-    EVENT_COUNTER_SHIM_CLASSES.put("0.20S", "org.apache.hadoop.log.metrics.EventCounter");
-    EVENT_COUNTER_SHIM_CLASSES.put("0.23", "org.apache.hadoop.log.metrics.EventCounter");
-  }
-
-  /**
-   * Factory method to get an instance of HadoopShims based on the
-   * version of Hadoop on the classpath.
-   */
-  public static synchronized HadoopShims getHadoopShims() {
-    if (hadoopShims == null) {
-      hadoopShims = loadShims(HADOOP_SHIM_CLASSES, HadoopShims.class);
-    }
-    return hadoopShims;
-  }
-
-  /**
-   * Factory method to get an instance of JettyShims based on the version
-   * of Hadoop on the classpath.
-   */
-  public static synchronized JettyShims getJettyShims() {
-    if (jettyShims == null) {
-      jettyShims = loadShims(JETTY_SHIM_CLASSES, JettyShims.class);
-    }
-    return jettyShims;
-  }
-
-  public static synchronized AppenderSkeleton getEventCounter() {
-    if (eventCounter == null) {
-      eventCounter = loadShims(EVENT_COUNTER_SHIM_CLASSES, AppenderSkeleton.class);
-    }
-    return eventCounter;
-  }
-
-  public static synchronized HadoopThriftAuthBridge getHadoopThriftAuthBridge() {
-      if (getHadoopShims().isSecureShimImpl()) {
-          return createShim("org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S",
-                            HadoopThriftAuthBridge.class);
-        } else {
-          return new HadoopThriftAuthBridge();
-        }
-      }
-
-  private static <T> T loadShims(Map<String, String> classMap, Class<T> xface) {
-    String vers = getMajorVersion();
-    String className = classMap.get(vers);
-    return createShim(className, xface);
-  }
-
-    private static <T> T createShim(String className, Class<T> xface) {
-    try {
-      Class<?> clazz = Class.forName(className);
-      return xface.cast(clazz.newInstance());
-    } catch (Exception e) {
-      throw new RuntimeException("Could not load shims in class " +
-          className, e);
-    }
-  }
-
-  /**
-   * Return the "major" version of Hadoop currently on the classpath.
-   * For releases in the 0.x series this is simply the first two
-   * components of the version, e.g. "0.20" or "0.23". Releases in
-   * the 1.x and 2.x series are mapped to the appropriate
-   * 0.x release series, e.g. 1.x is mapped to "0.20S" and 2.x
-   * is mapped to "0.23".
-   */
-  public static String getMajorVersion() {
-    String vers = VersionInfo.getVersion();
-
-    String[] parts = vers.split("\\.");
-    if (parts.length < 2) {
-      throw new RuntimeException("Illegal Hadoop Version: " + vers +
-          " (expected A.B.* format)");
-    }
-
-    // Special handling for Hadoop 1.x and 2.x
-    switch (Integer.parseInt(parts[0])) {
-    case 0:
-      break;
-    case 1:
-      return "0.20S";
-    case 2:
-      return "0.23";
-    default:
-      throw new IllegalArgumentException("Unrecognized Hadoop major version number: " + vers);
-    }
-
-    String majorVersion = parts[0] + "." + parts[1];
-
-    // If we are running a security release, we won't have UnixUserGroupInformation
-    // (removed by HADOOP-6299 when switching to JAAS for Login)
-    try {
-      Class.forName("org.apache.hadoop.security.UnixUserGroupInformation");
-    } catch (ClassNotFoundException cnf) {
-      if ("0.20".equals(majorVersion)) {
-        majorVersion += "S";
-      }
-    }
-    return majorVersion;
-  }
-
-  private ShimLoader() {
-    // prevent instantiation
-  }
-}
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java b/src/shims/src/common/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java
deleted file mode 100644
index 03f4e51..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
- package org.apache.hadoop.hive.thrift;
-
- import java.io.IOException;
-import java.net.InetAddress;
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.thrift.TProcessor;
-import org.apache.thrift.transport.TTransport;
-import org.apache.thrift.transport.TTransportException;
-import org.apache.thrift.transport.TTransportFactory;
-
- /**
-  * This class is only overridden by the secure hadoop shim. It allows
-  * the Thrift SASL support to bridge to Hadoop's UserGroupInformation
-  * & DelegationToken infrastructure.
-  */
- public class HadoopThriftAuthBridge {
-   public Client createClient() {
-     throw new UnsupportedOperationException(
-       "The current version of Hadoop does not support Authentication");
-   }
-
-   public Client createClientWithConf(String authType) {
-     throw new UnsupportedOperationException(
-       "The current version of Hadoop does not support Authentication");
-   }
-
-   public Server createServer(String keytabFile, String principalConf)
-     throws TTransportException {
-     throw new UnsupportedOperationException(
-       "The current version of Hadoop does not support Authentication");
-   }
-
-
-  /**
-   * Read and return Hadoop SASL configuration which can be configured using
-   * "hadoop.rpc.protection"
-   *
-   * @param conf
-   * @return Hadoop SASL configuration
-   */
-   public Map<String, String> getHadoopSaslProperties(Configuration conf) {
-     throw new UnsupportedOperationException(
-       "The current version of Hadoop does not support Authentication");
-   }
-
-   public static abstract class Client {
-   /**
-    *
-    * @param principalConfig In the case of Kerberos authentication this will
-    * be the kerberos principal name, for DIGEST-MD5 (delegation token) based
-    * authentication this will be null
-    * @param host The metastore server host name
-    * @param methodStr "KERBEROS" or "DIGEST"
-    * @param tokenStrForm This is url encoded string form of
-    * org.apache.hadoop.security.token.
-    * @param underlyingTransport the underlying transport
-    * @return the transport
-    * @throws IOException
-    */
-     public abstract TTransport createClientTransport(
-             String principalConfig, String host,
-             String methodStr, String tokenStrForm, TTransport underlyingTransport,
-             Map<String, String> saslProps)
-             throws IOException;
-   }
-
-   public static abstract class Server {
-     public abstract TTransportFactory createTransportFactory(Map<String, String> saslProps) throws TTransportException;
-     public abstract TProcessor wrapProcessor(TProcessor processor);
-     public abstract TProcessor wrapNonAssumingProcessor(TProcessor processor);
-     public abstract InetAddress getRemoteAddress();
-     public abstract void startDelegationTokenSecretManager(Configuration conf,
-       Object hmsHandler) throws IOException;
-     public abstract String getRemoteUser();
-     public abstract String getDelegationToken(String owner, String renewer)
-     throws IOException, InterruptedException;
-     public abstract long renewDelegationToken(String tokenStrForm) throws IOException;
-     public abstract void cancelDelegationToken(String tokenStrForm) throws IOException;
-   }
- }
-
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/thrift/TFilterTransport.java b/src/shims/src/common/java/org/apache/hadoop/hive/thrift/TFilterTransport.java
deleted file mode 100644
index b990ea5..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/thrift/TFilterTransport.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import org.apache.thrift.transport.TTransport;
-import org.apache.thrift.transport.TTransportException;
-
-/**
-  * Transport that simply wraps another transport.
-  * This is the equivalent of FilterInputStream for Thrift transports.
-  */
- public class TFilterTransport extends TTransport {
-   protected final TTransport wrapped;
-
-   public TFilterTransport(TTransport wrapped) {
-     this.wrapped = wrapped;
-   }
-
-   @Override
-   public void open() throws TTransportException {
-     wrapped.open();
-   }
-
-   @Override
-   public boolean isOpen() {
-     return wrapped.isOpen();
-   }
-
-   @Override
-   public boolean peek() {
-     return wrapped.peek();
-   }
-
-   @Override
-   public void close() {
-     wrapped.close();
-   }
-
-   @Override
-   public int read(byte[] buf, int off, int len) throws TTransportException {
-     return wrapped.read(buf, off, len);
-   }
-
-   @Override
-   public int readAll(byte[] buf, int off, int len) throws TTransportException {
-     return wrapped.readAll(buf, off, len);
-   }
-
-   @Override
-   public void write(byte[] buf) throws TTransportException {
-     wrapped.write(buf);
-   }
-
-   @Override
-   public void write(byte[] buf, int off, int len) throws TTransportException {
-     wrapped.write(buf, off, len);
-   }
-
-   @Override
-   public void flush() throws TTransportException {
-     wrapped.flush();
-   }
-
-   @Override
-   public byte[] getBuffer() {
-     return wrapped.getBuffer();
-   }
-
-   @Override
-   public int getBufferPosition() {
-     return wrapped.getBufferPosition();
-   }
-
-   @Override
-   public int getBytesRemainingInBuffer() {
-     return wrapped.getBytesRemainingInBuffer();
-   }
-
-   @Override
-   public void consumeBuffer(int len) {
-     wrapped.consumeBuffer(len);
-   }
- }
\ No newline at end of file
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java b/src/shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java
deleted file mode 100644
index 75f7297..0000000
--- a/src/shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java
+++ /dev/null
@@ -1,96 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.thrift;
-
-import java.net.Socket;
-import java.util.concurrent.ConcurrentMap;
-
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.thrift.transport.TSocket;
-import org.apache.thrift.transport.TTransport;
-import org.apache.thrift.transport.TTransportFactory;
-
-import com.google.common.collect.MapMaker;
-
-/** TUGIContainingTransport associates ugi information with connection (transport).
- *  Wraps underlying <code>TSocket</code> transport and annotates it with ugi.
-*/
-
-public class TUGIContainingTransport extends TFilterTransport {
-
-  private UserGroupInformation ugi;
-
-  public TUGIContainingTransport(TTransport wrapped) {
-    super(wrapped);
-  }
-
-  public UserGroupInformation getClientUGI(){
-    return ugi;
-  }
-
-  public void setClientUGI(UserGroupInformation ugi){
-    this.ugi = ugi;
-  }
-
-  /**
-   * If the underlying TTransport is an instance of TSocket, it returns the Socket object
-   * which it contains.  Otherwise it returns null.
-   */
-  public Socket getSocket() {
-    if (wrapped instanceof TSocket) {
-      return (((TSocket)wrapped).getSocket());
-    }
-
-    return null;
-  }
-
-  /** Factory to create TUGIContainingTransport.
-   */
-
-  public static class Factory extends TTransportFactory {
-
-    // Need a concurrent weakhashmap. WeakKeys() so that when underlying transport gets out of
-    // scope, it still can be GC'ed. Since value of map has a ref to key, need weekValues as well.
-    private static final ConcurrentMap<TTransport, TUGIContainingTransport> transMap =
-        new MapMaker().weakKeys().weakValues().makeMap();
-
-    /**
-     * Get a new <code>TUGIContainingTransport</code> instance, or reuse the
-     * existing one if a <code>TUGIContainingTransport</code> has already been
-     * created before using the given <code>TTransport</code> as an underlying
-     * transport. This ensures that a given underlying transport instance
-     * receives the same <code>TUGIContainingTransport</code>.
-     */
-    @Override
-    public TUGIContainingTransport getTransport(TTransport trans) {
-
-      // UGI information is not available at connection setup time, it will be set later
-      // via set_ugi() rpc.
-      TUGIContainingTransport tugiTrans = transMap.get(trans);
-      if (tugiTrans == null) {
-        tugiTrans = new TUGIContainingTransport(trans);
-        TUGIContainingTransport prev = transMap.putIfAbsent(trans, tugiTrans);
-        if (prev != null) {
-          return prev;
-        }
-      }
-      return tugiTrans;
-    }
-  }
-}
-- 
1.7.0.4

