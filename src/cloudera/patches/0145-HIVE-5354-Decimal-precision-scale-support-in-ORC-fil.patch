From 5a1e9039f9f2646669c0a69cb84a4cd395ee7426 Mon Sep 17 00:00:00 2001
From: Brock Noland <brock@apache.org>
Date: Mon, 4 Nov 2013 21:34:43 +0000
Subject: [PATCH 145/375] HIVE-5354 - Decimal precision/scale support in ORC file (Xuefu Zhang via Brock Noland)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1538780 13f79535-47bb-0310-9956-ffa450edef68

Conflicts:
	ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java

This is mostly a clean backport.
The only conflict is likely because OrcProto was generated using Protobuf 2.4.1, and now version is Protobuf 2.5.
---
 .../org/apache/hadoop/hive/ql/io/orc/OrcProto.java |  217 +++++++++++++++++---
 .../apache/hadoop/hive/ql/io/orc/OrcStruct.java    |    6 +-
 .../hadoop/hive/ql/io/orc/RecordReaderImpl.java    |   15 +-
 .../apache/hadoop/hive/ql/io/orc/WriterImpl.java   |    5 +-
 .../apache/hadoop/hive/ql/io/orc/orc_proto.proto   |    2 +
 5 files changed, 212 insertions(+), 33 deletions(-)

diff --git a/src/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java b/src/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java
index 7a96373..f239540 100644
--- a/src/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java
+++ b/src/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java
@@ -8935,6 +8935,26 @@ public Builder removeColumns(int index) {
      * <code>optional uint32 maximumLength = 4;</code>
      */
     int getMaximumLength();
+
+    // optional uint32 precision = 5;
+    /**
+     * <code>optional uint32 precision = 5;</code>
+     */
+    boolean hasPrecision();
+    /**
+     * <code>optional uint32 precision = 5;</code>
+     */
+    int getPrecision();
+
+    // optional uint32 scale = 6;
+    /**
+     * <code>optional uint32 scale = 6;</code>
+     */
+    boolean hasScale();
+    /**
+     * <code>optional uint32 scale = 6;</code>
+     */
+    int getScale();
   }
   /**
    * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Type}
@@ -9032,6 +9052,16 @@ private Type(
               maximumLength_ = input.readUInt32();
               break;
             }
+            case 40: {
+              bitField0_ |= 0x00000004;
+              precision_ = input.readUInt32();
+              break;
+            }
+            case 48: {
+              bitField0_ |= 0x00000008;
+              scale_ = input.readUInt32();
+              break;
+            }
           }
         }
       } catch (com.google.protobuf.InvalidProtocolBufferException e) {
@@ -9381,11 +9411,45 @@ public int getMaximumLength() {
       return maximumLength_;
     }
 
+    // optional uint32 precision = 5;
+    public static final int PRECISION_FIELD_NUMBER = 5;
+    private int precision_;
+    /**
+     * <code>optional uint32 precision = 5;</code>
+     */
+    public boolean hasPrecision() {
+      return ((bitField0_ & 0x00000004) == 0x00000004);
+    }
+    /**
+     * <code>optional uint32 precision = 5;</code>
+     */
+    public int getPrecision() {
+      return precision_;
+    }
+
+    // optional uint32 scale = 6;
+    public static final int SCALE_FIELD_NUMBER = 6;
+    private int scale_;
+    /**
+     * <code>optional uint32 scale = 6;</code>
+     */
+    public boolean hasScale() {
+      return ((bitField0_ & 0x00000008) == 0x00000008);
+    }
+    /**
+     * <code>optional uint32 scale = 6;</code>
+     */
+    public int getScale() {
+      return scale_;
+    }
+
     private void initFields() {
       kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.BOOLEAN;
       subtypes_ = java.util.Collections.emptyList();
       fieldNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
       maximumLength_ = 0;
+      precision_ = 0;
+      scale_ = 0;
     }
     private byte memoizedIsInitialized = -1;
     public final boolean isInitialized() {
@@ -9419,6 +9483,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       if (((bitField0_ & 0x00000002) == 0x00000002)) {
         output.writeUInt32(4, maximumLength_);
       }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        output.writeUInt32(5, precision_);
+      }
+      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+        output.writeUInt32(6, scale_);
+      }
       getUnknownFields().writeTo(output);
     }
 
@@ -9459,6 +9529,14 @@ public int getSerializedSize() {
         size += com.google.protobuf.CodedOutputStream
           .computeUInt32Size(4, maximumLength_);
       }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt32Size(5, precision_);
+      }
+      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt32Size(6, scale_);
+      }
       size += getUnknownFields().getSerializedSize();
       memoizedSerializedSize = size;
       return size;
@@ -9583,6 +9661,10 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000004);
         maximumLength_ = 0;
         bitField0_ = (bitField0_ & ~0x00000008);
+        precision_ = 0;
+        bitField0_ = (bitField0_ & ~0x00000010);
+        scale_ = 0;
+        bitField0_ = (bitField0_ & ~0x00000020);
         return this;
       }
 
@@ -9630,6 +9712,14 @@ public Builder clone() {
           to_bitField0_ |= 0x00000002;
         }
         result.maximumLength_ = maximumLength_;
+        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
+          to_bitField0_ |= 0x00000004;
+        }
+        result.precision_ = precision_;
+        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
+          to_bitField0_ |= 0x00000008;
+        }
+        result.scale_ = scale_;
         result.bitField0_ = to_bitField0_;
         onBuilt();
         return result;
@@ -9672,6 +9762,12 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.Type other) {
         if (other.hasMaximumLength()) {
           setMaximumLength(other.getMaximumLength());
         }
+        if (other.hasPrecision()) {
+          setPrecision(other.getPrecision());
+        }
+        if (other.hasScale()) {
+          setScale(other.getScale());
+        }
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
@@ -9931,6 +10027,72 @@ public Builder clearMaximumLength() {
         return this;
       }
 
+      // optional uint32 precision = 5;
+      private int precision_ ;
+      /**
+       * <code>optional uint32 precision = 5;</code>
+       */
+      public boolean hasPrecision() {
+        return ((bitField0_ & 0x00000010) == 0x00000010);
+      }
+      /**
+       * <code>optional uint32 precision = 5;</code>
+       */
+      public int getPrecision() {
+        return precision_;
+      }
+      /**
+       * <code>optional uint32 precision = 5;</code>
+       */
+      public Builder setPrecision(int value) {
+        bitField0_ |= 0x00000010;
+        precision_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>optional uint32 precision = 5;</code>
+       */
+      public Builder clearPrecision() {
+        bitField0_ = (bitField0_ & ~0x00000010);
+        precision_ = 0;
+        onChanged();
+        return this;
+      }
+
+      // optional uint32 scale = 6;
+      private int scale_ ;
+      /**
+       * <code>optional uint32 scale = 6;</code>
+       */
+      public boolean hasScale() {
+        return ((bitField0_ & 0x00000020) == 0x00000020);
+      }
+      /**
+       * <code>optional uint32 scale = 6;</code>
+       */
+      public int getScale() {
+        return scale_;
+      }
+      /**
+       * <code>optional uint32 scale = 6;</code>
+       */
+      public Builder setScale(int value) {
+        bitField0_ |= 0x00000020;
+        scale_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>optional uint32 scale = 6;</code>
+       */
+      public Builder clearScale() {
+        bitField0_ = (bitField0_ & ~0x00000020);
+        scale_ = 0;
+        onChanged();
+        return this;
+      }
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.Type)
     }
 
@@ -14425,34 +14587,35 @@ public Builder setMagicBytes(
       "treams\030\001 \003(\0132(.org.apache.hadoop.hive.ql" +
       ".io.orc.Stream\022A\n\007columns\030\002 \003(\01320.org.ap",
       "ache.hadoop.hive.ql.io.orc.ColumnEncodin" +
-      "g\"\314\002\n\004Type\0229\n\004kind\030\001 \002(\0162+.org.apache.ha" +
+      "g\"\356\002\n\004Type\0229\n\004kind\030\001 \002(\0162+.org.apache.ha" +
       "doop.hive.ql.io.orc.Type.Kind\022\024\n\010subtype" +
       "s\030\002 \003(\rB\002\020\001\022\022\n\nfieldNames\030\003 \003(\t\022\025\n\rmaxim" +
-      "umLength\030\004 \001(\r\"\307\001\n\004Kind\022\013\n\007BOOLEAN\020\000\022\010\n\004" +
-      "BYTE\020\001\022\t\n\005SHORT\020\002\022\007\n\003INT\020\003\022\010\n\004LONG\020\004\022\t\n\005" +
-      "FLOAT\020\005\022\n\n\006DOUBLE\020\006\022\n\n\006STRING\020\007\022\n\n\006BINAR" +
-      "Y\020\010\022\r\n\tTIMESTAMP\020\t\022\010\n\004LIST\020\n\022\007\n\003MAP\020\013\022\n\n" +
-      "\006STRUCT\020\014\022\t\n\005UNION\020\r\022\013\n\007DECIMAL\020\016\022\010\n\004DAT" +
-      "E\020\017\022\013\n\007VARCHAR\020\020\"x\n\021StripeInformation\022\016\n",
-      "\006offset\030\001 \001(\004\022\023\n\013indexLength\030\002 \001(\004\022\022\n\nda" +
-      "taLength\030\003 \001(\004\022\024\n\014footerLength\030\004 \001(\004\022\024\n\014" +
-      "numberOfRows\030\005 \001(\004\"/\n\020UserMetadataItem\022\014" +
-      "\n\004name\030\001 \002(\t\022\r\n\005value\030\002 \002(\014\"\356\002\n\006Footer\022\024" +
-      "\n\014headerLength\030\001 \001(\004\022\025\n\rcontentLength\030\002 " +
-      "\001(\004\022D\n\007stripes\030\003 \003(\01323.org.apache.hadoop" +
-      ".hive.ql.io.orc.StripeInformation\0225\n\005typ" +
-      "es\030\004 \003(\0132&.org.apache.hadoop.hive.ql.io." +
-      "orc.Type\022D\n\010metadata\030\005 \003(\01322.org.apache." +
-      "hadoop.hive.ql.io.orc.UserMetadataItem\022\024",
-      "\n\014numberOfRows\030\006 \001(\004\022F\n\nstatistics\030\007 \003(\013" +
-      "22.org.apache.hadoop.hive.ql.io.orc.Colu" +
-      "mnStatistics\022\026\n\016rowIndexStride\030\010 \001(\r\"\255\001\n" +
-      "\nPostScript\022\024\n\014footerLength\030\001 \001(\004\022F\n\013com" +
-      "pression\030\002 \001(\01621.org.apache.hadoop.hive." +
-      "ql.io.orc.CompressionKind\022\034\n\024compression" +
-      "BlockSize\030\003 \001(\004\022\023\n\007version\030\004 \003(\rB\002\020\001\022\016\n\005" +
-      "magic\030\300> \001(\t*:\n\017CompressionKind\022\010\n\004NONE\020" +
-      "\000\022\010\n\004ZLIB\020\001\022\n\n\006SNAPPY\020\002\022\007\n\003LZO\020\003"
+      "umLength\030\004 \001(\r\022\021\n\tprecision\030\005 \001(\r\022\r\n\005sca" +
+      "le\030\006 \001(\r\"\307\001\n\004Kind\022\013\n\007BOOLEAN\020\000\022\010\n\004BYTE\020\001" +
+      "\022\t\n\005SHORT\020\002\022\007\n\003INT\020\003\022\010\n\004LONG\020\004\022\t\n\005FLOAT\020" +
+      "\005\022\n\n\006DOUBLE\020\006\022\n\n\006STRING\020\007\022\n\n\006BINARY\020\010\022\r\n" +
+      "\tTIMESTAMP\020\t\022\010\n\004LIST\020\n\022\007\n\003MAP\020\013\022\n\n\006STRUC" +
+      "T\020\014\022\t\n\005UNION\020\r\022\013\n\007DECIMAL\020\016\022\010\n\004DATE\020\017\022\013\n",
+      "\007VARCHAR\020\020\"x\n\021StripeInformation\022\016\n\006offse" +
+      "t\030\001 \001(\004\022\023\n\013indexLength\030\002 \001(\004\022\022\n\ndataLeng" +
+      "th\030\003 \001(\004\022\024\n\014footerLength\030\004 \001(\004\022\024\n\014number" +
+      "OfRows\030\005 \001(\004\"/\n\020UserMetadataItem\022\014\n\004name" +
+      "\030\001 \002(\t\022\r\n\005value\030\002 \002(\014\"\356\002\n\006Footer\022\024\n\014head" +
+      "erLength\030\001 \001(\004\022\025\n\rcontentLength\030\002 \001(\004\022D\n" +
+      "\007stripes\030\003 \003(\01323.org.apache.hadoop.hive." +
+      "ql.io.orc.StripeInformation\0225\n\005types\030\004 \003" +
+      "(\0132&.org.apache.hadoop.hive.ql.io.orc.Ty" +
+      "pe\022D\n\010metadata\030\005 \003(\01322.org.apache.hadoop",
+      ".hive.ql.io.orc.UserMetadataItem\022\024\n\014numb" +
+      "erOfRows\030\006 \001(\004\022F\n\nstatistics\030\007 \003(\01322.org" +
+      ".apache.hadoop.hive.ql.io.orc.ColumnStat" +
+      "istics\022\026\n\016rowIndexStride\030\010 \001(\r\"\255\001\n\nPostS" +
+      "cript\022\024\n\014footerLength\030\001 \001(\004\022F\n\013compressi" +
+      "on\030\002 \001(\01621.org.apache.hadoop.hive.ql.io." +
+      "orc.CompressionKind\022\034\n\024compressionBlockS" +
+      "ize\030\003 \001(\004\022\023\n\007version\030\004 \003(\rB\002\020\001\022\016\n\005magic\030" +
+      "\300> \001(\t*:\n\017CompressionKind\022\010\n\004NONE\020\000\022\010\n\004Z" +
+      "LIB\020\001\022\n\n\006SNAPPY\020\002\022\007\n\003LZO\020\003"
     };
     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
@@ -14536,7 +14699,7 @@ public Builder setMagicBytes(
           internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor,
-              new java.lang.String[] { "Kind", "Subtypes", "FieldNames", "MaximumLength", });
+              new java.lang.String[] { "Kind", "Subtypes", "FieldNames", "MaximumLength", "Precision", "Scale", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor =
             getDescriptor().getMessageTypes().get(13);
           internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable = new
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java
index c993b37..a46ef53 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java
@@ -25,6 +25,7 @@
 import java.util.List;
 import java.util.Map;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -544,9 +545,10 @@ static ObjectInspector createObjectInspector(int columnId,
       case DATE:
         return PrimitiveObjectInspectorFactory.javaDateObjectInspector;
       case DECIMAL:
-        // TODO: get precision/scale from TYPE
+        int precision = type.hasPrecision() ? type.getPrecision() : HiveDecimal.MAX_PRECISION;
+        int scale =  type.hasScale()? type.getScale() : HiveDecimal.MAX_SCALE;
         return PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(
-            TypeInfoFactory.decimalTypeInfo);
+            TypeInfoFactory.getDecimalTypeInfo(precision, scale));
       case STRUCT:
         return new OrcStructInspector(columnId, types);
       case UNION:
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
index f3d621e..796dc42 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
@@ -44,6 +44,7 @@
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.HiveVarcharWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;
 import org.apache.hadoop.io.BooleanWritable;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.FloatWritable;
@@ -784,8 +785,13 @@ void skipRows(long items) throws IOException {
     private InStream valueStream;
     private IntegerReader scaleStream = null;
 
-    DecimalTreeReader(Path path, int columnId) {
+    private final int precision;
+    private final int scale;
+
+    DecimalTreeReader(Path path, int columnId, int precision, int scale) {
       super(path, columnId);
+      this.precision = precision;
+      this.scale = scale;
     }
 
     @Override
@@ -819,8 +825,9 @@ void seek(PositionProvider[] index) throws IOException {
     Object next(Object previous) throws IOException {
       super.next(previous);
       if (valuePresent) {
-        return HiveDecimal.create(SerializationUtils.readBigInteger(valueStream),
+        HiveDecimal dec = HiveDecimal.create(SerializationUtils.readBigInteger(valueStream),
             (int) scaleStream.next());
+        return HiveDecimalUtils.enforcePrecisionScale(dec, precision, scale);
       }
       return null;
     }
@@ -1468,7 +1475,9 @@ private static TreeReader createTreeReader(Path path,
       case DATE:
         return new DateTreeReader(path, columnId);
       case DECIMAL:
-        return new DecimalTreeReader(path, columnId);
+        int precision = type.hasPrecision() ? type.getPrecision() : HiveDecimal.MAX_PRECISION;
+        int scale =  type.hasScale()? type.getScale() : HiveDecimal.MAX_SCALE;
+        return new DecimalTreeReader(path, columnId, precision, scale);
       case STRUCT:
         return new StructTreeReader(path, columnId, types, included);
       case LIST:
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
index a23d388..3d8559e 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
@@ -61,6 +61,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
@@ -1622,8 +1623,10 @@ private static void writeTypes(OrcProto.Footer.Builder builder,
             type.setKind(OrcProto.Type.Kind.DATE);
             break;
           case DECIMAL:
-            // TODO: save precision/scale
+            DecimalTypeInfo decTypeInfo = (DecimalTypeInfo)((PrimitiveObjectInspector)treeWriter.inspector).getTypeInfo();
             type.setKind(OrcProto.Type.Kind.DECIMAL);
+            type.setPrecision(decTypeInfo.precision());
+            type.setScale(decTypeInfo.scale());
             break;
           default:
             throw new IllegalArgumentException("Unknown primitive category: " +
diff --git a/src/ql/src/protobuf/org/apache/hadoop/hive/ql/io/orc/orc_proto.proto b/src/ql/src/protobuf/org/apache/hadoop/hive/ql/io/orc/orc_proto.proto
index 1dd9c8d..949e278 100644
--- a/src/ql/src/protobuf/org/apache/hadoop/hive/ql/io/orc/orc_proto.proto
+++ b/src/ql/src/protobuf/org/apache/hadoop/hive/ql/io/orc/orc_proto.proto
@@ -127,6 +127,8 @@ message Type {
   repeated uint32 subtypes = 2 [packed=true];
   repeated string fieldNames = 3;
   optional uint32 maximumLength = 4;
+  optional uint32 precision = 5;
+  optional uint32 scale = 6;
 }
 
 message StripeInformation {
-- 
1.7.0.4

