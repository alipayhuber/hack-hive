From d12243b5e8540f6172c148a05c7d8c2a084168ff Mon Sep 17 00:00:00 2001
From: Ashutosh Chauhan <hashutosh@apache.org>
Date: Sun, 13 Oct 2013 16:15:04 +0000
Subject: [PATCH 083/375] HIVE-5474 : drop table hangs when concurrency=true (Jason Dere via Ashutosh Chauhan)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1531704 13f79535-47bb-0310-9956-ffa450edef68
---
 ql/src/java/org/apache/hadoop/hive/ql/Driver.java  |   23 ++++++++
 .../test/org/apache/hadoop/hive/ql/TestDriver.java |   57 ++++++++++++++++++++
 .../queries/clientpositive/drop_with_concurrency.q |    8 +++
 .../clientpositive/drop_with_concurrency.q.out     |   17 ++++++
 .../service/cli/thrift/ThriftCLIServiceTest.java   |    3 +
 5 files changed, 108 insertions(+), 0 deletions(-)
 create mode 100644 ql/src/test/org/apache/hadoop/hive/ql/TestDriver.java
 create mode 100644 ql/src/test/queries/clientpositive/drop_with_concurrency.q
 create mode 100644 ql/src/test/results/clientpositive/drop_with_concurrency.q.out

diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 55c76d4..d14bbcb 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -777,6 +777,28 @@ public QueryPlan getPlan() {
   }
 
   /**
+   * Dedup the list of lock objects so that there is only one lock per table/partition.
+   * If there is both a shared and exclusive lock for the same object, this will deduped
+   * to just a single exclusive lock.
+   * @param lockObjects
+   */
+  static void dedupLockObjects(List<HiveLockObj> lockObjects) {
+    Map<String, HiveLockObj> lockMap = new HashMap<String, HiveLockObj>();
+    for (HiveLockObj lockObj : lockObjects) {
+      String lockName = lockObj.getName();
+      HiveLockObj foundLock = lockMap.get(lockName);
+      if (foundLock == null || lockObj.getMode() == HiveLockMode.EXCLUSIVE) {
+        lockMap.put(lockName, lockObj);
+      }
+    }
+    // copy set of deduped locks back to original list
+    lockObjects.clear();
+    for (HiveLockObj lockObj : lockMap.values()) {
+      lockObjects.add(lockObj);
+    }
+  }
+
+  /**
    * Acquire read and write locks needed by the statement. The list of objects to be locked are
    * obtained from he inputs and outputs populated by the compiler. The lock acuisition scheme is
    * pretty simple. If all the locks cannot be obtained, error out. Deadlock is avoided by making
@@ -843,6 +865,7 @@ else if (output.getTyp() == WriteEntity.Type.DUMMYPARTITION) {
               )
           );
 
+      dedupLockObjects(lockObjects);
       List<HiveLock> hiveLocks = ctx.getHiveLockMgr().lock(lockObjects, false);
 
       if (hiveLocks == null) {
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/TestDriver.java b/src/ql/src/test/org/apache/hadoop/hive/ql/TestDriver.java
new file mode 100644
index 0000000..e00a73d
--- /dev/null
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/TestDriver.java
@@ -0,0 +1,57 @@
+package org.apache.hadoop.hive.ql;
+
+import java.util.*;
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.ql.lockmgr.HiveLockMode;
+import org.apache.hadoop.hive.ql.lockmgr.HiveLockObj;
+import org.apache.hadoop.hive.ql.lockmgr.HiveLockObject;
+import org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData;
+
+public class TestDriver extends TestCase {
+  public void testDedupLockObjects() {
+    List<HiveLockObj> lockObjs = new ArrayList<HiveLockObj>();
+    String path1 = "path1";
+    String path2 = "path2";
+    HiveLockObjectData lockData1 = new HiveLockObjectData(
+        "query1", "1", "IMPLICIT", "drop table table1");
+    HiveLockObjectData lockData2 = new HiveLockObjectData(
+        "query1", "1", "IMPLICIT", "drop table table1");
+
+    // Start with the following locks:
+    // [path1, shared]
+    // [path1, exclusive]
+    // [path2, shared]
+    // [path2, shared]
+    // [path2, shared]
+    lockObjs.add(new HiveLockObj(new HiveLockObject(path1, lockData1), HiveLockMode.SHARED));
+    String name1 = lockObjs.get(lockObjs.size() - 1).getName();
+    lockObjs.add(new HiveLockObj(new HiveLockObject(path1, lockData1), HiveLockMode.EXCLUSIVE));
+    lockObjs.add(new HiveLockObj(new HiveLockObject(path2, lockData2), HiveLockMode.SHARED));
+    String name2 = lockObjs.get(lockObjs.size() - 1).getName();
+    lockObjs.add(new HiveLockObj(new HiveLockObject(path2, lockData2), HiveLockMode.SHARED));
+    lockObjs.add(new HiveLockObj(new HiveLockObject(path2, lockData2), HiveLockMode.SHARED));
+
+    Driver.dedupLockObjects(lockObjs);
+
+    // After dedup we should be left with 2 locks:
+    // [path1, exclusive]
+    // [path2, shared]
+    assertEquals("Locks should be deduped", 2, lockObjs.size());
+
+    Comparator<HiveLockObj> cmp = new Comparator<HiveLockObj>() {
+      public int compare(HiveLockObj lock1, HiveLockObj lock2) {
+        return lock1.getName().compareTo(lock2.getName());
+      }
+    };
+    Collections.sort(lockObjs, cmp);
+
+    HiveLockObj lockObj = lockObjs.get(0);
+    assertEquals(name1, lockObj.getName());
+    assertEquals(HiveLockMode.EXCLUSIVE, lockObj.getMode());
+
+    lockObj = lockObjs.get(1);
+    assertEquals(name2, lockObj.getName());
+    assertEquals(HiveLockMode.SHARED, lockObj.getMode());
+  }
+}
diff --git a/src/ql/src/test/queries/clientpositive/drop_with_concurrency.q b/src/ql/src/test/queries/clientpositive/drop_with_concurrency.q
new file mode 100644
index 0000000..797a27c
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/drop_with_concurrency.q
@@ -0,0 +1,8 @@
+set hive.lock.numretries=1;
+set hive.lock.sleep.between.retries=1;
+set hive.support.concurrency=true;
+set hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager;
+
+drop table if exists drop_with_concurrency_1;
+create table drop_with_concurrency_1 (c1 int);
+drop table drop_with_concurrency_1;
diff --git a/src/ql/src/test/results/clientpositive/drop_with_concurrency.q.out b/src/ql/src/test/results/clientpositive/drop_with_concurrency.q.out
new file mode 100644
index 0000000..42aad9a
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/drop_with_concurrency.q.out
@@ -0,0 +1,17 @@
+PREHOOK: query: drop table if exists drop_with_concurrency_1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table if exists drop_with_concurrency_1
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table drop_with_concurrency_1 (c1 int)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table drop_with_concurrency_1 (c1 int)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@drop_with_concurrency_1
+PREHOOK: query: drop table drop_with_concurrency_1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@drop_with_concurrency_1
+PREHOOK: Output: default@drop_with_concurrency_1
+POSTHOOK: query: drop table drop_with_concurrency_1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@drop_with_concurrency_1
+POSTHOOK: Output: default@drop_with_concurrency_1
diff --git a/src/service/src/test/org/apache/hive/service/cli/thrift/ThriftCLIServiceTest.java b/src/service/src/test/org/apache/hive/service/cli/thrift/ThriftCLIServiceTest.java
index 0032925..ff7166d 100644
--- a/src/service/src/test/org/apache/hive/service/cli/thrift/ThriftCLIServiceTest.java
+++ b/src/service/src/test/org/apache/hive/service/cli/thrift/ThriftCLIServiceTest.java
@@ -197,6 +197,9 @@ public void testExecuteStatement() throws Exception {
     assertEquals("Query should be finished",
         OperationState.FINISHED, OperationState.getOperationState(opStatusResp.getOperationState()));
 
+    queryString = "DROP TABLE TEST_EXEC_THRIFT";
+    executeQuerySync(queryString, sessHandle);
+
     // Close the session; ignore exception if any
     TCloseSessionReq closeReq = new TCloseSessionReq(sessHandle);
     client.CloseSession(closeReq);
-- 
1.7.0.4

