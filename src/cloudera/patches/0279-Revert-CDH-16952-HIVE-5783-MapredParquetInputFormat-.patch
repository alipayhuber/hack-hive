From d2feb39dbffd0ccdf75509c27208915a465402c1 Mon Sep 17 00:00:00 2001
From: Szehon Ho <szehon@cloudera.com>
Date: Wed, 19 Feb 2014 15:40:24 -0800
Subject: [PATCH 279/375] Revert "CDH-16952: HIVE-5783: MapredParquetInputFormat incorrect reuses footers"

This reverts commit 25e4a238fa7d99b601e4b2c8036e596f3b107d1c.
---
 pom.xml                                            |   11 -
 ql/pom.xml                                         |   11 -
 .../ql/io/parquet/MapredParquetInputFormat.java    |   47 ----
 .../ql/io/parquet/MapredParquetOutputFormat.java   |  115 --------
 .../hive/ql/io/parquet/ProjectionPusher.java       |  161 ------------
 .../convert/ArrayWritableGroupConverter.java       |   80 ------
 .../convert/DataWritableGroupConverter.java        |  125 ---------
 .../convert/DataWritableRecordConverter.java       |   31 ---
 .../hive/ql/io/parquet/convert/ETypeConverter.java |  147 -----------
 .../ql/io/parquet/convert/HiveGroupConverter.java  |   32 ---
 .../ql/io/parquet/convert/HiveSchemaConverter.java |  120 ---------
 .../io/parquet/read/DataWritableReadSupport.java   |  120 ---------
 .../parquet/read/ParquetRecordReaderWrapper.java   |  228 ----------------
 .../parquet/serde/AbstractParquetMapInspector.java |  148 -----------
 .../serde/ArrayWritableObjectInspector.java        |  209 ---------------
 .../parquet/serde/DeepParquetHiveMapInspector.java |   69 -----
 .../parquet/serde/ParquetHiveArrayInspector.java   |  172 ------------
 .../hive/ql/io/parquet/serde/ParquetHiveSerDe.java |  272 --------------------
 .../serde/StandardParquetHiveMapInspector.java     |   52 ----
 .../serde/primitive/ParquetByteInspector.java      |   43 ---
 .../ParquetPrimitiveInspectorFactory.java          |   16 --
 .../serde/primitive/ParquetShortInspector.java     |   43 ---
 .../serde/primitive/ParquetStringInspector.java    |   85 ------
 .../ql/io/parquet/writable/BigDecimalWritable.java |  130 ----------
 .../ql/io/parquet/writable/BinaryWritable.java     |   80 ------
 .../io/parquet/write/DataWritableWriteSupport.java |   47 ----
 .../ql/io/parquet/write/DataWritableWriter.java    |  147 -----------
 .../parquet/write/ParquetRecordWriterWrapper.java  |   81 ------
 .../hadoop/hive/ql/parse/BaseSemanticAnalyzer.java |   17 --
 .../org/apache/hadoop/hive/ql/parse/HiveLexer.g    |    1 -
 .../org/apache/hadoop/hive/ql/parse/HiveParser.g   |    3 -
 .../parquet/hive/DeprecatedParquetInputFormat.java |   37 ---
 .../hive/DeprecatedParquetOutputFormat.java        |   36 ---
 .../parquet/hive/MapredParquetInputFormat.java     |   36 ---
 .../parquet/hive/MapredParquetOutputFormat.java    |   35 ---
 .../ql/io/parquet/TestHiveSchemaConverter.java     |  109 --------
 .../io/parquet/TestMapredParquetInputFormat.java   |   31 ---
 .../io/parquet/TestMapredParquetOutputFormat.java  |   83 ------
 .../hive/ql/io/parquet/TestParquetSerDe.java       |  135 ----------
 .../serde/TestAbstractParquetMapInspector.java     |   89 -------
 .../serde/TestDeepParquetHiveMapInspector.java     |   81 ------
 .../serde/TestParquetHiveArrayInspector.java       |   71 -----
 .../serde/TestStandardParquetHiveMapInspector.java |   79 ------
 43 files changed, 0 insertions(+), 3665 deletions(-)
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableGroupConverter.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetByteInspector.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetPrimitiveInspectorFactory.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetShortInspector.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetStringInspector.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BigDecimalWritable.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BinaryWritable.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriteSupport.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java
 delete mode 100644 ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java
 delete mode 100644 ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java
 delete mode 100644 ql/src/java/parquet/hive/MapredParquetInputFormat.java
 delete mode 100644 ql/src/java/parquet/hive/MapredParquetOutputFormat.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetInputFormat.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetOutputFormat.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java
 delete mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java

diff --git a/src/pom.xml b/src/pom.xml
index 155aac4..3c86ad0 100644
--- a/src/pom.xml
+++ b/src/pom.xml
@@ -244,17 +244,6 @@
         <version>${bonecp.version}</version>
       </dependency>
       <dependency>
-        <groupId>com.twitter</groupId>
-        <artifactId>parquet-hadoop-bundle</artifactId>
-        <version>${parquet.version}</version>
-      </dependency>
-      <dependency>
-        <groupId>com.twitter</groupId>
-        <artifactId>parquet-column</artifactId>
-        <version>${parquet.version}</version>
-        <classifier>tests</classifier>
-      </dependency>
-      <dependency>
         <groupId>com.sun.jersey</groupId>
         <artifactId>jersey-core</artifactId>
         <version>${jersey.version}</version>
diff --git a/src/ql/pom.xml b/src/ql/pom.xml
index ba6a877..6d79037 100644
--- a/src/ql/pom.xml
+++ b/src/ql/pom.xml
@@ -67,10 +67,6 @@
       <version>${kryo.version}</version>
     </dependency>
     <dependency>
-      <groupId>com.twitter</groupId>
-      <artifactId>parquet-hadoop-bundle</artifactId>
-    </dependency>
-    <dependency>
       <groupId>commons-codec</groupId>
       <artifactId>commons-codec</artifactId>
       <version>${commons-codec.version}</version>
@@ -203,12 +199,6 @@
     <!-- test intra-project -->
     <!-- test inter-project -->
     <dependency>
-      <groupId>com.twitter</groupId>
-      <artifactId>parquet-column</artifactId>
-      <classifier>tests</classifier>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
       <version>${junit.version}</version>
@@ -353,7 +343,6 @@
                   <include>org.apache.hive:hive-exec</include>
                   <include>org.apache.hive:hive-serde</include>
                   <include>com.esotericsoftware.kryo:kryo</include>
-                  <include>com.twiter:parquet-hadoop-bundle</include>
                   <include>org.apache.thrift:libthrift</include>
                   <include>commons-lang:commons-lang</include>
                   <include>org.json:json</include>
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java
deleted file mode 100644
index 13924e1..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport;
-import org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.RecordReader;
-
-import parquet.hadoop.ParquetInputFormat;
-
-
-/**
- *
- * A Parquet InputFormat for Hive (with the deprecated package mapred)
- *
- */
-public class MapredParquetInputFormat extends FileInputFormat<Void, ArrayWritable> {
-
-  public static final Log LOG = LogFactory.getLog(MapredParquetInputFormat.class);
-
-  private final ParquetInputFormat<ArrayWritable> realInput;
-
-  public MapredParquetInputFormat() {
-    this(new ParquetInputFormat<ArrayWritable>(DataWritableReadSupport.class));
-  }
-
-  protected MapredParquetInputFormat(final ParquetInputFormat<ArrayWritable> inputFormat) {
-    this.realInput = inputFormat;
-  }
-
-  @Override
-  public org.apache.hadoop.mapred.RecordReader<Void, ArrayWritable> getRecordReader(
-      final org.apache.hadoop.mapred.InputSplit split,
-      final org.apache.hadoop.mapred.JobConf job,
-      final org.apache.hadoop.mapred.Reporter reporter
-      ) throws IOException {
-    try {
-      return (RecordReader<Void, ArrayWritable>) new ParquetRecordReaderWrapper(realInput, split, job, reporter);
-    } catch (final InterruptedException e) {
-      throw new RuntimeException("Cannot create a RecordReaderWrapper", e);
-    }
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java
deleted file mode 100644
index 7023082..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java
+++ /dev/null
@@ -1,115 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Properties;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
-import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
-import org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter;
-import org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport;
-import org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.FileOutputFormat;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordWriter;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.OutputFormat;
-import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.hive.ql.io.FSRecordWriter;
-
-import parquet.hadoop.ParquetOutputFormat;
-
-/**
- *
- * A Parquet OutputFormat for Hive (with the deprecated package mapred)
- *
- */
-public class MapredParquetOutputFormat extends FileOutputFormat<Void, ArrayWritable> implements
-  HiveOutputFormat<Void, ArrayWritable> {
-
-  public static final Log LOG = LogFactory.getLog(MapredParquetOutputFormat.class);
-
-  protected ParquetOutputFormat<ArrayWritable> realOutputFormat;
-
-  public MapredParquetOutputFormat() {
-    realOutputFormat = new ParquetOutputFormat<ArrayWritable>(new DataWritableWriteSupport());
-  }
-
-  public MapredParquetOutputFormat(final OutputFormat<Void, ArrayWritable> mapreduceOutputFormat) {
-    realOutputFormat = (ParquetOutputFormat<ArrayWritable>) mapreduceOutputFormat;
-  }
-
-  @Override
-  public void checkOutputSpecs(final FileSystem ignored, final JobConf job) throws IOException {
-    realOutputFormat.checkOutputSpecs(ShimLoader.getHadoopShims().getHCatShim().createJobContext(job, null));
-  }
-
-  @Override
-  public RecordWriter<Void, ArrayWritable> getRecordWriter(
-      final FileSystem ignored,
-      final JobConf job,
-      final String name,
-      final Progressable progress
-      ) throws IOException {
-    throw new RuntimeException("Should never be used");
-  }
-
-  /**
-   *
-   * Create the parquet schema from the hive schema, and return the RecordWriterWrapper which
-   * contains the real output format
-   */
-  @Override
-  public FSRecordWriter getHiveRecordWriter(
-      final JobConf jobConf,
-      final Path finalOutPath,
-      final Class<? extends Writable> valueClass,
-      final boolean isCompressed,
-      final Properties tableProperties,
-      final Progressable progress) throws IOException {
-
-    LOG.info("getHiveRecordWriter " + this);
-    LOG.info("creating new record writer...");
-
-    // Seriously?  Hard coded property names?
-    final String columnNameProperty = tableProperties.getProperty("columns");
-    final String columnTypeProperty = tableProperties.getProperty("columns.types");
-    List<String> columnNames;
-    List<TypeInfo> columnTypes;
-
-    if (columnNameProperty.length() == 0) {
-      columnNames = new ArrayList<String>();
-    } else {
-      columnNames = Arrays.asList(columnNameProperty.split(","));
-    }
-
-    if (columnTypeProperty.length() == 0) {
-      columnTypes = new ArrayList<TypeInfo>();
-    } else {
-      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);
-    }
-
-    DataWritableWriteSupport.setSchema(HiveSchemaConverter.convert(columnNames, columnTypes), jobConf);
-    return getParquerRecordWriterWrapper(realOutputFormat, jobConf, finalOutPath.toString(), progress);
-  }
-
-  protected ParquetRecordWriterWrapper getParquerRecordWriterWrapper(
-      ParquetOutputFormat<ArrayWritable> realOutputFormat,
-      JobConf jobConf,
-      String finalOutPath,
-      Progressable progress
-      ) throws IOException {
-    return new ParquetRecordWriterWrapper(realOutputFormat, jobConf, finalOutPath.toString(), progress);
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java
deleted file mode 100644
index 5ece1ba..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java
+++ /dev/null
@@ -1,161 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet;
-
-import java.io.IOException;
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.exec.Operator;
-import org.apache.hadoop.hive.ql.exec.TableScanOperator;
-import org.apache.hadoop.hive.ql.exec.Utilities;
-import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
-import org.apache.hadoop.hive.ql.plan.MapWork;
-import org.apache.hadoop.hive.ql.plan.PartitionDesc;
-import org.apache.hadoop.hive.ql.plan.TableScanDesc;
-import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.util.StringUtils;
-
-public class ProjectionPusher {
-
-  public static final Log LOG = LogFactory.getLog(ProjectionPusher.class);
-
-  private final Map<String, PartitionDesc> pathToPartitionInfo =
-      new LinkedHashMap<String, PartitionDesc>();
-  /**
-   * MapWork is the Hive object which describes input files,
-   * columns projections, and filters.
-   */
-  private MapWork mapWork;
-
-  private static final List<String> virtualColumns;
-
-  static {
-    List<String> vcols =  new ArrayList<String>();
-    vcols.add("INPUT__FILE__NAME");
-    vcols.add("BLOCK__OFFSET__INSIDE__FILE");
-    vcols.add("ROW__OFFSET__INSIDE__BLOCK");
-    vcols.add("RAW__DATA__SIZE");
-    virtualColumns = Collections.unmodifiableList(vcols);
-  }
-
-  public List<String> getColumns(final String columns) {
-    final List<String> result = (List<String>) StringUtils.getStringCollection(columns);
-    result.removeAll(virtualColumns);
-    return result;
-  }
-
-  /**
-   * Sets the mapWork variable based on the current JobConf in order to get all partitions.
-   *
-   * @param job
-   */
-  private void updateMrWork(final JobConf job) {
-    final String plan = HiveConf.getVar(job, HiveConf.ConfVars.PLAN);
-    if (mapWork == null && plan != null && plan.length() > 0) {
-      mapWork = Utilities.getMapWork(job);
-      pathToPartitionInfo.clear();
-      for (final Map.Entry<String, PartitionDesc> entry : mapWork.getPathToPartitionInfo().entrySet()) {
-        pathToPartitionInfo.put(new Path(entry.getKey()).toUri().getPath().toString(), entry.getValue());
-      }
-    }
-  }
-
-  private void pushProjectionsAndFilters(final JobConf jobConf,
-      final String splitPath, final String splitPathWithNoSchema) {
-
-    if (mapWork == null) {
-      //LOG.debug("Not pushing projections and filters because MapWork is null");
-      return;
-    } else if (mapWork.getPathToAliases() == null) {
-      //LOG.debug("Not pushing projections and filters because pathToAliases is null");
-      return;
-    }
-
-    final ArrayList<String> aliases = new ArrayList<String>();
-    final Iterator<Entry<String, ArrayList<String>>> iterator = mapWork.getPathToAliases().entrySet().iterator();
-
-    while (iterator.hasNext()) {
-      final Entry<String, ArrayList<String>> entry = iterator.next();
-      final String key = new Path(entry.getKey()).toUri().getPath();
-
-      if (splitPath.equals(key) || splitPathWithNoSchema.equals(key)) {
-        final ArrayList<String> list = entry.getValue();
-        for (final String val : list) {
-          aliases.add(val);
-        }
-      }
-    }
-
-    for (final String alias : aliases) {
-      final Operator<? extends Serializable> op = mapWork.getAliasToWork().get(
-          alias);
-      if (op != null && op instanceof TableScanOperator) {
-        final TableScanOperator tableScan = (TableScanOperator) op;
-
-        // push down projections
-        final List<Integer> list = tableScan.getNeededColumnIDs();
-
-        if (list != null) {
-          ColumnProjectionUtils.appendReadColumnIDs(jobConf, list);
-        } else {
-          ColumnProjectionUtils.setFullyReadColumns(jobConf);
-        }
-
-        pushFilters(jobConf, tableScan);
-      }
-    }
-  }
-
-  private void pushFilters(final JobConf jobConf, final TableScanOperator tableScan) {
-
-    final TableScanDesc scanDesc = tableScan.getConf();
-    if (scanDesc == null) {
-      LOG.debug("Not pushing filters because TableScanDesc is null");
-      return;
-    }
-
-    // construct column name list for reference by filter push down
-    Utilities.setColumnNameList(jobConf, tableScan);
-
-    // push down filters
-    final ExprNodeGenericFuncDesc filterExpr = scanDesc.getFilterExpr();
-    if (filterExpr == null) {
-      LOG.debug("Not pushing filters because FilterExpr is null");
-      return;
-    }
-
-    final String filterText = filterExpr.getExprString();
-    final String filterExprSerialized = Utilities.serializeExpression(filterExpr);
-    jobConf.set(
-        TableScanDesc.FILTER_TEXT_CONF_STR,
-        filterText);
-    jobConf.set(
-        TableScanDesc.FILTER_EXPR_CONF_STR,
-        filterExprSerialized);
-  }
-
-
-  public JobConf pushProjectionsAndFilters(JobConf jobConf, Path path)
-      throws IOException {
-    updateMrWork(jobConf);  // TODO: refactor this
-    final JobConf cloneJobConf = new JobConf(jobConf);
-    final PartitionDesc part = pathToPartitionInfo.get(path.toString());
-
-    if ((part != null) && (part.getTableDesc() != null)) {
-      Utilities.copyTableJobPropertiesToConf(part.getTableDesc(), cloneJobConf);
-    }
-
-    pushProjectionsAndFilters(cloneJobConf, path.toString(), path.toUri().toString());
-    return cloneJobConf;
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java
deleted file mode 100644
index ce39cee..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java
+++ /dev/null
@@ -1,80 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.convert;
-
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-import parquet.io.ParquetDecodingException;
-import parquet.io.api.Converter;
-import parquet.schema.GroupType;
-
-/**
- *
- * A ArrayWritableGroupConverter
- *
- */
-public class ArrayWritableGroupConverter extends HiveGroupConverter {
-
-  private final Converter[] converters;
-  private final HiveGroupConverter parent;
-  private final int index;
-  private final boolean isMap;
-  private Writable currentValue;
-  private Writable[] mapPairContainer;
-
-  public ArrayWritableGroupConverter(final GroupType groupType, final HiveGroupConverter parent, final int index) {
-    this.parent = parent;
-    this.index = index;
-
-    if (groupType.getFieldCount() == 2) {
-      converters = new Converter[2];
-      converters[0] = getConverterFromDescription(groupType.getType(0), 0, this);
-      converters[1] = getConverterFromDescription(groupType.getType(1), 1, this);
-      isMap = true;
-    } else if (groupType.getFieldCount() == 1) {
-      converters = new Converter[1];
-      converters[0] = getConverterFromDescription(groupType.getType(0), 0, this);
-      isMap = false;
-    } else {
-      throw new RuntimeException("Invalid parquet hive schema: " + groupType);
-    }
-
-  }
-
-  @Override
-  public Converter getConverter(final int fieldIndex) {
-    return converters[fieldIndex];
-  }
-
-  @Override
-  public void start() {
-    if (isMap) {
-      mapPairContainer = new Writable[2];
-    }
-  }
-
-  @Override
-  public void end() {
-    if (isMap) {
-      currentValue = new ArrayWritable(Writable.class, mapPairContainer);
-    }
-    parent.add(index, currentValue);
-  }
-
-  @Override
-  protected void set(final int index, final Writable value) {
-    if (index != 0 && mapPairContainer == null || index > 1) {
-      throw new ParquetDecodingException("Repeated group can only have one or two fields for maps. Not allowed to set for the index : " + index);
-    }
-
-    if (isMap) {
-      mapPairContainer[index] = value;
-    } else {
-      currentValue = value;
-    }
-  }
-
-  @Override
-  protected void add(final int index, final Writable value) {
-    set(index, value);
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableGroupConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableGroupConverter.java
deleted file mode 100644
index 873a9cb..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableGroupConverter.java
+++ /dev/null
@@ -1,125 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.convert;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-import parquet.io.api.Converter;
-import parquet.schema.GroupType;
-import parquet.schema.Type;
-
-/**
- *
- * A MapWritableGroupConverter, real converter between hive and parquet types recursively for complex types.
- *
- */
-public class DataWritableGroupConverter extends HiveGroupConverter {
-
-  private final Converter[] converters;
-  private final HiveGroupConverter parent;
-  private final int index;
-  private final Object[] currentArr;
-  private Writable[] rootMap;
-
-  public DataWritableGroupConverter(final GroupType requestedSchema, final GroupType tableSchema) {
-    this(requestedSchema, null, 0, tableSchema);
-    final int fieldCount = tableSchema.getFieldCount();
-    this.rootMap = new Writable[fieldCount];
-  }
-
-  public DataWritableGroupConverter(final GroupType groupType, final HiveGroupConverter parent, final int index) {
-    this(groupType, parent, index, groupType);
-  }
-
-  public DataWritableGroupConverter(final GroupType selectedGroupType, final HiveGroupConverter parent, final int index, final GroupType containingGroupType) {
-    this.parent = parent;
-    this.index = index;
-    final int totalFieldCount = containingGroupType.getFieldCount();
-    final int selectedFieldCount = selectedGroupType.getFieldCount();
-
-    currentArr = new Object[totalFieldCount];
-    converters = new Converter[selectedFieldCount];
-
-    int i = 0;
-    for (final Type subtype : selectedGroupType.getFields()) {
-      if (containingGroupType.getFields().contains(subtype)) {
-        converters[i] = getConverterFromDescription(subtype, containingGroupType.getFieldIndex(subtype.getName()), this);
-      } else {
-        throw new RuntimeException("Group type [" + containingGroupType + "] does not contain requested field: " + subtype);
-      }
-      ++i;
-    }
-  }
-
-  final public ArrayWritable getCurrentArray() {
-    final Writable[] writableArr;
-    if (this.rootMap != null) { // We're at the root : we can safely re-use the same map to save perf
-      writableArr = this.rootMap;
-    } else {
-      writableArr = new Writable[currentArr.length];
-    }
-
-    for (int i = 0; i < currentArr.length; i++) {
-      final Object obj = currentArr[i];
-      if (obj instanceof List) {
-        final List<?> objList = (List<?>)obj;
-        final ArrayWritable arr = new ArrayWritable(Writable.class, objList.toArray(new Writable[objList.size()]));
-        writableArr[i] = arr;
-      } else {
-        writableArr[i] = (Writable) obj;
-      }
-    }
-    return new ArrayWritable(Writable.class, writableArr);
-  }
-
-  @Override
-  final protected void set(final int index, final Writable value) {
-    currentArr[index] = value;
-  }
-
-  @Override
-  public Converter getConverter(final int fieldIndex) {
-    return converters[fieldIndex];
-  }
-
-  @Override
-  public void start() {
-    for (int i = 0; i < currentArr.length; i++) {
-      currentArr[i] = null;
-    }
-  }
-
-  @Override
-  public void end() {
-    if (parent != null) {
-      parent.set(index, getCurrentArray());
-    }
-  }
-
-  @Override
-  protected void add(final int index, final Writable value) {
-
-    if (currentArr[index] != null) {
-
-      final Object obj = currentArr[index];
-      if (obj instanceof List) {
-        final List<Writable> list = (List<Writable>) obj;
-        list.add(value);
-      } else {
-        throw new RuntimeException("This should be a List: " + obj);
-      }
-
-    } else {
-      // create a list here because we don't know the final length of the object
-      // and it is more flexible than ArrayWritable.
-      //
-      // converted to ArrayWritable by getCurrentArray().
-      final List<Writable> buffer = new ArrayList<Writable>();
-      buffer.add(value);
-      currentArr[index] = (Object) buffer;
-    }
-
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java
deleted file mode 100644
index b537e15..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java
+++ /dev/null
@@ -1,31 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.convert;
-
-import org.apache.hadoop.io.ArrayWritable;
-
-import parquet.io.api.GroupConverter;
-import parquet.io.api.RecordMaterializer;
-import parquet.schema.GroupType;
-
-/**
- *
- * A MapWritableReadSupport, encapsulates the tuples
- *
- */
-public class DataWritableRecordConverter extends RecordMaterializer<ArrayWritable> {
-
-  private final DataWritableGroupConverter root;
-
-  public DataWritableRecordConverter(final GroupType requestedSchema, final GroupType tableSchema) {
-    this.root = new DataWritableGroupConverter(requestedSchema, tableSchema);
-  }
-
-  @Override
-  public ArrayWritable getCurrentRecord() {
-    return root.getCurrentArray();
-  }
-
-  @Override
-  public GroupConverter getRootConverter() {
-    return root;
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
deleted file mode 100644
index 028256a..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
+++ /dev/null
@@ -1,147 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.convert;
-
-import java.math.BigDecimal;
-
-import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable;
-import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable.DicBinaryWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.io.BooleanWritable;
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-
-import parquet.column.Dictionary;
-import parquet.io.api.Binary;
-import parquet.io.api.Converter;
-import parquet.io.api.PrimitiveConverter;
-
-/**
- *
- * ETypeConverter is an easy way to set the converter for the right type.
- *
- */
-public enum ETypeConverter {
-
-  EDOUBLE_CONVERTER(Double.TYPE) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addDouble(final double value) {
-          parent.set(index, new DoubleWritable(value));
-        }
-      };
-    }
-  },
-  EBOOLEAN_CONVERTER(Boolean.TYPE) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addBoolean(final boolean value) {
-          parent.set(index, new BooleanWritable(value));
-        }
-      };
-    }
-  },
-  EFLOAT_CONVERTER(Float.TYPE) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addFloat(final float value) {
-          parent.set(index, new FloatWritable(value));
-        }
-      };
-    }
-  },
-  EINT32_CONVERTER(Integer.TYPE) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addInt(final int value) {
-          parent.set(index, new IntWritable(value));
-        }
-      };
-    }
-  },
-  EINT64_CONVERTER(Long.TYPE) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addLong(final long value) {
-          parent.set(index, new LongWritable(value));
-        }
-      };
-    }
-  },
-  EINT96_CONVERTER(BigDecimal.class) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addDouble(final double value) {
-          parent.set(index, new DoubleWritable(value));
-        }
-      };
-    }
-  },
-  EBINARY_CONVERTER(Binary.class) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        private Binary[] dictBinary;
-        private String[] dict;
-
-        @Override
-        public boolean hasDictionarySupport() {
-          return true;
-        }
-
-        @Override
-        public void setDictionary(Dictionary dictionary) {
-          dictBinary = new Binary[dictionary.getMaxId() + 1];
-          dict = new String[dictionary.getMaxId() + 1];
-          for (int i = 0; i <= dictionary.getMaxId(); i++) {
-            Binary binary = dictionary.decodeToBinary(i);
-            dictBinary[i] = binary;
-            dict[i] = binary.toStringUsingUTF8();
-          }
-        }
-
-        @Override
-        public void addValueFromDictionary(int dictionaryId) {
-          parent.set(index, new DicBinaryWritable(dictBinary[dictionaryId],  dict[dictionaryId]));
-        }
-
-        @Override
-        final public void addBinary(Binary value) {
-          parent.set(index, new BinaryWritable(value));
-        }
-      };
-    }
-  };
-  final Class<?> _type;
-
-  private ETypeConverter(final Class<?> type) {
-    this._type = type;
-  }
-
-  private Class<?> getType() {
-    return _type;
-  }
-
-  abstract Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent);
-
-  static public Converter getNewConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-    for (final ETypeConverter eConverter : values()) {
-      if (eConverter.getType() == type) {
-        return eConverter.getConverter(type, index, parent);
-      }
-    }
-    throw new RuntimeException("Converter not found ... for type : " + type);
-  }
-
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java
deleted file mode 100644
index 7bf01e1..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java
+++ /dev/null
@@ -1,32 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.convert;
-
-import org.apache.hadoop.io.Writable;
-
-import parquet.io.api.Converter;
-import parquet.io.api.GroupConverter;
-import parquet.schema.Type;
-import parquet.schema.Type.Repetition;
-
-public abstract class HiveGroupConverter extends GroupConverter {
-
-  static protected Converter getConverterFromDescription(final Type type, final int index, final HiveGroupConverter parent) {
-    if (type == null) {
-      return null;
-    }
-
-    if (type.isPrimitive()) {
-      return ETypeConverter.getNewConverter(type.asPrimitiveType().getPrimitiveTypeName().javaType, index, parent);
-    } else {
-      if (type.asGroupType().getRepetition() == Repetition.REPEATED) {
-        return new ArrayWritableGroupConverter(type.asGroupType(), parent, index);
-      } else {
-        return new DataWritableGroupConverter(type.asGroupType(), parent, index);
-      }
-    }
-  }
-
-  abstract protected void set(int index, Writable value);
-
-  abstract protected void add(int index, Writable value);
-
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java
deleted file mode 100644
index 7bd1a10..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java
+++ /dev/null
@@ -1,120 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.convert;
-
-import java.util.List;
-
-import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
-import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-
-import parquet.Log;
-import parquet.schema.GroupType;
-import parquet.schema.MessageType;
-import parquet.schema.OriginalType;
-import parquet.schema.PrimitiveType;
-import parquet.schema.PrimitiveType.PrimitiveTypeName;
-import parquet.schema.Type;
-import parquet.schema.Type.Repetition;
-
-/**
- *
- * A HiveSchemaConverter
- *
- */
-public class HiveSchemaConverter {
-
-  private static final Log LOG = Log.getLog(HiveSchemaConverter.class);
-
-  static public MessageType convert(final List<String> columnNames, final List<TypeInfo> columnTypes) {
-    final MessageType schema = new MessageType("hive_schema", convertTypes(columnNames, columnTypes));
-    return schema;
-  }
-
-  static private Type[] convertTypes(final List<String> columnNames, final List<TypeInfo> columnTypes) {
-    if (columnNames.size() != columnTypes.size()) {
-      throw new RuntimeException("Mismatched Hive columns and types. Hive columns names found : " + columnNames
-              + " . And Hive types found : " + columnTypes);
-    }
-
-    final Type[] types = new Type[columnNames.size()];
-
-    for (int i = 0; i < columnNames.size(); ++i) {
-      types[i] = convertType(columnNames.get(i), columnTypes.get(i));
-    }
-
-    return types;
-  }
-
-  static private Type convertType(final String name, final TypeInfo typeInfo) {
-    return convertType(name, typeInfo, Repetition.OPTIONAL);
-  }
-
-  static private Type convertType(final String name, final TypeInfo typeInfo, final Repetition repetition) {
-    if (typeInfo.getCategory().equals(Category.PRIMITIVE)) {
-      if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.BINARY, name);
-      } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo) || typeInfo.equals(TypeInfoFactory.shortTypeInfo) || typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.INT32, name);
-      } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.INT64, name);
-      } else if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.DOUBLE, name);
-      } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.FLOAT, name);
-      } else if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.BOOLEAN, name);
-      } else if (typeInfo.equals(TypeInfoFactory.binaryTypeInfo)) {
-        // TODO : binaryTypeInfo is a byte array. Need to map it
-        throw new UnsupportedOperationException("Binary type not implemented");
-      } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {
-        throw new UnsupportedOperationException("Timestamp type not implemented");
-      } else if (typeInfo.equals(TypeInfoFactory.voidTypeInfo)) {
-        throw new UnsupportedOperationException("Void type not implemented");
-      } else if (typeInfo.equals(TypeInfoFactory.unknownTypeInfo)) {
-        throw new UnsupportedOperationException("Unknown type not implemented");
-      } else {
-        throw new RuntimeException("Unknown type: " + typeInfo);
-      }
-    } else if (typeInfo.getCategory().equals(Category.LIST)) {
-      return convertArrayType(name, (ListTypeInfo) typeInfo);
-    } else if (typeInfo.getCategory().equals(Category.STRUCT)) {
-      return convertStructType(name, (StructTypeInfo) typeInfo);
-    } else if (typeInfo.getCategory().equals(Category.MAP)) {
-      return convertMapType(name, (MapTypeInfo) typeInfo);
-    } else if (typeInfo.getCategory().equals(Category.UNION)) {
-      throw new UnsupportedOperationException("Union type not implemented");
-    } else {
-      throw new RuntimeException("Unknown type: " + typeInfo);
-    }
-  }
-
-  // An optional group containing a repeated anonymous group "bag", containing
-  // 1 anonymous element "array_element"
-  static private GroupType convertArrayType(final String name, final ListTypeInfo typeInfo) {
-    final TypeInfo subType = typeInfo.getListElementTypeInfo();
-    return listWrapper(name, OriginalType.LIST, new GroupType(Repetition.REPEATED, ParquetHiveSerDe.ARRAY.toString(), convertType("array_element", subType)));
-  }
-
-  // An optional group containing multiple elements
-  static private GroupType convertStructType(final String name, final StructTypeInfo typeInfo) {
-    final List<String> columnNames = typeInfo.getAllStructFieldNames();
-    final List<TypeInfo> columnTypes = typeInfo.getAllStructFieldTypeInfos();
-    return new GroupType(Repetition.OPTIONAL, name, convertTypes(columnNames, columnTypes));
-
-  }
-
-  // An optional group containing a repeated anonymous group "map", containing
-  // 2 elements: "key", "value"
-  static private GroupType convertMapType(final String name, final MapTypeInfo typeInfo) {
-    final Type keyType = convertType(ParquetHiveSerDe.MAP_KEY.toString(), typeInfo.getMapKeyTypeInfo(), Repetition.REQUIRED);
-    final Type valueType = convertType(ParquetHiveSerDe.MAP_VALUE.toString(), typeInfo.getMapValueTypeInfo());
-    return listWrapper(name, OriginalType.MAP_KEY_VALUE, new GroupType(Repetition.REPEATED, ParquetHiveSerDe.MAP.toString(), keyType, valueType));
-  }
-
-  static private GroupType listWrapper(final String name, final OriginalType originalType, final GroupType groupType) {
-    return new GroupType(Repetition.OPTIONAL, name, originalType, groupType);
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
deleted file mode 100644
index 7e416e8..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
+++ /dev/null
@@ -1,120 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.read;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter;
-import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.util.StringUtils;
-
-import parquet.hadoop.api.ReadSupport;
-import parquet.io.api.RecordMaterializer;
-import parquet.schema.MessageType;
-import parquet.schema.MessageTypeParser;
-import parquet.schema.PrimitiveType;
-import parquet.schema.PrimitiveType.PrimitiveTypeName;
-import parquet.schema.Type;
-import parquet.schema.Type.Repetition;
-
-/**
- *
- * A MapWritableReadSupport
- *
- * Manages the translation between Hive and Parquet
- *
- */
-public class DataWritableReadSupport extends ReadSupport<ArrayWritable> {
-
-  public static final String HIVE_SCHEMA_KEY = "HIVE_TABLE_SCHEMA";
-  private static final List<String> virtualColumns;
-
-  static {
-    List<String> vcols =  new ArrayList<String>();
-    vcols.add("INPUT__FILE__NAME");
-    vcols.add("BLOCK__OFFSET__INSIDE__FILE");
-    vcols.add("ROW__OFFSET__INSIDE__BLOCK");
-    vcols.add("RAW__DATA__SIZE");
-    virtualColumns = Collections.unmodifiableList(vcols);
-  }
-
-  /**
-   * From a string which columns names (including hive column), return a list
-   * of string columns
-   *
-   * @param comma separated list of columns
-   * @return list with virtual columns removed
-   */
-  private static List<String> getColumns(final String columns) {
-    final List<String> result = (List<String>) StringUtils.getStringCollection(columns);
-    result.removeAll(virtualColumns);
-    return result;
-  }
-  /**
-   *
-   * It creates the readContext for Parquet side with the requested schema during the init phase.
-   *
-   * @param configuration needed to get the wanted columns
-   * @param keyValueMetaData // unused
-   * @param fileSchema parquet file schema
-   * @return the parquet ReadContext
-   */
-  @Override
-  public parquet.hadoop.api.ReadSupport.ReadContext init(final Configuration configuration, final Map<String, String> keyValueMetaData, final MessageType fileSchema) {
-    final String columns = configuration.get("columns");
-    final Map<String, String> contextMetadata = new HashMap<String, String>();
-    if (columns != null) {
-      final List<String> listColumns = getColumns(columns);
-
-      final List<Type> typeListTable = new ArrayList<Type>();
-      for (final String col : listColumns) {
-        if (fileSchema.containsField(col)) {
-          typeListTable.add(fileSchema.getType(col));
-        } else { // dummy type, should not be called
-          typeListTable.add(new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, col));
-        }
-      }
-      MessageType tableSchema = new MessageType("table_schema", typeListTable);
-      contextMetadata.put(HIVE_SCHEMA_KEY, tableSchema.toString());
-
-      MessageType requestedSchemaByUser = tableSchema;
-      final List<Integer> indexColumnsWanted = ColumnProjectionUtils.getReadColumnIDs(configuration);
-
-      final List<Type> typeListWanted = new ArrayList<Type>();
-      for (final Integer idx : indexColumnsWanted) {
-        typeListWanted.add(tableSchema.getType(listColumns.get(idx)));
-      }
-      requestedSchemaByUser = new MessageType(fileSchema.getName(), typeListWanted);
-
-      return new ReadContext(requestedSchemaByUser, contextMetadata);
-    } else {
-      contextMetadata.put(HIVE_SCHEMA_KEY, fileSchema.toString());
-      return new ReadContext(fileSchema, contextMetadata);
-    }
-  }
-
-  /**
-   *
-   * It creates the hive read support to interpret data from parquet to hive
-   *
-   * @param configuration // unused
-   * @param keyValueMetaData
-   * @param fileSchema // unused
-   * @param readContext containing the requested schema and the schema of the hive table
-   * @return Record Materialize for Hive
-   */
-  @Override
-  public RecordMaterializer<ArrayWritable> prepareForRead(final Configuration configuration, final Map<String, String> keyValueMetaData, final MessageType fileSchema,
-          final parquet.hadoop.api.ReadSupport.ReadContext readContext) {
-    final Map<String, String> metadata = readContext.getReadSupportMetadata();
-    if (metadata == null) {
-      throw new RuntimeException("ReadContext not initialized properly. Don't know the Hive Schema.");
-    }
-    final MessageType tableSchema = MessageTypeParser.parseMessageType(metadata.get(HIVE_SCHEMA_KEY));
-    return new DataWritableRecordConverter(readContext.getRequestedSchema(), tableSchema);
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
deleted file mode 100644
index 9eb2f5a..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
+++ /dev/null
@@ -1,228 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.read;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.FileSplit;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-
-import parquet.hadoop.ParquetFileReader;
-import parquet.hadoop.ParquetInputFormat;
-import parquet.hadoop.ParquetInputSplit;
-import parquet.hadoop.api.ReadSupport.ReadContext;
-import parquet.hadoop.metadata.BlockMetaData;
-import parquet.hadoop.metadata.FileMetaData;
-import parquet.hadoop.metadata.ParquetMetadata;
-import parquet.hadoop.util.ContextUtil;
-import parquet.schema.MessageTypeParser;
-
-public class ParquetRecordReaderWrapper  implements RecordReader<Void, ArrayWritable> {
-  public static final Log LOG = LogFactory.getLog(ParquetRecordReaderWrapper.class);
-
-  private final long splitLen; // for getPos()
-
-  private org.apache.hadoop.mapreduce.RecordReader<Void, ArrayWritable> realReader;
-  // expect readReader return same Key & Value objects (common case)
-  // this avoids extra serialization & deserialization of these objects
-  private ArrayWritable valueObj = null;
-  private boolean firstRecord = false;
-  private boolean eof = false;
-  private int schemaSize;
-
-  private final ProjectionPusher projectionPusher;
-
-  public ParquetRecordReaderWrapper(
-      final ParquetInputFormat<ArrayWritable> newInputFormat,
-      final InputSplit oldSplit,
-      final JobConf oldJobConf,
-      final Reporter reporter)
-          throws IOException, InterruptedException {
-    this(newInputFormat, oldSplit, oldJobConf, reporter, new ProjectionPusher());
-  }
-
-  public ParquetRecordReaderWrapper(
-      final ParquetInputFormat<ArrayWritable> newInputFormat,
-      final InputSplit oldSplit,
-      final JobConf oldJobConf,
-      final Reporter reporter,
-      final ProjectionPusher pusher)
-          throws IOException, InterruptedException {
-    this.splitLen = oldSplit.getLength();
-    this.projectionPusher = pusher;
-
-    final ParquetInputSplit split = getSplit(oldSplit, oldJobConf);
-
-    TaskAttemptID taskAttemptID = TaskAttemptID.forName(oldJobConf.get("mapred.task.id"));
-    if (taskAttemptID == null) {
-      taskAttemptID = new TaskAttemptID();
-    }
-
-    // create a TaskInputOutputContext
-    final TaskAttemptContext taskContext = ContextUtil.newTaskAttemptContext(oldJobConf, taskAttemptID);
-
-    if (split != null) {
-      try {
-        realReader = newInputFormat.createRecordReader(split, taskContext);
-        realReader.initialize(split, taskContext);
-
-        // read once to gain access to key and value objects
-        if (realReader.nextKeyValue()) {
-          firstRecord = true;
-          valueObj = realReader.getCurrentValue();
-        } else {
-          eof = true;
-        }
-      } catch (final InterruptedException e) {
-        throw new IOException(e);
-      }
-    } else {
-      realReader = null;
-      eof = true;
-      if (valueObj == null) { // Should initialize the value for createValue
-        valueObj = new ArrayWritable(Writable.class, new Writable[schemaSize]);
-      }
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (realReader != null) {
-      realReader.close();
-    }
-  }
-
-  @Override
-  public Void createKey() {
-    return null;
-  }
-
-  @Override
-  public ArrayWritable createValue() {
-    return valueObj;
-  }
-
-  @Override
-  public long getPos() throws IOException {
-    return (long) (splitLen * getProgress());
-  }
-
-  @Override
-  public float getProgress() throws IOException {
-    if (realReader == null) {
-      return 1f;
-    } else {
-      try {
-        return realReader.getProgress();
-      } catch (final InterruptedException e) {
-        throw new IOException(e);
-      }
-    }
-  }
-
-  @Override
-  public boolean next(final Void key, final ArrayWritable value) throws IOException {
-    if (eof) {
-      return false;
-    }
-
-    try {
-      if (firstRecord) { // key & value are already read.
-        firstRecord = false;
-      } else if (!realReader.nextKeyValue()) {
-        eof = true; // strictly not required, just for consistency
-        return false;
-      }
-
-      final ArrayWritable tmpCurValue = realReader.getCurrentValue();
-
-      if (value != tmpCurValue) {
-        final Writable[] arrValue = value.get();
-        final Writable[] arrCurrent = tmpCurValue.get();
-        if (value != null && arrValue.length == arrCurrent.length) {
-          System.arraycopy(arrCurrent, 0, arrValue, 0, arrCurrent.length);
-        } else {
-          if (arrValue.length != arrCurrent.length) {
-            throw new IOException("DeprecatedParquetHiveInput : size of object differs. Value size :  " + arrValue.length + ", Current Object size : "
-                    + arrCurrent.length);
-          } else {
-            throw new IOException("DeprecatedParquetHiveInput can not support RecordReaders that don't return same key & value & value is null");
-          }
-        }
-      }
-      return true;
-
-    } catch (final InterruptedException e) {
-      throw new IOException(e);
-    }
-  }
-
-  /**
-   * gets a ParquetInputSplit corresponding to a split given by Hive
-   *
-   * @param oldSplit The split given by Hive
-   * @param conf The JobConf of the Hive job
-   * @return a ParquetInputSplit corresponding to the oldSplit
-   * @throws IOException if the config cannot be enhanced or if the footer cannot be read from the file
-   */
-  protected ParquetInputSplit getSplit(
-      final InputSplit oldSplit,
-      final JobConf conf
-      ) throws IOException {
-
-    ParquetInputSplit split;
-
-    if (oldSplit instanceof FileSplit) {
-      final Path finalPath = ((FileSplit) oldSplit).getPath();
-      final JobConf cloneJob = projectionPusher.pushProjectionsAndFilters(conf, finalPath.getParent());
-
-      final ParquetMetadata parquetMetadata = ParquetFileReader.readFooter(cloneJob, finalPath);
-      final List<BlockMetaData> blocks = parquetMetadata.getBlocks();
-      final FileMetaData fileMetaData = parquetMetadata.getFileMetaData();
-
-      final ReadContext readContext = new DataWritableReadSupport().init(cloneJob, fileMetaData.getKeyValueMetaData(), fileMetaData.getSchema());
-      schemaSize = MessageTypeParser.parseMessageType(readContext.getReadSupportMetadata().get(DataWritableReadSupport.HIVE_SCHEMA_KEY)).getFieldCount();
-
-      final List<BlockMetaData> splitGroup = new ArrayList<BlockMetaData>();
-      final long splitStart = ((FileSplit) oldSplit).getStart();
-      final long splitLength = ((FileSplit) oldSplit).getLength();
-      for (final BlockMetaData block : blocks) {
-        final long firstDataPage = block.getColumns().get(0).getFirstDataPageOffset();
-        if (firstDataPage >= splitStart && firstDataPage < splitStart + splitLength) {
-          splitGroup.add(block);
-        }
-      }
-
-      if (splitGroup.isEmpty()) {
-        LOG.warn("Skipping split, could not find row group in: " + (FileSplit) oldSplit);
-        split = null;
-      } else {
-        split = new ParquetInputSplit(finalPath,
-                splitStart,
-                splitLength,
-                ((FileSplit) oldSplit).getLocations(),
-                splitGroup,
-                readContext.getRequestedSchema().toString(),
-                fileMetaData.getSchema().toString(),
-                fileMetaData.getKeyValueMetaData(),
-                readContext.getReadSupportMetadata());
-      }
-
-    } else {
-      throw new IllegalArgumentException("Unknown split type: " + oldSplit);
-    }
-
-    return split;
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java
deleted file mode 100644
index 4684cec..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java
+++ /dev/null
@@ -1,148 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-public abstract class AbstractParquetMapInspector implements SettableMapObjectInspector {
-
-  protected final ObjectInspector keyInspector;
-  protected final ObjectInspector valueInspector;
-
-  public AbstractParquetMapInspector(final ObjectInspector keyInspector, final ObjectInspector valueInspector) {
-    this.keyInspector = keyInspector;
-    this.valueInspector = valueInspector;
-  }
-
-  @Override
-  public String getTypeName() {
-    return "map<" + keyInspector.getTypeName() + "," + valueInspector.getTypeName() + ">";
-  }
-
-  @Override
-  public Category getCategory() {
-    return Category.MAP;
-  }
-
-  @Override
-  public ObjectInspector getMapKeyObjectInspector() {
-    return keyInspector;
-  }
-
-  @Override
-  public ObjectInspector getMapValueObjectInspector() {
-    return valueInspector;
-  }
-
-  @Override
-  public Map<?, ?> getMap(final Object data) {
-    if (data == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
-
-      if (mapContainer == null || mapContainer.length == 0) {
-        return null;
-      }
-
-      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
-      final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
-
-      for (final Writable obj : mapArray) {
-        final ArrayWritable mapObj = (ArrayWritable) obj;
-        final Writable[] arr = mapObj.get();
-        map.put(arr[0], arr[1]);
-      }
-
-      return map;
-    }
-
-    if (data instanceof Map) {
-      return (Map) data;
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public int getMapSize(final Object data) {
-    if (data == null) {
-      return -1;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
-
-      if (mapContainer == null || mapContainer.length == 0) {
-        return -1;
-      } else {
-        return ((ArrayWritable) mapContainer[0]).get().length;
-      }
-    }
-
-    if (data instanceof Map) {
-      return ((Map) data).size();
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public Object create() {
-    Map<Object, Object> m = new HashMap<Object, Object>();
-    return m;
-  }
-
-  @Override
-  public Object put(Object map, Object key, Object value) {
-    Map<Object, Object> m = (HashMap<Object, Object>) map;
-    m.put(key, value);
-    return m;
-  }
-
-  @Override
-  public Object remove(Object map, Object key) {
-    Map<Object, Object> m = (HashMap<Object, Object>) map;
-    m.remove(key);
-    return m;
-  }
-
-  @Override
-  public Object clear(Object map) {
-    Map<Object, Object> m = (HashMap<Object, Object>) map;
-    m.clear();
-    return m;
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null) {
-      return false;
-    }
-    if (getClass() != obj.getClass()) {
-      return false;
-    }
-    final StandardParquetHiveMapInspector other = (StandardParquetHiveMapInspector) obj;
-    if (this.keyInspector != other.keyInspector && (this.keyInspector == null || !this.keyInspector.equals(other.keyInspector))) {
-      return false;
-    }
-    if (this.valueInspector != other.valueInspector && (this.valueInspector == null || !this.valueInspector.equals(other.valueInspector))) {
-      return false;
-    }
-    return true;
-  }
-
-  @Override
-  public int hashCode() {
-    int hash = 7;
-    hash = 59 * hash + (this.keyInspector != null ? this.keyInspector.hashCode() : 0);
-    hash = 59 * hash + (this.valueInspector != null ? this.valueInspector.hashCode() : 0);
-    return hash;
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java
deleted file mode 100644
index 9193eeb..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java
+++ /dev/null
@@ -1,209 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-
-import org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetPrimitiveInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.SettableStructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.StructField;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.io.ArrayWritable;
-
-/**
- *
- * The ArrayWritableObjectInspector will inspect an ArrayWritable, considering it as a Hive struct.<br />
- * It can also inspect a List if Hive decides to inspect the result of an inspection.
- *
- */
-public class ArrayWritableObjectInspector extends SettableStructObjectInspector {
-
-  private final TypeInfo typeInfo;
-  private final List<TypeInfo> fieldInfos;
-  private final List<String> fieldNames;
-  private final List<StructField> fields;
-  private final HashMap<String, StructFieldImpl> fieldsByName;
-
-  public ArrayWritableObjectInspector(final StructTypeInfo rowTypeInfo) {
-
-    typeInfo = rowTypeInfo;
-    fieldNames = rowTypeInfo.getAllStructFieldNames();
-    fieldInfos = rowTypeInfo.getAllStructFieldTypeInfos();
-    fields = new ArrayList<StructField>(fieldNames.size());
-    fieldsByName = new HashMap<String, StructFieldImpl>();
-
-    for (int i = 0; i < fieldNames.size(); ++i) {
-      final String name = fieldNames.get(i);
-      final TypeInfo fieldInfo = fieldInfos.get(i);
-
-      final StructFieldImpl field = new StructFieldImpl(name, getObjectInspector(fieldInfo), i);
-      fields.add(field);
-      fieldsByName.put(name, field);
-    }
-  }
-
-  private ObjectInspector getObjectInspector(final TypeInfo typeInfo) {
-    if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {
-      return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {
-      return PrimitiveObjectInspectorFactory.writableBooleanObjectInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {
-      return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo)) {
-      return PrimitiveObjectInspectorFactory.writableIntObjectInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {
-      return PrimitiveObjectInspectorFactory.writableLongObjectInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {
-      return ParquetPrimitiveInspectorFactory.parquetStringInspector;
-    } else if (typeInfo.getCategory().equals(Category.STRUCT)) {
-      return new ArrayWritableObjectInspector((StructTypeInfo) typeInfo);
-    } else if (typeInfo.getCategory().equals(Category.LIST)) {
-      final TypeInfo subTypeInfo = ((ListTypeInfo) typeInfo).getListElementTypeInfo();
-      return new ParquetHiveArrayInspector(getObjectInspector(subTypeInfo));
-    } else if (typeInfo.getCategory().equals(Category.MAP)) {
-      final TypeInfo keyTypeInfo = ((MapTypeInfo) typeInfo).getMapKeyTypeInfo();
-      final TypeInfo valueTypeInfo = ((MapTypeInfo) typeInfo).getMapValueTypeInfo();
-      if (keyTypeInfo.equals(TypeInfoFactory.stringTypeInfo) || keyTypeInfo.equals(TypeInfoFactory.byteTypeInfo)
-              || keyTypeInfo.equals(TypeInfoFactory.shortTypeInfo)) {
-        return new DeepParquetHiveMapInspector(getObjectInspector(keyTypeInfo), getObjectInspector(valueTypeInfo));
-      } else {
-        return new StandardParquetHiveMapInspector(getObjectInspector(keyTypeInfo), getObjectInspector(valueTypeInfo));
-      }
-    } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {
-      throw new UnsupportedOperationException("timestamp not implemented yet");
-    } else if (typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {
-      return ParquetPrimitiveInspectorFactory.parquetByteInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.shortTypeInfo)) {
-      return ParquetPrimitiveInspectorFactory.parquetShortInspector;
-    } else {
-      throw new RuntimeException("Unknown field info: " + typeInfo);
-    }
-
-  }
-
-  @Override
-  public Category getCategory() {
-    return Category.STRUCT;
-  }
-
-  @Override
-  public String getTypeName() {
-    return typeInfo.getTypeName();
-  }
-
-  @Override
-  public List<? extends StructField> getAllStructFieldRefs() {
-    return fields;
-  }
-
-  @Override
-  public Object getStructFieldData(final Object data, final StructField fieldRef) {
-    if (data == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final ArrayWritable arr = (ArrayWritable) data;
-      return arr.get()[((StructFieldImpl) fieldRef).getIndex()];
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public StructField getStructFieldRef(final String name) {
-    return fieldsByName.get(name);
-  }
-
-  @Override
-  public List<Object> getStructFieldsDataAsList(final Object data) {
-    if (data == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final ArrayWritable arr = (ArrayWritable) data;
-      final Object[] arrWritable = arr.get();
-      return new ArrayList<Object>(Arrays.asList(arrWritable));
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public Object create() {
-    final ArrayList<Object> list = new ArrayList<Object>(fields.size());
-    for (int i = 0; i < fields.size(); ++i) {
-      list.add(null);
-    }
-    return list;
-  }
-
-  @Override
-  public Object setStructFieldData(Object struct, StructField field, Object fieldValue) {
-    final ArrayList<Object> list = (ArrayList<Object>) struct;
-    list.set(((StructFieldImpl) field).getIndex(), fieldValue);
-    return list;
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null) {
-      return false;
-    }
-    if (getClass() != obj.getClass()) {
-      return false;
-    }
-    final ArrayWritableObjectInspector other = (ArrayWritableObjectInspector) obj;
-    if (this.typeInfo != other.typeInfo && (this.typeInfo == null || !this.typeInfo.equals(other.typeInfo))) {
-      return false;
-    }
-    return true;
-  }
-
-  @Override
-  public int hashCode() {
-    int hash = 5;
-    hash = 29 * hash + (this.typeInfo != null ? this.typeInfo.hashCode() : 0);
-    return hash;
-  }
-
-  class StructFieldImpl implements StructField {
-
-    private final String name;
-    private final ObjectInspector inspector;
-    private final int index;
-
-    public StructFieldImpl(final String name, final ObjectInspector inspector, final int index) {
-      this.name = name;
-      this.inspector = inspector;
-      this.index = index;
-    }
-
-    @Override
-    public String getFieldComment() {
-      return "";
-    }
-
-    @Override
-    public String getFieldName() {
-      return name;
-    }
-
-    public int getIndex() {
-      return index;
-    }
-
-    @Override
-    public ObjectInspector getFieldObjectInspector() {
-      return inspector;
-    }
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java
deleted file mode 100644
index 5dd3535..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java
+++ /dev/null
@@ -1,69 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde;
-
-import java.util.Map;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-/**
- * The DeepParquetHiveMapInspector will inspect an ArrayWritable, considering it as a Hive map.<br />
- * It can also inspect a Map if Hive decides to inspect the result of an inspection.<br />
- * When trying to access elements from the map it will iterate over all keys, inspecting them and comparing them to the
- * desired key.
- *
- */
-public class DeepParquetHiveMapInspector extends AbstractParquetMapInspector {
-
-  public DeepParquetHiveMapInspector(final ObjectInspector keyInspector, final ObjectInspector valueInspector) {
-    super(keyInspector, valueInspector);
-  }
-
-  @Override
-  public Object getMapValueElement(final Object data, final Object key) {
-    if (data == null || key == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
-
-      if (mapContainer == null || mapContainer.length == 0) {
-        return null;
-      }
-
-      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
-
-      for (final Writable obj : mapArray) {
-        final ArrayWritable mapObj = (ArrayWritable) obj;
-        final Writable[] arr = mapObj.get();
-        if (key.equals(arr[0]) || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveJavaObject(arr[0]))
-                || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveWritableObject(arr[0]))) {
-          return arr[1];
-        }
-      }
-
-      return null;
-    }
-
-    if (data instanceof Map) {
-      final Map<?, ?> map = (Map<?, ?>) data;
-
-      if (map.containsKey(key)) {
-        return map.get(key);
-      }
-
-      for (final Map.Entry<?, ?> entry : map.entrySet()) {
-        if (key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveJavaObject(entry.getKey()))
-                || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveWritableObject(entry.getKey()))) {
-          return entry.getValue();
-        }
-      }
-
-      return null;
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java
deleted file mode 100644
index 5d02b33..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java
+++ /dev/null
@@ -1,172 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.SettableListObjectInspector;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-/**
- * The ParquetHiveArrayInspector will inspect an ArrayWritable, considering it as an Hive array.<br />
- * It can also inspect a List if Hive decides to inspect the result of an inspection.
- *
- */
-public class ParquetHiveArrayInspector implements SettableListObjectInspector {
-
-  ObjectInspector arrayElementInspector;
-
-  public ParquetHiveArrayInspector(final ObjectInspector arrayElementInspector) {
-    this.arrayElementInspector = arrayElementInspector;
-  }
-
-  @Override
-  public String getTypeName() {
-    return "array<" + arrayElementInspector.getTypeName() + ">";
-  }
-
-  @Override
-  public Category getCategory() {
-    return Category.LIST;
-  }
-
-  @Override
-  public ObjectInspector getListElementObjectInspector() {
-    return arrayElementInspector;
-  }
-
-  @Override
-  public Object getListElement(final Object data, final int index) {
-    if (data == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] listContainer = ((ArrayWritable) data).get();
-
-      if (listContainer == null || listContainer.length == 0) {
-        return null;
-      }
-
-      final Writable subObj = listContainer[0];
-
-      if (subObj == null) {
-        return null;
-      }
-
-      if (index >= 0 && index < ((ArrayWritable) subObj).get().length) {
-        return ((ArrayWritable) subObj).get()[index];
-      } else {
-        return null;
-      }
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public int getListLength(final Object data) {
-    if (data == null) {
-      return -1;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] listContainer = ((ArrayWritable) data).get();
-
-      if (listContainer == null || listContainer.length == 0) {
-        return -1;
-      }
-
-      final Writable subObj = listContainer[0];
-
-      if (subObj == null) {
-        return 0;
-      }
-
-      return ((ArrayWritable) subObj).get().length;
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public List<?> getList(final Object data) {
-    if (data == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] listContainer = ((ArrayWritable) data).get();
-
-      if (listContainer == null || listContainer.length == 0) {
-        return null;
-      }
-
-      final Writable subObj = listContainer[0];
-
-      if (subObj == null) {
-        return null;
-      }
-
-      final Writable[] array = ((ArrayWritable) subObj).get();
-      final List<Writable> list = new ArrayList<Writable>();
-
-      for (final Writable obj : array) {
-        list.add(obj);
-      }
-
-      return list;
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public Object create(final int size) {
-    final ArrayList<Object> result = new ArrayList<Object>(size);
-    for (int i = 0; i < size; ++i) {
-      result.add(null);
-    }
-    return result;
-  }
-
-  @Override
-  public Object set(final Object list, final int index, final Object element) {
-    final ArrayList l = (ArrayList) list;
-    l.set(index, element);
-    return list;
-  }
-
-  @Override
-  public Object resize(final Object list, final int newSize) {
-    final ArrayList l = (ArrayList) list;
-    l.ensureCapacity(newSize);
-    while (l.size() < newSize) {
-      l.add(null);
-    }
-    while (l.size() > newSize) {
-      l.remove(l.size() - 1);
-    }
-    return list;
-  }
-
-  @Override
-  public boolean equals(final Object o) {
-    if (o == null || o.getClass() != getClass()) {
-      return false;
-    } else if (o == this) {
-      return true;
-    } else {
-      final ObjectInspector other = ((ParquetHiveArrayInspector) o).arrayElementInspector;
-      return other.equals(arrayElementInspector);
-    }
-  }
-
-  @Override
-  public int hashCode() {
-    int hash = 3;
-    hash = 29 * hash + (this.arrayElementInspector != null ? this.arrayElementInspector.hashCode() : 0);
-    return hash;
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
deleted file mode 100644
index 9647d67..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
+++ /dev/null
@@ -1,272 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Properties;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable;
-import org.apache.hadoop.hive.serde2.SerDe;
-import org.apache.hadoop.hive.serde2.SerDeException;
-import org.apache.hadoop.hive.serde2.SerDeStats;
-import org.apache.hadoop.hive.serde2.io.ByteWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.StructField;
-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
-import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.BooleanWritable;
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-
-import parquet.io.api.Binary;
-
-/**
- *
- * A ParquetHiveSerDe for Hive (with the deprecated package mapred)
- *
- */
-public class ParquetHiveSerDe implements SerDe {
-
-  public static Text MAP_KEY = new Text("key");
-  public static Text MAP_VALUE = new Text("value");
-  public static Text MAP = new Text("map");
-  public static Text ARRAY = new Text("bag");
-  private SerDeStats stats;
-  ObjectInspector objInspector;
-
-  private enum LAST_OPERATION {
-
-    SERIALIZE,
-    DESERIALIZE,
-    UNKNOWN
-  }
-  LAST_OPERATION status;
-  private long serializedSize;
-  private long deserializedSize;
-
-  @Override
-  final public void initialize(final Configuration conf, final Properties tbl) throws SerDeException {
-
-    final TypeInfo rowTypeInfo;
-    final List<String> columnNames;
-    final List<TypeInfo> columnTypes;
-    // Get column names and sort order
-    final String columnNameProperty = tbl.getProperty("columns");
-    final String columnTypeProperty = tbl.getProperty("columns.types");
-
-    if (columnNameProperty.length() == 0) {
-      columnNames = new ArrayList<String>();
-    } else {
-      columnNames = Arrays.asList(columnNameProperty.split(","));
-    }
-
-    if (columnTypeProperty.length() == 0) {
-      columnTypes = new ArrayList<TypeInfo>();
-    } else {
-      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);
-    }
-
-    if (columnNames.size() != columnTypes.size()) {
-      throw new RuntimeException("ParquetHiveSerde initialization failed. Number of column name and column type differs.");
-    }
-
-    // Create row related objects
-    rowTypeInfo = TypeInfoFactory.getStructTypeInfo(columnNames, columnTypes);
-    this.objInspector = new ArrayWritableObjectInspector((StructTypeInfo) rowTypeInfo);
-
-    // Stats part
-    stats = new SerDeStats();
-    serializedSize = 0;
-    deserializedSize = 0;
-    status = LAST_OPERATION.UNKNOWN;
-  }
-
-  @Override
-  public Object deserialize(final Writable blob) throws SerDeException {
-    status = LAST_OPERATION.DESERIALIZE;
-    deserializedSize = 0;
-    if (blob instanceof ArrayWritable) {
-      deserializedSize = ((ArrayWritable) blob).get().length;
-      return blob;
-    } else {
-      return null;
-    }
-  }
-
-  @Override
-  public ObjectInspector getObjectInspector() throws SerDeException {
-    return objInspector;
-  }
-
-  @Override
-  public Class<? extends Writable> getSerializedClass() {
-    return ArrayWritable.class;
-  }
-
-  @Override
-  public Writable serialize(final Object obj, final ObjectInspector objInspector) throws SerDeException {
-    if (!objInspector.getCategory().equals(Category.STRUCT)) {
-      throw new SerDeException("Cannot serialize " + objInspector.getCategory() + ". Can only serialize a struct");
-    }
-
-    final ArrayWritable serializeData = createStruct(obj, (StructObjectInspector) objInspector);
-
-    serializedSize = serializeData.get().length;
-    status = LAST_OPERATION.SERIALIZE;
-
-    return serializeData;
-  }
-
-  private ArrayWritable createStruct(final Object obj, final StructObjectInspector inspector) throws SerDeException {
-
-    final List<? extends StructField> fields = inspector.getAllStructFieldRefs();
-    final Writable[] arr = new Writable[fields.size()];
-
-    int i = 0;
-
-    for (final StructField field : fields) {
-      final Object subObj = inspector.getStructFieldData(obj, field);
-      final ObjectInspector subInspector = field.getFieldObjectInspector();
-
-      arr[i] = createObject(subObj, subInspector);
-      ++i;
-    }
-
-    return new ArrayWritable(Writable.class, arr);
-
-  }
-
-  private Writable createMap(final Object obj, final MapObjectInspector inspector) throws SerDeException {
-    final Map<?, ?> sourceMap = inspector.getMap(obj);
-    final ObjectInspector keyInspector = inspector.getMapKeyObjectInspector();
-    final ObjectInspector valueInspector = inspector.getMapValueObjectInspector();
-    final List<ArrayWritable> array = new ArrayList<ArrayWritable>();
-
-    if (sourceMap != null) {
-      for (final Entry<?, ?> keyValue : sourceMap.entrySet()) {
-        final Writable key = createObject(keyValue.getKey(), keyInspector);
-        final Writable value = createObject(keyValue.getValue(), valueInspector);
-
-        if (key != null) {
-          Writable[] arr = new Writable[2];
-          arr[0] = key;
-          arr[1] = value;
-          array.add(new ArrayWritable(Writable.class, arr));
-        }
-
-      }
-    }
-
-    if (array.size() > 0) {
-      final ArrayWritable subArray = new ArrayWritable(ArrayWritable.class, array.toArray(new ArrayWritable[array.size()]));
-      return new ArrayWritable(Writable.class, new Writable[] {subArray});
-    } else {
-      return null;
-    }
-  }
-
-  private ArrayWritable createArray(final Object obj, final ListObjectInspector inspector) throws SerDeException {
-    final List<?> sourceArray = inspector.getList(obj);
-    final ObjectInspector subInspector = inspector.getListElementObjectInspector();
-    final List<Writable> array = new ArrayList<Writable>();
-
-    if (sourceArray != null) {
-      for (final Object curObj : sourceArray) {
-        final Writable newObj = createObject(curObj, subInspector);
-        if (newObj != null) {
-          array.add(newObj);
-        }
-      }
-    }
-
-    if (array.size() > 0) {
-      final ArrayWritable subArray = new ArrayWritable(array.get(0).getClass(), array.toArray(new Writable[array.size()]));
-      return new ArrayWritable(Writable.class, new Writable[] {subArray});
-    } else {
-      return null;
-    }
-  }
-
-  private Writable createPrimitive(final Object obj, final PrimitiveObjectInspector inspector) throws SerDeException {
-
-    if (obj == null) {
-      return null;
-    }
-
-    switch (inspector.getPrimitiveCategory()) {
-    case VOID:
-      return null;
-    case BOOLEAN:
-      return new BooleanWritable(((BooleanObjectInspector) inspector).get(obj) ? Boolean.TRUE : Boolean.FALSE);
-    case BYTE:
-      return new ByteWritable((byte) ((ByteObjectInspector) inspector).get(obj));
-    case DOUBLE:
-      return new DoubleWritable(((DoubleObjectInspector) inspector).get(obj));
-    case FLOAT:
-      return new FloatWritable(((FloatObjectInspector) inspector).get(obj));
-    case INT:
-      return new IntWritable(((IntObjectInspector) inspector).get(obj));
-    case LONG:
-      return new LongWritable(((LongObjectInspector) inspector).get(obj));
-    case SHORT:
-      return new ShortWritable((short) ((ShortObjectInspector) inspector).get(obj));
-    case STRING:
-      return new BinaryWritable(Binary.fromString(((StringObjectInspector) inspector).getPrimitiveJavaObject(obj)));
-    default:
-      throw new SerDeException("Unknown primitive : " + inspector.getPrimitiveCategory());
-    }
-  }
-
-  private Writable createObject(final Object obj, final ObjectInspector inspector) throws SerDeException {
-    switch (inspector.getCategory()) {
-    case STRUCT:
-      return createStruct(obj, (StructObjectInspector) inspector);
-    case LIST:
-      return createArray(obj, (ListObjectInspector) inspector);
-    case MAP:
-      return createMap(obj, (MapObjectInspector) inspector);
-    case PRIMITIVE:
-      return createPrimitive(obj, (PrimitiveObjectInspector) inspector);
-    default:
-      throw new SerDeException("Unknown data type" + inspector.getCategory());
-    }
-  }
-
-  //
-  @Override
-  public SerDeStats getSerDeStats() {
-    // must be different
-    assert (status != LAST_OPERATION.UNKNOWN);
-
-    if (status == LAST_OPERATION.SERIALIZE) {
-      stats.setRawDataSize(serializedSize);
-    } else {
-      stats.setRawDataSize(deserializedSize);
-    }
-    return stats;
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java
deleted file mode 100644
index 995662d..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java
+++ /dev/null
@@ -1,52 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde;
-
-import java.util.Map;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-/**
- * The StandardParquetHiveMapInspector will inspect an ArrayWritable, considering it as a Hive map.<br />
- * It can also inspect a Map if Hive decides to inspect the result of an inspection.
- *
- */
-public class StandardParquetHiveMapInspector extends AbstractParquetMapInspector {
-
-  public StandardParquetHiveMapInspector(final ObjectInspector keyInspector, final ObjectInspector valueInspector) {
-    super(keyInspector, valueInspector);
-  }
-
-  @Override
-  public Object getMapValueElement(final Object data, final Object key) {
-    if (data == null || key == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
-
-      if (mapContainer == null || mapContainer.length == 0) {
-        return null;
-      }
-
-      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
-
-      for (final Writable obj : mapArray) {
-        final ArrayWritable mapObj = (ArrayWritable) obj;
-        final Writable[] arr = mapObj.get();
-        if (key.equals(arr[0])) {
-          return arr[1];
-        }
-      }
-
-      return null;
-    }
-
-    if (data instanceof Map) {
-      return ((Map) data).get(key);
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetByteInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetByteInspector.java
deleted file mode 100644
index c567102..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetByteInspector.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde.primitive;
-
-import org.apache.hadoop.hive.serde2.io.ByteWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableByteObjectInspector;
-import org.apache.hadoop.io.IntWritable;
-
-/**
- * The ParquetByteInspector can inspect both ByteWritables and IntWritables into bytes.
- *
- */
-public class ParquetByteInspector extends AbstractPrimitiveJavaObjectInspector implements SettableByteObjectInspector {
-
-  ParquetByteInspector() {
-    super(TypeInfoFactory.byteTypeInfo);
-  }
-
-  @Override
-  public Object getPrimitiveWritableObject(final Object o) {
-    return o == null ? null : new ByteWritable(get(o));
-  }
-
-  @Override
-  public Object create(final byte val) {
-    return new ByteWritable(val);
-  }
-
-  @Override
-  public Object set(final Object o, final byte val) {
-    ((ByteWritable) o).set(val);
-    return o;
-  }
-
-  @Override
-  public byte get(Object o) {
-    // Accept int writables and convert them.
-    if (o instanceof IntWritable) {
-      return (byte) ((IntWritable) o).get();
-    }
-    return ((ByteWritable) o).get();
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetPrimitiveInspectorFactory.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetPrimitiveInspectorFactory.java
deleted file mode 100644
index 4973b95..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetPrimitiveInspectorFactory.java
+++ /dev/null
@@ -1,16 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde.primitive;
-
-/**
- * The ParquetPrimitiveInspectorFactory allows us to be sure that the same object is inspected by the same inspector.
- *
- */
-public class ParquetPrimitiveInspectorFactory {
-
-  public static final ParquetByteInspector parquetByteInspector = new ParquetByteInspector();
-  public static final ParquetShortInspector parquetShortInspector = new ParquetShortInspector();
-  public static final ParquetStringInspector parquetStringInspector = new ParquetStringInspector();
-
-  private ParquetPrimitiveInspectorFactory() {
-    // prevent instantiation
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetShortInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetShortInspector.java
deleted file mode 100644
index f2b1176..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetShortInspector.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde.primitive;
-
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableShortObjectInspector;
-import org.apache.hadoop.io.IntWritable;
-
-/**
- * The ParquetShortInspector can inspect both ShortWritables and IntWritables into shorts.
- *
- */
-public class ParquetShortInspector extends AbstractPrimitiveJavaObjectInspector implements SettableShortObjectInspector {
-
-  ParquetShortInspector() {
-    super(TypeInfoFactory.shortTypeInfo);
-  }
-
-  @Override
-  public Object getPrimitiveWritableObject(final Object o) {
-    return o == null ? null : new ShortWritable(get(o));
-  }
-
-  @Override
-  public Object create(final short val) {
-    return new ShortWritable(val);
-  }
-
-  @Override
-  public Object set(final Object o, final short val) {
-    ((ShortWritable) o).set(val);
-    return o;
-  }
-
-  @Override
-  public short get(Object o) {
-    // Accept int writables and convert them.
-    if (o instanceof IntWritable) {
-      return (short) ((IntWritable) o).get();
-    }
-    return ((ShortWritable) o).get();
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetStringInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetStringInspector.java
deleted file mode 100644
index 869958e..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetStringInspector.java
+++ /dev/null
@@ -1,85 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde.primitive;
-
-import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableStringObjectInspector;
-import org.apache.hadoop.io.Text;
-
-import parquet.io.api.Binary;
-
-/**
- * The ParquetStringInspector inspects a BinaryWritable to give a Text or String.
- *
- */
-public class ParquetStringInspector extends AbstractPrimitiveJavaObjectInspector implements SettableStringObjectInspector {
-
-  ParquetStringInspector() {
-    super(TypeInfoFactory.stringTypeInfo);
-  }
-
-  @Override
-  public Text getPrimitiveWritableObject(final Object o) {
-    if (o == null) {
-      return null;
-    }
-
-    if (o instanceof BinaryWritable) {
-      return new Text(((BinaryWritable) o).getBytes());
-    }
-
-    if (o instanceof Text) {
-      return (Text) o;
-    }
-
-    if (o instanceof String) {
-      return new Text((String) o);
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + o.getClass().getCanonicalName());
-  }
-
-  @Override
-  public String getPrimitiveJavaObject(final Object o) {
-    if (o == null) {
-      return null;
-    }
-
-    if (o instanceof BinaryWritable) {
-      return ((BinaryWritable) o).getString();
-    }
-
-    if (o instanceof Text) {
-      return ((Text) o).toString();
-    }
-
-    if (o instanceof String) {
-      return (String) o;
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + o.getClass().getCanonicalName());
-  }
-
-  @Override
-  public Object set(final Object o, final Text text) {
-    return new BinaryWritable(text == null ? null : Binary.fromByteArray(text.getBytes()));
-  }
-
-  @Override
-  public Object set(final Object o, final String string) {
-    return new BinaryWritable(string == null ? null : Binary.fromString(string));
-  }
-
-  @Override
-  public Object create(final Text text) {
-    if (text == null) {
-      return null;
-    }
-    return text.toString();
-  }
-
-  @Override
-  public Object create(final String string) {
-    return string;
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BigDecimalWritable.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BigDecimalWritable.java
deleted file mode 100644
index d4498d3..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BigDecimalWritable.java
+++ /dev/null
@@ -1,130 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.writable;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.math.BigDecimal;
-import java.math.BigInteger;
-
-import org.apache.hadoop.hive.serde2.ByteStream.Output;
-import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils;
-import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.VInt;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.io.WritableUtils;
-
-/**
- * This file is taken from a patch to hive 0.11
- * Issue : https://issues.apache.org/jira/browse/HIVE-2693
- *
- */
-public class BigDecimalWritable implements WritableComparable<BigDecimalWritable> {
-
-  private byte[] internalStorage = new byte[0];
-  private int scale;
-
-  private final VInt vInt = new VInt(); // reusable integer
-
-  public BigDecimalWritable() {
-  }
-
-  public BigDecimalWritable(final byte[] bytes, final int scale) {
-    set(bytes, scale);
-  }
-
-  public BigDecimalWritable(final BigDecimalWritable writable) {
-    set(writable.getBigDecimal());
-  }
-
-  public BigDecimalWritable(final BigDecimal value) {
-    set(value);
-  }
-
-  public void set(BigDecimal value) {
-    value = value.stripTrailingZeros();
-    if (value.compareTo(BigDecimal.ZERO) == 0) {
-      // Special case for 0, because java doesn't strip zeros correctly on
-      // that number.
-      value = BigDecimal.ZERO;
-    }
-    set(value.unscaledValue().toByteArray(), value.scale());
-  }
-
-  public void set(final BigDecimalWritable writable) {
-    set(writable.getBigDecimal());
-  }
-
-  public void set(final byte[] bytes, final int scale) {
-    this.internalStorage = bytes;
-    this.scale = scale;
-  }
-
-  public void setFromBytes(final byte[] bytes, int offset, final int length) {
-    LazyBinaryUtils.readVInt(bytes, offset, vInt);
-    scale = vInt.value;
-    offset += vInt.length;
-    LazyBinaryUtils.readVInt(bytes, offset, vInt);
-    offset += vInt.length;
-    if (internalStorage.length != vInt.value) {
-      internalStorage = new byte[vInt.value];
-    }
-    System.arraycopy(bytes, offset, internalStorage, 0, vInt.value);
-  }
-
-  public BigDecimal getBigDecimal() {
-    return new BigDecimal(new BigInteger(internalStorage), scale);
-  }
-
-  @Override
-  public void readFields(final DataInput in) throws IOException {
-    scale = WritableUtils.readVInt(in);
-    final int byteArrayLen = WritableUtils.readVInt(in);
-    if (internalStorage.length != byteArrayLen) {
-      internalStorage = new byte[byteArrayLen];
-    }
-    in.readFully(internalStorage);
-  }
-
-  @Override
-  public void write(final DataOutput out) throws IOException {
-    WritableUtils.writeVInt(out, scale);
-    WritableUtils.writeVInt(out, internalStorage.length);
-    out.write(internalStorage);
-  }
-
-  @Override
-  public int compareTo(final BigDecimalWritable that) {
-    return getBigDecimal().compareTo(that.getBigDecimal());
-  }
-
-  public void writeToByteStream(final Output byteStream) {
-    LazyBinaryUtils.writeVInt(byteStream, scale);
-    LazyBinaryUtils.writeVInt(byteStream, internalStorage.length);
-    byteStream.write(internalStorage, 0, internalStorage.length);
-  }
-
-  @Override
-  public String toString() {
-    return getBigDecimal().toString();
-  }
-
-  @Override
-  public boolean equals(final Object other) {
-    if (other == null || !(other instanceof BigDecimalWritable)) {
-      return false;
-    }
-    final BigDecimalWritable bdw = (BigDecimalWritable) other;
-
-    // 'equals' and 'compareTo' are not compatible with BigDecimals. We want
-    // compareTo which returns true iff the numbers are equal (e.g.: 3.14 is
-        // the same as 3.140). 'Equals' returns true iff equal and the same
-    // scale
-    // is set in the decimals (e.g.: 3.14 is not the same as 3.140)
-    return getBigDecimal().compareTo(bdw.getBigDecimal()) == 0;
-  }
-
-  @Override
-  public int hashCode() {
-    return getBigDecimal().hashCode();
-  }
-
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BinaryWritable.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BinaryWritable.java
deleted file mode 100644
index dd92c81..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BinaryWritable.java
+++ /dev/null
@@ -1,80 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.writable;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.Writable;
-
-import parquet.io.api.Binary;
-
-/**
- *
- * A Wrapper to support constructor with Binary and String
- *
- * TODO : remove it, and call BytesWritable with the getBytes()
- *
- */
-public class BinaryWritable implements Writable {
-
-  private Binary binary;
-
-  public BinaryWritable(final Binary binary) {
-    this.binary = binary;
-  }
-
-  public Binary getBinary() {
-    return binary;
-  }
-
-  public byte[] getBytes() {
-    return binary.getBytes();
-  }
-
-  public String getString() {
-    return binary.toStringUsingUTF8();
-  }
-
-  @Override
-  public void readFields(DataInput input) throws IOException {
-    byte[] bytes = new byte[input.readInt()];
-    input.readFully(bytes);
-    binary = Binary.fromByteArray(bytes);
-  }
-
-  @Override
-  public void write(DataOutput output) throws IOException {
-    output.writeInt(binary.length());
-    binary.writeTo(output);
-  }
-
-  @Override
-  public int hashCode() {
-    return binary == null ? 0 : binary.hashCode();
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (obj instanceof BinaryWritable) {
-      final BinaryWritable other = (BinaryWritable)obj;
-      return binary.equals(other.binary);
-    }
-    return false;
-  }
-
-  public static class DicBinaryWritable extends BinaryWritable {
-
-    private final String string;
-
-    public DicBinaryWritable(Binary binary, String string) {
-      super(binary);
-      this.string = string;
-    }
-
-    @Override
-    public String getString() {
-      return string;
-    }
-  }
-
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriteSupport.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriteSupport.java
deleted file mode 100644
index 00ec229..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriteSupport.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.write;
-
-import java.util.HashMap;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.ArrayWritable;
-
-import parquet.hadoop.api.WriteSupport;
-import parquet.io.api.RecordConsumer;
-import parquet.schema.MessageType;
-import parquet.schema.MessageTypeParser;
-
-/**
- *
- * DataWritableWriteSupport is a WriteSupport for the DataWritableWriter
- *
- */
-public class DataWritableWriteSupport extends WriteSupport<ArrayWritable> {
-
-  public static final String PARQUET_HIVE_SCHEMA = "parquet.hive.schema";
-
-  public static void setSchema(final MessageType schema, final Configuration configuration) {
-    configuration.set(PARQUET_HIVE_SCHEMA, schema.toString());
-  }
-
-  public static MessageType getSchema(final Configuration configuration) {
-    return MessageTypeParser.parseMessageType(configuration.get(PARQUET_HIVE_SCHEMA));
-  }
-  private DataWritableWriter writer;
-  private MessageType schema;
-
-  @Override
-  public WriteContext init(final Configuration configuration) {
-    schema = getSchema(configuration);
-    return new WriteContext(schema, new HashMap<String, String>());
-  }
-
-  @Override
-  public void prepareForWrite(final RecordConsumer recordConsumer) {
-    writer = new DataWritableWriter(recordConsumer, schema);
-  }
-
-  @Override
-  public void write(final ArrayWritable record) {
-    writer.write(record);
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java
deleted file mode 100644
index 4bc2379..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java
+++ /dev/null
@@ -1,147 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.write;
-
-import org.apache.hadoop.hive.ql.io.parquet.writable.BigDecimalWritable;
-import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable;
-import org.apache.hadoop.hive.serde2.io.ByteWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.BooleanWritable;
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Writable;
-
-import parquet.io.ParquetEncodingException;
-import parquet.io.api.RecordConsumer;
-import parquet.schema.GroupType;
-import parquet.schema.Type;
-
-/**
- *
- * DataWritableWriter is a writer,
- * that will read an ArrayWritable and give the data to parquet
- * with the expected schema
- *
- */
-public class DataWritableWriter {
-
-  private final RecordConsumer recordConsumer;
-  private final GroupType schema;
-
-  public DataWritableWriter(final RecordConsumer recordConsumer, final GroupType schema) {
-    this.recordConsumer = recordConsumer;
-    this.schema = schema;
-  }
-
-  public void write(final ArrayWritable arr) {
-
-    if (arr == null) {
-      return;
-    }
-    recordConsumer.startMessage();
-    writeData(arr, schema);
-    recordConsumer.endMessage();
-  }
-
-  private void writeData(final ArrayWritable arr, final GroupType type) {
-
-    if (arr == null) {
-      return;
-    }
-
-    final int fieldCount = type.getFieldCount();
-    Writable[] values = arr.get();
-    for (int field = 0; field < fieldCount; ++field) {
-      final Type fieldType = type.getType(field);
-      final String fieldName = fieldType.getName();
-      final Writable value = values[field];
-      if (value == null) {
-        continue;
-      }
-      recordConsumer.startField(fieldName, field);
-
-      if (fieldType.isPrimitive()) {
-        writePrimitive(value);
-      } else {
-        recordConsumer.startGroup();
-        if (value instanceof ArrayWritable) {
-          if (fieldType.asGroupType().getRepetition().equals(Type.Repetition.REPEATED)) {
-            writeArray((ArrayWritable) value, fieldType.asGroupType());
-          } else {
-            writeData((ArrayWritable) value, fieldType.asGroupType());
-          }
-        } else if (value != null) {
-          throw new ParquetEncodingException("This should be an ArrayWritable or MapWritable: " + value);
-        }
-
-        recordConsumer.endGroup();
-      }
-
-      recordConsumer.endField(fieldName, field);
-    }
-  }
-
-  private void writeArray(final ArrayWritable array, final GroupType type) {
-    if (array == null) {
-      return;
-    }
-
-    final Writable[] subValues = array.get();
-
-    final int fieldCount = type.getFieldCount();
-    for (int field = 0; field < fieldCount; ++field) {
-      final Type subType = type.getType(field);
-      recordConsumer.startField(subType.getName(), field);
-      for (int i = 0; i < subValues.length; ++i) {
-        final Writable subValue = subValues[i];
-        if (subValue != null) {
-          if (subType.isPrimitive()) {
-            if (subValue instanceof ArrayWritable) {
-              writePrimitive(((ArrayWritable) subValue).get()[field]);// 0 ?
-            } else {
-              writePrimitive(subValue);
-            }
-          } else {
-            if (!(subValue instanceof ArrayWritable)) {
-              throw new RuntimeException("This should be a ArrayWritable: " + subValue);
-            } else {
-              recordConsumer.startGroup();
-              writeData((ArrayWritable) subValue, subType.asGroupType());
-              recordConsumer.endGroup();
-            }
-          }
-        }
-      }
-      recordConsumer.endField(subType.getName(), field);
-    }
-
-  }
-
-  private void writePrimitive(final Writable value) {
-    if (value == null) {
-      return;
-    }
-    if (value instanceof DoubleWritable) {
-      recordConsumer.addDouble(((DoubleWritable) value).get());
-    } else if (value instanceof BooleanWritable) {
-      recordConsumer.addBoolean(((BooleanWritable) value).get());
-    } else if (value instanceof FloatWritable) {
-      recordConsumer.addFloat(((FloatWritable) value).get());
-    } else if (value instanceof IntWritable) {
-      recordConsumer.addInteger(((IntWritable) value).get());
-    } else if (value instanceof LongWritable) {
-      recordConsumer.addLong(((LongWritable) value).get());
-    } else if (value instanceof ShortWritable) {
-      recordConsumer.addInteger(((ShortWritable) value).get());
-    } else if (value instanceof ByteWritable) {
-      recordConsumer.addInteger(((ByteWritable) value).get());
-    } else if (value instanceof BigDecimalWritable) {
-      throw new UnsupportedOperationException("BigDecimal writing not implemented");
-    } else if (value instanceof BinaryWritable) {
-      recordConsumer.addBinary(((BinaryWritable) value).getBinary());
-    } else {
-      throw new RuntimeException("Unknown value type: " + value + " " + value.getClass());
-    }
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java
deleted file mode 100644
index 1219975..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java
+++ /dev/null
@@ -1,81 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.write;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordWriter;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapreduce.OutputFormat;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.hive.ql.io.FSRecordWriter;
-
-import parquet.hadoop.ParquetOutputFormat;
-import parquet.hadoop.util.ContextUtil;
-
-public class ParquetRecordWriterWrapper implements RecordWriter<Void, ArrayWritable>,
-  FSRecordWriter {
-
-  public static final Log LOG = LogFactory.getLog(ParquetRecordWriterWrapper.class);
-
-  private final org.apache.hadoop.mapreduce.RecordWriter<Void, ArrayWritable> realWriter;
-  private TaskAttemptContext taskContext;
-
-  public ParquetRecordWriterWrapper(
-      final OutputFormat<Void, ArrayWritable> realOutputFormat,
-      final JobConf jobConf,
-      final String name,
-      final Progressable progress) throws IOException {
-    try {
-      // create a TaskInputOutputContext
-      TaskAttemptID taskAttemptID = TaskAttemptID.forName(jobConf.get("mapred.task.id"));
-      if (taskAttemptID == null) {
-        taskAttemptID = new TaskAttemptID();
-      }
-      taskContext = ContextUtil.newTaskAttemptContext(jobConf, taskAttemptID);
-
-      LOG.info("creating real writer to write at " + name);
-      realWriter = (org.apache.hadoop.mapreduce.RecordWriter<Void, ArrayWritable>) ((ParquetOutputFormat) realOutputFormat)
-          .getRecordWriter(taskContext, new Path(name));
-      LOG.info("real writer: " + realWriter);
-    } catch (final InterruptedException e) {
-      throw new IOException(e);
-    }
-  }
-
-  @Override
-  public void close(final Reporter reporter) throws IOException {
-    try {
-      realWriter.close(taskContext);
-    } catch (final InterruptedException e) {
-      throw new IOException(e);
-    }
-  }
-
-  @Override
-  public void write(final Void key, final ArrayWritable value) throws IOException {
-    try {
-      realWriter.write(key, value);
-    } catch (final InterruptedException e) {
-      throw new IOException(e);
-    }
-  }
-
-  @Override
-  public void close(final boolean abort) throws IOException {
-    close(null);
-  }
-
-  @Override
-  public void write(final Writable w) throws IOException {
-    write(null, (ArrayWritable) w);
-  }
-
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index 66acb37..61fa04b 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -54,9 +54,6 @@
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
 import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;
 import org.apache.hadoop.hive.ql.io.orc.OrcSerde;
-import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;
-import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;
-import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;
 import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -131,10 +128,6 @@
   protected static final String ORCFILE_SERDE = OrcSerde.class
       .getName();
 
-  protected static final String PARQUETFILE_INPUT = MapredParquetInputFormat.class.getName();
-  protected static final String PARQUETFILE_OUTPUT = MapredParquetOutputFormat.class.getName();
-  protected static final String PARQUETFILE_SERDE = ParquetHiveSerDe.class.getName();
-
   class RowFormatParams {
     String fieldDelim = null;
     String fieldEscape = null;
@@ -222,12 +215,6 @@ protected boolean fillStorageFormat(ASTNode child, AnalyzeCreateCommonVars share
         shared.serde = ORCFILE_SERDE;
         storageFormat = true;
         break;
-      case HiveParser.TOK_TBLPARQUETFILE:
-        inputFormat = PARQUETFILE_INPUT;
-        outputFormat = PARQUETFILE_OUTPUT;
-        shared.serde = PARQUETFILE_SERDE;
-        storageFormat = true;
-        break;
       case HiveParser.TOK_TABLEFILEFORMAT:
         inputFormat = unescapeSQLString(child.getChild(0).getText());
         outputFormat = unescapeSQLString(child.getChild(1).getText());
@@ -259,10 +246,6 @@ protected void fillDefaultStorageFormat(AnalyzeCreateCommonVars shared) {
           inputFormat = ORCFILE_INPUT;
           outputFormat = ORCFILE_OUTPUT;
           shared.serde = ORCFILE_SERDE;
-        } else if ("PARQUET".equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.HIVEDEFAULTFILEFORMAT))) {
-          inputFormat = PARQUETFILE_INPUT;
-          outputFormat = PARQUETFILE_OUTPUT;
-          shared.serde = PARQUETFILE_SERDE;
         } else {
           inputFormat = TEXTFILE_INPUT;
           outputFormat = TEXTFILE_OUTPUT;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
index 923b93c..ed9917d 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
@@ -135,7 +135,6 @@ KW_SEQUENCEFILE: 'SEQUENCEFILE';
 KW_TEXTFILE: 'TEXTFILE';
 KW_RCFILE: 'RCFILE';
 KW_ORCFILE: 'ORC';
-KW_PARQUETFILE: 'PARQUET';
 KW_INPUTFORMAT: 'INPUTFORMAT';
 KW_OUTPUTFORMAT: 'OUTPUTFORMAT';
 KW_INPUTDRIVER: 'INPUTDRIVER';
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
index c1a36fb..0875c23 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
@@ -180,7 +180,6 @@ TOK_TABLEROWFORMATMAPKEYS;
 TOK_TABLEROWFORMATLINES;
 TOK_TABLEROWFORMATNULL;
 TOK_TBLORCFILE;
-TOK_TBLPARQUETFILE;
 TOK_TBLSEQUENCEFILE;
 TOK_TBLTEXTFILE;
 TOK_TBLRCFILE;
@@ -1177,7 +1176,6 @@ fileFormat
     | KW_TEXTFILE  -> ^(TOK_TBLTEXTFILE)
     | KW_RCFILE  -> ^(TOK_TBLRCFILE)
     | KW_ORCFILE -> ^(TOK_TBLORCFILE)
-    | KW_PARQUETFILE -> ^(TOK_TBLPARQUETFILE)
     | KW_INPUTFORMAT inFmt=StringLiteral KW_OUTPUTFORMAT outFmt=StringLiteral (KW_INPUTDRIVER inDriver=StringLiteral KW_OUTPUTDRIVER outDriver=StringLiteral)?
       -> ^(TOK_TABLEFILEFORMAT $inFmt $outFmt $inDriver? $outDriver?)
     | genericSpec=identifier -> ^(TOK_FILEFORMAT_GENERIC $genericSpec)
@@ -1618,7 +1616,6 @@ tableFileFormat
       | KW_STORED KW_AS KW_TEXTFILE  -> TOK_TBLTEXTFILE
       | KW_STORED KW_AS KW_RCFILE  -> TOK_TBLRCFILE
       | KW_STORED KW_AS KW_ORCFILE -> TOK_TBLORCFILE
-      | KW_STORED KW_AS KW_PARQUETFILE -> TOK_TBLPARQUETFILE
       | KW_STORED KW_AS KW_INPUTFORMAT inFmt=StringLiteral KW_OUTPUTFORMAT outFmt=StringLiteral (KW_INPUTDRIVER inDriver=StringLiteral KW_OUTPUTDRIVER outDriver=StringLiteral)?
       -> ^(TOK_TABLEFILEFORMAT $inFmt $outFmt $inDriver? $outDriver?)
       | KW_STORED KW_BY storageHandler=StringLiteral
diff --git a/src/ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java b/src/ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java
deleted file mode 100644
index 1470957..0000000
--- a/src/ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;
-import org.apache.hadoop.io.ArrayWritable;
-
-import parquet.hadoop.ParquetInputFormat;
-
-/**
- * Deprecated name of the parquet-hive input format. This class exists
- * simply to provide backwards compatibility with users who specified
- * this name in the Hive metastore. All users should now use
- * {@link MapredParquetInputFormat MapredParquetInputFormat}
- */
-@Deprecated
-public class DeprecatedParquetInputFormat extends MapredParquetInputFormat {
-
-  public DeprecatedParquetInputFormat() {
-    super();
-  }
-
-  public DeprecatedParquetInputFormat(final ParquetInputFormat<ArrayWritable> realInputFormat) {
-    super(realInputFormat);
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java b/src/ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java
deleted file mode 100644
index 2063702..0000000
--- a/src/ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.mapreduce.OutputFormat;
-
-/**
- * Deprecated name of the parquet-hive output format. This class exists
- * simply to provide backwards compatibility with users who specified
- * this name in the Hive metastore. All users should now use
- * {@link MapredParquetOutputFormat MapredParquetOutputFormat}
- */
-@Deprecated
-public class DeprecatedParquetOutputFormat extends MapredParquetOutputFormat {
-
-  public DeprecatedParquetOutputFormat() {
-    super();
-  }
-
-  public DeprecatedParquetOutputFormat(final OutputFormat<Void, ArrayWritable> mapreduceOutputFormat) {
-    super(mapreduceOutputFormat);
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/MapredParquetInputFormat.java b/src/ql/src/java/parquet/hive/MapredParquetInputFormat.java
deleted file mode 100644
index cbdc750..0000000
--- a/src/ql/src/java/parquet/hive/MapredParquetInputFormat.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import org.apache.hadoop.io.ArrayWritable;
-
-import parquet.hadoop.ParquetInputFormat;
-
-/**
- * Deprecated name of the parquet-hive input format. This class exists
- * simply to provide backwards compatibility with users who specified
- * this name in the Hive metastore. All users should now use
- * {@link org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat MapredParquetInputFormat}
- */
-@Deprecated
-public class MapredParquetInputFormat extends org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat {
-
-  public MapredParquetInputFormat() {
-    super();
-  }
-
-  public MapredParquetInputFormat(final ParquetInputFormat<ArrayWritable> realInputFormat) {
-    super(realInputFormat);
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/MapredParquetOutputFormat.java b/src/ql/src/java/parquet/hive/MapredParquetOutputFormat.java
deleted file mode 100644
index 5ccdf70..0000000
--- a/src/ql/src/java/parquet/hive/MapredParquetOutputFormat.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.mapreduce.OutputFormat;
-
-/**
- * Deprecated name of the parquet-hive output format. This class exists
- * simply to provide backwards compatibility with users who specified
- * this name in the Hive metastore. All users should now use
- * {@link org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat MapredParquetOutputFormat}
- */
-@Deprecated
-public class MapredParquetOutputFormat extends org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat {
-
-  public MapredParquetOutputFormat () {
-    super();
-  }
-
-  public MapredParquetOutputFormat(final OutputFormat<Void, ArrayWritable> mapreduceOutputFormat) {
-    super(mapreduceOutputFormat);
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java
deleted file mode 100644
index f3657cc..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java
+++ /dev/null
@@ -1,109 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet;
-
-import static org.junit.Assert.assertEquals;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-import org.junit.Test;
-
-import parquet.schema.MessageType;
-import parquet.schema.MessageTypeParser;
-
-/**
- *
- * TestHiveSchemaConverter
- *
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- *
- */
-public class TestHiveSchemaConverter {
-
-  private List<String> createHiveColumnsFrom(final String columnNamesStr) {
-    List<String> columnNames;
-    if (columnNamesStr.length() == 0) {
-      columnNames = new ArrayList<String>();
-    } else {
-      columnNames = Arrays.asList(columnNamesStr.split(","));
-    }
-
-    return columnNames;
-  }
-
-  private List<TypeInfo> createHiveTypeInfoFrom(final String columnsTypeStr) {
-    List<TypeInfo> columnTypes;
-
-    if (columnsTypeStr.length() == 0) {
-      columnTypes = new ArrayList<TypeInfo>();
-    } else {
-      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnsTypeStr);
-    }
-
-    return columnTypes;
-  }
-
-  private void testConversion(final String columnNamesStr, final String columnsTypeStr, final String expectedSchema) throws Exception {
-    final List<String> columnNames = createHiveColumnsFrom(columnNamesStr);
-    final List<TypeInfo> columnTypes = createHiveTypeInfoFrom(columnsTypeStr);
-    final MessageType messageTypeFound = HiveSchemaConverter.convert(columnNames, columnTypes);
-    final MessageType expectedMT = MessageTypeParser.parseMessageType(expectedSchema);
-    assertEquals("converting " + columnNamesStr + ": " + columnsTypeStr + " to " + expectedSchema, expectedMT, messageTypeFound);
-  }
-
-  @Test
-  public void testSimpleType() throws Exception {
-    testConversion(
-            "a,b,c",
-            "int,double,boolean",
-            "message hive_schema {\n"
-            + "  optional int32 a;\n"
-            + "  optional double b;\n"
-            + "  optional boolean c;\n"
-            + "}\n");
-  }
-
-  @Test
-  public void testArray() throws Exception {
-    testConversion("arrayCol",
-            "array<int>",
-            "message hive_schema {\n"
-            + "  optional group arrayCol (LIST) {\n"
-            + "    repeated group bag {\n"
-            + "      optional int32 array_element;\n"
-            + "    }\n"
-            + "  }\n"
-            + "}\n");
-  }
-
-  @Test
-  public void testStruct() throws Exception {
-    testConversion("structCol",
-            "struct<a:int,b:double,c:boolean>",
-            "message hive_schema {\n"
-            + "  optional group structCol {\n"
-            + "    optional int32 a;\n"
-            + "    optional double b;\n"
-            + "    optional boolean c;\n"
-            + "  }\n"
-            + "}\n");
-  }
-
-  @Test
-  public void testMap() throws Exception {
-    testConversion("mapCol",
-            "map<string,string>",
-            "message hive_schema {\n"
-            + "  optional group mapCol (MAP) {\n"
-            + "    repeated group map (MAP_KEY_VALUE) {\n"
-            + "      required binary key;\n"
-            + "      optional binary value;\n"
-            + "    }\n"
-            + "  }\n"
-            + "}\n");
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetInputFormat.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetInputFormat.java
deleted file mode 100644
index 4bd5f43..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetInputFormat.java
+++ /dev/null
@@ -1,31 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet;
-
-import static org.mockito.Mockito.mock;
-
-import org.apache.hadoop.io.ArrayWritable;
-import org.junit.Test;
-
-import parquet.hadoop.ParquetInputFormat;
-
-/**
- *
- * Tests for MapredParquetInputFormat.
- *
- * @author Justin Coffey <j.coffey@criteo.com>
- *
- */
-public class TestMapredParquetInputFormat {
-  @Test
-  public void testDefaultConstructor() {
-    new MapredParquetInputFormat();
-  }
-
-  @SuppressWarnings("unchecked")
-  @Test
-  public void testConstructorWithParquetInputFormat() {
-    new MapredParquetInputFormat(
-        (ParquetInputFormat<ArrayWritable>) mock(ParquetInputFormat.class)
-        );
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetOutputFormat.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetOutputFormat.java
deleted file mode 100644
index 0104857..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetOutputFormat.java
+++ /dev/null
@@ -1,83 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.fail;
-import static org.mockito.Mockito.mock;
-
-import java.io.IOException;
-import java.util.Properties;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport;
-import org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.util.Progressable;
-import org.junit.Test;
-
-import parquet.hadoop.ParquetOutputFormat;
-
-/**
- * Tests for MapredParquetOutputFormat.
- *
- * @author Justin Coffey <j.coffey@criteo.com>
- *
- */
-public class TestMapredParquetOutputFormat {
-
-  @Test
-  public void testConstructor() {
-    new MapredParquetOutputFormat();
-  }
-
-  @SuppressWarnings("unchecked")
-  @Test
-  public void testConstructorWithFormat() {
-    new MapredParquetOutputFormat((ParquetOutputFormat<ArrayWritable>) mock(ParquetOutputFormat.class));
-  }
-
-  @Test
-  public void testGetRecordWriterThrowsException() {
-    try {
-      new MapredParquetOutputFormat().getRecordWriter(null, null, null, null);
-      fail("should throw runtime exception.");
-    } catch (Exception e) {
-      assertEquals("Should never be used", e.getMessage());
-    }
-  }
-
-  @SuppressWarnings("unchecked")
-  @Test
-  public void testGetHiveRecordWriter() throws IOException {
-    Properties tableProps = new Properties();
-    tableProps.setProperty("columns", "foo,bar");
-    tableProps.setProperty("columns.types", "int:int");
-
-    final Progressable mockProgress = mock(Progressable.class);
-    final ParquetOutputFormat<ArrayWritable> outputFormat = (ParquetOutputFormat<ArrayWritable>) mock(ParquetOutputFormat.class);
-
-    JobConf jobConf = new JobConf();
-
-    try {
-      new MapredParquetOutputFormat(outputFormat) {
-        @Override
-        protected ParquetRecordWriterWrapper getParquerRecordWriterWrapper(
-            ParquetOutputFormat<ArrayWritable> realOutputFormat,
-            JobConf jobConf,
-            String finalOutPath,
-            Progressable progress
-            ) throws IOException {
-          assertEquals(outputFormat, realOutputFormat);
-          assertNotNull(jobConf.get(DataWritableWriteSupport.PARQUET_HIVE_SCHEMA));
-          assertEquals("/foo", finalOutPath.toString());
-          assertEquals(mockProgress, progress);
-          throw new RuntimeException("passed tests");
-        }
-      }.getHiveRecordWriter(jobConf, new Path("/foo"), null, false, tableProps, mockProgress);
-      fail("should throw runtime exception.");
-    } catch (RuntimeException e) {
-      assertEquals("passed tests", e.getMessage());
-    }
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java
deleted file mode 100644
index 03f4fd6..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java
+++ /dev/null
@@ -1,135 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet;
-
-import java.util.Properties;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;
-import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable;
-import org.apache.hadoop.hive.serde2.SerDeException;
-import org.apache.hadoop.hive.serde2.io.ByteWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Writable;
-
-import parquet.io.api.Binary;
-
-/**
- *
- * testParquetHiveSerDe
- *
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- *
- */
-public class TestParquetSerDe extends TestCase {
-
-  public void testParquetHiveSerDe() throws Throwable {
-    try {
-      // Create the SerDe
-      System.out.println("test: testParquetHiveSerDe");
-
-      final ParquetHiveSerDe serDe = new ParquetHiveSerDe();
-      final Configuration conf = new Configuration();
-      final Properties tbl = createProperties();
-      serDe.initialize(conf, tbl);
-
-      // Data
-      final Writable[] arr = new Writable[8];
-
-      arr[0] = new ByteWritable((byte) 123);
-      arr[1] = new ShortWritable((short) 456);
-      arr[2] = new IntWritable(789);
-      arr[3] = new LongWritable(1000l);
-      arr[4] = new DoubleWritable((double) 5.3);
-      arr[5] = new BinaryWritable(Binary.fromString("hive and hadoop and parquet. Big family."));
-
-      final Writable[] mapContainer = new Writable[1];
-      final Writable[] map = new Writable[3];
-      for (int i = 0; i < 3; ++i) {
-        final Writable[] pair = new Writable[2];
-        pair[0] = new BinaryWritable(Binary.fromString("key_" + i));
-        pair[1] = new IntWritable(i);
-        map[i] = new ArrayWritable(Writable.class, pair);
-      }
-      mapContainer[0] = new ArrayWritable(Writable.class, map);
-      arr[6] = new ArrayWritable(Writable.class, mapContainer);
-
-      final Writable[] arrayContainer = new Writable[1];
-      final Writable[] array = new Writable[5];
-      for (int i = 0; i < 5; ++i) {
-        array[i] = new BinaryWritable(Binary.fromString("elem_" + i));
-      }
-      arrayContainer[0] = new ArrayWritable(Writable.class, array);
-      arr[7] = new ArrayWritable(Writable.class, arrayContainer);
-
-      final ArrayWritable arrWritable = new ArrayWritable(Writable.class, arr);
-      // Test
-      deserializeAndSerializeLazySimple(serDe, arrWritable);
-      System.out.println("test: testParquetHiveSerDe - OK");
-
-    } catch (final Throwable e) {
-      e.printStackTrace();
-      throw e;
-    }
-  }
-
-  private void deserializeAndSerializeLazySimple(final ParquetHiveSerDe serDe, final ArrayWritable t) throws SerDeException {
-
-    // Get the row structure
-    final StructObjectInspector oi = (StructObjectInspector) serDe.getObjectInspector();
-
-    // Deserialize
-    final Object row = serDe.deserialize(t);
-    assertEquals("deserialization gives the wrong object class", row.getClass(), ArrayWritable.class);
-    assertEquals("size correct after deserialization", serDe.getSerDeStats().getRawDataSize(), t.get().length);
-    assertEquals("deserialization gives the wrong object", t, row);
-
-    // Serialize
-    final ArrayWritable serializedArr = (ArrayWritable) serDe.serialize(row, oi);
-    assertEquals("size correct after serialization", serDe.getSerDeStats().getRawDataSize(), serializedArr.get().length);
-    assertTrue("serialized object should be equal to starting object", arrayWritableEquals(t, serializedArr));
-  }
-
-  private Properties createProperties() {
-    final Properties tbl = new Properties();
-
-    // Set the configuration parameters
-    tbl.setProperty("columns", "abyte,ashort,aint,along,adouble,astring,amap,alist");
-    tbl.setProperty("columns.types", "tinyint:smallint:int:bigint:double:string:map<string,int>:array<string>");
-    tbl.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_FORMAT, "NULL");
-    return tbl;
-  }
-
-  public static boolean arrayWritableEquals(final ArrayWritable a1, final ArrayWritable a2) {
-    final Writable[] a1Arr = a1.get();
-    final Writable[] a2Arr = a2.get();
-
-    if (a1Arr.length != a2Arr.length) {
-      return false;
-    }
-
-    for (int i = 0; i < a1Arr.length; ++i) {
-      if (a1Arr[i] instanceof ArrayWritable) {
-        if (!(a2Arr[i] instanceof ArrayWritable)) {
-          return false;
-        }
-        if (!arrayWritableEquals((ArrayWritable) a1Arr[i], (ArrayWritable) a2Arr[i])) {
-          return false;
-        }
-      } else {
-        if (!a1Arr[i].equals(a2Arr[i])) {
-          return false;
-        }
-      }
-
-    }
-    return true;
-  }
-
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java
deleted file mode 100644
index 66c5da6..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java
+++ /dev/null
@@ -1,89 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Writable;
-import org.junit.Test;
-
-/**
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class TestAbstractParquetMapInspector extends TestCase {
-
-  class TestableAbstractParquetMapInspector extends AbstractParquetMapInspector {
-
-    public TestableAbstractParquetMapInspector(ObjectInspector keyInspector, ObjectInspector valueInspector) {
-      super(keyInspector, valueInspector);
-    }
-
-    @Override
-    public Object getMapValueElement(Object o, Object o1) {
-      throw new UnsupportedOperationException("Should not be called");
-    }
-  }
-  private TestableAbstractParquetMapInspector inspector;
-
-  @Override
-  public void setUp() {
-    inspector = new TestableAbstractParquetMapInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector,
-            PrimitiveObjectInspectorFactory.javaIntObjectInspector);
-  }
-
-  @Test
-  public void testNullMap() {
-    assertEquals("Wrong size", -1, inspector.getMapSize(null));
-    assertNull("Should be null", inspector.getMap(null));
-  }
-
-  @Test
-  public void testNullContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);
-    assertEquals("Wrong size", -1, inspector.getMapSize(map));
-    assertNull("Should be null", inspector.getMap(map));
-  }
-
-  @Test
-  public void testEmptyContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
-    assertEquals("Wrong size", -1, inspector.getMapSize(map));
-    assertNull("Should be null", inspector.getMap(map));
-  }
-
-  @Test
-  public void testRegularMap() {
-    final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
-    final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
-
-    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
-      new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
-
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
-
-    final Map<Writable, Writable> expected = new HashMap<Writable, Writable>();
-    expected.put(new IntWritable(0), new IntWritable(1));
-    expected.put(new IntWritable(2), new IntWritable(3));
-
-    assertEquals("Wrong size", 2, inspector.getMapSize(map));
-    assertEquals("Wrong result of inspection", expected, inspector.getMap(map));
-  }
-
-  @Test
-  public void testHashMap() {
-    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
-    map.put(new IntWritable(0), new IntWritable(1));
-    map.put(new IntWritable(2), new IntWritable(3));
-    map.put(new IntWritable(4), new IntWritable(5));
-    map.put(new IntWritable(6), new IntWritable(7));
-
-    assertEquals("Wrong size", 4, inspector.getMapSize(map));
-    assertEquals("Wrong result of inspection", map, inspector.getMap(map));
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java
deleted file mode 100644
index 33db935..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java
+++ /dev/null
@@ -1,81 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetPrimitiveInspectorFactory;
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Writable;
-import org.junit.Test;
-
-/**
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class TestDeepParquetHiveMapInspector extends TestCase {
-
-  private DeepParquetHiveMapInspector inspector;
-
-  @Override
-  public void setUp() {
-    inspector = new DeepParquetHiveMapInspector(ParquetPrimitiveInspectorFactory.parquetShortInspector,
-            PrimitiveObjectInspectorFactory.javaIntObjectInspector);
-  }
-
-  @Test
-  public void testNullMap() {
-    assertNull("Should be null", inspector.getMapValueElement(null, new ShortWritable((short) 0)));
-  }
-
-  @Test
-  public void testNullContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);
-    assertNull("Should be null", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-  }
-
-  @Test
-  public void testEmptyContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
-    assertNull("Should be null", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-  }
-
-  @Test
-  public void testRegularMap() {
-    final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
-    final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
-
-    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
-      new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
-
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
-
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new ShortWritable((short) 2)));
-  }
-
-  @Test
-  public void testHashMap() {
-    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
-    map.put(new IntWritable(0), new IntWritable(1));
-    map.put(new IntWritable(2), new IntWritable(3));
-    map.put(new IntWritable(4), new IntWritable(5));
-    map.put(new IntWritable(6), new IntWritable(7));
-
-
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
-    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new IntWritable(4)));
-    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new IntWritable(6)));
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new ShortWritable((short) 2)));
-    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new ShortWritable((short) 4)));
-    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new ShortWritable((short) 6)));
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java
deleted file mode 100644
index 2232659..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java
+++ /dev/null
@@ -1,71 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Writable;
-import org.junit.Test;
-
-/**
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class TestParquetHiveArrayInspector extends TestCase {
-
-  private ParquetHiveArrayInspector inspector;
-
-  @Override
-  public void setUp() {
-    inspector = new ParquetHiveArrayInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector);
-  }
-
-  @Test
-  public void testNullArray() {
-    assertEquals("Wrong size", -1, inspector.getListLength(null));
-    assertNull("Should be null", inspector.getList(null));
-    assertNull("Should be null", inspector.getListElement(null, 0));
-  }
-
-  @Test
-  public void testNullContainer() {
-    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, null);
-    assertEquals("Wrong size", -1, inspector.getListLength(list));
-    assertNull("Should be null", inspector.getList(list));
-    assertNull("Should be null", inspector.getListElement(list, 0));
-  }
-
-  @Test
-  public void testEmptyContainer() {
-    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
-    assertEquals("Wrong size", -1, inspector.getListLength(list));
-    assertNull("Should be null", inspector.getList(list));
-    assertNull("Should be null", inspector.getListElement(list, 0));
-  }
-
-  @Test
-  public void testRegularList() {
-    final ArrayWritable internalList = new ArrayWritable(Writable.class,
-            new Writable[]{new IntWritable(3), new IntWritable(5), new IntWritable(1)});
-    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, new ArrayWritable[]{internalList});
-
-    final List<Writable> expected = new ArrayList<Writable>();
-    expected.add(new IntWritable(3));
-    expected.add(new IntWritable(5));
-    expected.add(new IntWritable(1));
-
-    assertEquals("Wrong size", 3, inspector.getListLength(list));
-    assertEquals("Wrong result of inspection", expected, inspector.getList(list));
-
-    for (int i = 0; i < expected.size(); ++i) {
-      assertEquals("Wrong result of inspection", expected.get(i), inspector.getListElement(list, i));
-
-    }
-
-    assertNull("Should be null", inspector.getListElement(list, 3));
-  }
-}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java
deleted file mode 100644
index 8cccc0f..0000000
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java
+++ /dev/null
@@ -1,79 +0,0 @@
-package org.apache.hadoop.hive.ql.io.parquet.serde;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Writable;
-import org.junit.Test;
-
-/**
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class TestStandardParquetHiveMapInspector extends TestCase {
-
-  private StandardParquetHiveMapInspector inspector;
-
-  @Override
-  public void setUp() {
-    inspector = new StandardParquetHiveMapInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector,
-            PrimitiveObjectInspectorFactory.javaIntObjectInspector);
-  }
-
-  @Test
-  public void testNullMap() {
-    assertNull("Should be null", inspector.getMapValueElement(null, new IntWritable(0)));
-  }
-
-  @Test
-  public void testNullContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);
-    assertNull("Should be null", inspector.getMapValueElement(map, new IntWritable(0)));
-  }
-
-  @Test
-  public void testEmptyContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
-    assertNull("Should be null", inspector.getMapValueElement(map, new IntWritable(0)));
-  }
-
-  @Test
-  public void testRegularMap() {
-    final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
-    final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
-
-    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
-      new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
-
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
-
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 2)));
-  }
-
-  @Test
-  public void testHashMap() {
-    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
-    map.put(new IntWritable(0), new IntWritable(1));
-    map.put(new IntWritable(2), new IntWritable(3));
-    map.put(new IntWritable(4), new IntWritable(5));
-    map.put(new IntWritable(6), new IntWritable(7));
-
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
-    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new IntWritable(4)));
-    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new IntWritable(6)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 2)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 4)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 6)));
-  }
-}
-- 
1.7.0.4

