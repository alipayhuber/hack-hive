From 023e84d1a9bc0fc731461871179f1879e3234afe Mon Sep 17 00:00:00 2001
From: Ashutosh Chauhan <hashutosh@apache.org>
Date: Sat, 14 Sep 2013 02:30:57 +0000
Subject: [PATCH 017/375] HIVE-5290 : Some HCatalog tests have been behaving flaky (Brock Noland via Ashutosh Chauhan)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1523178 13f79535-47bb-0310-9956-ffa450edef68
---
 .../org/apache/hcatalog/common/HCatContext.java    |    8 +-
 .../apache/hive/hcatalog/common/HCatContext.java   |   10 ++-
 .../hive/hcatalog/mapreduce/HCatMapRedUtil.java    |    5 +-
 .../java/org/apache/hcatalog/cli/TestPermsGrp.java |    4 +-
 .../mapreduce/TestSequenceFileReadWrite.java       |   60 ++++++++------
 .../org/apache/hive/hcatalog/cli/TestPermsGrp.java |    8 +-
 .../mapreduce/TestSequenceFileReadWrite.java       |   56 ++++++++-----
 .../org/apache/hcatalog/pig/TestHCatLoader.java    |   85 ++++++++-----------
 .../hcatalog/pig/TestHCatLoaderComplexSchema.java  |   19 ++++-
 .../apache/hive/hcatalog/pig/TestE2EScenarios.java |   24 ++++--
 .../apache/hive/hcatalog/pig/TestHCatLoader.java   |   81 +++++++++----------
 .../hcatalog/pig/TestHCatLoaderComplexSchema.java  |   20 +++++-
 .../apache/hadoop/hive/shims/Hadoop20Shims.java    |    6 ++
 .../apache/hadoop/hive/shims/Hadoop20SShims.java   |    5 +
 .../apache/hadoop/hive/shims/Hadoop23Shims.java    |    5 +
 .../org/apache/hadoop/hive/shims/HadoopShims.java  |    2 +
 16 files changed, 235 insertions(+), 163 deletions(-)

diff --git a/src/hcatalog/core/src/main/java/org/apache/hcatalog/common/HCatContext.java b/src/hcatalog/core/src/main/java/org/apache/hcatalog/common/HCatContext.java
index 9254071..4d2c3cb 100644
--- a/src/hcatalog/core/src/main/java/org/apache/hcatalog/common/HCatContext.java
+++ b/src/hcatalog/core/src/main/java/org/apache/hcatalog/common/HCatContext.java
@@ -67,9 +67,11 @@ public synchronized HCatContext setConf(Configuration newConf) {
     }
 
     if (conf != newConf) {
-      for (Map.Entry<String, String> entry : conf) {
-        if ((entry.getKey().matches("hcat.*")) && (newConf.get(entry.getKey()) == null)) {
-          newConf.set(entry.getKey(), entry.getValue());
+      synchronized (conf) {
+        for (Map.Entry<String, String> entry : conf) {
+          if ((entry.getKey().matches("hcat.*")) && (newConf.get(entry.getKey()) == null)) {
+            newConf.set(entry.getKey(), entry.getValue());
+          }
         }
       }
       conf = newConf;
diff --git a/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatContext.java b/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatContext.java
index 6f5d43c..f729750 100644
--- a/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatContext.java
+++ b/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatContext.java
@@ -65,10 +65,12 @@ public synchronized HCatContext setConf(Configuration newConf) {
     }
 
     if (conf != newConf) {
-      for (Map.Entry<String, String> entry : conf) {
-        if ((entry.getKey().matches("hcat.*")) && (newConf.get(entry.getKey()) == null)) {
-          newConf.set(entry.getKey(), entry.getValue());
-        }
+      synchronized (conf) {
+        for (Map.Entry<String, String> entry : conf) {
+          if ((entry.getKey().matches("hcat.*")) && (newConf.get(entry.getKey()) == null)) {
+            newConf.set(entry.getKey(), entry.getValue());
+          }
+        }        
       }
       conf = newConf;
     }
diff --git a/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatMapRedUtil.java b/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatMapRedUtil.java
index 4057a3e..b651cb3 100644
--- a/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatMapRedUtil.java
+++ b/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatMapRedUtil.java
@@ -27,6 +27,7 @@
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.TaskAttemptContext;
 import org.apache.hadoop.mapred.TaskAttemptID;
+import org.apache.hadoop.mapreduce.JobID;
 
 public class HCatMapRedUtil {
 
@@ -43,7 +44,9 @@ public static TaskAttemptContext createTaskAttemptContext(org.apache.hadoop.mapr
   public static TaskAttemptContext createTaskAttemptContext(JobConf conf, TaskAttemptID id, Progressable progressable) {
     return ShimLoader.getHadoopShims().getHCatShim().createTaskAttemptContext(conf, id, (Reporter) progressable);
   }
-
+  public static org.apache.hadoop.mapreduce.TaskAttemptID createTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id) {
+    return ShimLoader.getHadoopShims().newTaskAttemptID(jobId, isMap, taskId, id);
+  }
   public static org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapreduce.JobContext context) {
     return createJobContext((JobConf)context.getConfiguration(),
                 context.getJobID(),
diff --git a/src/hcatalog/core/src/test/java/org/apache/hcatalog/cli/TestPermsGrp.java b/src/hcatalog/core/src/test/java/org/apache/hcatalog/cli/TestPermsGrp.java
index e566ee0..2b0af67 100644
--- a/src/hcatalog/core/src/test/java/org/apache/hcatalog/cli/TestPermsGrp.java
+++ b/src/hcatalog/core/src/test/java/org/apache/hcatalog/cli/TestPermsGrp.java
@@ -57,7 +57,7 @@
 public class TestPermsGrp extends TestCase {
 
   private boolean isServerRunning = false;
-  private static final int msPort = 20101;
+  private int msPort;
   private HiveConf hcatConf;
   private Warehouse clientWH;
   private HiveMetaStoreClient msc;
@@ -75,6 +75,7 @@ protected void setUp() throws Exception {
       return;
     }
 
+    msPort = MetaStoreUtils.findFreePort();
     MetaStoreUtils.startMetaStore(msPort, ShimLoader.getHadoopThriftAuthBridge());
 
     isServerRunning = true;
@@ -87,6 +88,7 @@ protected void setUp() throws Exception {
     hcatConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://127.0.0.1:" + msPort);
     hcatConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
     hcatConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTFAILURERETRIES, 3);
+    hcatConf.setIntVar(HiveConf.ConfVars.METASTORE_CLIENT_SOCKET_TIMEOUT, 120);
 
     hcatConf.set(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK.varname, HCatSemanticAnalyzer.class.getName());
     hcatConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
diff --git a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java b/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java
index d70a258..dbcd195 100644
--- a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java
+++ b/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java
@@ -27,8 +27,7 @@
 import java.util.ArrayList;
 import java.util.Iterator;
 
-import junit.framework.TestCase;
-
+import org.apache.commons.io.FileUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -51,32 +50,40 @@
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 import org.apache.pig.data.Tuple;
+import org.junit.After;
+import org.junit.Before;
 import org.junit.Test;
 
 /**
  * @deprecated Use/modify {@link org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite} instead
  */
-public class TestSequenceFileReadWrite extends TestCase {
-  private static final String TEST_DATA_DIR = System.getProperty("user.dir") +
-      "/build/test/data/" + TestSequenceFileReadWrite.class.getCanonicalName();
-  private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
-  private static final String INPUT_FILE_NAME = TEST_DATA_DIR + "/input.data";
+public class TestSequenceFileReadWrite {
 
-  private static Driver driver;
-  private static PigServer server;
-  private static String[] input;
-  private static HiveConf hiveConf;
+  private File dataDir;
+  private String warehouseDir;
+  private String inputFileName;
+  private Driver driver;
+  private PigServer server;
+  private String[] input;
+  private HiveConf hiveConf;
 
-  public void Initialize() throws Exception {
+  @Before
+  public void setup() throws Exception {
+    dataDir = new File(System.getProperty("java.io.tmpdir") + File.separator + 
+        TestSequenceFileReadWrite.class.getCanonicalName() + "-" + System.currentTimeMillis());
     hiveConf = new HiveConf(this.getClass());
+    warehouseDir = new File(dataDir, "warehouse").getAbsolutePath();
+    inputFileName = new File(dataDir, "input.data").getAbsolutePath();
     hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
+    hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehouseDir);
     driver = new Driver(hiveConf);
     SessionState.start(new CliSessionState(hiveConf));
 
-    new File(TEST_WAREHOUSE_DIR).mkdirs();
+    if(!(new File(warehouseDir).mkdirs())) {
+      throw new RuntimeException("Could not create " + warehouseDir);
+    }
 
     int numRows = 3;
     input = new String[numRows];
@@ -85,13 +92,19 @@ public void Initialize() throws Exception {
       String col2 = "b" + i;
       input[i] = i + "," + col1 + "," + col2;
     }
-    HcatTestUtils.createTestDataFile(INPUT_FILE_NAME, input);
+    HcatTestUtils.createTestDataFile(inputFileName, input);
     server = new PigServer(ExecType.LOCAL);
   }
 
+  @After
+  public void teardown() throws IOException {
+    if(dataDir != null) {
+      FileUtils.deleteDirectory(dataDir);
+    }
+  }
+  
   @Test
   public void testSequenceTableWriteRead() throws Exception {
-    Initialize();
     String createTable = "CREATE TABLE demo_table(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
     driver.run("drop table demo_table");
     int retCode1 = driver.run(createTable).getResponseCode();
@@ -99,7 +112,7 @@ public void testSequenceTableWriteRead() throws Exception {
 
     server.setBatchOn();
     server.registerQuery("A = load '"
-        + INPUT_FILE_NAME
+        + inputFileName
         + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
     server.registerQuery("store A into 'demo_table' using org.apache.hcatalog.pig.HCatStorer();");
     server.executeBatch();
@@ -120,7 +133,6 @@ public void testSequenceTableWriteRead() throws Exception {
 
   @Test
   public void testTextTableWriteRead() throws Exception {
-    Initialize();
     String createTable = "CREATE TABLE demo_table_1(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
     driver.run("drop table demo_table_1");
     int retCode1 = driver.run(createTable).getResponseCode();
@@ -128,7 +140,7 @@ public void testTextTableWriteRead() throws Exception {
 
     server.setBatchOn();
     server.registerQuery("A = load '"
-        + INPUT_FILE_NAME
+        + inputFileName
         + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
     server.registerQuery("store A into 'demo_table_1' using org.apache.hcatalog.pig.HCatStorer();");
     server.executeBatch();
@@ -149,7 +161,6 @@ public void testTextTableWriteRead() throws Exception {
 
   @Test
   public void testSequenceTableWriteReadMR() throws Exception {
-    Initialize();
     String createTable = "CREATE TABLE demo_table_2(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
     driver.run("drop table demo_table_2");
     int retCode1 = driver.run(createTable).getResponseCode();
@@ -165,7 +176,7 @@ public void testSequenceTableWriteReadMR() throws Exception {
     job.setOutputKeyClass(NullWritable.class);
     job.setOutputValueClass(DefaultHCatRecord.class);
     job.setInputFormatClass(TextInputFormat.class);
-    TextInputFormat.setInputPaths(job, INPUT_FILE_NAME);
+    TextInputFormat.setInputPaths(job, inputFileName);
 
     HCatOutputFormat.setOutput(job, OutputJobInfo.create(
         MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_2", null));
@@ -196,7 +207,6 @@ public void testSequenceTableWriteReadMR() throws Exception {
 
   @Test
   public void testTextTableWriteReadMR() throws Exception {
-    Initialize();
     String createTable = "CREATE TABLE demo_table_3(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
     driver.run("drop table demo_table_3");
     int retCode1 = driver.run(createTable).getResponseCode();
@@ -213,7 +223,7 @@ public void testTextTableWriteReadMR() throws Exception {
     job.setOutputValueClass(DefaultHCatRecord.class);
     job.setInputFormatClass(TextInputFormat.class);
     job.setNumReduceTasks(0);
-    TextInputFormat.setInputPaths(job, INPUT_FILE_NAME);
+    TextInputFormat.setInputPaths(job, inputFileName);
 
     HCatOutputFormat.setOutput(job, OutputJobInfo.create(
         MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_3", null));
diff --git a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/TestPermsGrp.java b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/TestPermsGrp.java
index f3f674e..d61709b 100644
--- a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/TestPermsGrp.java
+++ b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/TestPermsGrp.java
@@ -55,7 +55,7 @@
 public class TestPermsGrp extends TestCase {
 
   private boolean isServerRunning = false;
-  private static final int msPort = 20101;
+  private int msPort;
   private HiveConf hcatConf;
   private Warehouse clientWH;
   private HiveMetaStoreClient msc;
@@ -72,7 +72,9 @@ protected void setUp() throws Exception {
     if (isServerRunning) {
       return;
     }
-
+    
+    
+    msPort = MetaStoreUtils.findFreePort();
     MetaStoreUtils.startMetaStore(msPort, ShimLoader.getHadoopThriftAuthBridge());
 
     isServerRunning = true;
@@ -85,6 +87,7 @@ protected void setUp() throws Exception {
     hcatConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://127.0.0.1:" + msPort);
     hcatConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
     hcatConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTFAILURERETRIES, 3);
+    hcatConf.setIntVar(HiveConf.ConfVars.METASTORE_CLIENT_SOCKET_TIMEOUT, 120);
 
     hcatConf.set(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK.varname, HCatSemanticAnalyzer.class.getName());
     hcatConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
@@ -97,7 +100,6 @@ protected void setUp() throws Exception {
     System.setProperty(HiveConf.ConfVars.POSTEXECHOOKS.varname, " ");
   }
 
-
   public void testCustomPerms() throws Exception {
 
     String dbName = MetaStoreUtils.DEFAULT_DATABASE_NAME;
diff --git a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
index 0bb3c41..7c315b8 100644
--- a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
+++ b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
@@ -27,8 +27,7 @@
 import java.util.ArrayList;
 import java.util.Iterator;
 
-import junit.framework.TestCase;
-
+import org.apache.commons.io.FileUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -51,29 +50,38 @@
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 import org.apache.pig.data.Tuple;
+import org.junit.After;
+import org.junit.Before;
 import org.junit.Test;
 
-public class TestSequenceFileReadWrite extends TestCase {
-  private static final String TEST_DATA_DIR =
-      "/tmp/build/test/data/" + TestSequenceFileReadWrite.class.getCanonicalName();
-  private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
-  private static final String INPUT_FILE_NAME = TEST_DATA_DIR + "/input.data";
+public class TestSequenceFileReadWrite {
 
-  private static Driver driver;
-  private static PigServer server;
-  private static String[] input;
-  private static HiveConf hiveConf;
+  private File dataDir;
+  private String warehouseDir;
+  private String inputFileName;
+  private Driver driver;
+  private PigServer server;
+  private String[] input;
+  private HiveConf hiveConf;
 
-  public void Initialize() throws Exception {
+  @Before
+  public void setup() throws Exception {
+    dataDir = new File(System.getProperty("java.io.tmpdir") + File.separator + 
+        TestSequenceFileReadWrite.class.getCanonicalName() + "-" + System.currentTimeMillis());
+    hiveConf = new HiveConf(this.getClass());
+    warehouseDir = new File(dataDir, "warehouse").getAbsolutePath();
+    inputFileName = new File(dataDir, "input.data").getAbsolutePath();
     hiveConf = new HiveConf(this.getClass());
     hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-    hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
+    hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehouseDir);
     driver = new Driver(hiveConf);
     SessionState.start(new CliSessionState(hiveConf));
 
-    new File(TEST_WAREHOUSE_DIR).mkdirs();
+    if(!(new File(warehouseDir).mkdirs())) {
+      throw new RuntimeException("Could not create " + warehouseDir);
+    }
 
     int numRows = 3;
     input = new String[numRows];
@@ -82,13 +90,18 @@ public void Initialize() throws Exception {
       String col2 = "b" + i;
       input[i] = i + "," + col1 + "," + col2;
     }
-    HcatTestUtils.createTestDataFile(INPUT_FILE_NAME, input);
+    HcatTestUtils.createTestDataFile(inputFileName, input);
     server = new PigServer(ExecType.LOCAL);
   }
+  @After
+  public void teardown() throws IOException {
+    if(dataDir != null) {
+      FileUtils.deleteDirectory(dataDir);
+    }
+  }
 
   @Test
   public void testSequenceTableWriteRead() throws Exception {
-    Initialize();
     String createTable = "CREATE TABLE demo_table(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
     driver.run("drop table demo_table");
     int retCode1 = driver.run(createTable).getResponseCode();
@@ -96,7 +109,7 @@ public void testSequenceTableWriteRead() throws Exception {
 
     server.setBatchOn();
     server.registerQuery("A = load '"
-        + INPUT_FILE_NAME
+        + inputFileName
         + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
     server.registerQuery("store A into 'demo_table' using org.apache.hive.hcatalog.pig.HCatStorer();");
     server.executeBatch();
@@ -117,7 +130,6 @@ public void testSequenceTableWriteRead() throws Exception {
 
   @Test
   public void testTextTableWriteRead() throws Exception {
-    Initialize();
     String createTable = "CREATE TABLE demo_table_1(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
     driver.run("drop table demo_table_1");
     int retCode1 = driver.run(createTable).getResponseCode();
@@ -125,7 +137,7 @@ public void testTextTableWriteRead() throws Exception {
 
     server.setBatchOn();
     server.registerQuery("A = load '"
-        + INPUT_FILE_NAME
+        + inputFileName
         + "' using PigStorage(',') as (a0:int,a1:chararray,a2:chararray);");
     server.registerQuery("store A into 'demo_table_1' using org.apache.hive.hcatalog.pig.HCatStorer();");
     server.executeBatch();
@@ -146,7 +158,6 @@ public void testTextTableWriteRead() throws Exception {
 
   @Test
   public void testSequenceTableWriteReadMR() throws Exception {
-    Initialize();
     String createTable = "CREATE TABLE demo_table_2(a0 int, a1 String, a2 String) STORED AS SEQUENCEFILE";
     driver.run("drop table demo_table_2");
     int retCode1 = driver.run(createTable).getResponseCode();
@@ -162,7 +173,7 @@ public void testSequenceTableWriteReadMR() throws Exception {
     job.setOutputKeyClass(NullWritable.class);
     job.setOutputValueClass(DefaultHCatRecord.class);
     job.setInputFormatClass(TextInputFormat.class);
-    TextInputFormat.setInputPaths(job, INPUT_FILE_NAME);
+    TextInputFormat.setInputPaths(job, inputFileName);
 
     HCatOutputFormat.setOutput(job, OutputJobInfo.create(
         MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_2", null));
@@ -193,7 +204,6 @@ public void testSequenceTableWriteReadMR() throws Exception {
 
   @Test
   public void testTextTableWriteReadMR() throws Exception {
-    Initialize();
     String createTable = "CREATE TABLE demo_table_3(a0 int, a1 String, a2 String) STORED AS TEXTFILE";
     driver.run("drop table demo_table_3");
     int retCode1 = driver.run(createTable).getResponseCode();
@@ -210,7 +220,7 @@ public void testTextTableWriteReadMR() throws Exception {
     job.setOutputValueClass(DefaultHCatRecord.class);
     job.setInputFormatClass(TextInputFormat.class);
     job.setNumReduceTasks(0);
-    TextInputFormat.setInputPaths(job, INPUT_FILE_NAME);
+    TextInputFormat.setInputPaths(job, inputFileName);
 
     HCatOutputFormat.setOutput(job, OutputJobInfo.create(
         MetaStoreUtils.DEFAULT_DATABASE_NAME, "demo_table_3", null));
diff --git a/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoader.java b/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoader.java
index cb9ce57..69158f1 100644
--- a/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoader.java
+++ b/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoader.java
@@ -18,6 +18,11 @@
  */
 package org.apache.hcatalog.pig;
 
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
+
 import java.io.File;
 import java.io.IOException;
 import java.io.RandomAccessFile;
@@ -29,8 +34,7 @@
 import java.util.Map;
 import java.util.Properties;
 
-import junit.framework.TestCase;
-
+import org.apache.commons.io.FileUtils;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -48,13 +52,16 @@
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
 import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
 
 /**
  * @deprecated Use/modify {@link org.apache.hive.hcatalog.pig.TestHCatLoader} instead
  */
-public class TestHCatLoader extends TestCase {
-  private static final String TEST_DATA_DIR = System.getProperty("user.dir") +
-    "/build/test/data/" + TestHCatLoader.class.getCanonicalName();
+public class TestHCatLoader {
+  private static final String TEST_DATA_DIR = System.getProperty("java.io.tmpdir") + File.separator
+      + TestHCatLoader.class.getCanonicalName() + "-" + System.currentTimeMillis();
   private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
   private static final String BASIC_FILE_NAME = TEST_DATA_DIR + "/basic.input.data";
   private static final String COMPLEX_FILE_NAME = TEST_DATA_DIR + "/complex.input.data";
@@ -63,13 +70,9 @@
   private static final String COMPLEX_TABLE = "junit_unparted_complex";
   private static final String PARTITIONED_TABLE = "junit_parted_basic";
   private static final String SPECIFIC_SIZE_TABLE = "junit_specific_size";
-  private static Driver driver;
-
-  private static int guardTestCount = 6; // ugh, instantiate using introspection in guardedSetupBeforeClass
-  private static boolean setupHasRun = false;
 
-
-  private static Map<Integer, Pair<Integer, String>> basicInputData;
+  private Driver driver;
+  private Map<Integer, Pair<Integer, String>> basicInputData;
 
   protected String storageFormat() {
     return "RCFILE tblproperties('hcat.isd'='org.apache.hcatalog.rcfile.RCFileInputDriver'," +
@@ -97,18 +100,16 @@ private void createTable(String tablename, String schema) throws IOException, Co
     createTable(tablename, schema, null);
   }
 
-  protected void guardedSetUpBeforeClass() throws Exception {
-    if (!setupHasRun) {
-      setupHasRun = true;
-    } else {
-      return;
-    }
+  @Before
+  public void setup() throws Exception {
 
     File f = new File(TEST_WAREHOUSE_DIR);
     if (f.exists()) {
       FileUtil.fullyDelete(f);
     }
-    new File(TEST_WAREHOUSE_DIR).mkdirs();
+    if(!(new File(TEST_WAREHOUSE_DIR).mkdirs())) {
+      throw new RuntimeException("Could not create " + TEST_WAREHOUSE_DIR);
+    }
 
     HiveConf hiveConf = new HiveConf(this.getClass());
     hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
@@ -118,8 +119,6 @@ protected void guardedSetUpBeforeClass() throws Exception {
     driver = new Driver(hiveConf);
     SessionState.start(new CliSessionState(hiveConf));
 
-    cleanup();
-
     createTable(BASIC_TABLE, "a int, b string");
     createTable(COMPLEX_TABLE,
       "name string, studentid int, "
@@ -172,29 +171,16 @@ protected void guardedSetUpBeforeClass() throws Exception {
 
   }
 
-  private void cleanup() throws IOException, CommandNeedRetryException {
-    dropTable(BASIC_TABLE);
-    dropTable(COMPLEX_TABLE);
-    dropTable(PARTITIONED_TABLE);
-    dropTable(SPECIFIC_SIZE_TABLE);
-  }
-
-  protected void guardedTearDownAfterClass() throws Exception {
-    guardTestCount--;
-    if (guardTestCount > 0) {
-      return;
+  @After
+  public void tearDown() throws Exception {
+    try {
+      dropTable(BASIC_TABLE);
+      dropTable(COMPLEX_TABLE);
+      dropTable(PARTITIONED_TABLE);
+      dropTable(SPECIFIC_SIZE_TABLE);
+    } finally {
+      FileUtils.deleteDirectory(new File(TEST_DATA_DIR));
     }
-    cleanup();
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    guardedSetUpBeforeClass();
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    guardedTearDownAfterClass();
   }
 
   public void testSchemaLoadBasic() throws IOException {
@@ -213,6 +199,7 @@ public void testSchemaLoadBasic() throws IOException {
 
   }
 
+  @Test
   public void testReadDataBasic() throws IOException {
     PigServer server = new PigServer(ExecType.LOCAL);
 
@@ -230,7 +217,7 @@ public void testReadDataBasic() throws IOException {
     }
     assertEquals(basicInputData.size(), numTuplesRead);
   }
-
+  @Test
   public void testSchemaLoadComplex() throws IOException {
 
     PigServer server = new PigServer(ExecType.LOCAL);
@@ -287,7 +274,7 @@ public void testSchemaLoadComplex() throws IOException {
     }
 
   }
-
+  @Test
   public void testReadPartitionedBasic() throws IOException, CommandNeedRetryException {
     PigServer server = new PigServer(ExecType.LOCAL);
 
@@ -350,7 +337,7 @@ public void testReadPartitionedBasic() throws IOException, CommandNeedRetryExcep
     }
     assertEquals(6, count2);
   }
-
+  @Test
   public void testProjectionsBasic() throws IOException {
 
     PigServer server = new PigServer(ExecType.LOCAL);
@@ -395,21 +382,21 @@ public void testProjectionsBasic() throws IOException {
     }
     assertEquals(basicInputData.size(), numTuplesRead);
   }
-
+  @Test
   public void testGetInputBytes() throws Exception {
     File file = new File(TEST_WAREHOUSE_DIR + "/" + SPECIFIC_SIZE_TABLE + "/part-m-00000");
     file.deleteOnExit();
     RandomAccessFile randomAccessFile = new RandomAccessFile(file, "rw");
     randomAccessFile.setLength(2L * 1024 * 1024 * 1024);
-
+    randomAccessFile.close();
     Job job = new Job();
     HCatLoader hCatLoader = new HCatLoader();
-    hCatLoader.setUDFContextSignature(this.getName());
+    hCatLoader.setUDFContextSignature("testGetInputBytes");
     hCatLoader.setLocation(SPECIFIC_SIZE_TABLE, job);
     ResourceStatistics statistics = hCatLoader.getStatistics(file.getAbsolutePath(), job);
     assertEquals(2048, (long) statistics.getmBytes());
   }
-
+  @Test
   public void testConvertBooleanToInt() throws Exception {
     String tbl = "test_convert_boolean_to_int";
     String inputFileName = TEST_DATA_DIR + "/testConvertBooleanToInt/data.txt";
diff --git a/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoaderComplexSchema.java b/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoaderComplexSchema.java
index 2085cf5..9db87cb 100644
--- a/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoaderComplexSchema.java
+++ b/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoaderComplexSchema.java
@@ -210,7 +210,7 @@ private void verifyWriteRead(String tablename, String pigSchema, String tableSch
       while (it.hasNext()) {
         Tuple input = data.get(i++);
         Tuple output = it.next();
-        Assert.assertEquals(input.toString(), output.toString());
+        compareTuples(input, output);
         LOG.info("tuple : {} ", output);
       }
       Schema dumpedXSchema = server.dumpSchema("X");
@@ -224,6 +224,23 @@ private void verifyWriteRead(String tablename, String pigSchema, String tableSch
       dropTable(tablename);
     }
   }
+  private void compareTuples(Tuple t1, Tuple t2) throws ExecException {
+    Assert.assertEquals("Tuple Sizes don't match", t1.size(), t2.size());
+    for (int i = 0; i < t1.size(); i++) {
+      Object f1 = t1.get(i);
+      Object f2 = t2.get(i);
+      Assert.assertNotNull("left", f1);
+      Assert.assertNotNull("right", f2);
+      String msg = "right: " + f1 + ", left: " + f2;
+      Assert.assertEquals(msg, noOrder(f1.toString()), noOrder(f2.toString()));
+    }
+  }
+  
+  private String noOrder(String s) {
+    char[] chars = s.toCharArray();
+    Arrays.sort(chars);
+    return new String(chars);
+  }
 
   private String compareIgnoreFiledNames(Schema expected, Schema got) throws FrontendException {
     if (expected == null || got == null) {
diff --git a/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestE2EScenarios.java b/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestE2EScenarios.java
index bf437d2..a4b55c8 100644
--- a/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestE2EScenarios.java
+++ b/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestE2EScenarios.java
@@ -25,6 +25,7 @@
 
 import junit.framework.TestCase;
 
+import org.apache.commons.io.FileUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.hive.cli.CliSessionState;
@@ -35,6 +36,7 @@
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.mapreduce.InputSplit;
 import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobID;
 import org.apache.hadoop.mapreduce.OutputCommitter;
 import org.apache.hadoop.mapreduce.RecordReader;
 import org.apache.hadoop.mapreduce.RecordWriter;
@@ -55,8 +57,8 @@
 
 public class TestE2EScenarios extends TestCase {
 
-  private static final String TEST_DATA_DIR = System.getProperty("user.dir") +
-    "/build/test/data/" + TestHCatLoader.class.getCanonicalName();
+  private static final String TEST_DATA_DIR = System.getProperty("java.io.tmpdir") + File.separator
+      + TestHCatLoader.class.getCanonicalName() + "-" + System.currentTimeMillis();
   private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
 
   private static final String TEXTFILE_LOCN = TEST_DATA_DIR + "/textfile";
@@ -74,7 +76,9 @@ protected void setUp() throws Exception {
     if (f.exists()) {
       FileUtil.fullyDelete(f);
     }
-    new File(TEST_WAREHOUSE_DIR).mkdirs();
+    if(!(new File(TEST_WAREHOUSE_DIR).mkdirs())) {
+      throw new RuntimeException("Could not create " + TEST_WAREHOUSE_DIR);
+    }
 
     HiveConf hiveConf = new HiveConf(this.getClass());
     hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
@@ -88,9 +92,13 @@ protected void setUp() throws Exception {
 
   @Override
   protected void tearDown() throws Exception {
-    dropTable("inpy");
-    dropTable("rc5318");
-    dropTable("orc5318");
+    try {
+      dropTable("inpy");
+      dropTable("rc5318");
+      dropTable("orc5318");
+    } finally {
+      FileUtils.deleteDirectory(new File(TEST_DATA_DIR));
+    }
   }
 
   private void dropTable(String tablename) throws IOException, CommandNeedRetryException {
@@ -191,9 +199,9 @@ private void copyTable(String in, String out) throws IOException, InterruptedExc
 
   private TaskAttemptContext createTaskAttemptContext(Configuration tconf) {
     Configuration conf = (tconf == null) ? (new Configuration()) : tconf;
-    TaskAttemptID taskId = new TaskAttemptID();
+    TaskAttemptID taskId = HCatMapRedUtil.createTaskAttemptID(new JobID("200908190029", 1), false, 1, 1);
     conf.setInt("mapred.task.partition", taskId.getId());
-    conf.set("mapred.task.id", "attempt__0000_r_000000_" + taskId.getId());
+    conf.set("mapred.task.id", taskId.toString());
     TaskAttemptContext rtaskContext = HCatMapRedUtil.createTaskAttemptContext(conf , taskId);
     return rtaskContext;
   }
diff --git a/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java b/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java
index bccd33d..d38bb8d 100644
--- a/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java
+++ b/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java
@@ -18,6 +18,11 @@
  */
 package org.apache.hive.hcatalog.pig;
 
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
+
 import java.io.File;
 import java.io.IOException;
 import java.io.RandomAccessFile;
@@ -29,8 +34,7 @@
 import java.util.Map;
 import java.util.Properties;
 
-import junit.framework.TestCase;
-
+import org.apache.commons.io.FileUtils;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -48,10 +52,13 @@
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
 import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
 
-public class TestHCatLoader extends TestCase {
-  private static final String TEST_DATA_DIR =
-    "/tmp/build/test/data/" + TestHCatLoader.class.getCanonicalName();
+public class TestHCatLoader {
+  private static final String TEST_DATA_DIR = System.getProperty("java.io.tmpdir") + File.separator
+      + TestHCatLoader.class.getCanonicalName() + "-" + System.currentTimeMillis();
   private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
   private static final String BASIC_FILE_NAME = TEST_DATA_DIR + "/basic.input.data";
   private static final String COMPLEX_FILE_NAME = TEST_DATA_DIR + "/complex.input.data";
@@ -60,13 +67,9 @@
   private static final String COMPLEX_TABLE = "junit_unparted_complex";
   private static final String PARTITIONED_TABLE = "junit_parted_basic";
   private static final String SPECIFIC_SIZE_TABLE = "junit_specific_size";
-  private static Driver driver;
-
-  private static int guardTestCount = 6; // ugh, instantiate using introspection in guardedSetupBeforeClass
-  private static boolean setupHasRun = false;
-
 
-  private static Map<Integer, Pair<Integer, String>> basicInputData;
+  private Driver driver;
+  private Map<Integer, Pair<Integer, String>> basicInputData;
 
   protected String storageFormat() {
     return "RCFILE tblproperties('hcat.isd'='org.apache.hive.hcatalog.rcfile.RCFileInputDriver'," +
@@ -94,18 +97,16 @@ private void createTable(String tablename, String schema) throws IOException, Co
     createTable(tablename, schema, null);
   }
 
-  protected void guardedSetUpBeforeClass() throws Exception {
-    if (!setupHasRun) {
-      setupHasRun = true;
-    } else {
-      return;
-    }
+  @Before
+  public void setup() throws Exception {
 
     File f = new File(TEST_WAREHOUSE_DIR);
     if (f.exists()) {
       FileUtil.fullyDelete(f);
     }
-    new File(TEST_WAREHOUSE_DIR).mkdirs();
+    if(!(new File(TEST_WAREHOUSE_DIR).mkdirs())) {
+      throw new RuntimeException("Could not create " + TEST_WAREHOUSE_DIR);
+    }
 
     HiveConf hiveConf = new HiveConf(this.getClass());
     hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
@@ -115,8 +116,6 @@ protected void guardedSetUpBeforeClass() throws Exception {
     driver = new Driver(hiveConf);
     SessionState.start(new CliSessionState(hiveConf));
 
-    cleanup();
-
     createTable(BASIC_TABLE, "a int, b string");
     createTable(COMPLEX_TABLE,
       "name string, studentid int, "
@@ -169,31 +168,19 @@ protected void guardedSetUpBeforeClass() throws Exception {
 
   }
 
-  private void cleanup() throws IOException, CommandNeedRetryException {
-    dropTable(BASIC_TABLE);
-    dropTable(COMPLEX_TABLE);
-    dropTable(PARTITIONED_TABLE);
-    dropTable(SPECIFIC_SIZE_TABLE);
-  }
-
-  protected void guardedTearDownAfterClass() throws Exception {
-    guardTestCount--;
-    if (guardTestCount > 0) {
-      return;
+  @After
+  public void tearDown() throws Exception {
+    try {
+      dropTable(BASIC_TABLE);
+      dropTable(COMPLEX_TABLE);
+      dropTable(PARTITIONED_TABLE);
+      dropTable(SPECIFIC_SIZE_TABLE);
+    } finally {
+      FileUtils.deleteDirectory(new File(TEST_DATA_DIR));
     }
-    cleanup();
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    guardedSetUpBeforeClass();
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    guardedTearDownAfterClass();
   }
 
+  @Test
   public void testSchemaLoadBasic() throws IOException {
 
     PigServer server = new PigServer(ExecType.LOCAL);
@@ -210,6 +197,7 @@ public void testSchemaLoadBasic() throws IOException {
 
   }
 
+  @Test
   public void testReadDataBasic() throws IOException {
     PigServer server = new PigServer(ExecType.LOCAL);
 
@@ -228,6 +216,7 @@ public void testReadDataBasic() throws IOException {
     assertEquals(basicInputData.size(), numTuplesRead);
   }
 
+  @Test
   public void testSchemaLoadComplex() throws IOException {
 
     PigServer server = new PigServer(ExecType.LOCAL);
@@ -285,6 +274,7 @@ public void testSchemaLoadComplex() throws IOException {
 
   }
 
+  @Test
   public void testReadPartitionedBasic() throws IOException, CommandNeedRetryException {
     PigServer server = new PigServer(ExecType.LOCAL);
 
@@ -348,6 +338,7 @@ public void testReadPartitionedBasic() throws IOException, CommandNeedRetryExcep
     assertEquals(6, count2);
   }
 
+  @Test
   public void testProjectionsBasic() throws IOException {
 
     PigServer server = new PigServer(ExecType.LOCAL);
@@ -393,20 +384,22 @@ public void testProjectionsBasic() throws IOException {
     assertEquals(basicInputData.size(), numTuplesRead);
   }
 
+  @Test
   public void testGetInputBytes() throws Exception {
     File file = new File(TEST_WAREHOUSE_DIR + "/" + SPECIFIC_SIZE_TABLE + "/part-m-00000");
     file.deleteOnExit();
     RandomAccessFile randomAccessFile = new RandomAccessFile(file, "rw");
     randomAccessFile.setLength(2L * 1024 * 1024 * 1024);
-
+    randomAccessFile.close();
     Job job = new Job();
     HCatLoader hCatLoader = new HCatLoader();
-    hCatLoader.setUDFContextSignature(this.getName());
+    hCatLoader.setUDFContextSignature("testGetInputBytes");
     hCatLoader.setLocation(SPECIFIC_SIZE_TABLE, job);
     ResourceStatistics statistics = hCatLoader.getStatistics(file.getAbsolutePath(), job);
     assertEquals(2048, (long) statistics.getmBytes());
   }
 
+  @Test
   public void testConvertBooleanToInt() throws Exception {
     String tbl = "test_convert_boolean_to_int";
     String inputFileName = TEST_DATA_DIR + "/testConvertBooleanToInt/data.txt";
diff --git a/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderComplexSchema.java b/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderComplexSchema.java
index fa7764b..eadbf20 100644
--- a/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderComplexSchema.java
+++ b/src/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderComplexSchema.java
@@ -211,7 +211,7 @@ private void verifyWriteRead(String tablename, String pigSchema, String tableSch
       while (it.hasNext()) {
         Tuple input = data.get(i++);
         Tuple output = it.next();
-        Assert.assertEquals(input.toString(), output.toString());
+        compareTuples(input, output);
         LOG.info("tuple : {} ", output);
       }
       Schema dumpedXSchema = server.dumpSchema("X");
@@ -225,6 +225,24 @@ private void verifyWriteRead(String tablename, String pigSchema, String tableSch
       dropTable(tablename);
     }
   }
+  
+  private void compareTuples(Tuple t1, Tuple t2) throws ExecException {
+    Assert.assertEquals("Tuple Sizes don't match", t1.size(), t2.size());
+    for (int i = 0; i < t1.size(); i++) {
+      Object f1 = t1.get(i);
+      Object f2 = t2.get(i);
+      Assert.assertNotNull("left", f1);
+      Assert.assertNotNull("right", f2);
+      String msg = "right: " + f1 + ", left: " + f2;
+      Assert.assertEquals(msg, noOrder(f1.toString()), noOrder(f2.toString()));
+    }
+  }
+  
+  private String noOrder(String s) {
+    char[] chars = s.toCharArray();
+    Arrays.sort(chars);
+    return new String(chars);
+  }
 
   private String compareIgnoreFiledNames(Schema expected, Schema got) throws FrontendException {
     if (expected == null || got == null) {
diff --git a/src/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java b/src/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
index 117e0a5..efc3211 100644
--- a/src/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
+++ b/src/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
@@ -65,6 +65,7 @@
 import org.apache.hadoop.mapred.lib.CombineFileSplit;
 import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
 import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.JobID;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.security.SecurityUtil;
 import org.apache.hadoop.security.UnixUserGroupInformation;
@@ -677,6 +678,11 @@ public void progress() {
   }
 
   @Override
+  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id) {
+    return new TaskAttemptID(jobId.getJtIdentifier(), jobId.getId(), isMap, taskId, id);
+  }
+
+  @Override
   public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job) {
     return new org.apache.hadoop.mapreduce.JobContext(job.getConfiguration(), job.getJobID());
   }
diff --git a/src/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java b/src/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
index cf5c175..9acfae9 100644
--- a/src/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
+++ b/src/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
@@ -88,6 +88,11 @@ public void progress() {
   }
 
   @Override
+  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id) {
+    return new TaskAttemptID(jobId.getJtIdentifier(), jobId.getId(), isMap, taskId, id);
+  }
+
+  @Override
   public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job) {
     return new org.apache.hadoop.mapreduce.JobContext(job.getConfiguration(), job.getJobID());
   }
diff --git a/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java b/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
index 9351411..c4d020f 100644
--- a/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
+++ b/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
@@ -100,6 +100,11 @@ public void progress() {
   }
 
   @Override
+  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id) {
+    return new TaskAttemptID(jobId.getJtIdentifier(), jobId.getId(), isMap ?  TaskType.MAP : TaskType.REDUCE, taskId, id);
+  }
+
+  @Override
   public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job) {
     return new JobContextImpl(job.getConfiguration(), job.getJobID());
   }
diff --git a/src/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java b/src/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
index 5b91267..a3d320b 100644
--- a/src/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
+++ b/src/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
@@ -316,6 +316,8 @@ void setTokenStr(UserGroupInformation ugi, String tokenStr, String tokenService)
 
   public TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable);
 
+  public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, int id);
+
   public JobContext newJobContext(Job job);
 
   /**
-- 
1.7.0.4

