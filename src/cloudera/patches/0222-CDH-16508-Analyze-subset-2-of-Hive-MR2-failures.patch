From 5f39b0766f0b538b37b5ac628de49510266ffeaa Mon Sep 17 00:00:00 2001
From: xzhang <xzhang@xzdt.(none)>
Date: Thu, 9 Jan 2014 11:28:08 -0800
Subject: [PATCH 222/375] CDH-16508: Analyze subset 2 of Hive MR2 failures

---
 .../test/queries/clientpositive/groupby1_limit.q   |    4 +-
 ql/src/test/queries/clientpositive/groupby1_map.q  |    2 +-
 .../test/queries/clientpositive/groupby2_noskew.q  |    2 +-
 .../groupby2_noskew_multi_distinct.q               |    2 +-
 .../test/queries/clientpositive/groupby5_noskew.q  |    2 +-
 ql/src/test/queries/clientpositive/groupby6_map.q  |    2 +-
 .../queries/clientpositive/groupby6_map_skew.q     |    2 +-
 .../test/queries/clientpositive/groupby7_noskew.q  |    4 +-
 ql/src/test/queries/clientpositive/groupby8_map.q  |    4 +-
 .../test/queries/clientpositive/groupby_map_ppr.q  |    2 +-
 .../test/queries/clientpositive/groupby_sort_1.q   |   34 +-
 .../results/clientpositive/groupby1_limit.q.out    |   30 +-
 .../test/results/clientpositive/groupby1_map.q.out |  114 +-
 .../results/clientpositive/groupby2_noskew.q.out   |    4 +-
 .../groupby2_noskew_multi_distinct.q.out           |    4 +-
 .../results/clientpositive/groupby5_noskew.q.out   |  114 +-
 .../test/results/clientpositive/groupby6_map.q.out |    4 +-
 .../results/clientpositive/groupby6_map_skew.q.out |    4 +-
 .../results/clientpositive/groupby7_noskew.q.out   |  192 +-
 .../test/results/clientpositive/groupby8_map.q.out |  192 +-
 .../results/clientpositive/groupby_map_ppr.q.out   |    4 +-
 .../results/clientpositive/groupby_sort_1.q.out    | 3030 +-------------------
 .../test/results/clientpositive/nullformat.q.out   |    1 +
 23 files changed, 397 insertions(+), 3356 deletions(-)

diff --git a/src/ql/src/test/queries/clientpositive/groupby1_limit.q b/src/ql/src/test/queries/clientpositive/groupby1_limit.q
index 1b6891e..287e666 100644
--- a/src/ql/src/test/queries/clientpositive/groupby1_limit.q
+++ b/src/ql/src/test/queries/clientpositive/groupby1_limit.q
@@ -3,8 +3,8 @@ set mapred.reduce.tasks=31;
 CREATE TABLE dest1(key INT, value DOUBLE) STORED AS TEXTFILE;
 
 EXPLAIN
-FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key LIMIT 5;
+FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key order by key LIMIT 5;
 
-FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key LIMIT 5;
+FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key order by key LIMIT 5;
 
 SELECT dest1.* FROM dest1 ORDER BY dest1.key ASC , dest1.value ASC;
diff --git a/src/ql/src/test/queries/clientpositive/groupby1_map.q b/src/ql/src/test/queries/clientpositive/groupby1_map.q
index 82cff36..9f7b4d6 100644
--- a/src/ql/src/test/queries/clientpositive/groupby1_map.q
+++ b/src/ql/src/test/queries/clientpositive/groupby1_map.q
@@ -9,4 +9,4 @@ FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) G
 
 FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;
 
-SELECT dest1.* FROM dest1;
+SELECT dest1.* FROM dest1 order by key;
diff --git a/src/ql/src/test/queries/clientpositive/groupby2_noskew.q b/src/ql/src/test/queries/clientpositive/groupby2_noskew.q
index c3c82d5..1159bce 100644
--- a/src/ql/src/test/queries/clientpositive/groupby2_noskew.q
+++ b/src/ql/src/test/queries/clientpositive/groupby2_noskew.q
@@ -11,4 +11,4 @@ INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr
 FROM src
 INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) GROUP BY substr(src.key,1,1);
 
-SELECT dest_g2.* FROM dest_g2;
+SELECT dest_g2.* FROM dest_g2 order by key;
diff --git a/src/ql/src/test/queries/clientpositive/groupby2_noskew_multi_distinct.q b/src/ql/src/test/queries/clientpositive/groupby2_noskew_multi_distinct.q
index b80c271..a3b77c9 100644
--- a/src/ql/src/test/queries/clientpositive/groupby2_noskew_multi_distinct.q
+++ b/src/ql/src/test/queries/clientpositive/groupby2_noskew_multi_distinct.q
@@ -11,4 +11,4 @@ INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr
 FROM src
 INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);
 
-SELECT dest_g2.* FROM dest_g2;
+SELECT dest_g2.* FROM dest_g2 order by key;
diff --git a/src/ql/src/test/queries/clientpositive/groupby5_noskew.q b/src/ql/src/test/queries/clientpositive/groupby5_noskew.q
index be60785..c84015c 100644
--- a/src/ql/src/test/queries/clientpositive/groupby5_noskew.q
+++ b/src/ql/src/test/queries/clientpositive/groupby5_noskew.q
@@ -16,5 +16,5 @@ SELECT src.key, sum(substr(src.value,5))
 FROM src
 GROUP BY src.key;
 
-SELECT dest1.* FROM dest1;
+SELECT dest1.* FROM dest1 order by key;
 
diff --git a/src/ql/src/test/queries/clientpositive/groupby6_map.q b/src/ql/src/test/queries/clientpositive/groupby6_map.q
index fbf761c..5aeb355 100644
--- a/src/ql/src/test/queries/clientpositive/groupby6_map.q
+++ b/src/ql/src/test/queries/clientpositive/groupby6_map.q
@@ -11,6 +11,6 @@ INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);
 FROM src
 INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);
 
-SELECT dest1.* FROM dest1;
+SELECT dest1.* FROM dest1 order by c1;
 
 
diff --git a/src/ql/src/test/queries/clientpositive/groupby6_map_skew.q b/src/ql/src/test/queries/clientpositive/groupby6_map_skew.q
index 0d3727b..911cf04 100644
--- a/src/ql/src/test/queries/clientpositive/groupby6_map_skew.q
+++ b/src/ql/src/test/queries/clientpositive/groupby6_map_skew.q
@@ -11,6 +11,6 @@ INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);
 FROM src
 INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);
 
-SELECT dest1.* FROM dest1 ORDER BY c1;
+SELECT dest1.* FROM dest1 order by dest1.c1;
 
 
diff --git a/src/ql/src/test/queries/clientpositive/groupby7_noskew.q b/src/ql/src/test/queries/clientpositive/groupby7_noskew.q
index 94a3dcf..e20a4d5 100644
--- a/src/ql/src/test/queries/clientpositive/groupby7_noskew.q
+++ b/src/ql/src/test/queries/clientpositive/groupby7_noskew.q
@@ -18,5 +18,5 @@ FROM SRC
 INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key
 INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key;
 
-SELECT DEST1.* FROM DEST1;
-SELECT DEST2.* FROM DEST2;
+SELECT DEST1.* FROM DEST1 order by key;
+SELECT DEST2.* FROM DEST2 order by key;
diff --git a/src/ql/src/test/queries/clientpositive/groupby8_map.q b/src/ql/src/test/queries/clientpositive/groupby8_map.q
index 62b6ff5..eec9afe 100644
--- a/src/ql/src/test/queries/clientpositive/groupby8_map.q
+++ b/src/ql/src/test/queries/clientpositive/groupby8_map.q
@@ -14,6 +14,6 @@ FROM SRC
 INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
 INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;
 
-SELECT DEST1.* FROM DEST1;
-SELECT DEST2.* FROM DEST2;
+SELECT DEST1.* FROM DEST1 order by key;
+SELECT DEST2.* FROM DEST2 order by key;
 
diff --git a/src/ql/src/test/queries/clientpositive/groupby_map_ppr.q b/src/ql/src/test/queries/clientpositive/groupby_map_ppr.q
index f0a8b72..04d33b6 100644
--- a/src/ql/src/test/queries/clientpositive/groupby_map_ppr.q
+++ b/src/ql/src/test/queries/clientpositive/groupby_map_ppr.q
@@ -17,4 +17,4 @@ SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(s
 WHERE src.ds = '2008-04-08'
 GROUP BY substr(src.key,1,1);
 
-SELECT dest1.* FROM dest1;
+SELECT dest1.* FROM dest1 order by key;
diff --git a/src/ql/src/test/queries/clientpositive/groupby_sort_1.q b/src/ql/src/test/queries/clientpositive/groupby_sort_1.q
index 7401a9c..c7f52d4 100644
--- a/src/ql/src/test/queries/clientpositive/groupby_sort_1.q
+++ b/src/ql/src/test/queries/clientpositive/groupby_sort_1.q
@@ -16,7 +16,7 @@ CREATE TABLE outputTbl1(key int, cnt int);
 -- The plan should be converted to a map-side group by if the group by key
 -- matches the sorted key
 -- addind a order by at the end to make the test results deterministic
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key, count(1) FROM T1 GROUP BY key;
 
@@ -28,7 +28,7 @@ SELECT * FROM outputTbl1 ORDER BY key;
 CREATE TABLE outputTbl2(key1 int, key2 string, cnt int);
 
 -- no map-side group by even if the group by key is a superset of sorted key
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl2
 SELECT key, val, count(1) FROM T1 GROUP BY key, val;
 
@@ -38,7 +38,7 @@ SELECT key, val, count(1) FROM T1 GROUP BY key, val;
 SELECT * FROM outputTbl2 ORDER BY key1, key2;
 
 -- It should work for sub-queries
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key;
 
@@ -48,7 +48,7 @@ SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key;
 SELECT * FROM outputTbl1 ORDER BY key;
 
 -- It should work for sub-queries with column aliases
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl1
 SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k;
 
@@ -61,7 +61,7 @@ CREATE TABLE outputTbl3(key1 int, key2 int, cnt int);
 
 -- The plan should be converted to a map-side group by if the group by key contains a constant followed
 -- by a match to the sorted key
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl3
 SELECT 1, key, count(1) FROM T1 GROUP BY 1, key;
 
@@ -73,7 +73,7 @@ SELECT * FROM outputTbl3 ORDER BY key1, key2;
 CREATE TABLE outputTbl4(key1 int, key2 int, key3 string, cnt int);
 
 -- no map-side group by if the group by key contains a constant followed by another column
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl4
 SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val;
 
@@ -83,7 +83,7 @@ SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val;
 SELECT * FROM outputTbl4 ORDER BY key1, key2, key3;
 
 -- no map-side group by if the group by key contains a function
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl3
 SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1;
 
@@ -96,7 +96,7 @@ SELECT * FROM outputTbl3 ORDER BY key1, key2;
 -- test various cases
 
 -- group by followed by another group by
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key + key, sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
@@ -110,7 +110,7 @@ group by key + key;
 SELECT * FROM outputTbl1 ORDER BY key;
 
 -- group by followed by a union
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
@@ -128,7 +128,7 @@ SELECT key, count(1) FROM T1 GROUP BY key
 SELECT * FROM outputTbl1 ORDER BY key;
 
 -- group by followed by a union where one of the sub-queries is map-side group by
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
@@ -146,7 +146,7 @@ SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
 SELECT * FROM outputTbl1 ORDER BY key;
 
 -- group by followed by a join
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT subq1.key, subq1.cnt+subq2.cnt FROM 
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
@@ -164,7 +164,7 @@ ON subq1.key = subq2.key;
 SELECT * FROM outputTbl1 ORDER BY key;
 
 -- group by followed by a join where one of the sub-queries can be performed in the mapper
-EXPLAIN EXTENDED 
+EXPLAIN 
 SELECT * FROM 
 (SELECT key, count(1) FROM T1 GROUP BY key) subq1
 JOIN
@@ -178,7 +178,7 @@ CLUSTERED BY (key, val) SORTED BY (key, val) INTO 2 BUCKETS STORED AS TEXTFILE;
 INSERT OVERWRITE TABLE T2 select key, val from T1;
 
 -- no mapside sort group by if the group by is a prefix of the sorted key
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key, count(1) FROM T2 GROUP BY key;
 
@@ -189,7 +189,7 @@ SELECT * FROM outputTbl1 ORDER BY key;
 
 -- The plan should be converted to a map-side group by if the group by key contains a constant in between the
 -- sorted keys
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl4
 SELECT key, 1, val, count(1) FROM T2 GROUP BY key, 1, val;
 
@@ -202,7 +202,7 @@ CREATE TABLE outputTbl5(key1 int, key2 int, key3 string, key4 int, cnt int);
 
 -- The plan should be converted to a map-side group by if the group by key contains a constant in between the
 -- sorted keys followed by anything
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl5
 SELECT key, 1, val, 2, count(1) FROM T2 GROUP BY key, 1, val, 2;
 
@@ -213,7 +213,7 @@ SELECT * FROM outputTbl5
 ORDER BY key1, key2, key3, key4;
 
 -- contants from sub-queries should work fine
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl4
 SELECT key, constant, val, count(1) from 
 (SELECT key, 1 as constant, val from T2)subq
@@ -227,7 +227,7 @@ group by key, constant, val;
 SELECT * FROM outputTbl4 ORDER BY key1, key2, key3;
 
 -- multiple levels of contants from sub-queries should work fine
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl4
 select key, constant3, val, count(1) from
 (
diff --git a/src/ql/src/test/results/clientpositive/groupby1_limit.q.out b/src/ql/src/test/results/clientpositive/groupby1_limit.q.out
index f9cf8b2..2eab31b 100644
--- a/src/ql/src/test/results/clientpositive/groupby1_limit.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby1_limit.q.out
@@ -4,13 +4,13 @@ POSTHOOK: query: CREATE TABLE dest1(key INT, value DOUBLE) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1
 PREHOOK: query: EXPLAIN
-FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key LIMIT 5
+FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key order by key LIMIT 5
 PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN
-FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key LIMIT 5
+FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key order by key LIMIT 5
 POSTHOOK: type: QUERY
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME dest1))) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL src) key)) (TOK_SELEXPR (TOK_FUNCTION sum (TOK_FUNCTION substr (. (TOK_TABLE_OR_COL src) value) 5)))) (TOK_GROUPBY (. (TOK_TABLE_OR_COL src) key)) (TOK_LIMIT 5)))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME dest1))) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL src) key)) (TOK_SELEXPR (TOK_FUNCTION sum (TOK_FUNCTION substr (. (TOK_TABLE_OR_COL src) value) 5)))) (TOK_GROUPBY (. (TOK_TABLE_OR_COL src) key)) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key))) (TOK_LIMIT 5)))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -70,14 +70,13 @@ STAGE PLANS:
                   expr: _col1
                   type: double
             outputColumnNames: _col0, _col1
-            Limit
-              File Output Operator
-                compressed: false
-                GlobalTableId: 0
-                table:
-                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
 
   Stage: Stage-2
     Map Reduce
@@ -85,7 +84,10 @@ STAGE PLANS:
 #### A masked pattern was here ####
           TableScan
             Reduce Output Operator
-              sort order: 
+              key expressions:
+                    expr: _col0
+                    type: string
+              sort order: +
               tag: -1
               value expressions:
                     expr: _col0
@@ -125,11 +127,11 @@ STAGE PLANS:
     Stats-Aggr Operator
 
 
-PREHOOK: query: FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key LIMIT 5
+PREHOOK: query: FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key order by key LIMIT 5
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Output: default@dest1
-POSTHOOK: query: FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key LIMIT 5
+POSTHOOK: query: FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key order by key LIMIT 5
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Output: default@dest1
diff --git a/src/ql/src/test/results/clientpositive/groupby1_map.q.out b/src/ql/src/test/results/clientpositive/groupby1_map.q.out
index b15ac58..b0d1c01 100644
--- a/src/ql/src/test/results/clientpositive/groupby1_map.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby1_map.q.out
@@ -102,30 +102,83 @@ POSTHOOK: Input: default@src
 POSTHOOK: Output: default@dest1
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: SELECT dest1.* FROM dest1
+PREHOOK: query: SELECT dest1.* FROM dest1 order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT dest1.* FROM dest1
+POSTHOOK: query: SELECT dest1.* FROM dest1 order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
 #### A masked pattern was here ####
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	0.0
+2	2.0
+4	4.0
+5	15.0
+8	8.0
+9	9.0
 10	10.0
+11	11.0
+12	24.0
+15	30.0
+17	17.0
+18	36.0
+19	19.0
+20	20.0
+24	48.0
+26	52.0
+27	27.0
+28	28.0
+30	30.0
+33	33.0
+34	34.0
+35	105.0
+37	74.0
+41	41.0
+42	84.0
+43	43.0
+44	44.0
+47	47.0
+51	102.0
+53	53.0
+54	54.0
+57	57.0
+58	116.0
+64	64.0
+65	65.0
+66	66.0
+67	134.0
+69	69.0
+70	210.0
+72	144.0
+74	74.0
+76	152.0
+77	77.0
+78	78.0
+80	80.0
+82	82.0
+83	166.0
+84	168.0
+85	85.0
+86	86.0
+87	87.0
+90	270.0
+92	92.0
+95	190.0
+96	96.0
+97	194.0
+98	196.0
 100	200.0
 103	206.0
 104	208.0
 105	105.0
-11	11.0
 111	111.0
 113	226.0
 114	114.0
 116	116.0
 118	236.0
 119	357.0
-12	24.0
 120	240.0
 125	250.0
 126	126.0
@@ -141,7 +194,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 145	145.0
 146	292.0
 149	298.0
-15	30.0
 150	150.0
 152	304.0
 153	153.0
@@ -158,7 +210,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 167	501.0
 168	168.0
 169	676.0
-17	17.0
 170	170.0
 172	344.0
 174	348.0
@@ -167,14 +218,12 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 177	177.0
 178	178.0
 179	358.0
-18	36.0
 180	180.0
 181	181.0
 183	183.0
 186	186.0
 187	561.0
 189	189.0
-19	19.0
 190	190.0
 191	382.0
 192	192.0
@@ -184,8 +233,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 196	196.0
 197	394.0
 199	597.0
-2	2.0
-20	20.0
 200	400.0
 201	201.0
 202	202.0
@@ -213,7 +260,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 237	474.0
 238	476.0
 239	478.0
-24	48.0
 241	241.0
 242	484.0
 244	244.0
@@ -225,20 +271,17 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 256	512.0
 257	257.0
 258	258.0
-26	52.0
 260	260.0
 262	262.0
 263	263.0
 265	530.0
 266	266.0
-27	27.0
 272	544.0
 273	819.0
 274	274.0
 275	275.0
 277	1108.0
 278	556.0
-28	28.0
 280	560.0
 281	562.0
 282	564.0
@@ -253,7 +296,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 292	292.0
 296	296.0
 298	894.0
-30	30.0
 302	302.0
 305	305.0
 306	306.0
@@ -271,7 +313,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 323	323.0
 325	650.0
 327	981.0
-33	33.0
 331	662.0
 332	332.0
 333	666.0
@@ -279,13 +320,11 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 336	336.0
 338	338.0
 339	339.0
-34	34.0
 341	341.0
 342	684.0
 344	688.0
 345	345.0
 348	1740.0
-35	105.0
 351	351.0
 353	706.0
 356	356.0
@@ -297,7 +336,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 367	734.0
 368	368.0
 369	1107.0
-37	74.0
 373	373.0
 374	374.0
 375	375.0
@@ -315,7 +353,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 396	1188.0
 397	794.0
 399	798.0
-4	4.0
 400	400.0
 401	2005.0
 402	402.0
@@ -324,19 +361,16 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 406	1624.0
 407	407.0
 409	1227.0
-41	41.0
 411	411.0
 413	826.0
 414	828.0
 417	1251.0
 418	418.0
 419	419.0
-42	84.0
 421	421.0
 424	848.0
 427	427.0
 429	858.0
-43	43.0
 430	1290.0
 431	1293.0
 432	432.0
@@ -345,7 +379,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 437	437.0
 438	1314.0
 439	878.0
-44	44.0
 443	443.0
 444	444.0
 446	446.0
@@ -365,7 +398,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 467	467.0
 468	1872.0
 469	2345.0
-47	47.0
 470	470.0
 472	472.0
 475	475.0
@@ -389,35 +421,3 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 496	496.0
 497	497.0
 498	1494.0
-5	15.0
-51	102.0
-53	53.0
-54	54.0
-57	57.0
-58	116.0
-64	64.0
-65	65.0
-66	66.0
-67	134.0
-69	69.0
-70	210.0
-72	144.0
-74	74.0
-76	152.0
-77	77.0
-78	78.0
-8	8.0
-80	80.0
-82	82.0
-83	166.0
-84	168.0
-85	85.0
-86	86.0
-87	87.0
-9	9.0
-90	270.0
-92	92.0
-95	190.0
-96	96.0
-97	194.0
-98	196.0
diff --git a/src/ql/src/test/results/clientpositive/groupby2_noskew.q.out b/src/ql/src/test/results/clientpositive/groupby2_noskew.q.out
index 9347270..87fbb1f 100644
--- a/src/ql/src/test/results/clientpositive/groupby2_noskew.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby2_noskew.q.out
@@ -100,11 +100,11 @@ POSTHOOK: Output: default@dest_g2
 POSTHOOK: Lineage: dest_g2.c1 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_g2.c2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_g2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: SELECT dest_g2.* FROM dest_g2
+PREHOOK: query: SELECT dest_g2.* FROM dest_g2 order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_g2
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT dest_g2.* FROM dest_g2
+POSTHOOK: query: SELECT dest_g2.* FROM dest_g2 order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_g2
 #### A masked pattern was here ####
diff --git a/src/ql/src/test/results/clientpositive/groupby2_noskew_multi_distinct.q.out b/src/ql/src/test/results/clientpositive/groupby2_noskew_multi_distinct.q.out
index 1750a5a..75c9097 100644
--- a/src/ql/src/test/results/clientpositive/groupby2_noskew_multi_distinct.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby2_noskew_multi_distinct.q.out
@@ -111,11 +111,11 @@ POSTHOOK: Lineage: dest_g2.c2 EXPRESSION [(src)src.FieldSchema(name:key, type:st
 POSTHOOK: Lineage: dest_g2.c3 EXPRESSION [(src)src.null, ]
 POSTHOOK: Lineage: dest_g2.c4 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_g2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: SELECT dest_g2.* FROM dest_g2
+PREHOOK: query: SELECT dest_g2.* FROM dest_g2 order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_g2
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT dest_g2.* FROM dest_g2
+POSTHOOK: query: SELECT dest_g2.* FROM dest_g2 order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_g2
 #### A masked pattern was here ####
diff --git a/src/ql/src/test/results/clientpositive/groupby5_noskew.q.out b/src/ql/src/test/results/clientpositive/groupby5_noskew.q.out
index 9bdf39e..b8d797b 100644
--- a/src/ql/src/test/results/clientpositive/groupby5_noskew.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby5_noskew.q.out
@@ -105,30 +105,83 @@ POSTHOOK: Input: default@src
 POSTHOOK: Output: default@dest1
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: SELECT dest1.* FROM dest1
+PREHOOK: query: SELECT dest1.* FROM dest1 order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT dest1.* FROM dest1
+POSTHOOK: query: SELECT dest1.* FROM dest1 order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
 #### A masked pattern was here ####
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	0.0
+2	2.0
+4	4.0
+5	15.0
+8	8.0
+9	9.0
 10	10.0
+11	11.0
+12	24.0
+15	30.0
+17	17.0
+18	36.0
+19	19.0
+20	20.0
+24	48.0
+26	52.0
+27	27.0
+28	28.0
+30	30.0
+33	33.0
+34	34.0
+35	105.0
+37	74.0
+41	41.0
+42	84.0
+43	43.0
+44	44.0
+47	47.0
+51	102.0
+53	53.0
+54	54.0
+57	57.0
+58	116.0
+64	64.0
+65	65.0
+66	66.0
+67	134.0
+69	69.0
+70	210.0
+72	144.0
+74	74.0
+76	152.0
+77	77.0
+78	78.0
+80	80.0
+82	82.0
+83	166.0
+84	168.0
+85	85.0
+86	86.0
+87	87.0
+90	270.0
+92	92.0
+95	190.0
+96	96.0
+97	194.0
+98	196.0
 100	200.0
 103	206.0
 104	208.0
 105	105.0
-11	11.0
 111	111.0
 113	226.0
 114	114.0
 116	116.0
 118	236.0
 119	357.0
-12	24.0
 120	240.0
 125	250.0
 126	126.0
@@ -144,7 +197,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 145	145.0
 146	292.0
 149	298.0
-15	30.0
 150	150.0
 152	304.0
 153	153.0
@@ -161,7 +213,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 167	501.0
 168	168.0
 169	676.0
-17	17.0
 170	170.0
 172	344.0
 174	348.0
@@ -170,14 +221,12 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 177	177.0
 178	178.0
 179	358.0
-18	36.0
 180	180.0
 181	181.0
 183	183.0
 186	186.0
 187	561.0
 189	189.0
-19	19.0
 190	190.0
 191	382.0
 192	192.0
@@ -187,8 +236,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 196	196.0
 197	394.0
 199	597.0
-2	2.0
-20	20.0
 200	400.0
 201	201.0
 202	202.0
@@ -216,7 +263,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 237	474.0
 238	476.0
 239	478.0
-24	48.0
 241	241.0
 242	484.0
 244	244.0
@@ -228,20 +274,17 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 256	512.0
 257	257.0
 258	258.0
-26	52.0
 260	260.0
 262	262.0
 263	263.0
 265	530.0
 266	266.0
-27	27.0
 272	544.0
 273	819.0
 274	274.0
 275	275.0
 277	1108.0
 278	556.0
-28	28.0
 280	560.0
 281	562.0
 282	564.0
@@ -256,7 +299,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 292	292.0
 296	296.0
 298	894.0
-30	30.0
 302	302.0
 305	305.0
 306	306.0
@@ -274,7 +316,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 323	323.0
 325	650.0
 327	981.0
-33	33.0
 331	662.0
 332	332.0
 333	666.0
@@ -282,13 +323,11 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 336	336.0
 338	338.0
 339	339.0
-34	34.0
 341	341.0
 342	684.0
 344	688.0
 345	345.0
 348	1740.0
-35	105.0
 351	351.0
 353	706.0
 356	356.0
@@ -300,7 +339,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 367	734.0
 368	368.0
 369	1107.0
-37	74.0
 373	373.0
 374	374.0
 375	375.0
@@ -318,7 +356,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 396	1188.0
 397	794.0
 399	798.0
-4	4.0
 400	400.0
 401	2005.0
 402	402.0
@@ -327,19 +364,16 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 406	1624.0
 407	407.0
 409	1227.0
-41	41.0
 411	411.0
 413	826.0
 414	828.0
 417	1251.0
 418	418.0
 419	419.0
-42	84.0
 421	421.0
 424	848.0
 427	427.0
 429	858.0
-43	43.0
 430	1290.0
 431	1293.0
 432	432.0
@@ -348,7 +382,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 437	437.0
 438	1314.0
 439	878.0
-44	44.0
 443	443.0
 444	444.0
 446	446.0
@@ -368,7 +401,6 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 467	467.0
 468	1872.0
 469	2345.0
-47	47.0
 470	470.0
 472	472.0
 475	475.0
@@ -392,35 +424,3 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 496	496.0
 497	497.0
 498	1494.0
-5	15.0
-51	102.0
-53	53.0
-54	54.0
-57	57.0
-58	116.0
-64	64.0
-65	65.0
-66	66.0
-67	134.0
-69	69.0
-70	210.0
-72	144.0
-74	74.0
-76	152.0
-77	77.0
-78	78.0
-8	8.0
-80	80.0
-82	82.0
-83	166.0
-84	168.0
-85	85.0
-86	86.0
-87	87.0
-9	9.0
-90	270.0
-92	92.0
-95	190.0
-96	96.0
-97	194.0
-98	196.0
diff --git a/src/ql/src/test/results/clientpositive/groupby6_map.q.out b/src/ql/src/test/results/clientpositive/groupby6_map.q.out
index 97c009c..dfee566 100644
--- a/src/ql/src/test/results/clientpositive/groupby6_map.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby6_map.q.out
@@ -94,11 +94,11 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Output: default@dest1
 POSTHOOK: Lineage: dest1.c1 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: SELECT dest1.* FROM dest1
+PREHOOK: query: SELECT dest1.* FROM dest1 order by c1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT dest1.* FROM dest1
+POSTHOOK: query: SELECT dest1.* FROM dest1 order by c1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
 #### A masked pattern was here ####
diff --git a/src/ql/src/test/results/clientpositive/groupby6_map_skew.q.out b/src/ql/src/test/results/clientpositive/groupby6_map_skew.q.out
index 827fa2d..e537c30 100644
--- a/src/ql/src/test/results/clientpositive/groupby6_map_skew.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby6_map_skew.q.out
@@ -125,11 +125,11 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Output: default@dest1
 POSTHOOK: Lineage: dest1.c1 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: SELECT dest1.* FROM dest1 ORDER BY c1
+PREHOOK: query: SELECT dest1.* FROM dest1 order by dest1.c1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT dest1.* FROM dest1 ORDER BY c1
+POSTHOOK: query: SELECT dest1.* FROM dest1 order by dest1.c1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
 #### A masked pattern was here ####
diff --git a/src/ql/src/test/results/clientpositive/groupby7_noskew.q.out b/src/ql/src/test/results/clientpositive/groupby7_noskew.q.out
index 35a8702..7d08d4d 100644
--- a/src/ql/src/test/results/clientpositive/groupby7_noskew.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby7_noskew.q.out
@@ -183,11 +183,11 @@ POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:str
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: SELECT DEST1.* FROM DEST1
+PREHOOK: query: SELECT DEST1.* FROM DEST1 order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT DEST1.* FROM DEST1
+POSTHOOK: query: SELECT DEST1.* FROM DEST1 order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
 #### A masked pattern was here ####
@@ -196,19 +196,72 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	0.0
+2	2.0
+4	4.0
+5	15.0
+8	8.0
+9	9.0
 10	10.0
+11	11.0
+12	24.0
+15	30.0
+17	17.0
+18	36.0
+19	19.0
+20	20.0
+24	48.0
+26	52.0
+27	27.0
+28	28.0
+30	30.0
+33	33.0
+34	34.0
+35	105.0
+37	74.0
+41	41.0
+42	84.0
+43	43.0
+44	44.0
+47	47.0
+51	102.0
+53	53.0
+54	54.0
+57	57.0
+58	116.0
+64	64.0
+65	65.0
+66	66.0
+67	134.0
+69	69.0
+70	210.0
+72	144.0
+74	74.0
+76	152.0
+77	77.0
+78	78.0
+80	80.0
+82	82.0
+83	166.0
+84	168.0
+85	85.0
+86	86.0
+87	87.0
+90	270.0
+92	92.0
+95	190.0
+96	96.0
+97	194.0
+98	196.0
 100	200.0
 103	206.0
 104	208.0
 105	105.0
-11	11.0
 111	111.0
 113	226.0
 114	114.0
 116	116.0
 118	236.0
 119	357.0
-12	24.0
 120	240.0
 125	250.0
 126	126.0
@@ -224,7 +277,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 145	145.0
 146	292.0
 149	298.0
-15	30.0
 150	150.0
 152	304.0
 153	153.0
@@ -241,7 +293,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 167	501.0
 168	168.0
 169	676.0
-17	17.0
 170	170.0
 172	344.0
 174	348.0
@@ -250,14 +301,12 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 177	177.0
 178	178.0
 179	358.0
-18	36.0
 180	180.0
 181	181.0
 183	183.0
 186	186.0
 187	561.0
 189	189.0
-19	19.0
 190	190.0
 191	382.0
 192	192.0
@@ -267,8 +316,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 196	196.0
 197	394.0
 199	597.0
-2	2.0
-20	20.0
 200	400.0
 201	201.0
 202	202.0
@@ -296,7 +343,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 237	474.0
 238	476.0
 239	478.0
-24	48.0
 241	241.0
 242	484.0
 244	244.0
@@ -308,20 +354,17 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 256	512.0
 257	257.0
 258	258.0
-26	52.0
 260	260.0
 262	262.0
 263	263.0
 265	530.0
 266	266.0
-27	27.0
 272	544.0
 273	819.0
 274	274.0
 275	275.0
 277	1108.0
 278	556.0
-28	28.0
 280	560.0
 281	562.0
 282	564.0
@@ -336,7 +379,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 292	292.0
 296	296.0
 298	894.0
-30	30.0
 302	302.0
 305	305.0
 306	306.0
@@ -354,7 +396,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 323	323.0
 325	650.0
 327	981.0
-33	33.0
 331	662.0
 332	332.0
 333	666.0
@@ -362,13 +403,11 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 336	336.0
 338	338.0
 339	339.0
-34	34.0
 341	341.0
 342	684.0
 344	688.0
 345	345.0
 348	1740.0
-35	105.0
 351	351.0
 353	706.0
 356	356.0
@@ -380,7 +419,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 367	734.0
 368	368.0
 369	1107.0
-37	74.0
 373	373.0
 374	374.0
 375	375.0
@@ -398,7 +436,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 396	1188.0
 397	794.0
 399	798.0
-4	4.0
 400	400.0
 401	2005.0
 402	402.0
@@ -407,19 +444,16 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 406	1624.0
 407	407.0
 409	1227.0
-41	41.0
 411	411.0
 413	826.0
 414	828.0
 417	1251.0
 418	418.0
 419	419.0
-42	84.0
 421	421.0
 424	848.0
 427	427.0
 429	858.0
-43	43.0
 430	1290.0
 431	1293.0
 432	432.0
@@ -428,7 +462,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 437	437.0
 438	1314.0
 439	878.0
-44	44.0
 443	443.0
 444	444.0
 446	446.0
@@ -448,7 +481,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 467	467.0
 468	1872.0
 469	2345.0
-47	47.0
 470	470.0
 472	472.0
 475	475.0
@@ -472,7 +504,46 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 496	496.0
 497	497.0
 498	1494.0
+PREHOOK: query: SELECT DEST2.* FROM DEST2 order by key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@dest2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT DEST2.* FROM DEST2 order by key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@dest2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+0	0.0
+2	2.0
+4	4.0
 5	15.0
+8	8.0
+9	9.0
+10	10.0
+11	11.0
+12	24.0
+15	30.0
+17	17.0
+18	36.0
+19	19.0
+20	20.0
+24	48.0
+26	52.0
+27	27.0
+28	28.0
+30	30.0
+33	33.0
+34	34.0
+35	105.0
+37	74.0
+41	41.0
+42	84.0
+43	43.0
+44	44.0
+47	47.0
 51	102.0
 53	53.0
 54	54.0
@@ -489,7 +560,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 76	152.0
 77	77.0
 78	78.0
-8	8.0
 80	80.0
 82	82.0
 83	166.0
@@ -497,39 +567,22 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 85	85.0
 86	86.0
 87	87.0
-9	9.0
 90	270.0
 92	92.0
 95	190.0
 96	96.0
 97	194.0
 98	196.0
-PREHOOK: query: SELECT DEST2.* FROM DEST2
-PREHOOK: type: QUERY
-PREHOOK: Input: default@dest2
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT DEST2.* FROM DEST2
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@dest2
-#### A masked pattern was here ####
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-0	0.0
-10	10.0
 100	200.0
 103	206.0
 104	208.0
 105	105.0
-11	11.0
 111	111.0
 113	226.0
 114	114.0
 116	116.0
 118	236.0
 119	357.0
-12	24.0
 120	240.0
 125	250.0
 126	126.0
@@ -545,7 +598,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 145	145.0
 146	292.0
 149	298.0
-15	30.0
 150	150.0
 152	304.0
 153	153.0
@@ -562,7 +614,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 167	501.0
 168	168.0
 169	676.0
-17	17.0
 170	170.0
 172	344.0
 174	348.0
@@ -571,14 +622,12 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 177	177.0
 178	178.0
 179	358.0
-18	36.0
 180	180.0
 181	181.0
 183	183.0
 186	186.0
 187	561.0
 189	189.0
-19	19.0
 190	190.0
 191	382.0
 192	192.0
@@ -588,8 +637,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 196	196.0
 197	394.0
 199	597.0
-2	2.0
-20	20.0
 200	400.0
 201	201.0
 202	202.0
@@ -617,7 +664,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 237	474.0
 238	476.0
 239	478.0
-24	48.0
 241	241.0
 242	484.0
 244	244.0
@@ -629,20 +675,17 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 256	512.0
 257	257.0
 258	258.0
-26	52.0
 260	260.0
 262	262.0
 263	263.0
 265	530.0
 266	266.0
-27	27.0
 272	544.0
 273	819.0
 274	274.0
 275	275.0
 277	1108.0
 278	556.0
-28	28.0
 280	560.0
 281	562.0
 282	564.0
@@ -657,7 +700,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 292	292.0
 296	296.0
 298	894.0
-30	30.0
 302	302.0
 305	305.0
 306	306.0
@@ -675,7 +717,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 323	323.0
 325	650.0
 327	981.0
-33	33.0
 331	662.0
 332	332.0
 333	666.0
@@ -683,13 +724,11 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 336	336.0
 338	338.0
 339	339.0
-34	34.0
 341	341.0
 342	684.0
 344	688.0
 345	345.0
 348	1740.0
-35	105.0
 351	351.0
 353	706.0
 356	356.0
@@ -701,7 +740,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 367	734.0
 368	368.0
 369	1107.0
-37	74.0
 373	373.0
 374	374.0
 375	375.0
@@ -719,7 +757,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 396	1188.0
 397	794.0
 399	798.0
-4	4.0
 400	400.0
 401	2005.0
 402	402.0
@@ -728,19 +765,16 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 406	1624.0
 407	407.0
 409	1227.0
-41	41.0
 411	411.0
 413	826.0
 414	828.0
 417	1251.0
 418	418.0
 419	419.0
-42	84.0
 421	421.0
 424	848.0
 427	427.0
 429	858.0
-43	43.0
 430	1290.0
 431	1293.0
 432	432.0
@@ -749,7 +783,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 437	437.0
 438	1314.0
 439	878.0
-44	44.0
 443	443.0
 444	444.0
 446	446.0
@@ -769,7 +802,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 467	467.0
 468	1872.0
 469	2345.0
-47	47.0
 470	470.0
 472	472.0
 475	475.0
@@ -793,35 +825,3 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 496	496.0
 497	497.0
 498	1494.0
-5	15.0
-51	102.0
-53	53.0
-54	54.0
-57	57.0
-58	116.0
-64	64.0
-65	65.0
-66	66.0
-67	134.0
-69	69.0
-70	210.0
-72	144.0
-74	74.0
-76	152.0
-77	77.0
-78	78.0
-8	8.0
-80	80.0
-82	82.0
-83	166.0
-84	168.0
-85	85.0
-86	86.0
-87	87.0
-9	9.0
-90	270.0
-92	92.0
-95	190.0
-96	96.0
-97	194.0
-98	196.0
diff --git a/src/ql/src/test/results/clientpositive/groupby8_map.q.out b/src/ql/src/test/results/clientpositive/groupby8_map.q.out
index 2f94e6f..ea88ce7 100644
--- a/src/ql/src/test/results/clientpositive/groupby8_map.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby8_map.q.out
@@ -215,11 +215,11 @@ POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:str
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: SELECT DEST1.* FROM DEST1
+PREHOOK: query: SELECT DEST1.* FROM DEST1 order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT DEST1.* FROM DEST1
+POSTHOOK: query: SELECT DEST1.* FROM DEST1 order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
 #### A masked pattern was here ####
@@ -228,19 +228,72 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	1
+2	1
+4	1
+5	1
+8	1
+9	1
 10	1
+11	1
+12	1
+15	1
+17	1
+18	1
+19	1
+20	1
+24	1
+26	1
+27	1
+28	1
+30	1
+33	1
+34	1
+35	1
+37	1
+41	1
+42	1
+43	1
+44	1
+47	1
+51	1
+53	1
+54	1
+57	1
+58	1
+64	1
+65	1
+66	1
+67	1
+69	1
+70	1
+72	1
+74	1
+76	1
+77	1
+78	1
+80	1
+82	1
+83	1
+84	1
+85	1
+86	1
+87	1
+90	1
+92	1
+95	1
+96	1
+97	1
+98	1
 100	1
 103	1
 104	1
 105	1
-11	1
 111	1
 113	1
 114	1
 116	1
 118	1
 119	1
-12	1
 120	1
 125	1
 126	1
@@ -256,7 +309,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 145	1
 146	1
 149	1
-15	1
 150	1
 152	1
 153	1
@@ -273,7 +325,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 167	1
 168	1
 169	1
-17	1
 170	1
 172	1
 174	1
@@ -282,14 +333,12 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 177	1
 178	1
 179	1
-18	1
 180	1
 181	1
 183	1
 186	1
 187	1
 189	1
-19	1
 190	1
 191	1
 192	1
@@ -299,8 +348,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 196	1
 197	1
 199	1
-2	1
-20	1
 200	1
 201	1
 202	1
@@ -328,7 +375,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 237	1
 238	1
 239	1
-24	1
 241	1
 242	1
 244	1
@@ -340,20 +386,17 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 256	1
 257	1
 258	1
-26	1
 260	1
 262	1
 263	1
 265	1
 266	1
-27	1
 272	1
 273	1
 274	1
 275	1
 277	1
 278	1
-28	1
 280	1
 281	1
 282	1
@@ -368,7 +411,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 292	1
 296	1
 298	1
-30	1
 302	1
 305	1
 306	1
@@ -386,7 +428,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 323	1
 325	1
 327	1
-33	1
 331	1
 332	1
 333	1
@@ -394,13 +435,11 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 336	1
 338	1
 339	1
-34	1
 341	1
 342	1
 344	1
 345	1
 348	1
-35	1
 351	1
 353	1
 356	1
@@ -412,7 +451,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 367	1
 368	1
 369	1
-37	1
 373	1
 374	1
 375	1
@@ -430,7 +468,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 396	1
 397	1
 399	1
-4	1
 400	1
 401	1
 402	1
@@ -439,19 +476,16 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 406	1
 407	1
 409	1
-41	1
 411	1
 413	1
 414	1
 417	1
 418	1
 419	1
-42	1
 421	1
 424	1
 427	1
 429	1
-43	1
 430	1
 431	1
 432	1
@@ -460,7 +494,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 437	1
 438	1
 439	1
-44	1
 443	1
 444	1
 446	1
@@ -480,7 +513,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 467	1
 468	1
 469	1
-47	1
 470	1
 472	1
 475	1
@@ -504,7 +536,46 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 496	1
 497	1
 498	1
+PREHOOK: query: SELECT DEST2.* FROM DEST2 order by key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@dest2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT DEST2.* FROM DEST2 order by key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@dest2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+0	1
+2	1
+4	1
 5	1
+8	1
+9	1
+10	1
+11	1
+12	1
+15	1
+17	1
+18	1
+19	1
+20	1
+24	1
+26	1
+27	1
+28	1
+30	1
+33	1
+34	1
+35	1
+37	1
+41	1
+42	1
+43	1
+44	1
+47	1
 51	1
 53	1
 54	1
@@ -521,7 +592,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 76	1
 77	1
 78	1
-8	1
 80	1
 82	1
 83	1
@@ -529,39 +599,22 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 85	1
 86	1
 87	1
-9	1
 90	1
 92	1
 95	1
 96	1
 97	1
 98	1
-PREHOOK: query: SELECT DEST2.* FROM DEST2
-PREHOOK: type: QUERY
-PREHOOK: Input: default@dest2
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT DEST2.* FROM DEST2
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@dest2
-#### A masked pattern was here ####
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-0	1
-10	1
 100	1
 103	1
 104	1
 105	1
-11	1
 111	1
 113	1
 114	1
 116	1
 118	1
 119	1
-12	1
 120	1
 125	1
 126	1
@@ -577,7 +630,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 145	1
 146	1
 149	1
-15	1
 150	1
 152	1
 153	1
@@ -594,7 +646,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 167	1
 168	1
 169	1
-17	1
 170	1
 172	1
 174	1
@@ -603,14 +654,12 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 177	1
 178	1
 179	1
-18	1
 180	1
 181	1
 183	1
 186	1
 187	1
 189	1
-19	1
 190	1
 191	1
 192	1
@@ -620,8 +669,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 196	1
 197	1
 199	1
-2	1
-20	1
 200	1
 201	1
 202	1
@@ -649,7 +696,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 237	1
 238	1
 239	1
-24	1
 241	1
 242	1
 244	1
@@ -661,20 +707,17 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 256	1
 257	1
 258	1
-26	1
 260	1
 262	1
 263	1
 265	1
 266	1
-27	1
 272	1
 273	1
 274	1
 275	1
 277	1
 278	1
-28	1
 280	1
 281	1
 282	1
@@ -689,7 +732,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 292	1
 296	1
 298	1
-30	1
 302	1
 305	1
 306	1
@@ -707,7 +749,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 323	1
 325	1
 327	1
-33	1
 331	1
 332	1
 333	1
@@ -715,13 +756,11 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 336	1
 338	1
 339	1
-34	1
 341	1
 342	1
 344	1
 345	1
 348	1
-35	1
 351	1
 353	1
 356	1
@@ -733,7 +772,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 367	1
 368	1
 369	1
-37	1
 373	1
 374	1
 375	1
@@ -751,7 +789,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 396	1
 397	1
 399	1
-4	1
 400	1
 401	1
 402	1
@@ -760,19 +797,16 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 406	1
 407	1
 409	1
-41	1
 411	1
 413	1
 414	1
 417	1
 418	1
 419	1
-42	1
 421	1
 424	1
 427	1
 429	1
-43	1
 430	1
 431	1
 432	1
@@ -781,7 +815,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 437	1
 438	1
 439	1
-44	1
 443	1
 444	1
 446	1
@@ -801,7 +834,6 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 467	1
 468	1
 469	1
-47	1
 470	1
 472	1
 475	1
@@ -825,35 +857,3 @@ POSTHOOK: Lineage: dest2.value EXPRESSION [(src)src.FieldSchema(name:value, type
 496	1
 497	1
 498	1
-5	1
-51	1
-53	1
-54	1
-57	1
-58	1
-64	1
-65	1
-66	1
-67	1
-69	1
-70	1
-72	1
-74	1
-76	1
-77	1
-78	1
-8	1
-80	1
-82	1
-83	1
-84	1
-85	1
-86	1
-87	1
-9	1
-90	1
-92	1
-95	1
-96	1
-97	1
-98	1
diff --git a/src/ql/src/test/results/clientpositive/groupby_map_ppr.q.out b/src/ql/src/test/results/clientpositive/groupby_map_ppr.q.out
index 317ce80..f2e9afe 100644
--- a/src/ql/src/test/results/clientpositive/groupby_map_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby_map_ppr.q.out
@@ -262,11 +262,11 @@ POSTHOOK: Output: default@dest1
 POSTHOOK: Lineage: dest1.c1 EXPRESSION [(srcpart)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c2 EXPRESSION [(srcpart)src.FieldSchema(name:key, type:string, comment:default), (srcpart)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.key EXPRESSION [(srcpart)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: SELECT dest1.* FROM dest1
+PREHOOK: query: SELECT dest1.* FROM dest1 order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT dest1.* FROM dest1
+POSTHOOK: query: SELECT dest1.* FROM dest1 order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
 #### A masked pattern was here ####
diff --git a/src/ql/src/test/results/clientpositive/groupby_sort_1.q.out b/src/ql/src/test/results/clientpositive/groupby_sort_1.q.out
index 7ecac4c..fdc2b67 100644
--- a/src/ql/src/test/results/clientpositive/groupby_sort_1.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby_sort_1.q.out
@@ -33,14 +33,14 @@ POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 PREHOOK: query: -- The plan should be converted to a map-side group by if the group by key
 -- matches the sorted key
 -- addind a order by at the end to make the test results deterministic
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key, count(1) FROM T1 GROUP BY key
 PREHOOK: type: QUERY
 POSTHOOK: query: -- The plan should be converted to a map-side group by if the group by key
 -- matches the sorted key
 -- addind a order by at the end to make the test results deterministic
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key, count(1) FROM T1 GROUP BY key
 POSTHOOK: type: QUERY
@@ -66,7 +66,6 @@ STAGE PLANS:
         t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -91,78 +90,11 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          bucket_count -1
-                          columns key,cnt
-                          columns.types int:int
-#### A masked pattern was here ####
-                          name default.outputtbl1
-                          serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                          serialization.format 1
-                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: default.outputtbl1
-                    TotalFiles: 1
-                    GatherStats: true
-                    MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [t1]
 
   Stage: Stage-7
     Conditional Operator
@@ -177,161 +109,42 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl1
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key,cnt
-              columns.types int:int
-#### A masked pattern was here ####
-              name default.outputtbl1
-              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl1
-            name: default.outputtbl1
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-5
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key,cnt
-              columns.types int:int
-#### A masked pattern was here ####
-              name default.outputtbl1
-              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl1
-            name: default.outputtbl1
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-6
     Move Operator
@@ -381,12 +194,12 @@ POSTHOOK: Lineage: outputtbl1.key EXPRESSION [(t1)t1.FieldSchema(name:key, type:
 POSTHOOK: Lineage: t1.key SIMPLE [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comment:null), ]
 PREHOOK: query: -- no map-side group by even if the group by key is a superset of sorted key
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl2
 SELECT key, val, count(1) FROM T1 GROUP BY key, val
 PREHOOK: type: QUERY
 POSTHOOK: query: -- no map-side group by even if the group by key is a superset of sorted key
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl2
 SELECT key, val, count(1) FROM T1 GROUP BY key, val
 POSTHOOK: type: QUERY
@@ -409,7 +222,6 @@ STAGE PLANS:
         t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -444,58 +256,6 @@ STAGE PLANS:
                   value expressions:
                         expr: _col2
                         type: bigint
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [t1]
-      Needs Tagging: false
       Reduce Operator Tree:
         Group By Operator
           aggregations:
@@ -520,53 +280,24 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
-#### A masked pattern was here ####
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,cnt
-                    columns.types int:string:int
-#### A masked pattern was here ####
-                    name default.outputtbl2
-                    serialization.ddl struct outputtbl2 { i32 key1, string key2, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl2
-              TotalFiles: 1
-              GatherStats: true
-              MultiFileSpray: false
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,cnt
-                columns.types int:string:int
-#### A masked pattern was here ####
-                name default.outputtbl2
-                serialization.ddl struct outputtbl2 { i32 key1, string key2, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl2
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
 
 PREHOOK: query: INSERT OVERWRITE TABLE outputTbl2
@@ -608,12 +339,12 @@ POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 8	18	1
 8	28	1
 PREHOOK: query: -- It should work for sub-queries
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key
 PREHOOK: type: QUERY
 POSTHOOK: query: -- It should work for sub-queries
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key
 POSTHOOK: type: QUERY
@@ -644,7 +375,6 @@ STAGE PLANS:
         subq1:t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -669,83 +399,11 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          bucket_count -1
-                          columns key,cnt
-                          columns.types int:int
-#### A masked pattern was here ####
-                          name default.outputtbl1
-                          numFiles 1
-                          numPartitions 0
-                          numRows 5
-                          rawDataSize 15
-                          serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                          serialization.format 1
-                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          totalSize 20
-#### A masked pattern was here ####
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: default.outputtbl1
-                    TotalFiles: 1
-                    GatherStats: true
-                    MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [subq1:t1]
 
   Stage: Stage-7
     Conditional Operator
@@ -760,196 +418,42 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 15
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 20
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl1
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    numFiles 1
-                    numPartitions 0
-                    numRows 5
-                    rawDataSize 15
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 20
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key,cnt
-              columns.types int:int
-#### A masked pattern was here ####
-              name default.outputtbl1
-              numFiles 1
-              numPartitions 0
-              numRows 5
-              rawDataSize 15
-              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 20
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 15
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 20
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl1
-            name: default.outputtbl1
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-5
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    numFiles 1
-                    numPartitions 0
-                    numRows 5
-                    rawDataSize 15
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 20
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key,cnt
-              columns.types int:int
-#### A masked pattern was here ####
-              name default.outputtbl1
-              numFiles 1
-              numPartitions 0
-              numRows 5
-              rawDataSize 15
-              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 20
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 15
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 20
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl1
-            name: default.outputtbl1
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-6
     Move Operator
@@ -1000,12 +504,12 @@ POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 7	1
 8	2
 PREHOOK: query: -- It should work for sub-queries with column aliases
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl1
 SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k
 PREHOOK: type: QUERY
 POSTHOOK: query: -- It should work for sub-queries with column aliases
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl1
 SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k
 POSTHOOK: type: QUERY
@@ -1038,7 +542,6 @@ STAGE PLANS:
         subq1:t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -1063,83 +566,11 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          bucket_count -1
-                          columns key,cnt
-                          columns.types int:int
-#### A masked pattern was here ####
-                          name default.outputtbl1
-                          numFiles 1
-                          numPartitions 0
-                          numRows 5
-                          rawDataSize 15
-                          serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                          serialization.format 1
-                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          totalSize 20
-#### A masked pattern was here ####
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: default.outputtbl1
-                    TotalFiles: 1
-                    GatherStats: true
-                    MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [subq1:t1]
 
   Stage: Stage-7
     Conditional Operator
@@ -1154,196 +585,42 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 15
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 20
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl1
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    numFiles 1
-                    numPartitions 0
-                    numRows 5
-                    rawDataSize 15
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 20
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key,cnt
-              columns.types int:int
-#### A masked pattern was here ####
-              name default.outputtbl1
-              numFiles 1
-              numPartitions 0
-              numRows 5
-              rawDataSize 15
-              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 20
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 15
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 20
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl1
-            name: default.outputtbl1
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-5
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    numFiles 1
-                    numPartitions 0
-                    numRows 5
-                    rawDataSize 15
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 20
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key,cnt
-              columns.types int:int
-#### A masked pattern was here ####
-              name default.outputtbl1
-              numFiles 1
-              numPartitions 0
-              numRows 5
-              rawDataSize 15
-              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 20
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 15
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 20
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl1
-            name: default.outputtbl1
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-6
     Move Operator
@@ -1415,13 +692,13 @@ POSTHOOK: Lineage: t1.key SIMPLE [(t1)t1.FieldSchema(name:key, type:string, comm
 POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comment:null), ]
 PREHOOK: query: -- The plan should be converted to a map-side group by if the group by key contains a constant followed
 -- by a match to the sorted key
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl3
 SELECT 1, key, count(1) FROM T1 GROUP BY 1, key
 PREHOOK: type: QUERY
 POSTHOOK: query: -- The plan should be converted to a map-side group by if the group by key contains a constant followed
 -- by a match to the sorted key
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl3
 SELECT 1, key, count(1) FROM T1 GROUP BY 1, key
 POSTHOOK: type: QUERY
@@ -1456,7 +733,6 @@ STAGE PLANS:
         t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -1485,78 +761,11 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          bucket_count -1
-                          columns key1,key2,cnt
-                          columns.types int:int:int
-#### A masked pattern was here ####
-                          name default.outputtbl3
-                          serialization.ddl struct outputtbl3 { i32 key1, i32 key2, i32 cnt}
-                          serialization.format 1
-                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: default.outputtbl3
-                    TotalFiles: 1
-                    GatherStats: true
-                    MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [t1]
 
   Stage: Stage-7
     Conditional Operator
@@ -1571,161 +780,42 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,cnt
-                columns.types int:int:int
-#### A masked pattern was here ####
-                name default.outputtbl3
-                serialization.ddl struct outputtbl3 { i32 key1, i32 key2, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl3
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,cnt
-                    columns.types int:int:int
-#### A masked pattern was here ####
-                    name default.outputtbl3
-                    serialization.ddl struct outputtbl3 { i32 key1, i32 key2, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl3
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key1,key2,cnt
-              columns.types int:int:int
-#### A masked pattern was here ####
-              name default.outputtbl3
-              serialization.ddl struct outputtbl3 { i32 key1, i32 key2, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,cnt
-                columns.types int:int:int
-#### A masked pattern was here ####
-                name default.outputtbl3
-                serialization.ddl struct outputtbl3 { i32 key1, i32 key2, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl3
-            name: default.outputtbl3
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-5
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,cnt
-                    columns.types int:int:int
-#### A masked pattern was here ####
-                    name default.outputtbl3
-                    serialization.ddl struct outputtbl3 { i32 key1, i32 key2, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl3
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key1,key2,cnt
-              columns.types int:int:int
-#### A masked pattern was here ####
-              name default.outputtbl3
-              serialization.ddl struct outputtbl3 { i32 key1, i32 key2, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,cnt
-                columns.types int:int:int
-#### A masked pattern was here ####
-                name default.outputtbl3
-                serialization.ddl struct outputtbl3 { i32 key1, i32 key2, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl3
-            name: default.outputtbl3
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-6
     Move Operator
@@ -1805,12 +895,12 @@ POSTHOOK: Lineage: outputtbl3.key2 EXPRESSION [(t1)t1.FieldSchema(name:key, type
 POSTHOOK: Lineage: t1.key SIMPLE [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comment:null), ]
 PREHOOK: query: -- no map-side group by if the group by key contains a constant followed by another column
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl4
 SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val
 PREHOOK: type: QUERY
 POSTHOOK: query: -- no map-side group by if the group by key contains a constant followed by another column
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl4
 SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val
 POSTHOOK: type: QUERY
@@ -1843,7 +933,6 @@ STAGE PLANS:
         t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -1884,58 +973,6 @@ STAGE PLANS:
                   value expressions:
                         expr: _col3
                         type: bigint
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [t1]
-      Needs Tagging: false
       Reduce Operator Tree:
         Group By Operator
           aggregations:
@@ -1964,53 +1001,24 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
-#### A masked pattern was here ####
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,key3,cnt
-                    columns.types int:int:string:int
-#### A masked pattern was here ####
-                    name default.outputtbl4
-                    serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl4
-              TotalFiles: 1
-              GatherStats: true
-              MultiFileSpray: false
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,cnt
-                columns.types int:int:string:int
-#### A masked pattern was here ####
-                name default.outputtbl4
-                serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl4
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
 
 PREHOOK: query: INSERT OVERWRITE TABLE outputTbl4
@@ -2074,12 +1082,12 @@ POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 8	1	18	1
 8	1	28	1
 PREHOOK: query: -- no map-side group by if the group by key contains a function
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl3
 SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1
 PREHOOK: type: QUERY
 POSTHOOK: query: -- no map-side group by if the group by key contains a function
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl3
 SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1
 POSTHOOK: type: QUERY
@@ -2116,7 +1124,6 @@ STAGE PLANS:
         t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -2149,58 +1156,6 @@ STAGE PLANS:
                   value expressions:
                         expr: _col2
                         type: bigint
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [t1]
-      Needs Tagging: false
       Reduce Operator Tree:
         Group By Operator
           aggregations:
@@ -2225,63 +1180,24 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
-#### A masked pattern was here ####
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,cnt
-                    columns.types int:int:int
-#### A masked pattern was here ####
-                    name default.outputtbl3
-                    numFiles 1
-                    numPartitions 0
-                    numRows 5
-                    rawDataSize 25
-                    serialization.ddl struct outputtbl3 { i32 key1, i32 key2, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 30
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl3
-              TotalFiles: 1
-              GatherStats: true
-              MultiFileSpray: false
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,cnt
-                columns.types int:int:int
-#### A masked pattern was here ####
-                name default.outputtbl3
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 25
-                serialization.ddl struct outputtbl3 { i32 key1, i32 key2, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl3
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
 
 PREHOOK: query: INSERT OVERWRITE TABLE outputTbl3
@@ -2353,7 +1269,7 @@ PREHOOK: query: -- it should not matter what follows the group by
 -- test various cases
 
 -- group by followed by another group by
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key + key, sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
@@ -2363,7 +1279,7 @@ POSTHOOK: query: -- it should not matter what follows the group by
 -- test various cases
 
 -- group by followed by another group by
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key + key, sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
@@ -2405,7 +1321,6 @@ STAGE PLANS:
         subq1:t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -2448,58 +1363,6 @@ STAGE PLANS:
                       value expressions:
                             expr: _col1
                             type: bigint
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [subq1:t1]
-      Needs Tagging: false
       Reduce Operator Tree:
         Group By Operator
           aggregations:
@@ -2520,63 +1383,24 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
-#### A masked pattern was here ####
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    numFiles 1
-                    numPartitions 0
-                    numRows 5
-                    rawDataSize 15
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 20
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: true
-              MultiFileSpray: false
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 15
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 20
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl1
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
 
 PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
@@ -2653,7 +1477,7 @@ POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 14	1
 16	2
 PREHOOK: query: -- group by followed by a union
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
@@ -2662,7 +1486,7 @@ SELECT key, count(1) FROM T1 GROUP BY key
 ) subq1
 PREHOOK: type: QUERY
 POSTHOOK: query: -- group by followed by a union
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
@@ -2713,7 +1537,6 @@ STAGE PLANS:
         null-subquery1:subq1-subquery1:t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -2746,36 +1569,14 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-#### A masked pattern was here ####
-                        NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
                             output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            properties:
-                              bucket_count -1
-                              columns key,cnt
-                              columns.types int:int
-#### A masked pattern was here ####
-                              name default.outputtbl1
-                              numFiles 1
-                              numPartitions 0
-                              numRows 5
-                              rawDataSize 17
-                              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                              serialization.format 1
-                              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              totalSize 22
-#### A masked pattern was here ####
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: default.outputtbl1
-                        TotalFiles: 1
-                        GatherStats: true
-                        MultiFileSpray: false
         null-subquery2:subq1-subquery2:t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -2808,83 +1609,11 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-#### A masked pattern was here ####
-                        NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
                             output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            properties:
-                              bucket_count -1
-                              columns key,cnt
-                              columns.types int:int
-#### A masked pattern was here ####
-                              name default.outputtbl1
-                              numFiles 1
-                              numPartitions 0
-                              numRows 5
-                              rawDataSize 17
-                              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                              serialization.format 1
-                              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              totalSize 22
-#### A masked pattern was here ####
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: default.outputtbl1
-                        TotalFiles: 1
-                        GatherStats: true
-                        MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [null-subquery1:subq1-subquery1:t1, null-subquery2:subq1-subquery2:t1]
 
   Stage: Stage-7
     Conditional Operator
@@ -2899,196 +1628,42 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 17
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 22
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl1
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    numFiles 1
-                    numPartitions 0
-                    numRows 5
-                    rawDataSize 17
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 22
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key,cnt
-              columns.types int:int
-#### A masked pattern was here ####
-              name default.outputtbl1
-              numFiles 1
-              numPartitions 0
-              numRows 5
-              rawDataSize 17
-              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 22
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 17
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 22
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl1
-            name: default.outputtbl1
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-5
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    numFiles 1
-                    numPartitions 0
-                    numRows 5
-                    rawDataSize 17
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 22
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key,cnt
-              columns.types int:int
-#### A masked pattern was here ####
-              name default.outputtbl1
-              numFiles 1
-              numPartitions 0
-              numRows 5
-              rawDataSize 17
-              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 22
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 17
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 22
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl1
-            name: default.outputtbl1
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-6
     Move Operator
@@ -3184,7 +1759,7 @@ POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 8	2
 8	2
 PREHOOK: query: -- group by followed by a union where one of the sub-queries is map-side group by
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
@@ -3193,7 +1768,7 @@ SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
 ) subq1
 PREHOOK: type: QUERY
 POSTHOOK: query: -- group by followed by a union where one of the sub-queries is map-side group by
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
@@ -3247,7 +1822,6 @@ STAGE PLANS:
         null-subquery2:subq1-subquery2:t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -3274,58 +1848,6 @@ STAGE PLANS:
                   value expressions:
                         expr: _col1
                         type: bigint
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [null-subquery2:subq1-subquery2:t1]
-      Needs Tagging: false
       Reduce Operator Tree:
         Group By Operator
           aggregations:
@@ -3346,27 +1868,16 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                  properties:
-                    columns _col0,_col1
-                    columns.types double,bigint
-                    escape.delim \
-                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                   serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             Union
               Select Operator
                 expressions:
@@ -3378,36 +1889,14 @@ STAGE PLANS:
                 File Output Operator
                   compressed: false
                   GlobalTableId: 1
-#### A masked pattern was here ####
-                  NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                      properties:
-                        bucket_count -1
-                        columns key,cnt
-                        columns.types int:int
-#### A masked pattern was here ####
-                        name default.outputtbl1
-                        numFiles 1
-                        numPartitions 0
-                        numRows 10
-                        rawDataSize 30
-                        serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                        serialization.format 1
-                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                        totalSize 40
-#### A masked pattern was here ####
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: default.outputtbl1
-                  TotalFiles: 1
-                  GatherStats: true
-                  MultiFileSpray: false
         null-subquery1:subq1-subquery1:t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -3440,104 +1929,11 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-#### A masked pattern was here ####
-                        NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
                             output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            properties:
-                              bucket_count -1
-                              columns key,cnt
-                              columns.types int:int
-#### A masked pattern was here ####
-                              name default.outputtbl1
-                              numFiles 1
-                              numPartitions 0
-                              numRows 10
-                              rawDataSize 30
-                              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                              serialization.format 1
-                              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              totalSize 40
-#### A masked pattern was here ####
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: default.outputtbl1
-                        TotalFiles: 1
-                        GatherStats: true
-                        MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -mr-10003
-            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-            properties:
-              columns _col0,_col1
-              columns.types double,bigint
-              escape.delim \
-              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-          
-              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-              properties:
-                columns _col0,_col1
-                columns.types double,bigint
-                escape.delim \
-                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [null-subquery1:subq1-subquery1:t1]
-#### A masked pattern was here ####
 
   Stage: Stage-8
     Conditional Operator
@@ -3552,196 +1948,42 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 10
-                rawDataSize 30
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 40
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl1
-#### A masked pattern was here ####
 
   Stage: Stage-3
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
   Stage: Stage-4
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    numFiles 1
-                    numPartitions 0
-                    numRows 10
-                    rawDataSize 30
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 40
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key,cnt
-              columns.types int:int
-#### A masked pattern was here ####
-              name default.outputtbl1
-              numFiles 1
-              numPartitions 0
-              numRows 10
-              rawDataSize 30
-              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 40
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 10
-                rawDataSize 30
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 40
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl1
-            name: default.outputtbl1
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-6
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    numFiles 1
-                    numPartitions 0
-                    numRows 10
-                    rawDataSize 30
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 40
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key,cnt
-              columns.types int:int
-#### A masked pattern was here ####
-              name default.outputtbl1
-              numFiles 1
-              numPartitions 0
-              numRows 10
-              rawDataSize 30
-              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 40
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 10
-                rawDataSize 30
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 40
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl1
-            name: default.outputtbl1
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-7
     Move Operator
@@ -3841,7 +2083,7 @@ POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 14	1
 16	2
 PREHOOK: query: -- group by followed by a join
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT subq1.key, subq1.cnt+subq2.cnt FROM 
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
@@ -3850,7 +2092,7 @@ JOIN
 ON subq1.key = subq2.key
 PREHOOK: type: QUERY
 POSTHOOK: query: -- group by followed by a join
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT subq1.key, subq1.cnt+subq2.cnt FROM 
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
@@ -3900,7 +2142,6 @@ STAGE PLANS:
         subq1:t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -3939,7 +2180,6 @@ STAGE PLANS:
         subq2:t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -3973,58 +2213,6 @@ STAGE PLANS:
                     value expressions:
                           expr: _col1
                           type: bigint
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [subq1:t1, subq2:t1]
-      Needs Tagging: true
       Reduce Operator Tree:
         Join Operator
           condition map:
@@ -4044,63 +2232,24 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
-#### A masked pattern was here ####
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    numFiles 1
-                    numPartitions 0
-                    numRows 10
-                    rawDataSize 32
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 42
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: true
-              MultiFileSpray: false
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 10
-                rawDataSize 32
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 42
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl1
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
 
 PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
@@ -4193,7 +2342,7 @@ POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 7	2
 8	4
 PREHOOK: query: -- group by followed by a join where one of the sub-queries can be performed in the mapper
-EXPLAIN EXTENDED 
+EXPLAIN 
 SELECT * FROM 
 (SELECT key, count(1) FROM T1 GROUP BY key) subq1
 JOIN
@@ -4201,7 +2350,7 @@ JOIN
 ON subq1.key = subq2.key
 PREHOOK: type: QUERY
 POSTHOOK: query: -- group by followed by a join where one of the sub-queries can be performed in the mapper
-EXPLAIN EXTENDED 
+EXPLAIN 
 SELECT * FROM 
 (SELECT key, count(1) FROM T1 GROUP BY key) subq1
 JOIN
@@ -4252,7 +2401,6 @@ STAGE PLANS:
         subq2:t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -4287,58 +2435,6 @@ STAGE PLANS:
                   value expressions:
                         expr: _col2
                         type: bigint
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [subq2:t1]
-      Needs Tagging: false
       Reduce Operator Tree:
         Group By Operator
           aggregations:
@@ -4363,27 +2459,16 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                  properties:
-                    columns _col0,_col1,_col2
-                    columns.types string,string,bigint
-                    escape.delim \
-                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                   serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
 
   Stage: Stage-1
     Map Reduce
       Alias -> Map Operator Tree:
         $INTNAME 
           TableScan
-            GatherStats: false
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -4403,7 +2488,6 @@ STAGE PLANS:
         subq1:t1 
           TableScan
             alias: t1
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -4439,79 +2523,6 @@ STAGE PLANS:
                           type: string
                           expr: _col1
                           type: bigint
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -mr-10002
-            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-            properties:
-              columns _col0,_col1,_col2
-              columns.types string,string,bigint
-              escape.delim \
-              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-          
-              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-              properties:
-                columns _col0,_col1,_col2
-                columns.types string,string,bigint
-                escape.delim \
-                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-#### A masked pattern was here ####
-          Partition
-            base file name: t1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t1
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t1 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t1
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t1 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t1
-            name: default.t1
-      Truncated Path -> Alias:
-        /t1 [subq1:t1]
-#### A masked pattern was here ####
-      Needs Tagging: true
       Reduce Operator Tree:
         Join Operator
           condition map:
@@ -4537,23 +2548,10 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
-#### A masked pattern was here ####
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    columns _col0,_col1,_col2,_col3,_col4
-                    columns.types string:bigint:string:string:bigint
-                    escape.delim \
-                    hive.serialization.extend.nesting.levels true
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
 
   Stage: Stage-0
     Fetch Operator
@@ -4638,12 +2636,12 @@ POSTHOOK: Lineage: t1.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 POSTHOOK: Lineage: t2.key SIMPLE [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: t2.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comment:null), ]
 PREHOOK: query: -- no mapside sort group by if the group by is a prefix of the sorted key
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key, count(1) FROM T2 GROUP BY key
 PREHOOK: type: QUERY
 POSTHOOK: query: -- no mapside sort group by if the group by is a prefix of the sorted key
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT key, count(1) FROM T2 GROUP BY key
 POSTHOOK: type: QUERY
@@ -4693,7 +2691,6 @@ STAGE PLANS:
         t2 
           TableScan
             alias: t2
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -4720,58 +2717,6 @@ STAGE PLANS:
                   value expressions:
                         expr: _col1
                         type: bigint
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t2
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t2
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t2 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t2
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t2 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t2
-            name: default.t2
-      Truncated Path -> Alias:
-        /t2 [t2]
-      Needs Tagging: false
       Reduce Operator Tree:
         Group By Operator
           aggregations:
@@ -4792,63 +2737,24 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
-#### A masked pattern was here ####
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key,cnt
-                    columns.types int:int
-#### A masked pattern was here ####
-                    name default.outputtbl1
-                    numFiles 1
-                    numPartitions 0
-                    numRows 5
-                    rawDataSize 15
-                    serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 20
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl1
-              TotalFiles: 1
-              GatherStats: true
-              MultiFileSpray: false
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,cnt
-                columns.types int:int
-#### A masked pattern was here ####
-                name default.outputtbl1
-                numFiles 1
-                numPartitions 0
-                numRows 5
-                rawDataSize 15
-                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 20
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl1
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
 
 PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
@@ -4942,13 +2848,13 @@ POSTHOOK: Lineage: t2.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 8	2
 PREHOOK: query: -- The plan should be converted to a map-side group by if the group by key contains a constant in between the
 -- sorted keys
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl4
 SELECT key, 1, val, count(1) FROM T2 GROUP BY key, 1, val
 PREHOOK: type: QUERY
 POSTHOOK: query: -- The plan should be converted to a map-side group by if the group by key contains a constant in between the
 -- sorted keys
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl4
 SELECT key, 1, val, count(1) FROM T2 GROUP BY key, 1, val
 POSTHOOK: type: QUERY
@@ -5005,7 +2911,6 @@ STAGE PLANS:
         t2 
           TableScan
             alias: t2
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -5040,83 +2945,11 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          bucket_count -1
-                          columns key1,key2,key3,cnt
-                          columns.types int:int:string:int
-#### A masked pattern was here ####
-                          name default.outputtbl4
-                          numFiles 1
-                          numPartitions 0
-                          numRows 6
-                          rawDataSize 48
-                          serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                          serialization.format 1
-                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          totalSize 54
-#### A masked pattern was here ####
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: default.outputtbl4
-                    TotalFiles: 1
-                    GatherStats: true
-                    MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t2
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t2
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t2 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t2
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t2 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t2
-            name: default.t2
-      Truncated Path -> Alias:
-        /t2 [t2]
 
   Stage: Stage-7
     Conditional Operator
@@ -5131,196 +2964,42 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,cnt
-                columns.types int:int:string:int
-#### A masked pattern was here ####
-                name default.outputtbl4
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 48
-                serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 54
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl4
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,key3,cnt
-                    columns.types int:int:string:int
-#### A masked pattern was here ####
-                    name default.outputtbl4
-                    numFiles 1
-                    numPartitions 0
-                    numRows 6
-                    rawDataSize 48
-                    serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 54
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl4
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key1,key2,key3,cnt
-              columns.types int:int:string:int
-#### A masked pattern was here ####
-              name default.outputtbl4
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 48
-              serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 54
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,cnt
-                columns.types int:int:string:int
-#### A masked pattern was here ####
-                name default.outputtbl4
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 48
-                serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 54
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl4
-            name: default.outputtbl4
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-5
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,key3,cnt
-                    columns.types int:int:string:int
-#### A masked pattern was here ####
-                    name default.outputtbl4
-                    numFiles 1
-                    numPartitions 0
-                    numRows 6
-                    rawDataSize 48
-                    serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 54
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl4
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key1,key2,key3,cnt
-              columns.types int:int:string:int
-#### A masked pattern was here ####
-              name default.outputtbl4
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 48
-              serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 54
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,cnt
-                columns.types int:int:string:int
-#### A masked pattern was here ####
-                name default.outputtbl4
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 48
-                serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 54
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl4
-            name: default.outputtbl4
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-6
     Move Operator
@@ -5471,13 +3150,13 @@ POSTHOOK: Lineage: t2.key SIMPLE [(t1)t1.FieldSchema(name:key, type:string, comm
 POSTHOOK: Lineage: t2.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comment:null), ]
 PREHOOK: query: -- The plan should be converted to a map-side group by if the group by key contains a constant in between the
 -- sorted keys followed by anything
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl5
 SELECT key, 1, val, 2, count(1) FROM T2 GROUP BY key, 1, val, 2
 PREHOOK: type: QUERY
 POSTHOOK: query: -- The plan should be converted to a map-side group by if the group by key contains a constant in between the
 -- sorted keys followed by anything
-EXPLAIN EXTENDED 
+EXPLAIN 
 INSERT OVERWRITE TABLE outputTbl5
 SELECT key, 1, val, 2, count(1) FROM T2 GROUP BY key, 1, val, 2
 POSTHOOK: type: QUERY
@@ -5538,7 +3217,6 @@ STAGE PLANS:
         t2 
           TableScan
             alias: t2
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -5577,78 +3255,11 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          bucket_count -1
-                          columns key1,key2,key3,key4,cnt
-                          columns.types int:int:string:int:int
-#### A masked pattern was here ####
-                          name default.outputtbl5
-                          serialization.ddl struct outputtbl5 { i32 key1, i32 key2, string key3, i32 key4, i32 cnt}
-                          serialization.format 1
-                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: default.outputtbl5
-                    TotalFiles: 1
-                    GatherStats: true
-                    MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t2
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t2
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t2 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t2
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t2 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t2
-            name: default.t2
-      Truncated Path -> Alias:
-        /t2 [t2]
 
   Stage: Stage-7
     Conditional Operator
@@ -5663,161 +3274,42 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,key4,cnt
-                columns.types int:int:string:int:int
-#### A masked pattern was here ####
-                name default.outputtbl5
-                serialization.ddl struct outputtbl5 { i32 key1, i32 key2, string key3, i32 key4, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl5
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,key3,key4,cnt
-                    columns.types int:int:string:int:int
-#### A masked pattern was here ####
-                    name default.outputtbl5
-                    serialization.ddl struct outputtbl5 { i32 key1, i32 key2, string key3, i32 key4, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl5
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key1,key2,key3,key4,cnt
-              columns.types int:int:string:int:int
-#### A masked pattern was here ####
-              name default.outputtbl5
-              serialization.ddl struct outputtbl5 { i32 key1, i32 key2, string key3, i32 key4, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,key4,cnt
-                columns.types int:int:string:int:int
-#### A masked pattern was here ####
-                name default.outputtbl5
-                serialization.ddl struct outputtbl5 { i32 key1, i32 key2, string key3, i32 key4, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl5
-            name: default.outputtbl5
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-5
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,key3,key4,cnt
-                    columns.types int:int:string:int:int
-#### A masked pattern was here ####
-                    name default.outputtbl5
-                    serialization.ddl struct outputtbl5 { i32 key1, i32 key2, string key3, i32 key4, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl5
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key1,key2,key3,key4,cnt
-              columns.types int:int:string:int:int
-#### A masked pattern was here ####
-              name default.outputtbl5
-              serialization.ddl struct outputtbl5 { i32 key1, i32 key2, string key3, i32 key4, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,key4,cnt
-                columns.types int:int:string:int:int
-#### A masked pattern was here ####
-                name default.outputtbl5
-                serialization.ddl struct outputtbl5 { i32 key1, i32 key2, string key3, i32 key4, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl5
-            name: default.outputtbl5
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-6
     Move Operator
@@ -5937,14 +3429,14 @@ POSTHOOK: Lineage: t2.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 8	1	18	2	1
 8	1	28	2	1
 PREHOOK: query: -- contants from sub-queries should work fine
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl4
 SELECT key, constant, val, count(1) from 
 (SELECT key, 1 as constant, val from T2)subq
 group by key, constant, val
 PREHOOK: type: QUERY
 POSTHOOK: query: -- contants from sub-queries should work fine
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl4
 SELECT key, constant, val, count(1) from 
 (SELECT key, 1 as constant, val from T2)subq
@@ -6012,7 +3504,6 @@ STAGE PLANS:
         subq:t2 
           TableScan
             alias: t2
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -6049,83 +3540,11 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          bucket_count -1
-                          columns key1,key2,key3,cnt
-                          columns.types int:int:string:int
-#### A masked pattern was here ####
-                          name default.outputtbl4
-                          numFiles 1
-                          numPartitions 0
-                          numRows 6
-                          rawDataSize 48
-                          serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                          serialization.format 1
-                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          totalSize 54
-#### A masked pattern was here ####
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: default.outputtbl4
-                    TotalFiles: 1
-                    GatherStats: true
-                    MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t2
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t2
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t2 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t2
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t2 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t2
-            name: default.t2
-      Truncated Path -> Alias:
-        /t2 [subq:t2]
 
   Stage: Stage-7
     Conditional Operator
@@ -6140,196 +3559,42 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,cnt
-                columns.types int:int:string:int
-#### A masked pattern was here ####
-                name default.outputtbl4
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 48
-                serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 54
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl4
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,key3,cnt
-                    columns.types int:int:string:int
-#### A masked pattern was here ####
-                    name default.outputtbl4
-                    numFiles 1
-                    numPartitions 0
-                    numRows 6
-                    rawDataSize 48
-                    serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 54
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl4
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key1,key2,key3,cnt
-              columns.types int:int:string:int
-#### A masked pattern was here ####
-              name default.outputtbl4
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 48
-              serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 54
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,cnt
-                columns.types int:int:string:int
-#### A masked pattern was here ####
-                name default.outputtbl4
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 48
-                serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 54
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl4
-            name: default.outputtbl4
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-5
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,key3,cnt
-                    columns.types int:int:string:int
-#### A masked pattern was here ####
-                    name default.outputtbl4
-                    numFiles 1
-                    numPartitions 0
-                    numRows 6
-                    rawDataSize 48
-                    serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 54
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl4
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key1,key2,key3,cnt
-              columns.types int:int:string:int
-#### A masked pattern was here ####
-              name default.outputtbl4
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 48
-              serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 54
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,cnt
-                columns.types int:int:string:int
-#### A masked pattern was here ####
-                name default.outputtbl4
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 48
-                serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 54
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl4
-            name: default.outputtbl4
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-6
     Move Operator
@@ -6459,7 +3724,7 @@ POSTHOOK: Lineage: t2.val SIMPLE [(t1)t1.FieldSchema(name:val, type:string, comm
 8	1	18	1
 8	1	28	1
 PREHOOK: query: -- multiple levels of contants from sub-queries should work fine
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl4
 select key, constant3, val, count(1) from
 (
@@ -6469,7 +3734,7 @@ SELECT key, constant as constant2, val, 2 as constant3 from
 group by key, constant3, val
 PREHOOK: type: QUERY
 POSTHOOK: query: -- multiple levels of contants from sub-queries should work fine
-EXPLAIN EXTENDED
+EXPLAIN
 INSERT OVERWRITE TABLE outputTbl4
 select key, constant3, val, count(1) from
 (
@@ -6544,7 +3809,6 @@ STAGE PLANS:
         subq2:subq:t2 
           TableScan
             alias: t2
-            GatherStats: false
             Select Operator
               expressions:
                     expr: key
@@ -6581,83 +3845,11 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
-#### A masked pattern was here ####
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          bucket_count -1
-                          columns key1,key2,key3,cnt
-                          columns.types int:int:string:int
-#### A masked pattern was here ####
-                          name default.outputtbl4
-                          numFiles 1
-                          numPartitions 0
-                          numRows 6
-                          rawDataSize 48
-                          serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                          serialization.format 1
-                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          totalSize 54
-#### A masked pattern was here ####
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: default.outputtbl4
-                    TotalFiles: 1
-                    GatherStats: true
-                    MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: t2
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              SORTBUCKETCOLSPREFIX TRUE
-              bucket_count 2
-              bucket_field_name key
-              columns key,val
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.t2
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 24
-              serialization.ddl struct t2 { string key, string val}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 30
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                SORTBUCKETCOLSPREFIX TRUE
-                bucket_count 2
-                bucket_field_name key
-                columns key,val
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.t2
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 24
-                serialization.ddl struct t2 { string key, string val}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 30
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.t2
-            name: default.t2
-      Truncated Path -> Alias:
-        /t2 [subq2:subq:t2]
 
   Stage: Stage-7
     Conditional Operator
@@ -6672,196 +3864,42 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-#### A masked pattern was here ####
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,cnt
-                columns.types int:int:string:int
-#### A masked pattern was here ####
-                name default.outputtbl4
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 48
-                serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 54
-#### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.outputtbl4
-#### A masked pattern was here ####
 
   Stage: Stage-2
     Stats-Aggr Operator
-#### A masked pattern was here ####
 
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,key3,cnt
-                    columns.types int:int:string:int
-#### A masked pattern was here ####
-                    name default.outputtbl4
-                    numFiles 1
-                    numPartitions 0
-                    numRows 6
-                    rawDataSize 48
-                    serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 54
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl4
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key1,key2,key3,cnt
-              columns.types int:int:string:int
-#### A masked pattern was here ####
-              name default.outputtbl4
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 48
-              serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 54
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,cnt
-                columns.types int:int:string:int
-#### A masked pattern was here ####
-                name default.outputtbl4
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 48
-                serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 54
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl4
-            name: default.outputtbl4
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-5
     Map Reduce
       Alias -> Map Operator Tree:
 #### A masked pattern was here ####
           TableScan
-            GatherStats: false
             File Output Operator
               compressed: false
               GlobalTableId: 0
-#### A masked pattern was here ####
-              NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    bucket_count -1
-                    columns key1,key2,key3,cnt
-                    columns.types int:int:string:int
-#### A masked pattern was here ####
-                    name default.outputtbl4
-                    numFiles 1
-                    numPartitions 0
-                    numRows 6
-                    rawDataSize 48
-                    serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    totalSize 54
-#### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: default.outputtbl4
-              TotalFiles: 1
-              GatherStats: false
-              MultiFileSpray: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -ext-10002
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            properties:
-              bucket_count -1
-              columns key1,key2,key3,cnt
-              columns.types int:int:string:int
-#### A masked pattern was here ####
-              name default.outputtbl4
-              numFiles 1
-              numPartitions 0
-              numRows 6
-              rawDataSize 48
-              serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 54
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key1,key2,key3,cnt
-                columns.types int:int:string:int
-#### A masked pattern was here ####
-                name default.outputtbl4
-                numFiles 1
-                numPartitions 0
-                numRows 6
-                rawDataSize 48
-                serialization.ddl struct outputtbl4 { i32 key1, i32 key2, string key3, i32 cnt}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 54
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.outputtbl4
-            name: default.outputtbl4
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
 
   Stage: Stage-6
     Move Operator
diff --git a/src/ql/src/test/results/clientpositive/nullformat.q.out b/src/ql/src/test/results/clientpositive/nullformat.q.out
index 82379d7..99b60bf 100644
--- a/src/ql/src/test/results/clientpositive/nullformat.q.out
+++ b/src/ql/src/test/results/clientpositive/nullformat.q.out
@@ -53,6 +53,7 @@ STAGE PLANS:
           name: null_tab1
           isExternal: false
 
+
 PREHOOK: query: CREATE TABLE null_tab1(a STRING, b STRING) ROW FORMAT DELIMITED NULL DEFINED AS 'fooNull'
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE null_tab1(a STRING, b STRING) ROW FORMAT DELIMITED NULL DEFINED AS 'fooNull'
-- 
1.7.0.4

