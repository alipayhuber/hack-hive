From 8f046dffe9ea64b5ad51d39d6465476942295e01 Mon Sep 17 00:00:00 2001
From: Szehon Ho <szehon@cloudera.com>
Date: Mon, 27 Jan 2014 15:54:36 -0800
Subject: [PATCH 254/375] FIXUP: CDH-16952: MapredParquetInputFormat incorrect reuses footers

This includes CDH-only fixes of the JIRA (build and other differences with upstream)
---
 pom.xml                                            |    6 ++++
 ql/pom.xml                                         |    5 ++-
 .../ql/io/parquet/MapredParquetOutputFormat.java   |    9 ++----
 .../parquet/write/ParquetRecordWriterWrapper.java  |   31 +++++--------------
 4 files changed, 20 insertions(+), 31 deletions(-)

diff --git a/src/pom.xml b/src/pom.xml
index 95576e0..bc24ebc 100644
--- a/src/pom.xml
+++ b/src/pom.xml
@@ -133,6 +133,7 @@
         requires netty < 3.6.0 we force hadoops version
       -->
     <netty.version>3.4.0.Final</netty.version>
+    <parquet.version>${cdh.parquet.version}</parquet.version>
     <pig.version>${cdh.pig.version}</pig.version>
     <protobuf.version>${cdh.protobuf.version}</protobuf.version>
     <rat.version>0.8</rat.version>
@@ -931,6 +932,11 @@
         <dependencies>
           <dependency>
             <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-client</artifactId>
+            <version>${hadoop-23.version}</version>
+          </dependency>
+          <dependency>
+            <groupId>org.apache.hadoop</groupId>
             <artifactId>hadoop-common</artifactId>
             <version>${hadoop-23.version}</version>
           </dependency>
diff --git a/src/ql/pom.xml b/src/ql/pom.xml
index ba6a877..5fe44f9 100644
--- a/src/ql/pom.xml
+++ b/src/ql/pom.xml
@@ -69,6 +69,7 @@
     <dependency>
       <groupId>com.twitter</groupId>
       <artifactId>parquet-hadoop-bundle</artifactId>
+      <version>${parquet.version}</version>
     </dependency>
     <dependency>
       <groupId>commons-codec</groupId>
@@ -205,7 +206,8 @@
     <dependency>
       <groupId>com.twitter</groupId>
       <artifactId>parquet-column</artifactId>
-      <classifier>tests</classifier>
+      <version>${parquet.version}</version>
+      <type>test-jar</type>
       <scope>test</scope>
     </dependency>
     <dependency>
@@ -353,7 +355,6 @@
                   <include>org.apache.hive:hive-exec</include>
                   <include>org.apache.hive:hive-serde</include>
                   <include>com.esotericsoftware.kryo:kryo</include>
-                  <include>com.twiter:parquet-hadoop-bundle</include>
                   <include>org.apache.thrift:libthrift</include>
                   <include>commons-lang:commons-lang</include>
                   <include>org.json:json</include>
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java
index 7023082..ba5ecf2 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java
@@ -10,7 +10,7 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
+import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;
 import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
 import org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter;
 import org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport;
@@ -22,11 +22,8 @@
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapred.FileOutputFormat;
 import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordWriter;
-import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.OutputFormat;
 import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.hive.ql.io.FSRecordWriter;
 
 import parquet.hadoop.ParquetOutputFormat;
 
@@ -56,7 +53,7 @@ public void checkOutputSpecs(final FileSystem ignored, final JobConf job) throws
   }
 
   @Override
-  public RecordWriter<Void, ArrayWritable> getRecordWriter(
+  public org.apache.hadoop.mapred.RecordWriter getRecordWriter(
       final FileSystem ignored,
       final JobConf job,
       final String name,
@@ -71,7 +68,7 @@ public void checkOutputSpecs(final FileSystem ignored, final JobConf job) throws
    * contains the real output format
    */
   @Override
-  public FSRecordWriter getHiveRecordWriter(
+  public RecordWriter getHiveRecordWriter(
       final JobConf jobConf,
       final Path finalOutPath,
       final Class<? extends Writable> valueClass,
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java
index 1219975..83504a6 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java
@@ -5,23 +5,19 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
+import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;
 import org.apache.hadoop.io.ArrayWritable;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordWriter;
-import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapreduce.OutputFormat;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.hive.ql.io.FSRecordWriter;
 
 import parquet.hadoop.ParquetOutputFormat;
 import parquet.hadoop.util.ContextUtil;
 
-public class ParquetRecordWriterWrapper implements RecordWriter<Void, ArrayWritable>,
-  FSRecordWriter {
+public class ParquetRecordWriterWrapper implements RecordWriter {
 
   public static final Log LOG = LogFactory.getLog(ParquetRecordWriterWrapper.class);
 
@@ -51,31 +47,20 @@ public ParquetRecordWriterWrapper(
   }
 
   @Override
-  public void close(final Reporter reporter) throws IOException {
+  public void close(final boolean abort) throws IOException {
     try {
-      realWriter.close(taskContext);
-    } catch (final InterruptedException e) {
+      realWriter.close(null);
+    } catch (InterruptedException e) {
       throw new IOException(e);
     }
   }
 
   @Override
-  public void write(final Void key, final ArrayWritable value) throws IOException {
+  public void write(final Writable w) throws IOException {
     try {
-      realWriter.write(key, value);
-    } catch (final InterruptedException e) {
+      realWriter.write(null, (ArrayWritable) w);
+    } catch (InterruptedException e) {
       throw new IOException(e);
     }
   }
-
-  @Override
-  public void close(final boolean abort) throws IOException {
-    close(null);
-  }
-
-  @Override
-  public void write(final Writable w) throws IOException {
-    write(null, (ArrayWritable) w);
-  }
-
 }
-- 
1.7.0.4

