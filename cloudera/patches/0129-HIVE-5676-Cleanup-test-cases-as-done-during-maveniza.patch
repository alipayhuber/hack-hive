From 6eb9feb6832a2e81ba2005e7630a3eeb69989aa7 Mon Sep 17 00:00:00 2001
From: Brock Noland <brock@apache.org>
Date: Tue, 29 Oct 2013 15:25:23 +0000
Subject: [PATCH 129/375] HIVE-5676 - Cleanup test cases as done during mavenization (Brock Noland reviewed by Ashutosh Chauhan)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1536755 13f79535-47bb-0310-9956-ffa450edef68
---
 .gitignore                                         |    5 +-
 .../hadoop/hive/cli/TestCliDriverMethods.java      |   15 ++--
 common/src/java/conf/hive-log4j.properties         |    2 +-
 .../org/apache/hadoop/hive/common/LogUtils.java    |    7 +-
 .../java/org/apache/hadoop/hive/conf/HiveConf.java |   22 ++--
 .../org/apache/hive/common/util/HiveTestUtils.java |   37 +++++++
 .../org/apache/hadoop/hive/conf/TestHiveConf.java  |    4 +-
 .../apache/hadoop/hive/conf/TestHiveLogging.java   |   58 ++++-------
 .../test/resources/hive-exec-log4j-test.properties |    2 +-
 .../src/test/resources/hive-log4j-test.properties  |    2 +-
 .../mapreduce/FileOutputCommitterContainer.java    |   10 +--
 .../hcatalog/mapreduce/TestPassProperties.java     |    4 +-
 .../hcatalog/mapreduce/TestPassProperties.java     |    4 +-
 .../hadoop/hive/metastore/HiveMetaStore.java       |    7 +-
 .../hadoop/hive/metastore/MetaStoreUtils.java      |   32 ++----
 .../apache/hadoop/hive/metastore/ObjectStore.java  |    2 +-
 .../hive/metastore/TestMarkPartitionRemote.java    |   15 ++-
 ql/src/java/conf/hive-exec-log4j.properties        |    3 +-
 ql/src/java/org/apache/hadoop/hive/ql/Driver.java  |   77 +++++++-------
 .../apache/hadoop/hive/ql/exec/mr/ExecDriver.java  |    6 +
 .../apache/hadoop/hive/ql/exec/mr/MapRedTask.java  |    2 +-
 .../hadoop/hive/ql/exec/mr/MapredLocalTask.java    |    2 +-
 .../formatting/MetaDataPrettyFormatUtils.java      |    6 +-
 .../hive/ql/udf/generic/GenericUDFBridge.java      |   46 ++++----
 .../apache/hadoop/hive/ql/TestLocationQueries.java |    8 ++
 .../org/apache/hadoop/hive/ql/TestMTQueries.java   |    7 ++
 .../apache/hadoop/hive/ql/exec/TestExecDriver.java |  111 ++++++--------------
 .../hadoop/hive/ql/io/orc/TestMemoryManager.java   |    2 +-
 .../hive/ql/udf/generic/TestGenericUDFBridge.java  |   39 +++++++
 .../service/auth/TestCustomAuthentication.java     |  104 +++++++------------
 .../apache/hadoop/hive/shims/Hadoop23Shims.java    |   10 +--
 31 files changed, 317 insertions(+), 334 deletions(-)
 create mode 100644 common/src/java/org/apache/hive/common/util/HiveTestUtils.java
 create mode 100644 ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFBridge.java

diff --git a/src/.gitignore b/src/.gitignore
index c0e9b3c..a4c23af 100644
--- a/src/.gitignore
+++ b/src/.gitignore
@@ -13,7 +13,8 @@ common/src/gen
 *.iml
 *.ipr
 *.iws
-ql/derby.log
 derby.log
+datanucleus.log
 .arc
-ql/TempStatsStore
+TempStatsStore/
+target/
diff --git a/src/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java b/src/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java
index 3e1f491..a0e9505 100644
--- a/src/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java
+++ b/src/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java
@@ -156,23 +156,22 @@ public void testRun() throws Exception {
       historyFile.delete();
     }
     HiveConf configuration = new HiveConf();
-    CliSessionState ss = new CliSessionState(configuration);
-    CliSessionState.start(ss);
-    String[] args = {};
+    configuration.setBoolVar(ConfVars.HIVE_SESSION_HISTORY_ENABLED, true);
     PrintStream oldOut = System.out;
     ByteArrayOutputStream dataOut = new ByteArrayOutputStream();
     System.setOut(new PrintStream(dataOut));
-
     PrintStream oldErr = System.err;
     ByteArrayOutputStream dataErr = new ByteArrayOutputStream();
     System.setErr(new PrintStream(dataErr));
-
+    CliSessionState ss = new CliSessionState(configuration);
+    CliSessionState.start(ss);
+    String[] args = {};
 
     try {
       new FakeCliDriver().run(args);
-      assertTrue(dataOut.toString().contains("test message"));
-      assertTrue(dataErr.toString().contains("Hive history file="));
-      assertTrue(dataErr.toString().contains("File: fakeFile is not a file."));
+      assertTrue(dataOut.toString(), dataOut.toString().contains("test message"));
+      assertTrue(dataErr.toString(), dataErr.toString().contains("Hive history file="));
+      assertTrue(dataErr.toString(), dataErr.toString().contains("File: fakeFile is not a file."));
       dataOut.reset();
       dataErr.reset();
 
diff --git a/src/common/src/java/conf/hive-log4j.properties b/src/common/src/java/conf/hive-log4j.properties
index 6a95ec0..41dffa9 100644
--- a/src/common/src/java/conf/hive-log4j.properties
+++ b/src/common/src/java/conf/hive-log4j.properties
@@ -17,7 +17,7 @@
 # Define some default values that can be overridden by system properties
 hive.log.threshold=ALL
 hive.root.logger=INFO,DRFA
-hive.log.dir=/tmp/${user.name}
+hive.log.dir=${java.io.tmpdir}/${user.name}
 hive.log.file=hive.log
 
 # Define the root logger to the system property "hadoop.root.logger".
diff --git a/src/common/src/java/org/apache/hadoop/hive/common/LogUtils.java b/src/common/src/java/org/apache/hadoop/hive/common/LogUtils.java
index 23b9921..9118675 100644
--- a/src/common/src/java/org/apache/hadoop/hive/common/LogUtils.java
+++ b/src/common/src/java/org/apache/hadoop/hive/common/LogUtils.java
@@ -89,8 +89,11 @@ private static String initHiveLog4jCommon(ConfVars confVarName)
         // property speficied file found in local file system
         // use the specified file
         if (confVarName == HiveConf.ConfVars.HIVE_EXEC_LOG4J_FILE) {
-          System.setProperty(HiveConf.ConfVars.HIVEQUERYID.toString(),
-            HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYID));
+          String queryId = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYID);
+          if(queryId == null || (queryId = queryId.trim()).isEmpty()) {
+            queryId = "unknown-" + System.currentTimeMillis();
+          }
+          System.setProperty(HiveConf.ConfVars.HIVEQUERYID.toString(), queryId);
         }
         LogManager.resetConfiguration();
         PropertyConfigurator.configure(log4jFileName);
diff --git a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index bccded6..ec55251 100644
--- a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -754,7 +754,7 @@
     // Number of async threads
     HIVE_SERVER2_ASYNC_EXEC_THREADS("hive.server2.async.exec.threads", 50),
     // Number of seconds HiveServer2 shutdown will wait for async threads to terminate
-    HIVE_SERVER2_ASYNC_EXEC_SHUTDOWN_TIMEOUT("hive.server2.async.exec.shutdown.timeout", 10),
+    HIVE_SERVER2_ASYNC_EXEC_SHUTDOWN_TIMEOUT("hive.server2.async.exec.shutdown.timeout", 10L),
 
 
     // HiveServer2 auth configuration
@@ -966,12 +966,12 @@ public void verifyAndSet(String name, String value) throws IllegalArgumentExcept
   }
 
   public static int getIntVar(Configuration conf, ConfVars var) {
-    assert (var.valClass == Integer.class);
+    assert (var.valClass == Integer.class) : var.varname;
     return conf.getInt(var.varname, var.defaultIntVal);
   }
 
   public static void setIntVar(Configuration conf, ConfVars var, int val) {
-    assert (var.valClass == Integer.class);
+    assert (var.valClass == Integer.class) : var.varname;
     conf.setInt(var.varname, val);
   }
 
@@ -984,7 +984,7 @@ public void setIntVar(ConfVars var, int val) {
   }
 
   public static long getLongVar(Configuration conf, ConfVars var) {
-    assert (var.valClass == Long.class);
+    assert (var.valClass == Long.class) : var.varname;
     return conf.getLong(var.varname, var.defaultLongVal);
   }
 
@@ -993,7 +993,7 @@ public static long getLongVar(Configuration conf, ConfVars var, long defaultVal)
   }
 
   public static void setLongVar(Configuration conf, ConfVars var, long val) {
-    assert (var.valClass == Long.class);
+    assert (var.valClass == Long.class) : var.varname;
     conf.setLong(var.varname, val);
   }
 
@@ -1006,7 +1006,7 @@ public void setLongVar(ConfVars var, long val) {
   }
 
   public static float getFloatVar(Configuration conf, ConfVars var) {
-    assert (var.valClass == Float.class);
+    assert (var.valClass == Float.class) : var.varname;
     return conf.getFloat(var.varname, var.defaultFloatVal);
   }
 
@@ -1015,7 +1015,7 @@ public static float getFloatVar(Configuration conf, ConfVars var, float defaultV
   }
 
   public static void setFloatVar(Configuration conf, ConfVars var, float val) {
-    assert (var.valClass == Float.class);
+    assert (var.valClass == Float.class) : var.varname;
     ShimLoader.getHadoopShims().setFloatConf(conf, var.varname, val);
   }
 
@@ -1028,7 +1028,7 @@ public void setFloatVar(ConfVars var, float val) {
   }
 
   public static boolean getBoolVar(Configuration conf, ConfVars var) {
-    assert (var.valClass == Boolean.class);
+    assert (var.valClass == Boolean.class) : var.varname;
     return conf.getBoolean(var.varname, var.defaultBoolVal);
   }
 
@@ -1037,7 +1037,7 @@ public static boolean getBoolVar(Configuration conf, ConfVars var, boolean defau
   }
 
   public static void setBoolVar(Configuration conf, ConfVars var, boolean val) {
-    assert (var.valClass == Boolean.class);
+    assert (var.valClass == Boolean.class) : var.varname;
     conf.setBoolean(var.varname, val);
   }
 
@@ -1050,7 +1050,7 @@ public void setBoolVar(ConfVars var, boolean val) {
   }
 
   public static String getVar(Configuration conf, ConfVars var) {
-    assert (var.valClass == String.class);
+    assert (var.valClass == String.class) : var.varname;
     return conf.get(var.varname, var.defaultVal);
   }
 
@@ -1059,7 +1059,7 @@ public static String getVar(Configuration conf, ConfVars var, String defaultVal)
   }
 
   public static void setVar(Configuration conf, ConfVars var, String val) {
-    assert (var.valClass == String.class);
+    assert (var.valClass == String.class) : var.varname;
     conf.set(var.varname, val);
   }
 
diff --git a/src/common/src/java/org/apache/hive/common/util/HiveTestUtils.java b/src/common/src/java/org/apache/hive/common/util/HiveTestUtils.java
new file mode 100644
index 0000000..db34494
--- /dev/null
+++ b/src/common/src/java/org/apache/hive/common/util/HiveTestUtils.java
@@ -0,0 +1,37 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.common.util;
+
+import java.net.URL;
+
+import org.apache.hadoop.hive.common.classification.InterfaceAudience;
+import org.apache.hadoop.hive.common.classification.InterfaceStability;
+
+@InterfaceAudience.Private
+@InterfaceStability.Unstable
+public class HiveTestUtils {
+
+  public static String getFileFromClasspath(String name) {
+    URL url = ClassLoader.getSystemResource(name);
+    if (url == null) {
+      throw new IllegalArgumentException("Could not find " + name);
+    }
+    return url.getPath();
+  }
+}
diff --git a/src/common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java b/src/common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java
index 25cefef..a31238b 100644
--- a/src/common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java
+++ b/src/common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java
@@ -20,6 +20,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hive.common.util.HiveTestUtils;
 import org.junit.Assert;
 import org.junit.Test;
 
@@ -33,8 +34,7 @@
 public class TestHiveConf {
   @Test
   public void testHiveSitePath() throws Exception {
-    String expectedPath =
-        new Path(System.getProperty("test.build.resources") + "/hive-site.xml").toUri().getPath();
+    String expectedPath = HiveTestUtils.getFileFromClasspath("hive-site.xml");
     Assert.assertEquals(expectedPath, new HiveConf().getHiveSiteLocation().getPath());
   }
 
diff --git a/src/common/src/test/org/apache/hadoop/hive/conf/TestHiveLogging.java b/src/common/src/test/org/apache/hadoop/hive/conf/TestHiveLogging.java
index 1a5dd72..ebd122b 100644
--- a/src/common/src/test/org/apache/hadoop/hive/conf/TestHiveLogging.java
+++ b/src/common/src/test/org/apache/hadoop/hive/conf/TestHiveLogging.java
@@ -24,7 +24,8 @@
 import junit.framework.TestCase;
 
 import org.apache.hadoop.hive.common.LogUtils;
-import org.apache.hadoop.hive.common.LogUtils.LogInitializationException;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hive.common.util.HiveTestUtils;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 
 /**
@@ -43,53 +44,34 @@ public TestHiveLogging() {
     process = null;
   }
 
-  private void configLog(String hiveLog4jTest, String hiveExecLog4jTest) {
-    System.setProperty(ConfVars.HIVE_LOG4J_FILE.varname,
-      System.getProperty("test.build.resources") + "/" + hiveLog4jTest);
-    System.setProperty(ConfVars.HIVE_EXEC_LOG4J_FILE.varname,
-      System.getProperty("test.build.resources") + "/" + hiveExecLog4jTest);
-
-    String expectedLog4jPath = System.getProperty("test.build.resources")
-      + "/" + hiveLog4jTest;
-    String expectedLog4jExecPath = System.getProperty("test.build.resources")
-      + "/" + hiveExecLog4jTest;
+  private void configLog(String hiveLog4jTest, String hiveExecLog4jTest) 
+  throws Exception {
+    String expectedLog4jTestPath = HiveTestUtils.getFileFromClasspath(hiveLog4jTest);
+    String expectedLog4jExecPath = HiveTestUtils.getFileFromClasspath(hiveExecLog4jTest);
+    System.setProperty(ConfVars.HIVE_LOG4J_FILE.varname, expectedLog4jTestPath);
+    System.setProperty(ConfVars.HIVE_EXEC_LOG4J_FILE.varname, expectedLog4jExecPath);
 
-    try {
-      LogUtils.initHiveLog4j();
-    } catch (LogInitializationException e) {
-    }
+    LogUtils.initHiveLog4j();
 
     HiveConf conf = new HiveConf();
-    assertEquals(expectedLog4jPath, conf.getVar(ConfVars.HIVE_LOG4J_FILE));
+    assertEquals(expectedLog4jTestPath, conf.getVar(ConfVars.HIVE_LOG4J_FILE));
     assertEquals(expectedLog4jExecPath, conf.getVar(ConfVars.HIVE_EXEC_LOG4J_FILE));
   }
 
-  private void runCmd(String cmd) {
-    try {
-      process = runTime.exec(cmd);
-    } catch (IOException e) {
-      e.printStackTrace();
-    }
-    try {
-      process.waitFor();
-    } catch (InterruptedException e) {
-      e.printStackTrace();
-    }
+  private void runCmd(String cmd) throws Exception {
+    process = runTime.exec(cmd);
+    process.waitFor();
   }
 
-  private void getCmdOutput(String logFile) {
+  private void getCmdOutput(String logFile) throws Exception {
     boolean logCreated = false;
     BufferedReader buf = new BufferedReader(
       new InputStreamReader(process.getInputStream()));
     String line = "";
-    try {
-      while((line = buf.readLine()) != null) {
-        if (line.equals(logFile)) {
-          logCreated = true;
-        }
+    while((line = buf.readLine()) != null) {
+      if (line.equals(logFile)) {
+        logCreated = true;
       }
-    } catch (IOException e) {
-      e.printStackTrace();
     }
     assertEquals(true, logCreated);
   }
@@ -112,12 +94,12 @@ private void RunTest(String cleanCmd, String findCmd, String logFile,
   }
 
   public void testHiveLogging() throws Exception {
-    // customized log4j config log file to be: /tmp/hiveLog4jTest.log
-    String customLogPath = "/tmp/";
+    // customized log4j config log file to be: /tmp/TestHiveLogging/hiveLog4jTest.log
+    String customLogPath = "/tmp/" + System.getProperty("user.name") + "-TestHiveLogging/";
     String customLogName = "hiveLog4jTest.log";
     String customLogFile = customLogPath + customLogName;
     String customCleanCmd = "rm -rf " + customLogFile;
-    String customFindCmd = "find /tmp -name " + customLogName;
+    String customFindCmd = "find " + customLogPath + " -name " + customLogName;
     RunTest(customCleanCmd, customFindCmd, customLogFile,
       "hive-log4j-test.properties", "hive-exec-log4j-test.properties");
   }
diff --git a/src/common/src/test/resources/hive-exec-log4j-test.properties b/src/common/src/test/resources/hive-exec-log4j-test.properties
index ece5875..29eceb2 100644
--- a/src/common/src/test/resources/hive-exec-log4j-test.properties
+++ b/src/common/src/test/resources/hive-exec-log4j-test.properties
@@ -1,6 +1,6 @@
 # Define some default values that can be overridden by system properties
 hive.root.logger=INFO,FA
-hive.log.dir=/tmp
+hive.log.dir=/tmp/${user.name}-TestHiveLogging
 hive.log.file=hiveExecLog4jTest.log
 
 # Define the root logger to the system property "hadoop.root.logger".
diff --git a/src/common/src/test/resources/hive-log4j-test.properties b/src/common/src/test/resources/hive-log4j-test.properties
index 2f08e9a..c6f7cc8 100644
--- a/src/common/src/test/resources/hive-log4j-test.properties
+++ b/src/common/src/test/resources/hive-log4j-test.properties
@@ -1,6 +1,6 @@
 # Define some default values that can be overridden by system properties
 hive.root.logger=WARN,DRFA
-hive.log.dir=/tmp
+hive.log.dir=/tmp/${user.name}-TestHiveLogging
 hive.log.file=hiveLog4jTest.log
 
 # Define the root logger to the system property "hadoop.root.logger".
diff --git a/src/hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/FileOutputCommitterContainer.java b/src/hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/FileOutputCommitterContainer.java
index b041a14..512647c 100644
--- a/src/hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/FileOutputCommitterContainer.java
+++ b/src/hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/FileOutputCommitterContainer.java
@@ -585,13 +585,6 @@ private void discoverPartitions(JobContext context) throws IOException {
         }
       }
 
-      //      for (Entry<String,Map<String,String>> spec : partitionsDiscoveredByPath.entrySet()){
-      //        LOG.info("Partition "+ spec.getKey());
-      //        for (Entry<String,String> e : spec.getValue().entrySet()){
-      //          LOG.info(e.getKey() + "=>" +e.getValue());
-      //        }
-      //      }
-
       this.partitionsDiscovered = true;
     }
   }
@@ -652,7 +645,6 @@ private void registerPartitions(JobContext context) throws IOException{
       for(Partition ptn : partitionsToAdd){
         ptnInfos.add(InternalUtil.createPtnKeyValueMap(new Table(tableInfo.getTable()), ptn));
       }
-
       //Publish the new partition(s)
       if (dynamicPartitioningUsed && harProcessor.isEnabled() && (!partitionsToAdd.isEmpty())){
 
@@ -678,7 +670,7 @@ private void registerPartitions(JobContext context) throws IOException{
           throw e;
         }
 
-      }else{
+      } else {
         // no harProcessor, regular operation
         updateTableSchema(client, table, jobInfo.getOutputSchema());
         LOG.info("HAR not is not being used. The table {} has new partitions {}.", table.getTableName(), ptnInfos);
diff --git a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestPassProperties.java b/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestPassProperties.java
index 1aad829..6517826 100644
--- a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestPassProperties.java
+++ b/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestPassProperties.java
@@ -51,7 +51,7 @@
  */
 public class TestPassProperties {
   private static final String TEST_DATA_DIR = System.getProperty("user.dir") +
-      "/build/test/data/" + TestSequenceFileReadWrite.class.getCanonicalName();
+      "/build/test/data/" + TestPassProperties.class.getCanonicalName();
   private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
   private static final String INPUT_FILE_NAME = TEST_DATA_DIR + "/input.data";
 
@@ -96,7 +96,7 @@ public void testSequenceTableWriteReadMR() throws Exception {
       conf.set("hive.metastore.uris", "thrift://no.such.machine:10888");
       conf.set("hive.metastore.local", "false");
       Job job = new Job(conf, "Write-hcat-seq-table");
-      job.setJarByClass(TestSequenceFileReadWrite.class);
+      job.setJarByClass(TestPassProperties.class);
 
       job.setMapperClass(Map.class);
       job.setOutputKeyClass(NullWritable.class);
diff --git a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestPassProperties.java b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestPassProperties.java
index 5e2b699..81df987 100644
--- a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestPassProperties.java
+++ b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestPassProperties.java
@@ -48,7 +48,7 @@
 
 public class TestPassProperties {
   private static final String TEST_DATA_DIR = System.getProperty("user.dir") +
-      "/build/test/data/" + TestSequenceFileReadWrite.class.getCanonicalName();
+      "/build/test/data/" + TestPassProperties.class.getCanonicalName();
   private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
   private static final String INPUT_FILE_NAME = TEST_DATA_DIR + "/input.data";
 
@@ -93,7 +93,7 @@ public void testSequenceTableWriteReadMR() throws Exception {
       conf.set("hive.metastore.uris", "thrift://no.such.machine:10888");
       conf.set("hive.metastore.local", "false");
       Job job = new Job(conf, "Write-hcat-seq-table");
-      job.setJarByClass(TestSequenceFileReadWrite.class);
+      job.setJarByClass(TestPassProperties.class);
 
       job.setMapperClass(Map.class);
       job.setOutputKeyClass(NullWritable.class);
diff --git a/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
index 24b1832..23c272f 100644
--- a/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
+++ b/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
@@ -334,9 +334,7 @@ private boolean init() throws MetaException {
         } catch (Exception e) {
           // log exception, but ignore inability to start
           LOG.error("error in Metrics init: " + e.getClass().getName() + " "
-              + e.getMessage());
-          MetaStoreUtils.printStackTrace(e);
-
+              + e.getMessage(), e);
         }
       }
 
@@ -466,8 +464,7 @@ public String startFunction(String function, String extraLogInfo) {
         Metrics.startScope(function);
       } catch (IOException e) {
         LOG.debug("Exception when starting metrics scope"
-            + e.getClass().getName() + " " + e.getMessage());
-        MetaStoreUtils.printStackTrace(e);
+            + e.getClass().getName() + " " + e.getMessage(), e);
       }
       return function;
     }
diff --git a/src/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java b/src/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
index 6d477eb..ffcdc02 100644
--- a/src/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
+++ b/src/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
@@ -76,21 +76,6 @@
 
   public static final String DATABASE_WAREHOUSE_SUFFIX = ".db";
 
-  /**
-   * printStackTrace
-   *
-   * Helper function to print an exception stack trace to the log and not stderr
-   *
-   * @param e
-   *          the exception
-   *
-   */
-  static public void printStackTrace(Exception e) {
-    for (StackTraceElement s : e.getStackTrace()) {
-      LOG.error(s);
-    }
-  }
-
   public static Table createColumnsetSchema(String name, List<String> columns,
       List<String> partCols, Configuration conf) throws MetaException {
 
@@ -175,14 +160,17 @@ static public void recursiveDelete(File f) throws IOException {
   static public Deserializer getDeserializer(Configuration conf,
       Properties schema) throws MetaException {
     try {
-      Deserializer deserializer = ReflectionUtils.newInstance(conf.getClassByName(
-      schema.getProperty(serdeConstants.SERIALIZATION_LIB)).asSubclass(Deserializer.class), conf);
+      String clazzName = schema.getProperty(serdeConstants.SERIALIZATION_LIB);
+      if(clazzName == null) {
+        throw new IllegalStateException("Property " + serdeConstants.SERIALIZATION_LIB + " cannot be null");
+      }
+      Deserializer deserializer = ReflectionUtils.newInstance(conf.getClassByName(clazzName)
+          .asSubclass(Deserializer.class), conf);
       deserializer.initialize(conf, schema);
       return deserializer;
     } catch (Exception e) {
       LOG.error("error in initSerDe: " + e.getClass().getName() + " "
-          + e.getMessage());
-      MetaStoreUtils.printStackTrace(e);
+          + e.getMessage(), e);
       throw new MetaException(e.getClass().getName() + " " + e.getMessage());
     }
   }
@@ -221,8 +209,7 @@ static public Deserializer getDeserializer(Configuration conf,
       throw e;
     } catch (Exception e) {
       LOG.error("error in initSerDe: " + e.getClass().getName() + " "
-          + e.getMessage());
-      MetaStoreUtils.printStackTrace(e);
+          + e.getMessage(), e);
       throw new MetaException(e.getClass().getName() + " " + e.getMessage());
     }
   }
@@ -258,8 +245,7 @@ static public Deserializer getDeserializer(Configuration conf,
       throw e;
     } catch (Exception e) {
       LOG.error("error in initSerDe: " + e.getClass().getName() + " "
-          + e.getMessage());
-      MetaStoreUtils.printStackTrace(e);
+          + e.getMessage(), e);
       throw new MetaException(e.getClass().getName() + " " + e.getMessage());
     }
   }
diff --git a/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java b/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
index 010cbd4..93535d4 100644
--- a/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
+++ b/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
@@ -1732,7 +1732,7 @@ private Collection getPartitionPsQueryResults(String dbName, String tableName,
       LOG.debug("Done executing query for listMPartitions");
       pm.retrieveAll(mparts);
       success = commitTransaction();
-      LOG.debug("Done retrieving all objects for listMPartitions");
+      LOG.debug("Done retrieving all objects for listMPartitions " + mparts);
     } finally {
       if (!success) {
         rollbackTransaction();
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java
index 3e48a42..7576f39 100644
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java
+++ b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestMarkPartitionRemote.java
@@ -19,15 +19,21 @@
 package org.apache.hadoop.hive.metastore;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 
-public class TestMarkPartitionRemote extends TestMarkPartition{
+public class TestMarkPartitionRemote extends TestMarkPartition {
 
   private static class RunMS implements Runnable {
 
+    private final int port;
+
+    public RunMS(int port) {
+      this.port = port;
+    }
     @Override
     public void run() {
       try {
-        HiveMetaStore.main(new String[] { "29111" });
+        HiveMetaStore.main(new String[] { String.valueOf(port) });
       } catch (Throwable e) {
         e.printStackTrace(System.err);
         assert false;
@@ -38,10 +44,11 @@ public void run() {
   @Override
   protected void setUp() throws Exception {
     super.setUp();
-    Thread t = new Thread(new RunMS());
+    int port = MetaStoreUtils.findFreePort();
+    Thread t = new Thread(new RunMS(port));
     t.setDaemon(true);
     t.start();
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:29111");
+    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
     hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
     Thread.sleep(30000);
   }
diff --git a/src/ql/src/java/conf/hive-exec-log4j.properties b/src/ql/src/java/conf/hive-exec-log4j.properties
index 7121379..74d62d5 100644
--- a/src/ql/src/java/conf/hive-exec-log4j.properties
+++ b/src/ql/src/java/conf/hive-exec-log4j.properties
@@ -17,7 +17,8 @@
 # Define some default values that can be overridden by system properties
 hive.log.threshold=ALL
 hive.root.logger=INFO,FA
-hive.log.dir=/tmp/${user.name}
+hive.log.dir=${java.io.tmpdir}/${user.name}
+hive.query.id=hadoop
 hive.log.file=${hive.query.id}.log
 
 # Define the root logger to the system property "hadoop.root.logger".
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 45d49cf..339cdb9 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -145,55 +145,45 @@
 
   private String userName;
 
-  private boolean checkLockManager() {
+  private boolean checkConcurrency() throws SemanticException {
     boolean supportConcurrency = conf.getBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY);
     if (!supportConcurrency) {
+      LOG.info("Concurrency mode is disabled, not creating a lock manager");
       return false;
     }
-    if ((hiveLockMgr == null)) {
-      try {
-        setLockManager();
-      } catch (SemanticException e) {
-        errorMessage = "FAILED: Error in semantic analysis: " + e.getMessage();
-        SQLState = ErrorMsg.findSQLState(e.getMessage());
-        downstreamError = e;
-        console.printError(errorMessage, "\n"
-            + org.apache.hadoop.util.StringUtils.stringifyException(e));
-        return false;
-      }
-    }
+    createLockManager();
     // the reason that we set the lock manager for the cxt here is because each
     // query has its own ctx object. The hiveLockMgr is shared accross the
     // same instance of Driver, which can run multiple queries.
     ctx.setHiveLockMgr(hiveLockMgr);
-    return hiveLockMgr != null;
+    return true;
   }
 
-  private void setLockManager() throws SemanticException {
-    boolean supportConcurrency = conf.getBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY);
-    if (supportConcurrency) {
-      String lockMgr = conf.getVar(HiveConf.ConfVars.HIVE_LOCK_MANAGER);
-      if ((lockMgr == null) || (lockMgr.isEmpty())) {
-        throw new SemanticException(ErrorMsg.LOCKMGR_NOT_SPECIFIED.getMsg());
-      }
-
-      try {
-        hiveLockMgr = (HiveLockManager) ReflectionUtils.newInstance(conf.getClassByName(lockMgr),
-            conf);
-        hiveLockMgr.setContext(new HiveLockManagerCtx(conf));
-      } catch (Exception e) {
-        // set hiveLockMgr to null just in case this invalid manager got set to
-        // next query's ctx.
-        if (hiveLockMgr != null) {
-          try {
-            hiveLockMgr.close();
-          } catch (LockException e1) {
-            //nothing can do here
-          }
-          hiveLockMgr = null;
+  private void createLockManager() throws SemanticException {
+    if (hiveLockMgr != null) {
+      return;
+    }
+    String lockMgr = conf.getVar(HiveConf.ConfVars.HIVE_LOCK_MANAGER);
+    LOG.info("Creating lock manager of type " + lockMgr);
+    if ((lockMgr == null) || (lockMgr.isEmpty())) {
+      throw new SemanticException(ErrorMsg.LOCKMGR_NOT_SPECIFIED.getMsg());
+    }
+    try {
+      hiveLockMgr = (HiveLockManager) ReflectionUtils.newInstance(conf.getClassByName(lockMgr),
+          conf);
+      hiveLockMgr.setContext(new HiveLockManagerCtx(conf));
+    } catch (Exception e1) {
+      // set hiveLockMgr to null just in case this invalid manager got set to
+      // next query's ctx.
+      if (hiveLockMgr != null) {
+        try {
+          hiveLockMgr.close();
+        } catch (LockException e2) {
+          //nothing can do here
         }
-        throw new SemanticException(ErrorMsg.LOCKMGR_NOT_INITIALIZED.getMsg() + e.getMessage());
+        hiveLockMgr = null;
       }
+      throw new SemanticException(ErrorMsg.LOCKMGR_NOT_INITIALIZED.getMsg() + e1.getMessage(), e1);
     }
   }
 
@@ -1023,7 +1013,18 @@ private CommandProcessorResponse runInternal(String command) throws CommandNeedR
     }
 
     boolean requireLock = false;
-    boolean ckLock = checkLockManager();
+    boolean ckLock = false;
+    try {
+      ckLock = checkConcurrency();
+    } catch (SemanticException e) {
+      errorMessage = "FAILED: Error in semantic analysis: " + e.getMessage();
+      SQLState = ErrorMsg.findSQLState(e.getMessage());
+      downstreamError = e;
+      console.printError(errorMessage, "\n"
+          + org.apache.hadoop.util.StringUtils.stringifyException(e));
+      ret = 10;
+      return new CommandProcessorResponse(ret, errorMessage, SQLState);
+    }
 
     if (ckLock) {
       boolean lockOnlyMapred = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_LOCK_MAPRED_ONLY);
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
index cde76bf..47c23ff 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
@@ -659,6 +659,12 @@ public static void main(String[] args) throws IOException, HiveException {
 
     boolean isSilent = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVESESSIONSILENT);
 
+    String queryId = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYID, "").trim();
+    if(queryId.isEmpty()) {
+      queryId = "unknown-" + System.currentTimeMillis();
+    }
+    System.setProperty(HiveConf.ConfVars.HIVEQUERYID.toString(), queryId);
+
     if (noLog) {
       // If started from main(), and noLog is on, we should not output
       // any logs. To turn the log on, please set -Dtest.silent=false
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
index fdf1629..99ec216 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
@@ -61,7 +61,7 @@
   static final String HIVE_DEBUG_RECURSIVE = "HIVE_DEBUG_RECURSIVE";
   static final String HIVE_MAIN_CLIENT_DEBUG_OPTS = "HIVE_MAIN_CLIENT_DEBUG_OPTS";
   static final String HIVE_CHILD_CLIENT_DEBUG_OPTS = "HIVE_CHILD_CLIENT_DEBUG_OPTS";
-  static final String[] HIVE_SYS_PROP = {"build.dir", "build.dir.hive"};
+  static final String[] HIVE_SYS_PROP = {"build.dir", "build.dir.hive", "hive.query.id"};
 
   private transient ContentSummary inputSummary = null;
   private transient boolean runningViaChild = false;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java
index ac451d4..540a9a1 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java
@@ -87,7 +87,7 @@
   public static transient final Log l4j = LogFactory.getLog(MapredLocalTask.class);
   static final String HADOOP_MEM_KEY = "HADOOP_HEAPSIZE";
   static final String HADOOP_OPTS_KEY = "HADOOP_OPTS";
-  static final String[] HIVE_SYS_PROP = {"build.dir", "build.dir.hive"};
+  static final String[] HIVE_SYS_PROP = {"build.dir", "build.dir.hive", "hive.query.id"};
   public static MemoryMXBean memoryMXBean;
   private static final Log LOG = LogFactory.getLog(MapredLocalTask.class);
 
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataPrettyFormatUtils.java b/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataPrettyFormatUtils.java
index e35249c..86da780 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataPrettyFormatUtils.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataPrettyFormatUtils.java
@@ -25,8 +25,6 @@
 import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 
-import jline.Terminal;
-
 /**
  * This class provides methods to format the output of DESCRIBE PRETTY
  * in a human-readable way.
@@ -116,8 +114,8 @@ private static String breakCommentIntoMultipleLines(String comment,
       int columnsAlreadyConsumed, int prettyOutputNumCols) {
 
     if (prettyOutputNumCols == -1) {
-      Terminal terminal = Terminal.getTerminal();
-      prettyOutputNumCols = terminal.getTerminalWidth() - 1;
+      // XXX fixed to 80 to remove jline dep
+      prettyOutputNumCols = 80 - 1;
     }
 
     int commentNumCols = prettyOutputNumCols - columnsAlreadyConsumed;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.java b/src/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.java
index c3c8ddc..5c3d5f7 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.java
@@ -43,21 +43,41 @@
  *
  */
 public class GenericUDFBridge extends GenericUDF implements Serializable {
+  private static final long serialVersionUID = 4994861742809511113L;
+
   /**
    * The name of the UDF.
    */
-  String udfName;
+  private String udfName;
 
   /**
    * Whether the UDF is an operator or not. This controls how the display string
    * is generated.
    */
-  boolean isOperator;
+  private boolean isOperator;
 
   /**
    * The underlying UDF class Name.
    */
-  String udfClassName;
+  private String udfClassName;
+
+  /**
+   * The underlying method of the UDF class.
+   */
+  private transient Method udfMethod;
+
+  /**
+   * Helper to convert the parameters before passing to udfMethod.
+   */
+  private transient ConversionHelper conversionHelper;
+  /**
+   * The actual udf object.
+   */
+  private transient UDF udf;
+  /**
+   * The non-deferred real arguments for method invocation.
+   */
+  private transient Object[] realArguments;
 
   /**
    * Create a new GenericUDFBridge object.
@@ -73,7 +93,7 @@ public GenericUDFBridge(String udfName, boolean isOperator,
     this.isOperator = isOperator;
     this.udfClassName = udfClassName;
   }
-
+ 
   // For Java serialization only
   public GenericUDFBridge() {
   }
@@ -110,24 +130,6 @@ public void setOperator(boolean isOperator) {
     }
   }
 
-  /**
-   * The underlying method of the UDF class.
-   */
-  transient Method udfMethod;
-
-  /**
-   * Helper to convert the parameters before passing to udfMethod.
-   */
-  transient ConversionHelper conversionHelper;
-  /**
-   * The actual udf object.
-   */
-  transient UDF udf;
-  /**
-   * The non-deferred real arguments for method invocation.
-   */
-  transient Object[] realArguments;
-
   @Override
   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
 
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/TestLocationQueries.java b/src/ql/src/test/org/apache/hadoop/hive/ql/TestLocationQueries.java
index 0ddc1d5..9384a35 100644
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/TestLocationQueries.java
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/TestLocationQueries.java
@@ -32,6 +32,14 @@
  *  ignored.
  */
 public class TestLocationQueries extends BaseTestQueries {
+
+  public TestLocationQueries() {
+    File logDirFile = new File(logDir);
+    if (!(logDirFile.exists() || logDirFile.mkdirs())) {
+      fail("Could not create " + logDir);
+    }
+  }
+
   /**
    * Our own checker - validate the location of the partition.
    */
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/TestMTQueries.java b/src/ql/src/test/org/apache/hadoop/hive/ql/TestMTQueries.java
index 870a0d4..378de03 100644
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/TestMTQueries.java
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/TestMTQueries.java
@@ -25,6 +25,13 @@
  */
 public class TestMTQueries extends BaseTestQueries {
 
+  public TestMTQueries() {
+    File logDirFile = new File(logDir);
+    if (!(logDirFile.exists() || logDirFile.mkdirs())) {
+      fail("Could not create " + logDir);
+    }
+  }
+
   public void testMTQueries1() throws Exception {
     String[] testNames = new String[] {"join1.q", "join2.q", "groupby1.q",
         "groupby2.q", "join3.q", "input1.q", "input19.q"};
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/exec/TestExecDriver.java b/src/ql/src/test/org/apache/hadoop/hive/ql/exec/TestExecDriver.java
index 68c319c..8525720 100644
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/exec/TestExecDriver.java
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/exec/TestExecDriver.java
@@ -131,8 +131,7 @@
       }
 
     } catch (Throwable e) {
-      e.printStackTrace();
-      throw new RuntimeException("Encountered throwable");
+      throw new RuntimeException("Encountered throwable", e);
     }
   }
 
@@ -472,118 +471,70 @@ private void executePlan() throws Exception {
   public void testMapPlan1() throws Exception {
 
     LOG.info("Beginning testMapPlan1");
-
-    try {
-      populateMapPlan1(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, "src"));
-      executePlan();
-      fileDiff("lt100.txt.deflate", "mapplan1.out");
-    } catch (Throwable e) {
-      e.printStackTrace();
-      fail("Got Throwable");
-    }
+    populateMapPlan1(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, "src"));
+    executePlan();
+    fileDiff("lt100.txt.deflate", "mapplan1.out");
   }
 
   public void testMapPlan2() throws Exception {
 
     LOG.info("Beginning testMapPlan2");
-
-    try {
-      populateMapPlan2(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, "src"));
-      executePlan();
-      fileDiff("lt100.txt", "mapplan2.out");
-    } catch (Throwable e) {
-      e.printStackTrace();
-      fail("Got Throwable");
-    }
+    populateMapPlan2(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, "src"));
+    executePlan();
+    fileDiff("lt100.txt", "mapplan2.out");
   }
 
   public void testMapRedPlan1() throws Exception {
 
     LOG.info("Beginning testMapRedPlan1");
-
-    try {
-      populateMapRedPlan1(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
-          "src"));
-      executePlan();
-      fileDiff("kv1.val.sorted.txt", "mapredplan1.out");
-    } catch (Throwable e) {
-      e.printStackTrace();
-      fail("Got Throwable");
-    }
+    populateMapRedPlan1(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
+        "src"));
+    executePlan();
+    fileDiff("kv1.val.sorted.txt", "mapredplan1.out");
   }
 
   public void testMapRedPlan2() throws Exception {
 
     LOG.info("Beginning testMapPlan2");
-
-    try {
-      populateMapRedPlan2(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
-          "src"));
-      executePlan();
-      fileDiff("lt100.sorted.txt", "mapredplan2.out");
-    } catch (Throwable e) {
-      e.printStackTrace();
-      fail("Got Throwable");
-    }
+    populateMapRedPlan2(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
+        "src"));
+    executePlan();
+    fileDiff("lt100.sorted.txt", "mapredplan2.out");
   }
 
   public void testMapRedPlan3() throws Exception {
 
     LOG.info("Beginning testMapPlan3");
-
-    try {
-      populateMapRedPlan3(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
-          "src"), db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, "src2"));
-      executePlan();
-      fileDiff("kv1kv2.cogroup.txt", "mapredplan3.out");
-    } catch (Throwable e) {
-      e.printStackTrace();
-      fail("Got Throwable");
-    }
+    populateMapRedPlan3(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
+        "src"), db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, "src2"));
+    executePlan();
+    fileDiff("kv1kv2.cogroup.txt", "mapredplan3.out");
   }
 
   public void testMapRedPlan4() throws Exception {
 
     LOG.info("Beginning testMapPlan4");
-
-    try {
-      populateMapRedPlan4(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
-          "src"));
-      executePlan();
-      fileDiff("kv1.string-sorted.txt", "mapredplan4.out");
-    } catch (Throwable e) {
-      e.printStackTrace();
-      fail("Got Throwable");
-    }
+    populateMapRedPlan4(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
+        "src"));
+    executePlan();
+    fileDiff("kv1.string-sorted.txt", "mapredplan4.out");
   }
 
   public void testMapRedPlan5() throws Exception {
 
     LOG.info("Beginning testMapPlan5");
-
-    try {
-      populateMapRedPlan5(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
-          "src"));
-      executePlan();
-      fileDiff("kv1.string-sorted.txt", "mapredplan5.out");
-    } catch (Throwable e) {
-      e.printStackTrace();
-      fail("Got Throwable");
-    }
+    populateMapRedPlan5(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
+        "src"));
+    executePlan();
+    fileDiff("kv1.string-sorted.txt", "mapredplan5.out");
   }
 
   public void testMapRedPlan6() throws Exception {
 
     LOG.info("Beginning testMapPlan6");
-
-    try {
-      populateMapRedPlan6(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
-          "src"));
-      executePlan();
-      fileDiff("lt100.sorted.txt", "mapredplan6.out");
-    } catch (Throwable e) {
-      e.printStackTrace();
-      fail("Got Throwable");
-    }
+    populateMapRedPlan6(db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME,
+        "src"));
+    executePlan();
+    fileDiff("lt100.sorted.txt", "mapredplan6.out");
   }
 }
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestMemoryManager.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestMemoryManager.java
index bb51bab..fb6be16 100644
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestMemoryManager.java
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestMemoryManager.java
@@ -51,7 +51,7 @@ public void testBasics() throws Exception {
     NullCallback callback = new NullCallback();
     long poolSize = mgr.getTotalMemoryPool();
     assertEquals(Math.round(ManagementFactory.getMemoryMXBean().
-        getHeapMemoryUsage().getMax() * 0.5f), poolSize);
+        getHeapMemoryUsage().getMax() * 0.5d), poolSize);
     assertEquals(1.0, mgr.getAllocationScale(), 0.00001);
     mgr.addWriter(new Path("p1"), 1000, callback);
     assertEquals(1.0, mgr.getAllocationScale(), 0.00001);
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFBridge.java b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFBridge.java
new file mode 100644
index 0000000..effe842
--- /dev/null
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFBridge.java
@@ -0,0 +1,39 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.junit.Test;
+
+public class TestGenericUDFBridge {
+
+  @Test(expected = UDFArgumentException.class)
+  public void testInvalidName() throws Exception {
+    GenericUDFBridge udf = new GenericUDFBridge("someudf", false, "not a class name");
+    udf.initialize(new ObjectInspector[0]);
+    udf.close();
+  }
+
+  @Test(expected = UDFArgumentException.class)
+  public void testNullName() throws Exception {
+    GenericUDFBridge udf = new GenericUDFBridge("someudf", false, null);
+    udf.initialize(new ObjectInspector[0]);
+    udf.close();
+  }
+}
diff --git a/src/service/src/test/org/apache/hive/service/auth/TestCustomAuthentication.java b/src/service/src/test/org/apache/hive/service/auth/TestCustomAuthentication.java
index 8db6313..ece54a8 100644
--- a/src/service/src/test/org/apache/hive/service/auth/TestCustomAuthentication.java
+++ b/src/service/src/test/org/apache/hive/service/auth/TestCustomAuthentication.java
@@ -26,102 +26,72 @@
 import org.junit.Test;
 
 import javax.security.sasl.AuthenticationException;
+import java.io.ByteArrayOutputStream;
 import java.io.File;
 import java.io.FileOutputStream;
 import java.sql.Connection;
 import java.sql.DriverManager;
+import java.sql.SQLException;
 import java.util.HashMap;
 import java.util.Map;
 
 public class TestCustomAuthentication {
 
-  private static HiveServer2 hiveserver2 = null;
-
-  private static File configFile = null;
+  private static HiveServer2 hiveserver2;
+  private static HiveConf hiveConf;
+  private static byte[] hiveConfBackup;
 
   @BeforeClass
   public static void setUp() throws Exception {
-    createConfig();
-    startServer();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    stopServer();
-    removeConfig();
-  }
-
-  private static void startServer() throws Exception{
-
-    HiveConf hiveConf = new HiveConf();
+    hiveConf = new HiveConf();
+    ByteArrayOutputStream baos = new ByteArrayOutputStream();
+    hiveConf.writeXml(baos);
+    baos.close();
+    hiveConfBackup = baos.toByteArray();
+    hiveConf.set("hive.server2.authentication", "CUSTOM");
+    hiveConf.set("hive.server2.custom.authentication.class",
+        "org.apache.hive.service.auth.TestCustomAuthentication$SimpleAuthenticationProviderImpl");
+    FileOutputStream fos = new FileOutputStream(new File(hiveConf.getHiveSiteLocation().toURI()));
+    hiveConf.writeXml(fos);
+    fos.close();
     hiveserver2 = new HiveServer2();
     hiveserver2.init(hiveConf);
     hiveserver2.start();
     Thread.sleep(1000);
     System.out.println("hiveServer2 start ......");
-
   }
 
-  private static void stopServer(){
-    try {
-      if (hiveserver2 != null) {
-        hiveserver2.stop();
-        hiveserver2 = null;
-      }
-      Thread.sleep(1000);
-    } catch (Exception e) {
-      e.printStackTrace();
+  @AfterClass
+  public static void tearDown() throws Exception {
+    if(hiveConf != null && hiveConfBackup != null) {
+      FileOutputStream fos = new FileOutputStream(new File(hiveConf.getHiveSiteLocation().toURI()));
+      fos.write(hiveConfBackup);
+      fos.close();
     }
-    System.out.println("hiveServer2 stop ......");
-  }
-
-  private static void createConfig() throws Exception{
-
-    Configuration conf = new Configuration(false);
-    conf.set("hive.server2.authentication", "CUSTOM");
-    conf.set("hive.server2.custom.authentication.class",
-        "org.apache.hive.service.auth.TestCustomAuthentication$SimpleAuthenticationProviderImpl");
-
-    configFile = new File("../build/service/test/resources","hive-site.xml");
-
-    FileOutputStream out = new FileOutputStream(configFile);
-    conf.writeXml(out);
-  }
-
-  private static void removeConfig(){
-    try {
-      configFile.delete();
-    } catch (Exception e){
-      System.out.println(e.getMessage());
+    if (hiveserver2 != null) {
+      hiveserver2.stop();
+      hiveserver2 = null;
     }
+    Thread.sleep(1000);
+    System.out.println("hiveServer2 stop ......");
   }
 
   @Test
-  public void testCustomAuthentication() throws Exception{
+  public void testCustomAuthentication() throws Exception {
 
     String url = "jdbc:hive2://localhost:10000/default";
+    Class.forName("org.apache.hive.jdbc.HiveDriver");
 
-    Exception exception = null;
-    try{
-      Class.forName("org.apache.hive.jdbc.HiveDriver");
-      Connection connection =  DriverManager.getConnection(url, "wronguser", "pwd");
-      connection.close();
-    } catch (Exception e){
-      exception = e;
-    }
-
-    Assert.assertNotNull(exception);
-
-    exception = null;
-    try{
-      Class.forName("org.apache.hive.jdbc.HiveDriver");
-      Connection connection =  DriverManager.getConnection(url, "hiveuser", "hive");
-      connection.close();
-    } catch (Exception e){
-      exception = e;
+    try {
+      DriverManager.getConnection(url, "wronguser", "pwd");
+      Assert.fail("Expected Exception");
+    } catch(SQLException e) {
+      Assert.assertNotNull(e.getMessage());
+      Assert.assertTrue(e.getMessage(), e.getMessage().contains("Peer indicated failure: Error validating the login"));
     }
 
-    Assert.assertNull(exception);
+    Connection connection = DriverManager.getConnection(url, "hiveuser", "hive");
+    connection.close();
 
     System.out.println(">>> PASSED testCustomAuthentication");
   }
diff --git a/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java b/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
index 06db19a..bd13ad9 100644
--- a/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
+++ b/src/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
@@ -82,7 +82,6 @@ public String getTaskAttemptLogUrl(JobConf conf,
 
   @Override
   public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception {
-    JobTrackerState state;
     switch (clusterStatus.getJobTrackerStatus()) {
     case INITIALIZING:
       return JobTrackerState.INITIALIZING;
@@ -211,19 +210,16 @@ public int getJobTrackerPort() throws UnsupportedOperationException {
     public void shutdown() throws IOException {
       mr.shutdown();
     }
-    
+
     @Override
     public void setupConfiguration(Configuration conf) {
       JobConf jConf = mr.createJobConf();
       for (Map.Entry<String, String> pair: jConf) {
-	//System.out.println("XXX Var: "+pair.getKey() +"="+pair.getValue());
-        //if (conf.get(pair.getKey()) == null) {
-          conf.set(pair.getKey(), pair.getValue());
-	  //}
+        conf.set(pair.getKey(), pair.getValue());
       }
     }
   }
-  
+
   // Don't move this code to the parent class. There's a binary
   // incompatibility between hadoop 1 and 2 wrt MiniDFSCluster and we
   // need to have two different shim classes even though they are
-- 
1.7.0.4

