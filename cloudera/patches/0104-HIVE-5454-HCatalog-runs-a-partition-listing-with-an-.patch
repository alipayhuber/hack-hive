From 6d7f946f62c11abb65e1f24544065fb77e1ba1b4 Mon Sep 17 00:00:00 2001
From: Brock Noland <brock@apache.org>
Date: Wed, 23 Oct 2013 15:13:20 +0000
Subject: [PATCH 104/375] HIVE-5454 - HCatalog runs a partition listing with an empty filter (Harsh J via Brock Noland)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1535051 13f79535-47bb-0310-9956-ffa450edef68
---
 .../data/transfer/impl/HCatInputFormatReader.java  |    2 +-
 .../hive/hcatalog/mapreduce/HCatInputFormat.java   |   45 +++++++++++--------
 .../hive/hcatalog/mapreduce/HCatMapReduceTest.java |    2 +-
 .../org/apache/hive/hcatalog/pig/HCatLoader.java   |    2 +-
 .../documentation/content/xdocs/inputoutput.xml    |   24 +++++-----
 .../org/apache/hcatalog/utils/HBaseReadWrite.java  |    4 +-
 .../org/apache/hive/hcatalog/utils/GroupByAge.java |    5 +-
 .../org/apache/hive/hcatalog/utils/ReadJson.java   |    5 +-
 .../org/apache/hive/hcatalog/utils/ReadRC.java     |    5 +-
 .../org/apache/hive/hcatalog/utils/ReadText.java   |    5 +-
 .../org/apache/hive/hcatalog/utils/ReadWrite.java  |    5 +-
 .../org/apache/hive/hcatalog/utils/SimpleRead.java |    5 +-
 .../apache/hive/hcatalog/utils/StoreComplex.java   |    5 +-
 .../org/apache/hive/hcatalog/utils/StoreDemo.java  |    5 +-
 .../apache/hive/hcatalog/utils/StoreNumbers.java   |    5 +-
 .../org/apache/hive/hcatalog/utils/SumNumbers.java |    5 +-
 .../apache/hive/hcatalog/utils/TypeDataCheck.java  |    5 +-
 .../org/apache/hive/hcatalog/utils/WriteJson.java  |    5 +-
 .../org/apache/hive/hcatalog/utils/WriteRC.java    |    5 +-
 .../org/apache/hive/hcatalog/utils/WriteText.java  |    5 +-
 .../hive/hcatalog/utils/WriteTextPartitioned.java  |    5 +-
 .../hive/hcatalog/hbase/TestHBaseInputFormat.java  |   10 +---
 22 files changed, 75 insertions(+), 89 deletions(-)

diff --git a/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/transfer/impl/HCatInputFormatReader.java b/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/transfer/impl/HCatInputFormatReader.java
index 2272a1e..9e7a548 100644
--- a/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/transfer/impl/HCatInputFormatReader.java
+++ b/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/transfer/impl/HCatInputFormatReader.java
@@ -63,7 +63,7 @@ public ReaderContext prepareRead() throws HCatException {
     try {
       Job job = new Job(conf);
       HCatInputFormat hcif = HCatInputFormat.setInput(
-        job, re.getDbName(), re.getTableName()).setFilter(re.getFilterString());
+        job, re.getDbName(), re.getTableName(), re.getFilterString());
       ReaderContext cntxt = new ReaderContext();
       cntxt.setInputSplits(hcif.getSplits(
           ShimLoader.getHadoopShims().getHCatShim().createJobContext(job.getConfiguration(), null)));
diff --git a/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java b/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java
index 2f24124..5733662 100644
--- a/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java
+++ b/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatInputFormat.java
@@ -39,28 +39,33 @@
   private InputJobInfo inputJobInfo;
 
   /**
-   * @deprecated as of release 0.5, and will be removed in a future release
+   * Initializes the input with a null filter.
+   * See {@link #setInput(org.apache.hadoop.conf.Configuration, String, String, String)}
    */
-  @Deprecated
-  public static void setInput(Job job, InputJobInfo inputJobInfo) throws IOException {
-    setInput(job.getConfiguration(), inputJobInfo);
+  public static HCatInputFormat setInput(
+          Job job, String dbName, String tableName)
+    throws IOException {
+    return setInput(job.getConfiguration(), dbName, tableName, null);
   }
 
   /**
-   * @deprecated as of release 0.5, and will be removed in a future release
+   * Initializes the input with a provided filter.
+   * See {@link #setInput(org.apache.hadoop.conf.Configuration, String, String, String)}
    */
-  @Deprecated
-  public static void setInput(Configuration conf, InputJobInfo inputJobInfo) throws IOException {
-    setInput(conf, inputJobInfo.getDatabaseName(), inputJobInfo.getTableName())
-      .setFilter(inputJobInfo.getFilter())
-      .setProperties(inputJobInfo.getProperties());
+  public static HCatInputFormat setInput(
+          Job job, String dbName, String tableName, String filter)
+    throws IOException {
+    return setInput(job.getConfiguration(), dbName, tableName, filter);
   }
 
   /**
-   * See {@link #setInput(org.apache.hadoop.conf.Configuration, String, String)}
+   * Initializes the input with a null filter.
+   * See {@link #setInput(org.apache.hadoop.conf.Configuration, String, String, String)}
    */
-  public static HCatInputFormat setInput(Job job, String dbName, String tableName) throws IOException {
-    return setInput(job.getConfiguration(), dbName, tableName);
+  public static HCatInputFormat setInput(
+          Configuration conf, String dbName, String tableName)
+    throws IOException {
+    return setInput(conf, dbName, tableName, null);
   }
 
   /**
@@ -69,9 +74,11 @@ public static HCatInputFormat setInput(Job job, String dbName, String tableName)
    * @param conf the job configuration
    * @param dbName database name, which if null 'default' is used
    * @param tableName table name
+   * @param filter the partition filter to use, can be null for no filter
    * @throws IOException on all errors
    */
-  public static HCatInputFormat setInput(Configuration conf, String dbName, String tableName)
+  public static HCatInputFormat setInput(
+          Configuration conf, String dbName, String tableName, String filter)
     throws IOException {
 
     Preconditions.checkNotNull(conf, "required argument 'conf' is null");
@@ -79,7 +86,7 @@ public static HCatInputFormat setInput(Configuration conf, String dbName, String
 
     HCatInputFormat hCatInputFormat = new HCatInputFormat();
     hCatInputFormat.conf = conf;
-    hCatInputFormat.inputJobInfo = InputJobInfo.create(dbName, tableName, null, null);
+    hCatInputFormat.inputJobInfo = InputJobInfo.create(dbName, tableName, filter, null);
 
     try {
       InitializeInput.setInput(conf, hCatInputFormat.inputJobInfo);
@@ -91,11 +98,11 @@ public static HCatInputFormat setInput(Configuration conf, String dbName, String
   }
 
   /**
-   * Set a filter on the input table.
-   * @param filter the filter specification, which may be null
-   * @return this
-   * @throws IOException on all errors
+   * @deprecated As of 0.13
+   * Use {@link #setInput(org.apache.hadoop.conf.Configuration, String, String, String)} instead,
+   * to specify a partition filter to directly initialize the input with.
    */
+  @Deprecated
   public HCatInputFormat setFilter(String filter) throws IOException {
     // null filters are supported to simplify client code
     if (filter != null) {
diff --git a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatMapReduceTest.java b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatMapReduceTest.java
index dd63559..77bdb9d 100644
--- a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatMapReduceTest.java
+++ b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatMapReduceTest.java
@@ -341,7 +341,7 @@ Job runMRCreate(Map<String, String> partitionValues,
     job.setInputFormatClass(HCatInputFormat.class);
     job.setOutputFormatClass(TextOutputFormat.class);
 
-    HCatInputFormat.setInput(job, dbName, tableName).setFilter(filter);
+    HCatInputFormat.setInput(job, dbName, tableName, filter);
 
     job.setMapOutputKeyClass(BytesWritable.class);
     job.setMapOutputValueClass(Text.class);
diff --git a/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java b/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java
index a4f93c7..b03337d 100644
--- a/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java
+++ b/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java
@@ -116,7 +116,7 @@ public void setLocation(String location, Job job) throws IOException {
       }
     } else {
       Job clone = new Job(job.getConfiguration());
-      HCatInputFormat.setInput(job, dbName, tableName).setFilter(getPartitionFilterString());
+      HCatInputFormat.setInput(job, dbName, tableName, getPartitionFilterString());
 
       // We will store all the new /changed properties in the job in the
       // udf context, so the the HCatInputFormat.setInput method need not
diff --git a/src/hcatalog/src/docs/src/documentation/content/xdocs/inputoutput.xml b/src/hcatalog/src/docs/src/documentation/content/xdocs/inputoutput.xml
index 300df73..fec9e73 100644
--- a/src/hcatalog/src/docs/src/documentation/content/xdocs/inputoutput.xml
+++ b/src/hcatalog/src/docs/src/documentation/content/xdocs/inputoutput.xml
@@ -45,9 +45,7 @@
 	    <li><code>getTableSchema</code></li>
 	</ul>
 
-	<p>To use HCatInputFormat to read data, first instantiate an <code>InputJobInfo</code>
-	with the necessary information from the table being read
-	and then call setInput with the <code>InputJobInfo</code>.</p>
+	<p>To use HCatInputFormat to read data, call setInput with the database name, tablename and an optional partition filter.</p>
 
 <p>You can use the <code>setOutputSchema</code> method to include a projection schema, to
 specify the output fields. If a schema is not specified, all the columns in the table
@@ -62,11 +60,13 @@ will be returned.</p>
    * the information in the conf object. The inputInfo object is updated with
    * information needed in the client context
    * @param job the job object
-   * @param inputJobInfo the input info for table to read
+   * @param dbName the database where the table lies
+   * @param tableName the table to read
+   * @param filter the partition filter to use
    * @throws IOException the exception in communicating with the metadata server
    */
   public static void setInput(Job job,
-      InputJobInfo inputJobInfo) throws IOException;
+      String dbName, String tableName, String filter) throws IOException;
 
   /**
    * Set the schema for the HCatRecord data returned by HCatInputFormat.
@@ -354,8 +354,8 @@ public class GroupByAge extends Configured implements Tool {
         String dbName = null;
 
         Job job = new Job(conf, "GroupByAge");
-        HCatInputFormat.setInput(job, InputJobInfo.create(dbName,
-                inputTableName, null));
+        HCatInputFormat.setInput(job, dbName,
+                inputTableName);
         // initialize HCatOutputFormat
 
         job.setInputFormatClass(HCatInputFormat.class);
@@ -388,8 +388,8 @@ public class GroupByAge extends Configured implements Tool {
 <li>The implementation of Map takes HCatRecord as an input and the implementation of Reduce produces it as an output.</li>
 <li>This example program assumes the schema of the input, but it could also retrieve the schema via
 HCatOutputFormat.getOutputSchema() and retrieve fields based on the results of that call.</li>
-<li>The input descriptor for the table to be read is created by calling InputJobInfo.create.  It requires the database name,
-table name, and partition filter.  In this example the partition filter is null, so all partitions of the table
+<li>The input descriptor for the table to be read is created by passing the database name,
+table name, and an optional partition filter to HCatInputFormat.setInput. In this example the partition filter is null, so all partitions of the table
 will be read.</li>
 <li>The output descriptor for the table to be written is created by calling OutputJobInfo.create.  It requires the
 database name, the table name, and a Map of partition keys and values that describe the partition being written.
@@ -397,7 +397,7 @@ In this example it is assumed the table is unpartitioned, so this Map is null.</
 </ol>
 
 <p>To scan just selected partitions of a table, a filter describing the desired partitions can be passed to
-InputJobInfo.create.  To scan a single partition, the filter string should look like: "<code>ds=20120401</code>"
+HCatInputFormat.setInput.  To scan a single partition, the filter string should look like: "<code>ds=20120401</code>"
 where the datestamp "<code>ds</code>" is the partition column name and "<code>20120401</code>" is the value
 you want to read (year, month, and day).</p>
 </section>
@@ -420,14 +420,14 @@ you want to read (year, month, and day).</p>
 
 <p>Assume for example you have a web_logs table that is partitioned by the column "<code>ds</code>".  You could select one partition of the table by changing</p>
 <source>
-HCatInputFormat.setInput(job, InputJobInfo.create(dbName, inputTableName, null));
+HCatInputFormat.setInput(job, dbName, inputTableName);
 </source>
 <p>
 to
 </p>
 <source>
 HCatInputFormat.setInput(job,
-                         InputJobInfo.create(dbName, inputTableName, "ds=\"20110924\""));
+                         dbName, inputTableName, "ds=\"20110924\"");
 </source>
 <p>
 This filter must reference only partition columns.  Values from other columns will cause the job to fail.</p>
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hcatalog/utils/HBaseReadWrite.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hcatalog/utils/HBaseReadWrite.java
index 18e0ea2..78955be 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hcatalog/utils/HBaseReadWrite.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hcatalog/utils/HBaseReadWrite.java
@@ -41,7 +41,6 @@
 import org.apache.hcatalog.data.HCatRecord;
 import org.apache.hcatalog.mapreduce.HCatInputFormat;
 import org.apache.hcatalog.mapreduce.HCatOutputFormat;
-import org.apache.hcatalog.mapreduce.InputJobInfo;
 import org.apache.hcatalog.mapreduce.OutputJobInfo;
 
 /**
@@ -165,8 +164,7 @@ public int run(String[] args) throws Exception {
     if (!succ) return 1;
 
     job = new Job(conf, "HBaseRead");
-    HCatInputFormat.setInput(job, InputJobInfo.create(dbName, tableName,
-      null));
+    HCatInputFormat.setInput(job, dbName, tableName);
 
     job.setInputFormatClass(HCatInputFormat.class);
     job.setOutputFormatClass(TextOutputFormat.class);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/GroupByAge.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/GroupByAge.java
index 2352bdd..a2d8086 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/GroupByAge.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/GroupByAge.java
@@ -38,7 +38,6 @@
 import org.apache.hive.hcatalog.data.schema.HCatSchema;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
 import org.apache.hive.hcatalog.mapreduce.HCatOutputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 import org.apache.hive.hcatalog.mapreduce.OutputJobInfo;
 
 /**
@@ -105,8 +104,8 @@ public int run(String[] args) throws Exception {
     if (principalID != null)
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "GroupByAge");
-    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,
-      inputTableName, null));
+    HCatInputFormat.setInput(job, dbName,
+      inputTableName);
     // initialize HCatOutputFormat
 
     job.setInputFormatClass(HCatInputFormat.class);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadJson.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadJson.java
index 9ecfa43..cc8bff2 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadJson.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadJson.java
@@ -37,7 +37,6 @@
 import org.apache.hive.hcatalog.data.DefaultHCatRecord;
 import org.apache.hive.hcatalog.data.HCatRecord;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 
 /**
  * This is a map reduce test for testing hcat which goes against the "numbers"
@@ -90,8 +89,8 @@ public int run(String[] args) throws Exception {
     if (principalID != null)
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "ReadJson");
-    HCatInputFormat.setInput(job, InputJobInfo.create(
-      dbName, tableName, null));
+    HCatInputFormat.setInput(job,
+      dbName, tableName);
     // initialize HCatOutputFormat
 
     job.setInputFormatClass(HCatInputFormat.class);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadRC.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadRC.java
index b5c2fb7..369cb96 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadRC.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadRC.java
@@ -37,7 +37,6 @@
 import org.apache.hive.hcatalog.data.DefaultHCatRecord;
 import org.apache.hive.hcatalog.data.HCatRecord;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 
 /**
  * This is a map reduce test for testing hcat which goes against the "numbers"
@@ -91,8 +90,8 @@ public int run(String[] args) throws Exception {
     if (principalID != null)
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "ReadRC");
-    HCatInputFormat.setInput(job, InputJobInfo.create(
-      dbName, tableName, null));
+    HCatInputFormat.setInput(job,
+      dbName, tableName);
     // initialize HCatOutputFormat
 
     job.setInputFormatClass(HCatInputFormat.class);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadText.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadText.java
index dae7fa2..5b9fc43 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadText.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadText.java
@@ -37,7 +37,6 @@
 import org.apache.hive.hcatalog.data.DefaultHCatRecord;
 import org.apache.hive.hcatalog.data.HCatRecord;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 
 /**
  * This is a map reduce test for testing hcat which goes against the "numbers"
@@ -102,8 +101,8 @@ public int run(String[] args) throws Exception {
     if (principalID != null)
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "ReadText");
-    HCatInputFormat.setInput(job, InputJobInfo.create(
-      dbName, tableName, null));
+    HCatInputFormat.setInput(job,
+      dbName, tableName);
     // initialize HCatOutputFormat
 
     job.setInputFormatClass(HCatInputFormat.class);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadWrite.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadWrite.java
index e15d128..26c09f4 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadWrite.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/ReadWrite.java
@@ -36,7 +36,6 @@
 import org.apache.hive.hcatalog.data.schema.HCatSchema;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
 import org.apache.hive.hcatalog.mapreduce.HCatOutputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 import org.apache.hive.hcatalog.mapreduce.OutputJobInfo;
 
 /**
@@ -86,8 +85,8 @@ public int run(String[] args) throws Exception {
     if (principalID != null)
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "ReadWrite");
-    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,
-      inputTableName, null));
+    HCatInputFormat.setInput(job, dbName,
+      inputTableName);
     // initialize HCatOutputFormat
 
     job.setInputFormatClass(HCatInputFormat.class);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SimpleRead.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SimpleRead.java
index 550fb46..9b13ee3 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SimpleRead.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SimpleRead.java
@@ -37,7 +37,6 @@
 import org.apache.hive.hcatalog.common.HCatConstants;
 import org.apache.hive.hcatalog.data.HCatRecord;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 
 /**
  * This is a map reduce test for testing hcat which goes against the "numbers"
@@ -87,8 +86,8 @@ public int run(String[] args) throws Exception {
     if (principalID != null)
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "SimpleRead");
-    HCatInputFormat.setInput(job, InputJobInfo.create(
-      dbName, tableName, null));
+    HCatInputFormat.setInput(job,
+      dbName, tableName, null);
     // initialize HCatOutputFormat
 
     job.setInputFormatClass(HCatInputFormat.class);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreComplex.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreComplex.java
index 579db17..fd648f5 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreComplex.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreComplex.java
@@ -36,7 +36,6 @@
 import org.apache.hive.hcatalog.data.schema.HCatSchema;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
 import org.apache.hive.hcatalog.mapreduce.HCatOutputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 import org.apache.hive.hcatalog.mapreduce.OutputJobInfo;
 
 /**
@@ -103,8 +102,8 @@ public static void main(String[] args) throws Exception {
     Job job = new Job(conf, "storecomplex");
     // initialize HCatInputFormat
 
-    HCatInputFormat.setInput(job, InputJobInfo.create(
-      dbName, tableName, null));
+    HCatInputFormat.setInput(job,
+      dbName, tableName);
     // initialize HCatOutputFormat
     HCatOutputFormat.setOutput(job, OutputJobInfo.create(
       dbName, outputTableName, outputPartitionKvps));
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreDemo.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreDemo.java
index fb07561..74e4b89 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreDemo.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreDemo.java
@@ -35,7 +35,6 @@
 import org.apache.hive.hcatalog.data.schema.HCatSchema;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
 import org.apache.hive.hcatalog.mapreduce.HCatOutputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 import org.apache.hive.hcatalog.mapreduce.OutputJobInfo;
 
 /**
@@ -114,8 +113,8 @@ public static void main(String[] args) throws Exception {
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "storedemo");
     // initialize HCatInputFormat
-    HCatInputFormat.setInput(job, InputJobInfo.create(
-      dbName, tableName, null));
+    HCatInputFormat.setInput(job,
+      dbName, tableName);
     // initialize HCatOutputFormat
     HCatOutputFormat.setOutput(job, OutputJobInfo.create(
       dbName, outputTableName, outputPartitionKvps));
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreNumbers.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreNumbers.java
index 5d9ef05..f6ea671 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreNumbers.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/StoreNumbers.java
@@ -39,7 +39,6 @@
 import org.apache.hive.hcatalog.data.schema.HCatSchema;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
 import org.apache.hive.hcatalog.mapreduce.HCatOutputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 import org.apache.hive.hcatalog.mapreduce.OutputJobInfo;
 
 /**
@@ -178,8 +177,8 @@ public static void main(String[] args) throws Exception {
     Job job = new Job(conf, "storenumbers");
 
     // initialize HCatInputFormat
-    HCatInputFormat.setInput(job, InputJobInfo.create(
-      dbName, tableName, null));
+    HCatInputFormat.setInput(job,
+      dbName, tableName);
     // initialize HCatOutputFormat
     HCatOutputFormat.setOutput(job, OutputJobInfo.create(
       dbName, outputTableName, outputPartitionKvps));
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SumNumbers.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SumNumbers.java
index 08725c4..b0cf396 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SumNumbers.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/SumNumbers.java
@@ -41,7 +41,6 @@
 import org.apache.hive.hcatalog.common.HCatConstants;
 import org.apache.hive.hcatalog.data.HCatRecord;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 
 /**
  * This is a map reduce test for testing hcat which goes against the "numbers"
@@ -162,8 +161,8 @@ public static void main(String[] args) throws Exception {
     if (principalID != null)
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "sumnumbers");
-    HCatInputFormat.setInput(job, InputJobInfo.create(
-      dbName, tableName, null));
+    HCatInputFormat.setInput(job,
+      dbName, tableName);
     // initialize HCatOutputFormat
 
     job.setInputFormatClass(HCatInputFormat.class);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/TypeDataCheck.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/TypeDataCheck.java
index 503b2ff..4933747 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/TypeDataCheck.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/TypeDataCheck.java
@@ -37,7 +37,6 @@
 import org.apache.hive.hcatalog.data.HCatRecord;
 import org.apache.hive.hcatalog.data.schema.HCatSchema;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 
 /**
  * This is a map reduce test for testing hcat that checks that the columns
@@ -150,8 +149,8 @@ public int run(String[] args) {
       }
       Job job = new Job(conf, "typedatacheck");
       // initialize HCatInputFormat
-      HCatInputFormat.setInput(job, InputJobInfo.create(
-        dbName, tableName, null));
+      HCatInputFormat.setInput(job,
+        dbName, tableName);
       HCatSchema s = HCatInputFormat.getTableSchema(job);
       job.getConfiguration().set(SCHEMA_KEY, schemaStr);
       job.getConfiguration().set(DELIM, outputdelim);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteJson.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteJson.java
index a54cc0a..0bee02f 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteJson.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteJson.java
@@ -35,7 +35,6 @@
 import org.apache.hive.hcatalog.data.schema.HCatSchema;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
 import org.apache.hive.hcatalog.mapreduce.HCatOutputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 import org.apache.hive.hcatalog.mapreduce.OutputJobInfo;
 
 /**
@@ -91,8 +90,8 @@ public int run(String[] args) throws Exception {
     if (principalID != null)
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "WriteJson");
-    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,
-      inputTableName, null));
+    HCatInputFormat.setInput(job, dbName,
+      inputTableName);
     // initialize HCatOutputFormat
 
     job.setInputFormatClass(HCatInputFormat.class);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteRC.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteRC.java
index 4e90b31..fa7df02 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteRC.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteRC.java
@@ -35,7 +35,6 @@
 import org.apache.hive.hcatalog.data.schema.HCatSchema;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
 import org.apache.hive.hcatalog.mapreduce.HCatOutputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 import org.apache.hive.hcatalog.mapreduce.OutputJobInfo;
 
 /**
@@ -93,8 +92,8 @@ public int run(String[] args) throws Exception {
     if (principalID != null)
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "WriteRC");
-    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,
-      inputTableName, null));
+    HCatInputFormat.setInput(job, dbName,
+      inputTableName);
     // initialize HCatOutputFormat
 
     job.setInputFormatClass(HCatInputFormat.class);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteText.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteText.java
index e1102bc..19b3e9d 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteText.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteText.java
@@ -35,7 +35,6 @@
 import org.apache.hive.hcatalog.data.schema.HCatSchema;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
 import org.apache.hive.hcatalog.mapreduce.HCatOutputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 import org.apache.hive.hcatalog.mapreduce.OutputJobInfo;
 
 /**
@@ -103,8 +102,8 @@ public int run(String[] args) throws Exception {
     if (principalID != null)
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "WriteText");
-    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,
-      inputTableName, null));
+    HCatInputFormat.setInput(job, dbName,
+      inputTableName);
     // initialize HCatOutputFormat
 
     job.setInputFormatClass(HCatInputFormat.class);
diff --git a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteTextPartitioned.java b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteTextPartitioned.java
index 1414cbc..6ba0f0b 100644
--- a/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteTextPartitioned.java
+++ b/src/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteTextPartitioned.java
@@ -39,7 +39,6 @@
 import org.apache.hive.hcatalog.data.schema.HCatFieldSchema;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
 import org.apache.hive.hcatalog.mapreduce.HCatOutputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 import org.apache.hive.hcatalog.mapreduce.OutputJobInfo;
 
 /**
@@ -94,8 +93,8 @@ public int run(String[] args) throws Exception {
     if (principalID != null)
       conf.set(HCatConstants.HCAT_METASTORE_PRINCIPAL, principalID);
     Job job = new Job(conf, "WriteTextPartitioned");
-    HCatInputFormat.setInput(job, InputJobInfo.create(dbName,
-      inputTableName, filter));
+    HCatInputFormat.setInput(job, dbName,
+      inputTableName, filter);
     // initialize HCatOutputFormat
 
     job.setInputFormatClass(HCatInputFormat.class);
diff --git a/src/hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestHBaseInputFormat.java b/src/hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestHBaseInputFormat.java
index 5cbe86d..8bd97ca 100644
--- a/src/hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestHBaseInputFormat.java
+++ b/src/hcatalog/storage-handlers/hbase/src/test/org/apache/hive/hcatalog/hbase/TestHBaseInputFormat.java
@@ -62,7 +62,6 @@
 import org.apache.hive.hcatalog.data.schema.HCatFieldSchema;
 import org.apache.hive.hcatalog.data.schema.HCatSchema;
 import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
 import org.junit.Test;
 
 public class TestHBaseInputFormat extends SkeletonHBaseTest {
@@ -160,9 +159,7 @@ public void TestHBaseTableReadMR() throws Exception {
     MapReadHTable.resetCounters();
 
     job.setInputFormatClass(HCatInputFormat.class);
-    InputJobInfo inputJobInfo = InputJobInfo.create(databaseName, tableName,
-                null);
-    HCatInputFormat.setInput(job, inputJobInfo);
+    HCatInputFormat.setInput(job, databaseName, tableName);
     job.setOutputFormatClass(TextOutputFormat.class);
     TextOutputFormat.setOutputPath(job, outputDir);
     job.setMapOutputKeyClass(BytesWritable.class);
@@ -225,10 +222,9 @@ public void TestHBaseTableProjectionReadMR() throws Exception {
     job.setJarByClass(this.getClass());
     job.setMapperClass(MapReadProjHTable.class);
     job.setInputFormatClass(HCatInputFormat.class);
-    InputJobInfo inputJobInfo = InputJobInfo.create(
-      MetaStoreUtils.DEFAULT_DATABASE_NAME, tableName, null);
     HCatInputFormat.setOutputSchema(job, getProjectionSchema());
-    HCatInputFormat.setInput(job, inputJobInfo);
+    HCatInputFormat.setInput(job,
+      MetaStoreUtils.DEFAULT_DATABASE_NAME, tableName);
     job.setOutputFormatClass(TextOutputFormat.class);
     TextOutputFormat.setOutputPath(job, outputDir);
     job.setMapOutputKeyClass(BytesWritable.class);
-- 
1.7.0.4

