From 90ecc2280ec89c580fa4eb0ba8d20fd14c0146bb Mon Sep 17 00:00:00 2001
From: Ashutosh Chauhan <hashutosh@apache.org>
Date: Tue, 10 Sep 2013 15:08:29 +0000
Subject: [PATCH 018/375] HIVE-5112 : Upgrade protobuf to 2.5 from 2.4 (Owen O'malley and Brock Noland via Ashutosh Chauhan)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1521520 13f79535-47bb-0310-9956-ffa450edef68
---
 build.properties                                   |    2 +-
 hcatalog/pom.xml                                   |    2 +-
 ivy/libraries.properties                           |    2 +-
 .../org/apache/hadoop/hive/ql/io/orc/OrcProto.java | 7875 ++++++++++++++------
 4 files changed, 5540 insertions(+), 2341 deletions(-)

diff --git a/src/build.properties b/src/build.properties
index e58a709..b1e745b 100644
--- a/src/build.properties
+++ b/src/build.properties
@@ -28,7 +28,7 @@ javac.args.warnings=
 
 hadoop-0.20.version=0.20.2
 hadoop-0.20S.version=1.1.2
-hadoop-0.23.version=2.0.5-alpha
+hadoop-0.23.version=2.1.0-beta
 hadoop.version=${hadoop-0.20.version}
 hadoop.security.version=${hadoop-0.20S.version}
 # Used to determine which set of Hadoop artifacts we depend on.
diff --git a/src/hcatalog/pom.xml b/src/hcatalog/pom.xml
index 34295d4..504f32b 100644
--- a/src/hcatalog/pom.xml
+++ b/src/hcatalog/pom.xml
@@ -27,7 +27,7 @@
       <commons-io.version>2.4</commons-io.version>
       <guava.version>11.0.2</guava.version>
       <hadoop20.version>1.0.3</hadoop20.version>
-      <hadoop23.version>2.0.3-alpha</hadoop23.version>
+      <hadoop23.version>2.1.0-beta</hadoop23.version>
       <hbase.version>0.94.5</hbase.version>
       <hcatalog.version>${project.version}</hcatalog.version>
       <hive.version>${project.version}</hive.version>
diff --git a/src/ivy/libraries.properties b/src/ivy/libraries.properties
index 3790649..226e87a 100644
--- a/src/ivy/libraries.properties
+++ b/src/ivy/libraries.properties
@@ -59,7 +59,7 @@ libthrift.version=0.9.0
 log4j.version=1.2.16
 maven-ant-tasks.version=2.1.0
 mockito-all.version=1.8.2
-protobuf.version=2.4.1
+protobuf.version=2.5.0
 rat.version=0.8
 slf4j-api.version=1.6.1
 slf4j-log4j12.version=1.6.1
diff --git a/src/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java b/src/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java
index c4ff684..7a96373 100644
--- a/src/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java
+++ b/src/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java
@@ -8,22 +8,49 @@ private OrcProto() {}
   public static void registerAllExtensions(
       com.google.protobuf.ExtensionRegistry registry) {
   }
+  /**
+   * Protobuf enum {@code org.apache.hadoop.hive.ql.io.orc.CompressionKind}
+   */
   public enum CompressionKind
       implements com.google.protobuf.ProtocolMessageEnum {
+    /**
+     * <code>NONE = 0;</code>
+     */
     NONE(0, 0),
+    /**
+     * <code>ZLIB = 1;</code>
+     */
     ZLIB(1, 1),
+    /**
+     * <code>SNAPPY = 2;</code>
+     */
     SNAPPY(2, 2),
+    /**
+     * <code>LZO = 3;</code>
+     */
     LZO(3, 3),
     ;
-    
+
+    /**
+     * <code>NONE = 0;</code>
+     */
     public static final int NONE_VALUE = 0;
+    /**
+     * <code>ZLIB = 1;</code>
+     */
     public static final int ZLIB_VALUE = 1;
+    /**
+     * <code>SNAPPY = 2;</code>
+     */
     public static final int SNAPPY_VALUE = 2;
+    /**
+     * <code>LZO = 3;</code>
+     */
     public static final int LZO_VALUE = 3;
-    
-    
+
+
     public final int getNumber() { return value; }
-    
+
     public static CompressionKind valueOf(int value) {
       switch (value) {
         case 0: return NONE;
@@ -33,7 +60,7 @@ public static CompressionKind valueOf(int value) {
         default: return null;
       }
     }
-    
+
     public static com.google.protobuf.Internal.EnumLiteMap<CompressionKind>
         internalGetValueMap() {
       return internalValueMap;
@@ -45,7 +72,7 @@ public CompressionKind findValueByNumber(int number) {
               return CompressionKind.valueOf(number);
             }
           };
-    
+
     public final com.google.protobuf.Descriptors.EnumValueDescriptor
         getValueDescriptor() {
       return getDescriptor().getValues().get(index);
@@ -58,11 +85,9 @@ public CompressionKind findValueByNumber(int number) {
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.getDescriptor().getEnumTypes().get(0);
     }
-    
-    private static final CompressionKind[] VALUES = {
-      NONE, ZLIB, SNAPPY, LZO, 
-    };
-    
+
+    private static final CompressionKind[] VALUES = values();
+
     public static CompressionKind valueOf(
         com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
       if (desc.getType() != getDescriptor()) {
@@ -71,92 +96,205 @@ public static CompressionKind valueOf(
       }
       return VALUES[desc.getIndex()];
     }
-    
+
     private final int index;
     private final int value;
-    
+
     private CompressionKind(int index, int value) {
       this.index = index;
       this.value = value;
     }
-    
+
     // @@protoc_insertion_point(enum_scope:org.apache.hadoop.hive.ql.io.orc.CompressionKind)
   }
-  
+
   public interface IntegerStatisticsOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // optional sint64 minimum = 1;
+    /**
+     * <code>optional sint64 minimum = 1;</code>
+     */
     boolean hasMinimum();
+    /**
+     * <code>optional sint64 minimum = 1;</code>
+     */
     long getMinimum();
-    
+
     // optional sint64 maximum = 2;
+    /**
+     * <code>optional sint64 maximum = 2;</code>
+     */
     boolean hasMaximum();
+    /**
+     * <code>optional sint64 maximum = 2;</code>
+     */
     long getMaximum();
-    
+
     // optional sint64 sum = 3;
+    /**
+     * <code>optional sint64 sum = 3;</code>
+     */
     boolean hasSum();
+    /**
+     * <code>optional sint64 sum = 3;</code>
+     */
     long getSum();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.IntegerStatistics}
+   */
   public static final class IntegerStatistics extends
       com.google.protobuf.GeneratedMessage
       implements IntegerStatisticsOrBuilder {
     // Use IntegerStatistics.newBuilder() to construct.
-    private IntegerStatistics(Builder builder) {
+    private IntegerStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private IntegerStatistics(boolean noInit) {}
-    
+    private IntegerStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final IntegerStatistics defaultInstance;
     public static IntegerStatistics getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public IntegerStatistics getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private IntegerStatistics(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              bitField0_ |= 0x00000001;
+              minimum_ = input.readSInt64();
+              break;
+            }
+            case 16: {
+              bitField0_ |= 0x00000002;
+              maximum_ = input.readSInt64();
+              break;
+            }
+            case 24: {
+              bitField0_ |= 0x00000004;
+              sum_ = input.readSInt64();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<IntegerStatistics> PARSER =
+        new com.google.protobuf.AbstractParser<IntegerStatistics>() {
+      public IntegerStatistics parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new IntegerStatistics(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<IntegerStatistics> getParserForType() {
+      return PARSER;
     }
-    
+
     private int bitField0_;
     // optional sint64 minimum = 1;
     public static final int MINIMUM_FIELD_NUMBER = 1;
     private long minimum_;
+    /**
+     * <code>optional sint64 minimum = 1;</code>
+     */
     public boolean hasMinimum() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
+    /**
+     * <code>optional sint64 minimum = 1;</code>
+     */
     public long getMinimum() {
       return minimum_;
     }
-    
+
     // optional sint64 maximum = 2;
     public static final int MAXIMUM_FIELD_NUMBER = 2;
     private long maximum_;
+    /**
+     * <code>optional sint64 maximum = 2;</code>
+     */
     public boolean hasMaximum() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
+    /**
+     * <code>optional sint64 maximum = 2;</code>
+     */
     public long getMaximum() {
       return maximum_;
     }
-    
+
     // optional sint64 sum = 3;
     public static final int SUM_FIELD_NUMBER = 3;
     private long sum_;
+    /**
+     * <code>optional sint64 sum = 3;</code>
+     */
     public boolean hasSum() {
       return ((bitField0_ & 0x00000004) == 0x00000004);
     }
+    /**
+     * <code>optional sint64 sum = 3;</code>
+     */
     public long getSum() {
       return sum_;
     }
-    
+
     private void initFields() {
       minimum_ = 0L;
       maximum_ = 0L;
@@ -166,11 +304,11 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -185,12 +323,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -208,94 +346,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.IntegerStatistics}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatisticsOrBuilder {
@@ -303,18 +430,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -325,7 +455,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         minimum_ = 0L;
@@ -336,20 +466,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics result = buildPartial();
         if (!result.isInitialized()) {
@@ -357,17 +487,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics(this);
         int from_bitField0_ = bitField0_;
@@ -388,7 +508,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics)other);
@@ -397,7 +517,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getDefaultInstance()) return this;
         if (other.hasMinimum()) {
@@ -412,203 +532,327 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatis
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              minimum_ = input.readSInt64();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              maximum_ = input.readSInt64();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              sum_ = input.readSInt64();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // optional sint64 minimum = 1;
       private long minimum_ ;
+      /**
+       * <code>optional sint64 minimum = 1;</code>
+       */
       public boolean hasMinimum() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
+      /**
+       * <code>optional sint64 minimum = 1;</code>
+       */
       public long getMinimum() {
         return minimum_;
       }
+      /**
+       * <code>optional sint64 minimum = 1;</code>
+       */
       public Builder setMinimum(long value) {
         bitField0_ |= 0x00000001;
         minimum_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional sint64 minimum = 1;</code>
+       */
       public Builder clearMinimum() {
         bitField0_ = (bitField0_ & ~0x00000001);
         minimum_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // optional sint64 maximum = 2;
       private long maximum_ ;
+      /**
+       * <code>optional sint64 maximum = 2;</code>
+       */
       public boolean hasMaximum() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
+      /**
+       * <code>optional sint64 maximum = 2;</code>
+       */
       public long getMaximum() {
         return maximum_;
       }
+      /**
+       * <code>optional sint64 maximum = 2;</code>
+       */
       public Builder setMaximum(long value) {
         bitField0_ |= 0x00000002;
         maximum_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional sint64 maximum = 2;</code>
+       */
       public Builder clearMaximum() {
         bitField0_ = (bitField0_ & ~0x00000002);
         maximum_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // optional sint64 sum = 3;
       private long sum_ ;
+      /**
+       * <code>optional sint64 sum = 3;</code>
+       */
       public boolean hasSum() {
         return ((bitField0_ & 0x00000004) == 0x00000004);
       }
+      /**
+       * <code>optional sint64 sum = 3;</code>
+       */
       public long getSum() {
         return sum_;
       }
+      /**
+       * <code>optional sint64 sum = 3;</code>
+       */
       public Builder setSum(long value) {
         bitField0_ |= 0x00000004;
         sum_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional sint64 sum = 3;</code>
+       */
       public Builder clearSum() {
         bitField0_ = (bitField0_ & ~0x00000004);
         sum_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.IntegerStatistics)
     }
-    
+
     static {
       defaultInstance = new IntegerStatistics(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.IntegerStatistics)
   }
-  
+
   public interface DoubleStatisticsOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // optional double minimum = 1;
+    /**
+     * <code>optional double minimum = 1;</code>
+     */
     boolean hasMinimum();
+    /**
+     * <code>optional double minimum = 1;</code>
+     */
     double getMinimum();
-    
+
     // optional double maximum = 2;
+    /**
+     * <code>optional double maximum = 2;</code>
+     */
     boolean hasMaximum();
+    /**
+     * <code>optional double maximum = 2;</code>
+     */
     double getMaximum();
-    
+
     // optional double sum = 3;
+    /**
+     * <code>optional double sum = 3;</code>
+     */
     boolean hasSum();
+    /**
+     * <code>optional double sum = 3;</code>
+     */
     double getSum();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DoubleStatistics}
+   */
   public static final class DoubleStatistics extends
       com.google.protobuf.GeneratedMessage
       implements DoubleStatisticsOrBuilder {
     // Use DoubleStatistics.newBuilder() to construct.
-    private DoubleStatistics(Builder builder) {
+    private DoubleStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private DoubleStatistics(boolean noInit) {}
-    
+    private DoubleStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final DoubleStatistics defaultInstance;
     public static DoubleStatistics getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public DoubleStatistics getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private DoubleStatistics(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 9: {
+              bitField0_ |= 0x00000001;
+              minimum_ = input.readDouble();
+              break;
+            }
+            case 17: {
+              bitField0_ |= 0x00000002;
+              maximum_ = input.readDouble();
+              break;
+            }
+            case 25: {
+              bitField0_ |= 0x00000004;
+              sum_ = input.readDouble();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<DoubleStatistics> PARSER =
+        new com.google.protobuf.AbstractParser<DoubleStatistics>() {
+      public DoubleStatistics parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new DoubleStatistics(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<DoubleStatistics> getParserForType() {
+      return PARSER;
     }
-    
+
     private int bitField0_;
     // optional double minimum = 1;
     public static final int MINIMUM_FIELD_NUMBER = 1;
     private double minimum_;
+    /**
+     * <code>optional double minimum = 1;</code>
+     */
     public boolean hasMinimum() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
+    /**
+     * <code>optional double minimum = 1;</code>
+     */
     public double getMinimum() {
       return minimum_;
     }
-    
+
     // optional double maximum = 2;
     public static final int MAXIMUM_FIELD_NUMBER = 2;
     private double maximum_;
+    /**
+     * <code>optional double maximum = 2;</code>
+     */
     public boolean hasMaximum() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
+    /**
+     * <code>optional double maximum = 2;</code>
+     */
     public double getMaximum() {
       return maximum_;
     }
-    
+
     // optional double sum = 3;
     public static final int SUM_FIELD_NUMBER = 3;
     private double sum_;
+    /**
+     * <code>optional double sum = 3;</code>
+     */
     public boolean hasSum() {
       return ((bitField0_ & 0x00000004) == 0x00000004);
     }
+    /**
+     * <code>optional double sum = 3;</code>
+     */
     public double getSum() {
       return sum_;
     }
-    
+
     private void initFields() {
       minimum_ = 0D;
       maximum_ = 0D;
@@ -618,11 +862,11 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -637,12 +881,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -660,94 +904,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DoubleStatistics}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatisticsOrBuilder {
@@ -755,18 +988,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -777,7 +1013,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         minimum_ = 0D;
@@ -788,20 +1024,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics result = buildPartial();
         if (!result.isInitialized()) {
@@ -809,17 +1045,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics(this);
         int from_bitField0_ = bitField0_;
@@ -840,7 +1066,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics)other);
@@ -849,7 +1075,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getDefaultInstance()) return this;
         if (other.hasMinimum()) {
@@ -864,233 +1090,360 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatist
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 9: {
-              bitField0_ |= 0x00000001;
-              minimum_ = input.readDouble();
-              break;
-            }
-            case 17: {
-              bitField0_ |= 0x00000002;
-              maximum_ = input.readDouble();
-              break;
-            }
-            case 25: {
-              bitField0_ |= 0x00000004;
-              sum_ = input.readDouble();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // optional double minimum = 1;
       private double minimum_ ;
+      /**
+       * <code>optional double minimum = 1;</code>
+       */
       public boolean hasMinimum() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
+      /**
+       * <code>optional double minimum = 1;</code>
+       */
       public double getMinimum() {
         return minimum_;
       }
+      /**
+       * <code>optional double minimum = 1;</code>
+       */
       public Builder setMinimum(double value) {
         bitField0_ |= 0x00000001;
         minimum_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional double minimum = 1;</code>
+       */
       public Builder clearMinimum() {
         bitField0_ = (bitField0_ & ~0x00000001);
         minimum_ = 0D;
         onChanged();
         return this;
       }
-      
+
       // optional double maximum = 2;
       private double maximum_ ;
+      /**
+       * <code>optional double maximum = 2;</code>
+       */
       public boolean hasMaximum() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
+      /**
+       * <code>optional double maximum = 2;</code>
+       */
       public double getMaximum() {
         return maximum_;
       }
+      /**
+       * <code>optional double maximum = 2;</code>
+       */
       public Builder setMaximum(double value) {
         bitField0_ |= 0x00000002;
         maximum_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional double maximum = 2;</code>
+       */
       public Builder clearMaximum() {
         bitField0_ = (bitField0_ & ~0x00000002);
         maximum_ = 0D;
         onChanged();
         return this;
       }
-      
+
       // optional double sum = 3;
       private double sum_ ;
+      /**
+       * <code>optional double sum = 3;</code>
+       */
       public boolean hasSum() {
         return ((bitField0_ & 0x00000004) == 0x00000004);
       }
+      /**
+       * <code>optional double sum = 3;</code>
+       */
       public double getSum() {
         return sum_;
       }
+      /**
+       * <code>optional double sum = 3;</code>
+       */
       public Builder setSum(double value) {
         bitField0_ |= 0x00000004;
         sum_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional double sum = 3;</code>
+       */
       public Builder clearSum() {
         bitField0_ = (bitField0_ & ~0x00000004);
         sum_ = 0D;
         onChanged();
         return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.DoubleStatistics)
     }
-    
+
     static {
       defaultInstance = new DoubleStatistics(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.DoubleStatistics)
   }
-  
+
   public interface StringStatisticsOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // optional string minimum = 1;
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
     boolean hasMinimum();
-    String getMinimum();
-    
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
+    java.lang.String getMinimum();
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
+    com.google.protobuf.ByteString
+        getMinimumBytes();
+
     // optional string maximum = 2;
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
     boolean hasMaximum();
-    String getMaximum();
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
+    java.lang.String getMaximum();
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
+    com.google.protobuf.ByteString
+        getMaximumBytes();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StringStatistics}
+   */
   public static final class StringStatistics extends
       com.google.protobuf.GeneratedMessage
       implements StringStatisticsOrBuilder {
     // Use StringStatistics.newBuilder() to construct.
-    private StringStatistics(Builder builder) {
+    private StringStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private StringStatistics(boolean noInit) {}
-    
+    private StringStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final StringStatistics defaultInstance;
     public static StringStatistics getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public StringStatistics getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private StringStatistics(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              minimum_ = input.readBytes();
+              break;
+            }
+            case 18: {
+              bitField0_ |= 0x00000002;
+              maximum_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.class);
     }
-    
+
+    public static com.google.protobuf.Parser<StringStatistics> PARSER =
+        new com.google.protobuf.AbstractParser<StringStatistics>() {
+      public StringStatistics parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new StringStatistics(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<StringStatistics> getParserForType() {
+      return PARSER;
+    }
+
     private int bitField0_;
     // optional string minimum = 1;
     public static final int MINIMUM_FIELD_NUMBER = 1;
     private java.lang.Object minimum_;
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
     public boolean hasMinimum() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
-    public String getMinimum() {
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
+    public java.lang.String getMinimum() {
       java.lang.Object ref = minimum_;
-      if (ref instanceof String) {
-        return (String) ref;
+      if (ref instanceof java.lang.String) {
+        return (java.lang.String) ref;
       } else {
         com.google.protobuf.ByteString bs = 
             (com.google.protobuf.ByteString) ref;
-        String s = bs.toStringUtf8();
-        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+        java.lang.String s = bs.toStringUtf8();
+        if (bs.isValidUtf8()) {
           minimum_ = s;
         }
         return s;
       }
     }
-    private com.google.protobuf.ByteString getMinimumBytes() {
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
+    public com.google.protobuf.ByteString
+        getMinimumBytes() {
       java.lang.Object ref = minimum_;
-      if (ref instanceof String) {
+      if (ref instanceof java.lang.String) {
         com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+            com.google.protobuf.ByteString.copyFromUtf8(
+                (java.lang.String) ref);
         minimum_ = b;
         return b;
       } else {
         return (com.google.protobuf.ByteString) ref;
       }
     }
-    
+
     // optional string maximum = 2;
     public static final int MAXIMUM_FIELD_NUMBER = 2;
     private java.lang.Object maximum_;
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
     public boolean hasMaximum() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
-    public String getMaximum() {
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
+    public java.lang.String getMaximum() {
       java.lang.Object ref = maximum_;
-      if (ref instanceof String) {
-        return (String) ref;
+      if (ref instanceof java.lang.String) {
+        return (java.lang.String) ref;
       } else {
         com.google.protobuf.ByteString bs = 
             (com.google.protobuf.ByteString) ref;
-        String s = bs.toStringUtf8();
-        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+        java.lang.String s = bs.toStringUtf8();
+        if (bs.isValidUtf8()) {
           maximum_ = s;
         }
         return s;
       }
     }
-    private com.google.protobuf.ByteString getMaximumBytes() {
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
+    public com.google.protobuf.ByteString
+        getMaximumBytes() {
       java.lang.Object ref = maximum_;
-      if (ref instanceof String) {
+      if (ref instanceof java.lang.String) {
         com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+            com.google.protobuf.ByteString.copyFromUtf8(
+                (java.lang.String) ref);
         maximum_ = b;
         return b;
       } else {
         return (com.google.protobuf.ByteString) ref;
       }
     }
-    
+
     private void initFields() {
       minimum_ = "";
       maximum_ = "";
@@ -1099,11 +1452,11 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -1115,12 +1468,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -1134,94 +1487,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StringStatistics}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatisticsOrBuilder {
@@ -1229,18 +1571,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -1251,7 +1596,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         minimum_ = "";
@@ -1260,20 +1605,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics result = buildPartial();
         if (!result.isInitialized()) {
@@ -1281,17 +1626,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics(this);
         int from_bitField0_ = bitField0_;
@@ -1308,7 +1643,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics)other);
@@ -1317,78 +1652,89 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getDefaultInstance()) return this;
         if (other.hasMinimum()) {
-          setMinimum(other.getMinimum());
+          bitField0_ |= 0x00000001;
+          minimum_ = other.minimum_;
+          onChanged();
         }
         if (other.hasMaximum()) {
-          setMaximum(other.getMaximum());
+          bitField0_ |= 0x00000002;
+          maximum_ = other.maximum_;
+          onChanged();
         }
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              minimum_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              maximum_ = input.readBytes();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // optional string minimum = 1;
       private java.lang.Object minimum_ = "";
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
       public boolean hasMinimum() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
-      public String getMinimum() {
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
+      public java.lang.String getMinimum() {
         java.lang.Object ref = minimum_;
-        if (!(ref instanceof String)) {
-          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+        if (!(ref instanceof java.lang.String)) {
+          java.lang.String s = ((com.google.protobuf.ByteString) ref)
+              .toStringUtf8();
           minimum_ = s;
           return s;
         } else {
-          return (String) ref;
+          return (java.lang.String) ref;
         }
       }
-      public Builder setMinimum(String value) {
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
+      public com.google.protobuf.ByteString
+          getMinimumBytes() {
+        java.lang.Object ref = minimum_;
+        if (ref instanceof String) {
+          com.google.protobuf.ByteString b = 
+              com.google.protobuf.ByteString.copyFromUtf8(
+                  (java.lang.String) ref);
+          minimum_ = b;
+          return b;
+        } else {
+          return (com.google.protobuf.ByteString) ref;
+        }
+      }
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
+      public Builder setMinimum(
+          java.lang.String value) {
         if (value == null) {
     throw new NullPointerException();
   }
@@ -1397,34 +1743,72 @@ public Builder setMinimum(String value) {
         onChanged();
         return this;
       }
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
       public Builder clearMinimum() {
         bitField0_ = (bitField0_ & ~0x00000001);
         minimum_ = getDefaultInstance().getMinimum();
         onChanged();
         return this;
       }
-      void setMinimum(com.google.protobuf.ByteString value) {
-        bitField0_ |= 0x00000001;
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
+      public Builder setMinimumBytes(
+          com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
         minimum_ = value;
         onChanged();
+        return this;
       }
-      
+
       // optional string maximum = 2;
       private java.lang.Object maximum_ = "";
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
       public boolean hasMaximum() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
-      public String getMaximum() {
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
+      public java.lang.String getMaximum() {
         java.lang.Object ref = maximum_;
-        if (!(ref instanceof String)) {
-          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+        if (!(ref instanceof java.lang.String)) {
+          java.lang.String s = ((com.google.protobuf.ByteString) ref)
+              .toStringUtf8();
           maximum_ = s;
           return s;
         } else {
-          return (String) ref;
+          return (java.lang.String) ref;
         }
       }
-      public Builder setMaximum(String value) {
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
+      public com.google.protobuf.ByteString
+          getMaximumBytes() {
+        java.lang.Object ref = maximum_;
+        if (ref instanceof String) {
+          com.google.protobuf.ByteString b = 
+              com.google.protobuf.ByteString.copyFromUtf8(
+                  (java.lang.String) ref);
+          maximum_ = b;
+          return b;
+        } else {
+          return (com.google.protobuf.ByteString) ref;
+        }
+      }
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
+      public Builder setMaximum(
+          java.lang.String value) {
         if (value == null) {
     throw new NullPointerException();
   }
@@ -1433,92 +1817,207 @@ public Builder setMaximum(String value) {
         onChanged();
         return this;
       }
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
       public Builder clearMaximum() {
         bitField0_ = (bitField0_ & ~0x00000002);
         maximum_ = getDefaultInstance().getMaximum();
         onChanged();
         return this;
       }
-      void setMaximum(com.google.protobuf.ByteString value) {
-        bitField0_ |= 0x00000002;
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
+      public Builder setMaximumBytes(
+          com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000002;
         maximum_ = value;
         onChanged();
+        return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.StringStatistics)
     }
-    
+
     static {
       defaultInstance = new StringStatistics(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.StringStatistics)
   }
-  
+
   public interface BucketStatisticsOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // repeated uint64 count = 1 [packed = true];
+    /**
+     * <code>repeated uint64 count = 1 [packed = true];</code>
+     */
     java.util.List<java.lang.Long> getCountList();
+    /**
+     * <code>repeated uint64 count = 1 [packed = true];</code>
+     */
     int getCountCount();
+    /**
+     * <code>repeated uint64 count = 1 [packed = true];</code>
+     */
     long getCount(int index);
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.BucketStatistics}
+   */
   public static final class BucketStatistics extends
       com.google.protobuf.GeneratedMessage
       implements BucketStatisticsOrBuilder {
     // Use BucketStatistics.newBuilder() to construct.
-    private BucketStatistics(Builder builder) {
+    private BucketStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private BucketStatistics(boolean noInit) {}
-    
+    private BucketStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final BucketStatistics defaultInstance;
     public static BucketStatistics getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public BucketStatistics getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private BucketStatistics(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
+                count_ = new java.util.ArrayList<java.lang.Long>();
+                mutable_bitField0_ |= 0x00000001;
+              }
+              count_.add(input.readUInt64());
+              break;
+            }
+            case 10: {
+              int length = input.readRawVarint32();
+              int limit = input.pushLimit(length);
+              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001) && input.getBytesUntilLimit() > 0) {
+                count_ = new java.util.ArrayList<java.lang.Long>();
+                mutable_bitField0_ |= 0x00000001;
+              }
+              while (input.getBytesUntilLimit() > 0) {
+                count_.add(input.readUInt64());
+              }
+              input.popLimit(limit);
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
+          count_ = java.util.Collections.unmodifiableList(count_);
+        }
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<BucketStatistics> PARSER =
+        new com.google.protobuf.AbstractParser<BucketStatistics>() {
+      public BucketStatistics parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new BucketStatistics(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<BucketStatistics> getParserForType() {
+      return PARSER;
     }
-    
+
     // repeated uint64 count = 1 [packed = true];
     public static final int COUNT_FIELD_NUMBER = 1;
     private java.util.List<java.lang.Long> count_;
+    /**
+     * <code>repeated uint64 count = 1 [packed = true];</code>
+     */
     public java.util.List<java.lang.Long>
         getCountList() {
       return count_;
     }
+    /**
+     * <code>repeated uint64 count = 1 [packed = true];</code>
+     */
     public int getCountCount() {
       return count_.size();
     }
+    /**
+     * <code>repeated uint64 count = 1 [packed = true];</code>
+     */
     public long getCount(int index) {
       return count_.get(index);
     }
     private int countMemoizedSerializedSize = -1;
-    
+
     private void initFields() {
-      count_ = java.util.Collections.emptyList();;
+      count_ = java.util.Collections.emptyList();
     }
     private byte memoizedIsInitialized = -1;
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -1531,12 +2030,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       {
         int dataSize = 0;
@@ -1556,94 +2055,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.BucketStatistics}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatisticsOrBuilder {
@@ -1651,18 +2139,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -1673,27 +2164,27 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
-        count_ = java.util.Collections.emptyList();;
+        count_ = java.util.Collections.emptyList();
         bitField0_ = (bitField0_ & ~0x00000001);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics result = buildPartial();
         if (!result.isInitialized()) {
@@ -1701,17 +2192,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics(this);
         int from_bitField0_ = bitField0_;
@@ -1723,7 +2204,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics)other);
@@ -1732,7 +2213,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getDefaultInstance()) return this;
         if (!other.count_.isEmpty()) {
@@ -1748,72 +2229,60 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatist
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              ensureCountIsMutable();
-              count_.add(input.readUInt64());
-              break;
-            }
-            case 10: {
-              int length = input.readRawVarint32();
-              int limit = input.pushLimit(length);
-              while (input.getBytesUntilLimit() > 0) {
-                addCount(input.readUInt64());
-              }
-              input.popLimit(limit);
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // repeated uint64 count = 1 [packed = true];
-      private java.util.List<java.lang.Long> count_ = java.util.Collections.emptyList();;
+      private java.util.List<java.lang.Long> count_ = java.util.Collections.emptyList();
       private void ensureCountIsMutable() {
         if (!((bitField0_ & 0x00000001) == 0x00000001)) {
           count_ = new java.util.ArrayList<java.lang.Long>(count_);
           bitField0_ |= 0x00000001;
          }
       }
+      /**
+       * <code>repeated uint64 count = 1 [packed = true];</code>
+       */
       public java.util.List<java.lang.Long>
           getCountList() {
         return java.util.Collections.unmodifiableList(count_);
       }
+      /**
+       * <code>repeated uint64 count = 1 [packed = true];</code>
+       */
       public int getCountCount() {
         return count_.size();
       }
+      /**
+       * <code>repeated uint64 count = 1 [packed = true];</code>
+       */
       public long getCount(int index) {
         return count_.get(index);
       }
+      /**
+       * <code>repeated uint64 count = 1 [packed = true];</code>
+       */
       public Builder setCount(
           int index, long value) {
         ensureCountIsMutable();
@@ -1821,12 +2290,18 @@ public Builder setCount(
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint64 count = 1 [packed = true];</code>
+       */
       public Builder addCount(long value) {
         ensureCountIsMutable();
         count_.add(value);
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint64 count = 1 [packed = true];</code>
+       */
       public Builder addAllCount(
           java.lang.Iterable<? extends java.lang.Long> values) {
         ensureCountIsMutable();
@@ -1834,164 +2309,310 @@ public Builder addAllCount(
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint64 count = 1 [packed = true];</code>
+       */
       public Builder clearCount() {
-        count_ = java.util.Collections.emptyList();;
+        count_ = java.util.Collections.emptyList();
         bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.BucketStatistics)
     }
-    
+
     static {
       defaultInstance = new BucketStatistics(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.BucketStatistics)
   }
-  
+
   public interface DecimalStatisticsOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // optional string minimum = 1;
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
     boolean hasMinimum();
-    String getMinimum();
-    
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
+    java.lang.String getMinimum();
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
+    com.google.protobuf.ByteString
+        getMinimumBytes();
+
     // optional string maximum = 2;
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
     boolean hasMaximum();
-    String getMaximum();
-    
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
+    java.lang.String getMaximum();
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
+    com.google.protobuf.ByteString
+        getMaximumBytes();
+
     // optional string sum = 3;
+    /**
+     * <code>optional string sum = 3;</code>
+     */
     boolean hasSum();
-    String getSum();
+    /**
+     * <code>optional string sum = 3;</code>
+     */
+    java.lang.String getSum();
+    /**
+     * <code>optional string sum = 3;</code>
+     */
+    com.google.protobuf.ByteString
+        getSumBytes();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DecimalStatistics}
+   */
   public static final class DecimalStatistics extends
       com.google.protobuf.GeneratedMessage
       implements DecimalStatisticsOrBuilder {
     // Use DecimalStatistics.newBuilder() to construct.
-    private DecimalStatistics(Builder builder) {
+    private DecimalStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private DecimalStatistics(boolean noInit) {}
-    
+    private DecimalStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final DecimalStatistics defaultInstance;
     public static DecimalStatistics getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public DecimalStatistics getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private DecimalStatistics(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              minimum_ = input.readBytes();
+              break;
+            }
+            case 18: {
+              bitField0_ |= 0x00000002;
+              maximum_ = input.readBytes();
+              break;
+            }
+            case 26: {
+              bitField0_ |= 0x00000004;
+              sum_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<DecimalStatistics> PARSER =
+        new com.google.protobuf.AbstractParser<DecimalStatistics>() {
+      public DecimalStatistics parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new DecimalStatistics(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<DecimalStatistics> getParserForType() {
+      return PARSER;
     }
-    
+
     private int bitField0_;
     // optional string minimum = 1;
     public static final int MINIMUM_FIELD_NUMBER = 1;
     private java.lang.Object minimum_;
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
     public boolean hasMinimum() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
-    public String getMinimum() {
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
+    public java.lang.String getMinimum() {
       java.lang.Object ref = minimum_;
-      if (ref instanceof String) {
-        return (String) ref;
+      if (ref instanceof java.lang.String) {
+        return (java.lang.String) ref;
       } else {
         com.google.protobuf.ByteString bs = 
             (com.google.protobuf.ByteString) ref;
-        String s = bs.toStringUtf8();
-        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+        java.lang.String s = bs.toStringUtf8();
+        if (bs.isValidUtf8()) {
           minimum_ = s;
         }
         return s;
       }
     }
-    private com.google.protobuf.ByteString getMinimumBytes() {
+    /**
+     * <code>optional string minimum = 1;</code>
+     */
+    public com.google.protobuf.ByteString
+        getMinimumBytes() {
       java.lang.Object ref = minimum_;
-      if (ref instanceof String) {
+      if (ref instanceof java.lang.String) {
         com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+            com.google.protobuf.ByteString.copyFromUtf8(
+                (java.lang.String) ref);
         minimum_ = b;
         return b;
       } else {
         return (com.google.protobuf.ByteString) ref;
       }
     }
-    
+
     // optional string maximum = 2;
     public static final int MAXIMUM_FIELD_NUMBER = 2;
     private java.lang.Object maximum_;
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
     public boolean hasMaximum() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
-    public String getMaximum() {
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
+    public java.lang.String getMaximum() {
       java.lang.Object ref = maximum_;
-      if (ref instanceof String) {
-        return (String) ref;
+      if (ref instanceof java.lang.String) {
+        return (java.lang.String) ref;
       } else {
         com.google.protobuf.ByteString bs = 
             (com.google.protobuf.ByteString) ref;
-        String s = bs.toStringUtf8();
-        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+        java.lang.String s = bs.toStringUtf8();
+        if (bs.isValidUtf8()) {
           maximum_ = s;
         }
         return s;
       }
     }
-    private com.google.protobuf.ByteString getMaximumBytes() {
+    /**
+     * <code>optional string maximum = 2;</code>
+     */
+    public com.google.protobuf.ByteString
+        getMaximumBytes() {
       java.lang.Object ref = maximum_;
-      if (ref instanceof String) {
+      if (ref instanceof java.lang.String) {
         com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+            com.google.protobuf.ByteString.copyFromUtf8(
+                (java.lang.String) ref);
         maximum_ = b;
         return b;
       } else {
         return (com.google.protobuf.ByteString) ref;
       }
     }
-    
+
     // optional string sum = 3;
     public static final int SUM_FIELD_NUMBER = 3;
     private java.lang.Object sum_;
+    /**
+     * <code>optional string sum = 3;</code>
+     */
     public boolean hasSum() {
       return ((bitField0_ & 0x00000004) == 0x00000004);
     }
-    public String getSum() {
+    /**
+     * <code>optional string sum = 3;</code>
+     */
+    public java.lang.String getSum() {
       java.lang.Object ref = sum_;
-      if (ref instanceof String) {
-        return (String) ref;
+      if (ref instanceof java.lang.String) {
+        return (java.lang.String) ref;
       } else {
         com.google.protobuf.ByteString bs = 
             (com.google.protobuf.ByteString) ref;
-        String s = bs.toStringUtf8();
-        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+        java.lang.String s = bs.toStringUtf8();
+        if (bs.isValidUtf8()) {
           sum_ = s;
         }
         return s;
       }
     }
-    private com.google.protobuf.ByteString getSumBytes() {
+    /**
+     * <code>optional string sum = 3;</code>
+     */
+    public com.google.protobuf.ByteString
+        getSumBytes() {
       java.lang.Object ref = sum_;
-      if (ref instanceof String) {
+      if (ref instanceof java.lang.String) {
         com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+            com.google.protobuf.ByteString.copyFromUtf8(
+                (java.lang.String) ref);
         sum_ = b;
         return b;
       } else {
         return (com.google.protobuf.ByteString) ref;
       }
     }
-    
+
     private void initFields() {
       minimum_ = "";
       maximum_ = "";
@@ -2001,11 +2622,11 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -2020,12 +2641,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -2043,94 +2664,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DecimalStatistics}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatisticsOrBuilder {
@@ -2138,18 +2748,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -2160,7 +2773,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         minimum_ = "";
@@ -2171,20 +2784,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics result = buildPartial();
         if (!result.isInitialized()) {
@@ -2192,17 +2805,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics(this);
         int from_bitField0_ = bitField0_;
@@ -2223,7 +2826,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics)other);
@@ -2232,86 +2835,94 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getDefaultInstance()) return this;
         if (other.hasMinimum()) {
-          setMinimum(other.getMinimum());
+          bitField0_ |= 0x00000001;
+          minimum_ = other.minimum_;
+          onChanged();
         }
         if (other.hasMaximum()) {
-          setMaximum(other.getMaximum());
+          bitField0_ |= 0x00000002;
+          maximum_ = other.maximum_;
+          onChanged();
         }
         if (other.hasSum()) {
-          setSum(other.getSum());
+          bitField0_ |= 0x00000004;
+          sum_ = other.sum_;
+          onChanged();
         }
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              minimum_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              maximum_ = input.readBytes();
-              break;
-            }
-            case 26: {
-              bitField0_ |= 0x00000004;
-              sum_ = input.readBytes();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // optional string minimum = 1;
       private java.lang.Object minimum_ = "";
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
       public boolean hasMinimum() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
-      public String getMinimum() {
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
+      public java.lang.String getMinimum() {
         java.lang.Object ref = minimum_;
-        if (!(ref instanceof String)) {
-          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+        if (!(ref instanceof java.lang.String)) {
+          java.lang.String s = ((com.google.protobuf.ByteString) ref)
+              .toStringUtf8();
           minimum_ = s;
           return s;
         } else {
-          return (String) ref;
+          return (java.lang.String) ref;
         }
       }
-      public Builder setMinimum(String value) {
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
+      public com.google.protobuf.ByteString
+          getMinimumBytes() {
+        java.lang.Object ref = minimum_;
+        if (ref instanceof String) {
+          com.google.protobuf.ByteString b = 
+              com.google.protobuf.ByteString.copyFromUtf8(
+                  (java.lang.String) ref);
+          minimum_ = b;
+          return b;
+        } else {
+          return (com.google.protobuf.ByteString) ref;
+        }
+      }
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
+      public Builder setMinimum(
+          java.lang.String value) {
         if (value == null) {
     throw new NullPointerException();
   }
@@ -2320,34 +2931,72 @@ public Builder setMinimum(String value) {
         onChanged();
         return this;
       }
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
       public Builder clearMinimum() {
         bitField0_ = (bitField0_ & ~0x00000001);
         minimum_ = getDefaultInstance().getMinimum();
         onChanged();
         return this;
       }
-      void setMinimum(com.google.protobuf.ByteString value) {
-        bitField0_ |= 0x00000001;
+      /**
+       * <code>optional string minimum = 1;</code>
+       */
+      public Builder setMinimumBytes(
+          com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
         minimum_ = value;
         onChanged();
+        return this;
       }
-      
+
       // optional string maximum = 2;
       private java.lang.Object maximum_ = "";
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
       public boolean hasMaximum() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
-      public String getMaximum() {
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
+      public java.lang.String getMaximum() {
         java.lang.Object ref = maximum_;
-        if (!(ref instanceof String)) {
-          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+        if (!(ref instanceof java.lang.String)) {
+          java.lang.String s = ((com.google.protobuf.ByteString) ref)
+              .toStringUtf8();
           maximum_ = s;
           return s;
         } else {
-          return (String) ref;
+          return (java.lang.String) ref;
         }
       }
-      public Builder setMaximum(String value) {
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
+      public com.google.protobuf.ByteString
+          getMaximumBytes() {
+        java.lang.Object ref = maximum_;
+        if (ref instanceof String) {
+          com.google.protobuf.ByteString b = 
+              com.google.protobuf.ByteString.copyFromUtf8(
+                  (java.lang.String) ref);
+          maximum_ = b;
+          return b;
+        } else {
+          return (com.google.protobuf.ByteString) ref;
+        }
+      }
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
+      public Builder setMaximum(
+          java.lang.String value) {
         if (value == null) {
     throw new NullPointerException();
   }
@@ -2356,34 +3005,72 @@ public Builder setMaximum(String value) {
         onChanged();
         return this;
       }
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
       public Builder clearMaximum() {
         bitField0_ = (bitField0_ & ~0x00000002);
         maximum_ = getDefaultInstance().getMaximum();
         onChanged();
         return this;
       }
-      void setMaximum(com.google.protobuf.ByteString value) {
-        bitField0_ |= 0x00000002;
+      /**
+       * <code>optional string maximum = 2;</code>
+       */
+      public Builder setMaximumBytes(
+          com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000002;
         maximum_ = value;
         onChanged();
+        return this;
       }
-      
+
       // optional string sum = 3;
       private java.lang.Object sum_ = "";
+      /**
+       * <code>optional string sum = 3;</code>
+       */
       public boolean hasSum() {
         return ((bitField0_ & 0x00000004) == 0x00000004);
       }
-      public String getSum() {
+      /**
+       * <code>optional string sum = 3;</code>
+       */
+      public java.lang.String getSum() {
         java.lang.Object ref = sum_;
-        if (!(ref instanceof String)) {
-          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+        if (!(ref instanceof java.lang.String)) {
+          java.lang.String s = ((com.google.protobuf.ByteString) ref)
+              .toStringUtf8();
           sum_ = s;
           return s;
         } else {
-          return (String) ref;
+          return (java.lang.String) ref;
+        }
+      }
+      /**
+       * <code>optional string sum = 3;</code>
+       */
+      public com.google.protobuf.ByteString
+          getSumBytes() {
+        java.lang.Object ref = sum_;
+        if (ref instanceof String) {
+          com.google.protobuf.ByteString b = 
+              com.google.protobuf.ByteString.copyFromUtf8(
+                  (java.lang.String) ref);
+          sum_ = b;
+          return b;
+        } else {
+          return (com.google.protobuf.ByteString) ref;
         }
       }
-      public Builder setSum(String value) {
+      /**
+       * <code>optional string sum = 3;</code>
+       */
+      public Builder setSum(
+          java.lang.String value) {
         if (value == null) {
     throw new NullPointerException();
   }
@@ -2392,89 +3079,212 @@ public Builder setSum(String value) {
         onChanged();
         return this;
       }
+      /**
+       * <code>optional string sum = 3;</code>
+       */
       public Builder clearSum() {
         bitField0_ = (bitField0_ & ~0x00000004);
         sum_ = getDefaultInstance().getSum();
         onChanged();
         return this;
       }
-      void setSum(com.google.protobuf.ByteString value) {
-        bitField0_ |= 0x00000004;
+      /**
+       * <code>optional string sum = 3;</code>
+       */
+      public Builder setSumBytes(
+          com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000004;
         sum_ = value;
         onChanged();
+        return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.DecimalStatistics)
     }
-    
+
     static {
       defaultInstance = new DecimalStatistics(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.DecimalStatistics)
   }
-  
+
   public interface DateStatisticsOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // optional sint32 minimum = 1;
+    /**
+     * <code>optional sint32 minimum = 1;</code>
+     *
+     * <pre>
+     * min,max values saved as days since epoch
+     * </pre>
+     */
     boolean hasMinimum();
+    /**
+     * <code>optional sint32 minimum = 1;</code>
+     *
+     * <pre>
+     * min,max values saved as days since epoch
+     * </pre>
+     */
     int getMinimum();
-    
+
     // optional sint32 maximum = 2;
+    /**
+     * <code>optional sint32 maximum = 2;</code>
+     */
     boolean hasMaximum();
+    /**
+     * <code>optional sint32 maximum = 2;</code>
+     */
     int getMaximum();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DateStatistics}
+   */
   public static final class DateStatistics extends
       com.google.protobuf.GeneratedMessage
       implements DateStatisticsOrBuilder {
     // Use DateStatistics.newBuilder() to construct.
-    private DateStatistics(Builder builder) {
+    private DateStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private DateStatistics(boolean noInit) {}
-    
+    private DateStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final DateStatistics defaultInstance;
     public static DateStatistics getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public DateStatistics getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private DateStatistics(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              bitField0_ |= 0x00000001;
+              minimum_ = input.readSInt32();
+              break;
+            }
+            case 16: {
+              bitField0_ |= 0x00000002;
+              maximum_ = input.readSInt32();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<DateStatistics> PARSER =
+        new com.google.protobuf.AbstractParser<DateStatistics>() {
+      public DateStatistics parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new DateStatistics(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<DateStatistics> getParserForType() {
+      return PARSER;
     }
-    
+
     private int bitField0_;
     // optional sint32 minimum = 1;
     public static final int MINIMUM_FIELD_NUMBER = 1;
     private int minimum_;
+    /**
+     * <code>optional sint32 minimum = 1;</code>
+     *
+     * <pre>
+     * min,max values saved as days since epoch
+     * </pre>
+     */
     public boolean hasMinimum() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
+    /**
+     * <code>optional sint32 minimum = 1;</code>
+     *
+     * <pre>
+     * min,max values saved as days since epoch
+     * </pre>
+     */
     public int getMinimum() {
       return minimum_;
     }
-    
+
     // optional sint32 maximum = 2;
     public static final int MAXIMUM_FIELD_NUMBER = 2;
     private int maximum_;
+    /**
+     * <code>optional sint32 maximum = 2;</code>
+     */
     public boolean hasMaximum() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
+    /**
+     * <code>optional sint32 maximum = 2;</code>
+     */
     public int getMaximum() {
       return maximum_;
     }
-    
+
     private void initFields() {
       minimum_ = 0;
       maximum_ = 0;
@@ -2483,11 +3293,11 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -2499,12 +3309,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -2518,94 +3328,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.DateStatistics}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatisticsOrBuilder {
@@ -2613,18 +3412,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -2635,7 +3437,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         minimum_ = 0;
@@ -2644,20 +3446,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics result = buildPartial();
         if (!result.isInitialized()) {
@@ -2665,17 +3467,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics(this);
         int from_bitField0_ = bitField0_;
@@ -2692,7 +3484,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics)other);
@@ -2701,7 +3493,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getDefaultInstance()) return this;
         if (other.hasMinimum()) {
@@ -2713,257 +3505,542 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistic
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              minimum_ = input.readSInt32();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              maximum_ = input.readSInt32();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // optional sint32 minimum = 1;
       private int minimum_ ;
+      /**
+       * <code>optional sint32 minimum = 1;</code>
+       *
+       * <pre>
+       * min,max values saved as days since epoch
+       * </pre>
+       */
       public boolean hasMinimum() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
+      /**
+       * <code>optional sint32 minimum = 1;</code>
+       *
+       * <pre>
+       * min,max values saved as days since epoch
+       * </pre>
+       */
       public int getMinimum() {
         return minimum_;
       }
+      /**
+       * <code>optional sint32 minimum = 1;</code>
+       *
+       * <pre>
+       * min,max values saved as days since epoch
+       * </pre>
+       */
       public Builder setMinimum(int value) {
         bitField0_ |= 0x00000001;
         minimum_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional sint32 minimum = 1;</code>
+       *
+       * <pre>
+       * min,max values saved as days since epoch
+       * </pre>
+       */
       public Builder clearMinimum() {
         bitField0_ = (bitField0_ & ~0x00000001);
         minimum_ = 0;
         onChanged();
         return this;
       }
-      
+
       // optional sint32 maximum = 2;
       private int maximum_ ;
+      /**
+       * <code>optional sint32 maximum = 2;</code>
+       */
       public boolean hasMaximum() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
+      /**
+       * <code>optional sint32 maximum = 2;</code>
+       */
       public int getMaximum() {
         return maximum_;
       }
+      /**
+       * <code>optional sint32 maximum = 2;</code>
+       */
       public Builder setMaximum(int value) {
         bitField0_ |= 0x00000002;
         maximum_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional sint32 maximum = 2;</code>
+       */
       public Builder clearMaximum() {
         bitField0_ = (bitField0_ & ~0x00000002);
         maximum_ = 0;
         onChanged();
         return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.DateStatistics)
     }
-    
+
     static {
       defaultInstance = new DateStatistics(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.DateStatistics)
   }
-  
+
   public interface ColumnStatisticsOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // optional uint64 numberOfValues = 1;
+    /**
+     * <code>optional uint64 numberOfValues = 1;</code>
+     */
     boolean hasNumberOfValues();
+    /**
+     * <code>optional uint64 numberOfValues = 1;</code>
+     */
     long getNumberOfValues();
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+     */
     boolean hasIntStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics getIntStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatisticsOrBuilder getIntStatisticsOrBuilder();
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+     */
     boolean hasDoubleStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics getDoubleStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatisticsOrBuilder getDoubleStatisticsOrBuilder();
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+     */
     boolean hasStringStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics getStringStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatisticsOrBuilder getStringStatisticsOrBuilder();
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+     */
     boolean hasBucketStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics getBucketStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatisticsOrBuilder getBucketStatisticsOrBuilder();
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+     */
     boolean hasDecimalStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics getDecimalStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatisticsOrBuilder getDecimalStatisticsOrBuilder();
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+     */
     boolean hasDateStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics getDateStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatisticsOrBuilder getDateStatisticsOrBuilder();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.ColumnStatistics}
+   */
   public static final class ColumnStatistics extends
       com.google.protobuf.GeneratedMessage
       implements ColumnStatisticsOrBuilder {
     // Use ColumnStatistics.newBuilder() to construct.
-    private ColumnStatistics(Builder builder) {
+    private ColumnStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private ColumnStatistics(boolean noInit) {}
-    
+    private ColumnStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final ColumnStatistics defaultInstance;
     public static ColumnStatistics getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public ColumnStatistics getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private ColumnStatistics(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              bitField0_ |= 0x00000001;
+              numberOfValues_ = input.readUInt64();
+              break;
+            }
+            case 18: {
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder subBuilder = null;
+              if (((bitField0_ & 0x00000002) == 0x00000002)) {
+                subBuilder = intStatistics_.toBuilder();
+              }
+              intStatistics_ = input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.PARSER, extensionRegistry);
+              if (subBuilder != null) {
+                subBuilder.mergeFrom(intStatistics_);
+                intStatistics_ = subBuilder.buildPartial();
+              }
+              bitField0_ |= 0x00000002;
+              break;
+            }
+            case 26: {
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder subBuilder = null;
+              if (((bitField0_ & 0x00000004) == 0x00000004)) {
+                subBuilder = doubleStatistics_.toBuilder();
+              }
+              doubleStatistics_ = input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.PARSER, extensionRegistry);
+              if (subBuilder != null) {
+                subBuilder.mergeFrom(doubleStatistics_);
+                doubleStatistics_ = subBuilder.buildPartial();
+              }
+              bitField0_ |= 0x00000004;
+              break;
+            }
+            case 34: {
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder subBuilder = null;
+              if (((bitField0_ & 0x00000008) == 0x00000008)) {
+                subBuilder = stringStatistics_.toBuilder();
+              }
+              stringStatistics_ = input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.PARSER, extensionRegistry);
+              if (subBuilder != null) {
+                subBuilder.mergeFrom(stringStatistics_);
+                stringStatistics_ = subBuilder.buildPartial();
+              }
+              bitField0_ |= 0x00000008;
+              break;
+            }
+            case 42: {
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder subBuilder = null;
+              if (((bitField0_ & 0x00000010) == 0x00000010)) {
+                subBuilder = bucketStatistics_.toBuilder();
+              }
+              bucketStatistics_ = input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.PARSER, extensionRegistry);
+              if (subBuilder != null) {
+                subBuilder.mergeFrom(bucketStatistics_);
+                bucketStatistics_ = subBuilder.buildPartial();
+              }
+              bitField0_ |= 0x00000010;
+              break;
+            }
+            case 50: {
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder subBuilder = null;
+              if (((bitField0_ & 0x00000020) == 0x00000020)) {
+                subBuilder = decimalStatistics_.toBuilder();
+              }
+              decimalStatistics_ = input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.PARSER, extensionRegistry);
+              if (subBuilder != null) {
+                subBuilder.mergeFrom(decimalStatistics_);
+                decimalStatistics_ = subBuilder.buildPartial();
+              }
+              bitField0_ |= 0x00000020;
+              break;
+            }
+            case 58: {
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder subBuilder = null;
+              if (((bitField0_ & 0x00000040) == 0x00000040)) {
+                subBuilder = dateStatistics_.toBuilder();
+              }
+              dateStatistics_ = input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.PARSER, extensionRegistry);
+              if (subBuilder != null) {
+                subBuilder.mergeFrom(dateStatistics_);
+                dateStatistics_ = subBuilder.buildPartial();
+              }
+              bitField0_ |= 0x00000040;
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<ColumnStatistics> PARSER =
+        new com.google.protobuf.AbstractParser<ColumnStatistics>() {
+      public ColumnStatistics parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new ColumnStatistics(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<ColumnStatistics> getParserForType() {
+      return PARSER;
     }
-    
+
     private int bitField0_;
     // optional uint64 numberOfValues = 1;
     public static final int NUMBEROFVALUES_FIELD_NUMBER = 1;
     private long numberOfValues_;
+    /**
+     * <code>optional uint64 numberOfValues = 1;</code>
+     */
     public boolean hasNumberOfValues() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
+    /**
+     * <code>optional uint64 numberOfValues = 1;</code>
+     */
     public long getNumberOfValues() {
       return numberOfValues_;
     }
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;
     public static final int INTSTATISTICS_FIELD_NUMBER = 2;
     private org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics intStatistics_;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+     */
     public boolean hasIntStatistics() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics getIntStatistics() {
       return intStatistics_;
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatisticsOrBuilder getIntStatisticsOrBuilder() {
       return intStatistics_;
     }
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;
     public static final int DOUBLESTATISTICS_FIELD_NUMBER = 3;
     private org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics doubleStatistics_;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+     */
     public boolean hasDoubleStatistics() {
       return ((bitField0_ & 0x00000004) == 0x00000004);
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics getDoubleStatistics() {
       return doubleStatistics_;
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatisticsOrBuilder getDoubleStatisticsOrBuilder() {
       return doubleStatistics_;
     }
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;
     public static final int STRINGSTATISTICS_FIELD_NUMBER = 4;
     private org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics stringStatistics_;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+     */
     public boolean hasStringStatistics() {
       return ((bitField0_ & 0x00000008) == 0x00000008);
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics getStringStatistics() {
       return stringStatistics_;
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatisticsOrBuilder getStringStatisticsOrBuilder() {
       return stringStatistics_;
     }
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;
     public static final int BUCKETSTATISTICS_FIELD_NUMBER = 5;
     private org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics bucketStatistics_;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+     */
     public boolean hasBucketStatistics() {
       return ((bitField0_ & 0x00000010) == 0x00000010);
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics getBucketStatistics() {
       return bucketStatistics_;
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatisticsOrBuilder getBucketStatisticsOrBuilder() {
       return bucketStatistics_;
     }
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;
     public static final int DECIMALSTATISTICS_FIELD_NUMBER = 6;
     private org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics decimalStatistics_;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+     */
     public boolean hasDecimalStatistics() {
       return ((bitField0_ & 0x00000020) == 0x00000020);
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics getDecimalStatistics() {
       return decimalStatistics_;
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatisticsOrBuilder getDecimalStatisticsOrBuilder() {
       return decimalStatistics_;
     }
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;
     public static final int DATESTATISTICS_FIELD_NUMBER = 7;
     private org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics dateStatistics_;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+     */
     public boolean hasDateStatistics() {
       return ((bitField0_ & 0x00000040) == 0x00000040);
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics getDateStatistics() {
       return dateStatistics_;
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatisticsOrBuilder getDateStatisticsOrBuilder() {
       return dateStatistics_;
     }
-    
+
     private void initFields() {
       numberOfValues_ = 0L;
       intStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getDefaultInstance();
@@ -2977,11 +4054,11 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -3008,12 +4085,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -3047,94 +4124,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.ColumnStatistics}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder {
@@ -3142,18 +4208,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -3170,7 +4239,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         numberOfValues_ = 0L;
@@ -3213,20 +4282,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000040);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics result = buildPartial();
         if (!result.isInitialized()) {
@@ -3234,17 +4303,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics(this);
         int from_bitField0_ = bitField0_;
@@ -3305,7 +4364,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics)other);
@@ -3314,7 +4373,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDefaultInstance()) return this;
         if (other.hasNumberOfValues()) {
@@ -3341,127 +4400,76 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatist
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              numberOfValues_ = input.readUInt64();
-              break;
-            }
-            case 18: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.newBuilder();
-              if (hasIntStatistics()) {
-                subBuilder.mergeFrom(getIntStatistics());
-              }
-              input.readMessage(subBuilder, extensionRegistry);
-              setIntStatistics(subBuilder.buildPartial());
-              break;
-            }
-            case 26: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.newBuilder();
-              if (hasDoubleStatistics()) {
-                subBuilder.mergeFrom(getDoubleStatistics());
-              }
-              input.readMessage(subBuilder, extensionRegistry);
-              setDoubleStatistics(subBuilder.buildPartial());
-              break;
-            }
-            case 34: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.newBuilder();
-              if (hasStringStatistics()) {
-                subBuilder.mergeFrom(getStringStatistics());
-              }
-              input.readMessage(subBuilder, extensionRegistry);
-              setStringStatistics(subBuilder.buildPartial());
-              break;
-            }
-            case 42: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.newBuilder();
-              if (hasBucketStatistics()) {
-                subBuilder.mergeFrom(getBucketStatistics());
-              }
-              input.readMessage(subBuilder, extensionRegistry);
-              setBucketStatistics(subBuilder.buildPartial());
-              break;
-            }
-            case 50: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.newBuilder();
-              if (hasDecimalStatistics()) {
-                subBuilder.mergeFrom(getDecimalStatistics());
-              }
-              input.readMessage(subBuilder, extensionRegistry);
-              setDecimalStatistics(subBuilder.buildPartial());
-              break;
-            }
-            case 58: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.newBuilder();
-              if (hasDateStatistics()) {
-                subBuilder.mergeFrom(getDateStatistics());
-              }
-              input.readMessage(subBuilder, extensionRegistry);
-              setDateStatistics(subBuilder.buildPartial());
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // optional uint64 numberOfValues = 1;
       private long numberOfValues_ ;
+      /**
+       * <code>optional uint64 numberOfValues = 1;</code>
+       */
       public boolean hasNumberOfValues() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
+      /**
+       * <code>optional uint64 numberOfValues = 1;</code>
+       */
       public long getNumberOfValues() {
         return numberOfValues_;
       }
+      /**
+       * <code>optional uint64 numberOfValues = 1;</code>
+       */
       public Builder setNumberOfValues(long value) {
         bitField0_ |= 0x00000001;
         numberOfValues_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 numberOfValues = 1;</code>
+       */
       public Builder clearNumberOfValues() {
         bitField0_ = (bitField0_ & ~0x00000001);
         numberOfValues_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;
       private org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics intStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatisticsOrBuilder> intStatisticsBuilder_;
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+       */
       public boolean hasIntStatistics() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics getIntStatistics() {
         if (intStatisticsBuilder_ == null) {
           return intStatistics_;
@@ -3469,6 +4477,9 @@ public boolean hasIntStatistics() {
           return intStatisticsBuilder_.getMessage();
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+       */
       public Builder setIntStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics value) {
         if (intStatisticsBuilder_ == null) {
           if (value == null) {
@@ -3482,6 +4493,9 @@ public Builder setIntStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.Intege
         bitField0_ |= 0x00000002;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+       */
       public Builder setIntStatistics(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder builderForValue) {
         if (intStatisticsBuilder_ == null) {
@@ -3493,6 +4507,9 @@ public Builder setIntStatistics(
         bitField0_ |= 0x00000002;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+       */
       public Builder mergeIntStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics value) {
         if (intStatisticsBuilder_ == null) {
           if (((bitField0_ & 0x00000002) == 0x00000002) &&
@@ -3509,6 +4526,9 @@ public Builder mergeIntStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.Inte
         bitField0_ |= 0x00000002;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+       */
       public Builder clearIntStatistics() {
         if (intStatisticsBuilder_ == null) {
           intStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getDefaultInstance();
@@ -3519,11 +4539,17 @@ public Builder clearIntStatistics() {
         bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder getIntStatisticsBuilder() {
         bitField0_ |= 0x00000002;
         onChanged();
         return getIntStatisticsFieldBuilder().getBuilder();
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatisticsOrBuilder getIntStatisticsOrBuilder() {
         if (intStatisticsBuilder_ != null) {
           return intStatisticsBuilder_.getMessageOrBuilder();
@@ -3531,6 +4557,9 @@ public Builder clearIntStatistics() {
           return intStatistics_;
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.IntegerStatistics intStatistics = 2;</code>
+       */
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatisticsOrBuilder> 
           getIntStatisticsFieldBuilder() {
@@ -3544,14 +4573,20 @@ public Builder clearIntStatistics() {
         }
         return intStatisticsBuilder_;
       }
-      
+
       // optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;
       private org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics doubleStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatisticsOrBuilder> doubleStatisticsBuilder_;
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+       */
       public boolean hasDoubleStatistics() {
         return ((bitField0_ & 0x00000004) == 0x00000004);
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics getDoubleStatistics() {
         if (doubleStatisticsBuilder_ == null) {
           return doubleStatistics_;
@@ -3559,6 +4594,9 @@ public boolean hasDoubleStatistics() {
           return doubleStatisticsBuilder_.getMessage();
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+       */
       public Builder setDoubleStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics value) {
         if (doubleStatisticsBuilder_ == null) {
           if (value == null) {
@@ -3572,6 +4610,9 @@ public Builder setDoubleStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.Dou
         bitField0_ |= 0x00000004;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+       */
       public Builder setDoubleStatistics(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder builderForValue) {
         if (doubleStatisticsBuilder_ == null) {
@@ -3583,6 +4624,9 @@ public Builder setDoubleStatistics(
         bitField0_ |= 0x00000004;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+       */
       public Builder mergeDoubleStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics value) {
         if (doubleStatisticsBuilder_ == null) {
           if (((bitField0_ & 0x00000004) == 0x00000004) &&
@@ -3599,6 +4643,9 @@ public Builder mergeDoubleStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.D
         bitField0_ |= 0x00000004;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+       */
       public Builder clearDoubleStatistics() {
         if (doubleStatisticsBuilder_ == null) {
           doubleStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getDefaultInstance();
@@ -3609,11 +4656,17 @@ public Builder clearDoubleStatistics() {
         bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder getDoubleStatisticsBuilder() {
         bitField0_ |= 0x00000004;
         onChanged();
         return getDoubleStatisticsFieldBuilder().getBuilder();
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatisticsOrBuilder getDoubleStatisticsOrBuilder() {
         if (doubleStatisticsBuilder_ != null) {
           return doubleStatisticsBuilder_.getMessageOrBuilder();
@@ -3621,6 +4674,9 @@ public Builder clearDoubleStatistics() {
           return doubleStatistics_;
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DoubleStatistics doubleStatistics = 3;</code>
+       */
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatisticsOrBuilder> 
           getDoubleStatisticsFieldBuilder() {
@@ -3634,14 +4690,20 @@ public Builder clearDoubleStatistics() {
         }
         return doubleStatisticsBuilder_;
       }
-      
+
       // optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;
       private org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics stringStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatisticsOrBuilder> stringStatisticsBuilder_;
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+       */
       public boolean hasStringStatistics() {
         return ((bitField0_ & 0x00000008) == 0x00000008);
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics getStringStatistics() {
         if (stringStatisticsBuilder_ == null) {
           return stringStatistics_;
@@ -3649,6 +4711,9 @@ public boolean hasStringStatistics() {
           return stringStatisticsBuilder_.getMessage();
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+       */
       public Builder setStringStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics value) {
         if (stringStatisticsBuilder_ == null) {
           if (value == null) {
@@ -3662,6 +4727,9 @@ public Builder setStringStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.Str
         bitField0_ |= 0x00000008;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+       */
       public Builder setStringStatistics(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder builderForValue) {
         if (stringStatisticsBuilder_ == null) {
@@ -3673,6 +4741,9 @@ public Builder setStringStatistics(
         bitField0_ |= 0x00000008;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+       */
       public Builder mergeStringStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics value) {
         if (stringStatisticsBuilder_ == null) {
           if (((bitField0_ & 0x00000008) == 0x00000008) &&
@@ -3689,6 +4760,9 @@ public Builder mergeStringStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.S
         bitField0_ |= 0x00000008;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+       */
       public Builder clearStringStatistics() {
         if (stringStatisticsBuilder_ == null) {
           stringStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getDefaultInstance();
@@ -3699,11 +4773,17 @@ public Builder clearStringStatistics() {
         bitField0_ = (bitField0_ & ~0x00000008);
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder getStringStatisticsBuilder() {
         bitField0_ |= 0x00000008;
         onChanged();
         return getStringStatisticsFieldBuilder().getBuilder();
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatisticsOrBuilder getStringStatisticsOrBuilder() {
         if (stringStatisticsBuilder_ != null) {
           return stringStatisticsBuilder_.getMessageOrBuilder();
@@ -3711,6 +4791,9 @@ public Builder clearStringStatistics() {
           return stringStatistics_;
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.StringStatistics stringStatistics = 4;</code>
+       */
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatisticsOrBuilder> 
           getStringStatisticsFieldBuilder() {
@@ -3724,14 +4807,20 @@ public Builder clearStringStatistics() {
         }
         return stringStatisticsBuilder_;
       }
-      
+
       // optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;
       private org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics bucketStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatisticsOrBuilder> bucketStatisticsBuilder_;
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+       */
       public boolean hasBucketStatistics() {
         return ((bitField0_ & 0x00000010) == 0x00000010);
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics getBucketStatistics() {
         if (bucketStatisticsBuilder_ == null) {
           return bucketStatistics_;
@@ -3739,6 +4828,9 @@ public boolean hasBucketStatistics() {
           return bucketStatisticsBuilder_.getMessage();
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+       */
       public Builder setBucketStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics value) {
         if (bucketStatisticsBuilder_ == null) {
           if (value == null) {
@@ -3752,6 +4844,9 @@ public Builder setBucketStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.Buc
         bitField0_ |= 0x00000010;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+       */
       public Builder setBucketStatistics(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder builderForValue) {
         if (bucketStatisticsBuilder_ == null) {
@@ -3763,6 +4858,9 @@ public Builder setBucketStatistics(
         bitField0_ |= 0x00000010;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+       */
       public Builder mergeBucketStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics value) {
         if (bucketStatisticsBuilder_ == null) {
           if (((bitField0_ & 0x00000010) == 0x00000010) &&
@@ -3779,6 +4877,9 @@ public Builder mergeBucketStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.B
         bitField0_ |= 0x00000010;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+       */
       public Builder clearBucketStatistics() {
         if (bucketStatisticsBuilder_ == null) {
           bucketStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getDefaultInstance();
@@ -3789,11 +4890,17 @@ public Builder clearBucketStatistics() {
         bitField0_ = (bitField0_ & ~0x00000010);
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder getBucketStatisticsBuilder() {
         bitField0_ |= 0x00000010;
         onChanged();
         return getBucketStatisticsFieldBuilder().getBuilder();
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatisticsOrBuilder getBucketStatisticsOrBuilder() {
         if (bucketStatisticsBuilder_ != null) {
           return bucketStatisticsBuilder_.getMessageOrBuilder();
@@ -3801,6 +4908,9 @@ public Builder clearBucketStatistics() {
           return bucketStatistics_;
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.BucketStatistics bucketStatistics = 5;</code>
+       */
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatisticsOrBuilder> 
           getBucketStatisticsFieldBuilder() {
@@ -3814,14 +4924,20 @@ public Builder clearBucketStatistics() {
         }
         return bucketStatisticsBuilder_;
       }
-      
+
       // optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;
       private org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics decimalStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatisticsOrBuilder> decimalStatisticsBuilder_;
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+       */
       public boolean hasDecimalStatistics() {
         return ((bitField0_ & 0x00000020) == 0x00000020);
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics getDecimalStatistics() {
         if (decimalStatisticsBuilder_ == null) {
           return decimalStatistics_;
@@ -3829,6 +4945,9 @@ public boolean hasDecimalStatistics() {
           return decimalStatisticsBuilder_.getMessage();
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+       */
       public Builder setDecimalStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics value) {
         if (decimalStatisticsBuilder_ == null) {
           if (value == null) {
@@ -3842,6 +4961,9 @@ public Builder setDecimalStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.De
         bitField0_ |= 0x00000020;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+       */
       public Builder setDecimalStatistics(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder builderForValue) {
         if (decimalStatisticsBuilder_ == null) {
@@ -3853,6 +4975,9 @@ public Builder setDecimalStatistics(
         bitField0_ |= 0x00000020;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+       */
       public Builder mergeDecimalStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics value) {
         if (decimalStatisticsBuilder_ == null) {
           if (((bitField0_ & 0x00000020) == 0x00000020) &&
@@ -3869,6 +4994,9 @@ public Builder mergeDecimalStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.
         bitField0_ |= 0x00000020;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+       */
       public Builder clearDecimalStatistics() {
         if (decimalStatisticsBuilder_ == null) {
           decimalStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getDefaultInstance();
@@ -3879,11 +5007,17 @@ public Builder clearDecimalStatistics() {
         bitField0_ = (bitField0_ & ~0x00000020);
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder getDecimalStatisticsBuilder() {
         bitField0_ |= 0x00000020;
         onChanged();
         return getDecimalStatisticsFieldBuilder().getBuilder();
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatisticsOrBuilder getDecimalStatisticsOrBuilder() {
         if (decimalStatisticsBuilder_ != null) {
           return decimalStatisticsBuilder_.getMessageOrBuilder();
@@ -3891,6 +5025,9 @@ public Builder clearDecimalStatistics() {
           return decimalStatistics_;
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DecimalStatistics decimalStatistics = 6;</code>
+       */
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatisticsOrBuilder> 
           getDecimalStatisticsFieldBuilder() {
@@ -3904,14 +5041,20 @@ public Builder clearDecimalStatistics() {
         }
         return decimalStatisticsBuilder_;
       }
-      
+
       // optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;
       private org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics dateStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatisticsOrBuilder> dateStatisticsBuilder_;
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+       */
       public boolean hasDateStatistics() {
         return ((bitField0_ & 0x00000040) == 0x00000040);
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics getDateStatistics() {
         if (dateStatisticsBuilder_ == null) {
           return dateStatistics_;
@@ -3919,6 +5062,9 @@ public boolean hasDateStatistics() {
           return dateStatisticsBuilder_.getMessage();
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+       */
       public Builder setDateStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics value) {
         if (dateStatisticsBuilder_ == null) {
           if (value == null) {
@@ -3932,6 +5078,9 @@ public Builder setDateStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.DateS
         bitField0_ |= 0x00000040;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+       */
       public Builder setDateStatistics(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder builderForValue) {
         if (dateStatisticsBuilder_ == null) {
@@ -3943,6 +5092,9 @@ public Builder setDateStatistics(
         bitField0_ |= 0x00000040;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+       */
       public Builder mergeDateStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics value) {
         if (dateStatisticsBuilder_ == null) {
           if (((bitField0_ & 0x00000040) == 0x00000040) &&
@@ -3959,6 +5111,9 @@ public Builder mergeDateStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.Dat
         bitField0_ |= 0x00000040;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+       */
       public Builder clearDateStatistics() {
         if (dateStatisticsBuilder_ == null) {
           dateStatistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getDefaultInstance();
@@ -3969,11 +5124,17 @@ public Builder clearDateStatistics() {
         bitField0_ = (bitField0_ & ~0x00000040);
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder getDateStatisticsBuilder() {
         bitField0_ |= 0x00000040;
         onChanged();
         return getDateStatisticsFieldBuilder().getBuilder();
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatisticsOrBuilder getDateStatisticsOrBuilder() {
         if (dateStatisticsBuilder_ != null) {
           return dateStatisticsBuilder_.getMessageOrBuilder();
@@ -3981,6 +5142,9 @@ public Builder clearDateStatistics() {
           return dateStatistics_;
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.DateStatistics dateStatistics = 7;</code>
+       */
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatisticsOrBuilder> 
           getDateStatisticsFieldBuilder() {
@@ -3994,101 +5158,236 @@ public Builder clearDateStatistics() {
         }
         return dateStatisticsBuilder_;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.ColumnStatistics)
     }
-    
+
     static {
       defaultInstance = new ColumnStatistics(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.ColumnStatistics)
   }
-  
+
   public interface RowIndexEntryOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // repeated uint64 positions = 1 [packed = true];
+    /**
+     * <code>repeated uint64 positions = 1 [packed = true];</code>
+     */
     java.util.List<java.lang.Long> getPositionsList();
+    /**
+     * <code>repeated uint64 positions = 1 [packed = true];</code>
+     */
     int getPositionsCount();
+    /**
+     * <code>repeated uint64 positions = 1 [packed = true];</code>
+     */
     long getPositions(int index);
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+     */
     boolean hasStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics getStatistics();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.RowIndexEntry}
+   */
   public static final class RowIndexEntry extends
       com.google.protobuf.GeneratedMessage
       implements RowIndexEntryOrBuilder {
     // Use RowIndexEntry.newBuilder() to construct.
-    private RowIndexEntry(Builder builder) {
+    private RowIndexEntry(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private RowIndexEntry(boolean noInit) {}
-    
+    private RowIndexEntry(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final RowIndexEntry defaultInstance;
     public static RowIndexEntry getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public RowIndexEntry getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private RowIndexEntry(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
+                positions_ = new java.util.ArrayList<java.lang.Long>();
+                mutable_bitField0_ |= 0x00000001;
+              }
+              positions_.add(input.readUInt64());
+              break;
+            }
+            case 10: {
+              int length = input.readRawVarint32();
+              int limit = input.pushLimit(length);
+              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001) && input.getBytesUntilLimit() > 0) {
+                positions_ = new java.util.ArrayList<java.lang.Long>();
+                mutable_bitField0_ |= 0x00000001;
+              }
+              while (input.getBytesUntilLimit() > 0) {
+                positions_.add(input.readUInt64());
+              }
+              input.popLimit(limit);
+              break;
+            }
+            case 18: {
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder subBuilder = null;
+              if (((bitField0_ & 0x00000001) == 0x00000001)) {
+                subBuilder = statistics_.toBuilder();
+              }
+              statistics_ = input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.PARSER, extensionRegistry);
+              if (subBuilder != null) {
+                subBuilder.mergeFrom(statistics_);
+                statistics_ = subBuilder.buildPartial();
+              }
+              bitField0_ |= 0x00000001;
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
+          positions_ = java.util.Collections.unmodifiableList(positions_);
+        }
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<RowIndexEntry> PARSER =
+        new com.google.protobuf.AbstractParser<RowIndexEntry>() {
+      public RowIndexEntry parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new RowIndexEntry(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<RowIndexEntry> getParserForType() {
+      return PARSER;
     }
-    
+
     private int bitField0_;
     // repeated uint64 positions = 1 [packed = true];
     public static final int POSITIONS_FIELD_NUMBER = 1;
     private java.util.List<java.lang.Long> positions_;
+    /**
+     * <code>repeated uint64 positions = 1 [packed = true];</code>
+     */
     public java.util.List<java.lang.Long>
         getPositionsList() {
       return positions_;
     }
+    /**
+     * <code>repeated uint64 positions = 1 [packed = true];</code>
+     */
     public int getPositionsCount() {
       return positions_.size();
     }
+    /**
+     * <code>repeated uint64 positions = 1 [packed = true];</code>
+     */
     public long getPositions(int index) {
       return positions_.get(index);
     }
     private int positionsMemoizedSerializedSize = -1;
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;
     public static final int STATISTICS_FIELD_NUMBER = 2;
     private org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics statistics_;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+     */
     public boolean hasStatistics() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics getStatistics() {
       return statistics_;
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder() {
       return statistics_;
     }
-    
+
     private void initFields() {
-      positions_ = java.util.Collections.emptyList();;
+      positions_ = java.util.Collections.emptyList();
       statistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDefaultInstance();
     }
     private byte memoizedIsInitialized = -1;
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -4104,12 +5403,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       {
         int dataSize = 0;
@@ -4133,94 +5432,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.RowIndexEntry}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntryOrBuilder {
@@ -4228,18 +5516,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -4251,10 +5542,10 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
-        positions_ = java.util.Collections.emptyList();;
+        positions_ = java.util.Collections.emptyList();
         bitField0_ = (bitField0_ & ~0x00000001);
         if (statisticsBuilder_ == null) {
           statistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDefaultInstance();
@@ -4264,20 +5555,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry result = buildPartial();
         if (!result.isInitialized()) {
@@ -4285,17 +5576,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry(this);
         int from_bitField0_ = bitField0_;
@@ -4317,7 +5598,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry)other);
@@ -4326,7 +5607,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getDefaultInstance()) return this;
         if (!other.positions_.isEmpty()) {
@@ -4345,81 +5626,60 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              ensurePositionsIsMutable();
-              positions_.add(input.readUInt64());
-              break;
-            }
-            case 10: {
-              int length = input.readRawVarint32();
-              int limit = input.pushLimit(length);
-              while (input.getBytesUntilLimit() > 0) {
-                addPositions(input.readUInt64());
-              }
-              input.popLimit(limit);
-              break;
-            }
-            case 18: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.newBuilder();
-              if (hasStatistics()) {
-                subBuilder.mergeFrom(getStatistics());
-              }
-              input.readMessage(subBuilder, extensionRegistry);
-              setStatistics(subBuilder.buildPartial());
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // repeated uint64 positions = 1 [packed = true];
-      private java.util.List<java.lang.Long> positions_ = java.util.Collections.emptyList();;
+      private java.util.List<java.lang.Long> positions_ = java.util.Collections.emptyList();
       private void ensurePositionsIsMutable() {
         if (!((bitField0_ & 0x00000001) == 0x00000001)) {
           positions_ = new java.util.ArrayList<java.lang.Long>(positions_);
           bitField0_ |= 0x00000001;
          }
       }
+      /**
+       * <code>repeated uint64 positions = 1 [packed = true];</code>
+       */
       public java.util.List<java.lang.Long>
           getPositionsList() {
         return java.util.Collections.unmodifiableList(positions_);
       }
+      /**
+       * <code>repeated uint64 positions = 1 [packed = true];</code>
+       */
       public int getPositionsCount() {
         return positions_.size();
       }
+      /**
+       * <code>repeated uint64 positions = 1 [packed = true];</code>
+       */
       public long getPositions(int index) {
         return positions_.get(index);
       }
+      /**
+       * <code>repeated uint64 positions = 1 [packed = true];</code>
+       */
       public Builder setPositions(
           int index, long value) {
         ensurePositionsIsMutable();
@@ -4427,12 +5687,18 @@ public Builder setPositions(
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint64 positions = 1 [packed = true];</code>
+       */
       public Builder addPositions(long value) {
         ensurePositionsIsMutable();
         positions_.add(value);
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint64 positions = 1 [packed = true];</code>
+       */
       public Builder addAllPositions(
           java.lang.Iterable<? extends java.lang.Long> values) {
         ensurePositionsIsMutable();
@@ -4440,20 +5706,29 @@ public Builder addAllPositions(
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint64 positions = 1 [packed = true];</code>
+       */
       public Builder clearPositions() {
-        positions_ = java.util.Collections.emptyList();;
+        positions_ = java.util.Collections.emptyList();
         bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
-      
+
       // optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;
       private org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics statistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder> statisticsBuilder_;
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+       */
       public boolean hasStatistics() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics getStatistics() {
         if (statisticsBuilder_ == null) {
           return statistics_;
@@ -4461,6 +5736,9 @@ public boolean hasStatistics() {
           return statisticsBuilder_.getMessage();
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+       */
       public Builder setStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics value) {
         if (statisticsBuilder_ == null) {
           if (value == null) {
@@ -4474,6 +5752,9 @@ public Builder setStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnSta
         bitField0_ |= 0x00000002;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+       */
       public Builder setStatistics(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder builderForValue) {
         if (statisticsBuilder_ == null) {
@@ -4485,6 +5766,9 @@ public Builder setStatistics(
         bitField0_ |= 0x00000002;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+       */
       public Builder mergeStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics value) {
         if (statisticsBuilder_ == null) {
           if (((bitField0_ & 0x00000002) == 0x00000002) &&
@@ -4501,6 +5785,9 @@ public Builder mergeStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnS
         bitField0_ |= 0x00000002;
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+       */
       public Builder clearStatistics() {
         if (statisticsBuilder_ == null) {
           statistics_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDefaultInstance();
@@ -4511,11 +5798,17 @@ public Builder clearStatistics() {
         bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder getStatisticsBuilder() {
         bitField0_ |= 0x00000002;
         onChanged();
         return getStatisticsFieldBuilder().getBuilder();
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder() {
         if (statisticsBuilder_ != null) {
           return statisticsBuilder_.getMessageOrBuilder();
@@ -4523,6 +5816,9 @@ public Builder clearStatistics() {
           return statistics_;
         }
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 2;</code>
+       */
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder> 
           getStatisticsFieldBuilder() {
@@ -4536,80 +5832,183 @@ public Builder clearStatistics() {
         }
         return statisticsBuilder_;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.RowIndexEntry)
     }
-    
+
     static {
       defaultInstance = new RowIndexEntry(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.RowIndexEntry)
   }
-  
+
   public interface RowIndexOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+     */
     java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry> 
         getEntryList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry getEntry(int index);
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+     */
     int getEntryCount();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+     */
     java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntryOrBuilder> 
         getEntryOrBuilderList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntryOrBuilder getEntryOrBuilder(
         int index);
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.RowIndex}
+   */
   public static final class RowIndex extends
       com.google.protobuf.GeneratedMessage
       implements RowIndexOrBuilder {
     // Use RowIndex.newBuilder() to construct.
-    private RowIndex(Builder builder) {
+    private RowIndex(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private RowIndex(boolean noInit) {}
-    
+    private RowIndex(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final RowIndex defaultInstance;
     public static RowIndex getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public RowIndex getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private RowIndex(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 10: {
+              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
+                entry_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry>();
+                mutable_bitField0_ |= 0x00000001;
+              }
+              entry_.add(input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.PARSER, extensionRegistry));
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
+          entry_ = java.util.Collections.unmodifiableList(entry_);
+        }
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<RowIndex> PARSER =
+        new com.google.protobuf.AbstractParser<RowIndex>() {
+      public RowIndex parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new RowIndex(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<RowIndex> getParserForType() {
+      return PARSER;
     }
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;
     public static final int ENTRY_FIELD_NUMBER = 1;
     private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry> entry_;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+     */
     public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry> getEntryList() {
       return entry_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+     */
     public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntryOrBuilder> 
         getEntryOrBuilderList() {
       return entry_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+     */
     public int getEntryCount() {
       return entry_.size();
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry getEntry(int index) {
       return entry_.get(index);
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntryOrBuilder getEntryOrBuilder(
         int index) {
       return entry_.get(index);
     }
-    
+
     private void initFields() {
       entry_ = java.util.Collections.emptyList();
     }
@@ -4617,11 +6016,11 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -4630,12 +6029,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       for (int i = 0; i < entry_.size(); i++) {
         size += com.google.protobuf.CodedOutputStream
@@ -4645,94 +6044,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.RowIndex}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexOrBuilder {
@@ -4740,18 +6128,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -4763,7 +6154,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         if (entryBuilder_ == null) {
@@ -4774,20 +6165,20 @@ public Builder clear() {
         }
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex result = buildPartial();
         if (!result.isInitialized()) {
@@ -4795,17 +6186,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex(this);
         int from_bitField0_ = bitField0_;
@@ -4821,7 +6202,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex)other);
@@ -4830,7 +6211,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getDefaultInstance()) return this;
         if (entryBuilder_ == null) {
@@ -4862,46 +6243,30 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex othe
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.newBuilder();
-              input.readMessage(subBuilder, extensionRegistry);
-              addEntry(subBuilder.buildPartial());
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;
       private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry> entry_ =
         java.util.Collections.emptyList();
@@ -4911,10 +6276,13 @@ private void ensureEntryIsMutable() {
           bitField0_ |= 0x00000001;
          }
       }
-      
+
       private com.google.protobuf.RepeatedFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry, org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntryOrBuilder> entryBuilder_;
-      
+
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry> getEntryList() {
         if (entryBuilder_ == null) {
           return java.util.Collections.unmodifiableList(entry_);
@@ -4922,6 +6290,9 @@ private void ensureEntryIsMutable() {
           return entryBuilder_.getMessageList();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public int getEntryCount() {
         if (entryBuilder_ == null) {
           return entry_.size();
@@ -4929,6 +6300,9 @@ public int getEntryCount() {
           return entryBuilder_.getCount();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry getEntry(int index) {
         if (entryBuilder_ == null) {
           return entry_.get(index);
@@ -4936,6 +6310,9 @@ public int getEntryCount() {
           return entryBuilder_.getMessage(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public Builder setEntry(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry value) {
         if (entryBuilder_ == null) {
@@ -4950,6 +6327,9 @@ public Builder setEntry(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public Builder setEntry(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder builderForValue) {
         if (entryBuilder_ == null) {
@@ -4961,6 +6341,9 @@ public Builder setEntry(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public Builder addEntry(org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry value) {
         if (entryBuilder_ == null) {
           if (value == null) {
@@ -4974,6 +6357,9 @@ public Builder addEntry(org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry 
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public Builder addEntry(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry value) {
         if (entryBuilder_ == null) {
@@ -4988,6 +6374,9 @@ public Builder addEntry(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public Builder addEntry(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder builderForValue) {
         if (entryBuilder_ == null) {
@@ -4999,6 +6388,9 @@ public Builder addEntry(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public Builder addEntry(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder builderForValue) {
         if (entryBuilder_ == null) {
@@ -5010,6 +6402,9 @@ public Builder addEntry(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public Builder addAllEntry(
           java.lang.Iterable<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry> values) {
         if (entryBuilder_ == null) {
@@ -5021,6 +6416,9 @@ public Builder addAllEntry(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public Builder clearEntry() {
         if (entryBuilder_ == null) {
           entry_ = java.util.Collections.emptyList();
@@ -5031,6 +6429,9 @@ public Builder clearEntry() {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public Builder removeEntry(int index) {
         if (entryBuilder_ == null) {
           ensureEntryIsMutable();
@@ -5041,10 +6442,16 @@ public Builder removeEntry(int index) {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder getEntryBuilder(
           int index) {
         return getEntryFieldBuilder().getBuilder(index);
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntryOrBuilder getEntryOrBuilder(
           int index) {
         if (entryBuilder_ == null) {
@@ -5052,6 +6459,9 @@ public Builder removeEntry(int index) {
           return entryBuilder_.getMessageOrBuilder(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntryOrBuilder> 
            getEntryOrBuilderList() {
         if (entryBuilder_ != null) {
@@ -5060,15 +6470,24 @@ public Builder removeEntry(int index) {
           return java.util.Collections.unmodifiableList(entry_);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder addEntryBuilder() {
         return getEntryFieldBuilder().addBuilder(
             org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder addEntryBuilder(
           int index) {
         return getEntryFieldBuilder().addBuilder(
             index, org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.RowIndexEntry entry = 1;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder> 
            getEntryBuilderList() {
         return getEntryFieldBuilder().getBuilderList();
@@ -5087,83 +6506,234 @@ public Builder removeEntry(int index) {
         }
         return entryBuilder_;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.RowIndex)
     }
-    
+
     static {
       defaultInstance = new RowIndex(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.RowIndex)
   }
-  
+
   public interface StreamOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
+     */
     boolean hasKind();
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind getKind();
-    
+
     // optional uint32 column = 2;
+    /**
+     * <code>optional uint32 column = 2;</code>
+     */
     boolean hasColumn();
+    /**
+     * <code>optional uint32 column = 2;</code>
+     */
     int getColumn();
-    
+
     // optional uint64 length = 3;
+    /**
+     * <code>optional uint64 length = 3;</code>
+     */
     boolean hasLength();
+    /**
+     * <code>optional uint64 length = 3;</code>
+     */
     long getLength();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Stream}
+   */
   public static final class Stream extends
       com.google.protobuf.GeneratedMessage
       implements StreamOrBuilder {
     // Use Stream.newBuilder() to construct.
-    private Stream(Builder builder) {
+    private Stream(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private Stream(boolean noInit) {}
-    
+    private Stream(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final Stream defaultInstance;
     public static Stream getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public Stream getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private Stream(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              int rawValue = input.readEnum();
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind value = org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.valueOf(rawValue);
+              if (value == null) {
+                unknownFields.mergeVarintField(1, rawValue);
+              } else {
+                bitField0_ |= 0x00000001;
+                kind_ = value;
+              }
+              break;
+            }
+            case 16: {
+              bitField0_ |= 0x00000002;
+              column_ = input.readUInt32();
+              break;
+            }
+            case 24: {
+              bitField0_ |= 0x00000004;
+              length_ = input.readUInt64();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<Stream> PARSER =
+        new com.google.protobuf.AbstractParser<Stream>() {
+      public Stream parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new Stream(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<Stream> getParserForType() {
+      return PARSER;
     }
-    
+
+    /**
+     * Protobuf enum {@code org.apache.hadoop.hive.ql.io.orc.Stream.Kind}
+     *
+     * <pre>
+     * if you add new index stream kinds, you need to make sure to update
+     * StreamName to ensure it is added to the stripe in the right area
+     * </pre>
+     */
     public enum Kind
         implements com.google.protobuf.ProtocolMessageEnum {
+      /**
+       * <code>PRESENT = 0;</code>
+       */
       PRESENT(0, 0),
+      /**
+       * <code>DATA = 1;</code>
+       */
       DATA(1, 1),
+      /**
+       * <code>LENGTH = 2;</code>
+       */
       LENGTH(2, 2),
+      /**
+       * <code>DICTIONARY_DATA = 3;</code>
+       */
       DICTIONARY_DATA(3, 3),
+      /**
+       * <code>DICTIONARY_COUNT = 4;</code>
+       */
       DICTIONARY_COUNT(4, 4),
+      /**
+       * <code>SECONDARY = 5;</code>
+       */
       SECONDARY(5, 5),
+      /**
+       * <code>ROW_INDEX = 6;</code>
+       */
       ROW_INDEX(6, 6),
       ;
-      
+
+      /**
+       * <code>PRESENT = 0;</code>
+       */
       public static final int PRESENT_VALUE = 0;
+      /**
+       * <code>DATA = 1;</code>
+       */
       public static final int DATA_VALUE = 1;
+      /**
+       * <code>LENGTH = 2;</code>
+       */
       public static final int LENGTH_VALUE = 2;
+      /**
+       * <code>DICTIONARY_DATA = 3;</code>
+       */
       public static final int DICTIONARY_DATA_VALUE = 3;
+      /**
+       * <code>DICTIONARY_COUNT = 4;</code>
+       */
       public static final int DICTIONARY_COUNT_VALUE = 4;
+      /**
+       * <code>SECONDARY = 5;</code>
+       */
       public static final int SECONDARY_VALUE = 5;
+      /**
+       * <code>ROW_INDEX = 6;</code>
+       */
       public static final int ROW_INDEX_VALUE = 6;
-      
-      
+
+
       public final int getNumber() { return value; }
-      
+
       public static Kind valueOf(int value) {
         switch (value) {
           case 0: return PRESENT;
@@ -5176,7 +6746,7 @@ public static Kind valueOf(int value) {
           default: return null;
         }
       }
-      
+
       public static com.google.protobuf.Internal.EnumLiteMap<Kind>
           internalGetValueMap() {
         return internalValueMap;
@@ -5188,7 +6758,7 @@ public Kind findValueByNumber(int number) {
                 return Kind.valueOf(number);
               }
             };
-      
+
       public final com.google.protobuf.Descriptors.EnumValueDescriptor
           getValueDescriptor() {
         return getDescriptor().getValues().get(index);
@@ -5201,11 +6771,9 @@ public Kind findValueByNumber(int number) {
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getDescriptor().getEnumTypes().get(0);
       }
-      
-      private static final Kind[] VALUES = {
-        PRESENT, DATA, LENGTH, DICTIONARY_DATA, DICTIONARY_COUNT, SECONDARY, ROW_INDEX, 
-      };
-      
+
+      private static final Kind[] VALUES = values();
+
       public static Kind valueOf(
           com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
         if (desc.getType() != getDescriptor()) {
@@ -5214,49 +6782,67 @@ public static Kind valueOf(
         }
         return VALUES[desc.getIndex()];
       }
-      
+
       private final int index;
       private final int value;
-      
+
       private Kind(int index, int value) {
         this.index = index;
         this.value = value;
       }
-      
+
       // @@protoc_insertion_point(enum_scope:org.apache.hadoop.hive.ql.io.orc.Stream.Kind)
     }
-    
+
     private int bitField0_;
     // required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;
     public static final int KIND_FIELD_NUMBER = 1;
     private org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind kind_;
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
+     */
     public boolean hasKind() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind getKind() {
       return kind_;
     }
-    
+
     // optional uint32 column = 2;
     public static final int COLUMN_FIELD_NUMBER = 2;
     private int column_;
+    /**
+     * <code>optional uint32 column = 2;</code>
+     */
     public boolean hasColumn() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
+    /**
+     * <code>optional uint32 column = 2;</code>
+     */
     public int getColumn() {
       return column_;
     }
-    
+
     // optional uint64 length = 3;
     public static final int LENGTH_FIELD_NUMBER = 3;
     private long length_;
+    /**
+     * <code>optional uint64 length = 3;</code>
+     */
     public boolean hasLength() {
       return ((bitField0_ & 0x00000004) == 0x00000004);
     }
+    /**
+     * <code>optional uint64 length = 3;</code>
+     */
     public long getLength() {
       return length_;
     }
-    
+
     private void initFields() {
       kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.PRESENT;
       column_ = 0;
@@ -5266,7 +6852,7 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       if (!hasKind()) {
         memoizedIsInitialized = 0;
         return false;
@@ -5274,7 +6860,7 @@ public final boolean isInitialized() {
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -5289,12 +6875,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -5312,94 +6898,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Stream}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.StreamOrBuilder {
@@ -5407,18 +6982,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -5429,7 +7007,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.PRESENT;
@@ -5440,20 +7018,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream result = buildPartial();
         if (!result.isInitialized()) {
@@ -5461,17 +7039,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream(this);
         int from_bitField0_ = bitField0_;
@@ -5492,7 +7060,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream)other);
@@ -5501,7 +7069,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getDefaultInstance()) return this;
         if (other.hasKind()) {
@@ -5516,7 +7084,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream other)
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         if (!hasKind()) {
           
@@ -5524,65 +7092,43 @@ public final boolean isInitialized() {
         }
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              int rawValue = input.readEnum();
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind value = org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.valueOf(rawValue);
-              if (value == null) {
-                unknownFields.mergeVarintField(1, rawValue);
-              } else {
-                bitField0_ |= 0x00000001;
-                kind_ = value;
-              }
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              column_ = input.readUInt32();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              length_ = input.readUInt64();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;
       private org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.PRESENT;
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
+       */
       public boolean hasKind() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind getKind() {
         return kind_;
       }
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
+       */
       public Builder setKind(org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind value) {
         if (value == null) {
           throw new NullPointerException();
@@ -5592,121 +7138,265 @@ public Builder setKind(org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind val
         onChanged();
         return this;
       }
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.Stream.Kind kind = 1;</code>
+       */
       public Builder clearKind() {
         bitField0_ = (bitField0_ & ~0x00000001);
         kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.PRESENT;
         onChanged();
         return this;
       }
-      
+
       // optional uint32 column = 2;
       private int column_ ;
+      /**
+       * <code>optional uint32 column = 2;</code>
+       */
       public boolean hasColumn() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
+      /**
+       * <code>optional uint32 column = 2;</code>
+       */
       public int getColumn() {
         return column_;
       }
+      /**
+       * <code>optional uint32 column = 2;</code>
+       */
       public Builder setColumn(int value) {
         bitField0_ |= 0x00000002;
         column_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint32 column = 2;</code>
+       */
       public Builder clearColumn() {
         bitField0_ = (bitField0_ & ~0x00000002);
         column_ = 0;
         onChanged();
         return this;
       }
-      
+
       // optional uint64 length = 3;
       private long length_ ;
+      /**
+       * <code>optional uint64 length = 3;</code>
+       */
       public boolean hasLength() {
         return ((bitField0_ & 0x00000004) == 0x00000004);
       }
+      /**
+       * <code>optional uint64 length = 3;</code>
+       */
       public long getLength() {
         return length_;
       }
+      /**
+       * <code>optional uint64 length = 3;</code>
+       */
       public Builder setLength(long value) {
         bitField0_ |= 0x00000004;
         length_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 length = 3;</code>
+       */
       public Builder clearLength() {
         bitField0_ = (bitField0_ & ~0x00000004);
         length_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.Stream)
     }
-    
+
     static {
       defaultInstance = new Stream(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.Stream)
   }
-  
+
   public interface ColumnEncodingOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
+     */
     boolean hasKind();
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind getKind();
-    
+
     // optional uint32 dictionarySize = 2;
+    /**
+     * <code>optional uint32 dictionarySize = 2;</code>
+     */
     boolean hasDictionarySize();
+    /**
+     * <code>optional uint32 dictionarySize = 2;</code>
+     */
     int getDictionarySize();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.ColumnEncoding}
+   */
   public static final class ColumnEncoding extends
       com.google.protobuf.GeneratedMessage
       implements ColumnEncodingOrBuilder {
     // Use ColumnEncoding.newBuilder() to construct.
-    private ColumnEncoding(Builder builder) {
+    private ColumnEncoding(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private ColumnEncoding(boolean noInit) {}
-    
+    private ColumnEncoding(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final ColumnEncoding defaultInstance;
     public static ColumnEncoding getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public ColumnEncoding getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private ColumnEncoding(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              int rawValue = input.readEnum();
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind value = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.valueOf(rawValue);
+              if (value == null) {
+                unknownFields.mergeVarintField(1, rawValue);
+              } else {
+                bitField0_ |= 0x00000001;
+                kind_ = value;
+              }
+              break;
+            }
+            case 16: {
+              bitField0_ |= 0x00000002;
+              dictionarySize_ = input.readUInt32();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.class);
     }
-    
+
+    public static com.google.protobuf.Parser<ColumnEncoding> PARSER =
+        new com.google.protobuf.AbstractParser<ColumnEncoding>() {
+      public ColumnEncoding parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new ColumnEncoding(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<ColumnEncoding> getParserForType() {
+      return PARSER;
+    }
+
+    /**
+     * Protobuf enum {@code org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind}
+     */
     public enum Kind
         implements com.google.protobuf.ProtocolMessageEnum {
+      /**
+       * <code>DIRECT = 0;</code>
+       */
       DIRECT(0, 0),
+      /**
+       * <code>DICTIONARY = 1;</code>
+       */
       DICTIONARY(1, 1),
+      /**
+       * <code>DIRECT_V2 = 2;</code>
+       */
       DIRECT_V2(2, 2),
+      /**
+       * <code>DICTIONARY_V2 = 3;</code>
+       */
       DICTIONARY_V2(3, 3),
       ;
-      
+
+      /**
+       * <code>DIRECT = 0;</code>
+       */
       public static final int DIRECT_VALUE = 0;
+      /**
+       * <code>DICTIONARY = 1;</code>
+       */
       public static final int DICTIONARY_VALUE = 1;
+      /**
+       * <code>DIRECT_V2 = 2;</code>
+       */
       public static final int DIRECT_V2_VALUE = 2;
+      /**
+       * <code>DICTIONARY_V2 = 3;</code>
+       */
       public static final int DICTIONARY_V2_VALUE = 3;
-      
-      
+
+
       public final int getNumber() { return value; }
-      
+
       public static Kind valueOf(int value) {
         switch (value) {
           case 0: return DIRECT;
@@ -5716,7 +7406,7 @@ public static Kind valueOf(int value) {
           default: return null;
         }
       }
-      
+
       public static com.google.protobuf.Internal.EnumLiteMap<Kind>
           internalGetValueMap() {
         return internalValueMap;
@@ -5728,7 +7418,7 @@ public Kind findValueByNumber(int number) {
                 return Kind.valueOf(number);
               }
             };
-      
+
       public final com.google.protobuf.Descriptors.EnumValueDescriptor
           getValueDescriptor() {
         return getDescriptor().getValues().get(index);
@@ -5741,11 +7431,9 @@ public Kind findValueByNumber(int number) {
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getDescriptor().getEnumTypes().get(0);
       }
-      
-      private static final Kind[] VALUES = {
-        DIRECT, DICTIONARY, DIRECT_V2, DICTIONARY_V2, 
-      };
-      
+
+      private static final Kind[] VALUES = values();
+
       public static Kind valueOf(
           com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
         if (desc.getType() != getDescriptor()) {
@@ -5754,39 +7442,51 @@ public static Kind valueOf(
         }
         return VALUES[desc.getIndex()];
       }
-      
+
       private final int index;
       private final int value;
-      
+
       private Kind(int index, int value) {
         this.index = index;
         this.value = value;
       }
-      
+
       // @@protoc_insertion_point(enum_scope:org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind)
     }
-    
+
     private int bitField0_;
     // required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;
     public static final int KIND_FIELD_NUMBER = 1;
     private org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind kind_;
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
+     */
     public boolean hasKind() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind getKind() {
       return kind_;
     }
-    
+
     // optional uint32 dictionarySize = 2;
     public static final int DICTIONARYSIZE_FIELD_NUMBER = 2;
     private int dictionarySize_;
+    /**
+     * <code>optional uint32 dictionarySize = 2;</code>
+     */
     public boolean hasDictionarySize() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
+    /**
+     * <code>optional uint32 dictionarySize = 2;</code>
+     */
     public int getDictionarySize() {
       return dictionarySize_;
     }
-    
+
     private void initFields() {
       kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.DIRECT;
       dictionarySize_ = 0;
@@ -5795,7 +7495,7 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       if (!hasKind()) {
         memoizedIsInitialized = 0;
         return false;
@@ -5803,7 +7503,7 @@ public final boolean isInitialized() {
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -5815,12 +7515,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -5834,94 +7534,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.ColumnEncoding}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncodingOrBuilder {
@@ -5929,18 +7618,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -5951,7 +7643,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.DIRECT;
@@ -5960,20 +7652,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding result = buildPartial();
         if (!result.isInitialized()) {
@@ -5981,17 +7673,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding(this);
         int from_bitField0_ = bitField0_;
@@ -6008,7 +7690,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding)other);
@@ -6017,7 +7699,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getDefaultInstance()) return this;
         if (other.hasKind()) {
@@ -6029,7 +7711,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncodin
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         if (!hasKind()) {
           
@@ -6037,60 +7719,43 @@ public final boolean isInitialized() {
         }
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              int rawValue = input.readEnum();
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind value = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.valueOf(rawValue);
-              if (value == null) {
-                unknownFields.mergeVarintField(1, rawValue);
-              } else {
-                bitField0_ |= 0x00000001;
-                kind_ = value;
-              }
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              dictionarySize_ = input.readUInt32();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;
       private org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.DIRECT;
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
+       */
       public boolean hasKind() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind getKind() {
         return kind_;
       }
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
+       */
       public Builder setKind(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind value) {
         if (value == null) {
           throw new NullPointerException();
@@ -6100,138 +7765,297 @@ public Builder setKind(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.
         onChanged();
         return this;
       }
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding.Kind kind = 1;</code>
+       */
       public Builder clearKind() {
         bitField0_ = (bitField0_ & ~0x00000001);
         kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.DIRECT;
         onChanged();
         return this;
       }
-      
+
       // optional uint32 dictionarySize = 2;
       private int dictionarySize_ ;
+      /**
+       * <code>optional uint32 dictionarySize = 2;</code>
+       */
       public boolean hasDictionarySize() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
+      /**
+       * <code>optional uint32 dictionarySize = 2;</code>
+       */
       public int getDictionarySize() {
         return dictionarySize_;
       }
+      /**
+       * <code>optional uint32 dictionarySize = 2;</code>
+       */
       public Builder setDictionarySize(int value) {
         bitField0_ |= 0x00000002;
         dictionarySize_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint32 dictionarySize = 2;</code>
+       */
       public Builder clearDictionarySize() {
         bitField0_ = (bitField0_ & ~0x00000002);
         dictionarySize_ = 0;
         onChanged();
         return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.ColumnEncoding)
     }
-    
+
     static {
       defaultInstance = new ColumnEncoding(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.ColumnEncoding)
   }
-  
+
   public interface StripeFooterOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+     */
     java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream> 
         getStreamsList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream getStreams(int index);
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+     */
     int getStreamsCount();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+     */
     java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.StreamOrBuilder> 
         getStreamsOrBuilderList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.StreamOrBuilder getStreamsOrBuilder(
         int index);
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+     */
     java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding> 
         getColumnsList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding getColumns(int index);
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+     */
     int getColumnsCount();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+     */
     java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncodingOrBuilder> 
         getColumnsOrBuilderList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncodingOrBuilder getColumnsOrBuilder(
         int index);
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StripeFooter}
+   */
   public static final class StripeFooter extends
       com.google.protobuf.GeneratedMessage
       implements StripeFooterOrBuilder {
     // Use StripeFooter.newBuilder() to construct.
-    private StripeFooter(Builder builder) {
+    private StripeFooter(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private StripeFooter(boolean noInit) {}
-    
+    private StripeFooter(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final StripeFooter defaultInstance;
     public static StripeFooter getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public StripeFooter getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private StripeFooter(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 10: {
+              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
+                streams_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream>();
+                mutable_bitField0_ |= 0x00000001;
+              }
+              streams_.add(input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.PARSER, extensionRegistry));
+              break;
+            }
+            case 18: {
+              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
+                columns_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding>();
+                mutable_bitField0_ |= 0x00000002;
+              }
+              columns_.add(input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.PARSER, extensionRegistry));
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
+          streams_ = java.util.Collections.unmodifiableList(streams_);
+        }
+        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
+          columns_ = java.util.Collections.unmodifiableList(columns_);
+        }
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<StripeFooter> PARSER =
+        new com.google.protobuf.AbstractParser<StripeFooter>() {
+      public StripeFooter parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new StripeFooter(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<StripeFooter> getParserForType() {
+      return PARSER;
     }
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;
     public static final int STREAMS_FIELD_NUMBER = 1;
     private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream> streams_;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+     */
     public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream> getStreamsList() {
       return streams_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+     */
     public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.StreamOrBuilder> 
         getStreamsOrBuilderList() {
       return streams_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+     */
     public int getStreamsCount() {
       return streams_.size();
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream getStreams(int index) {
       return streams_.get(index);
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.StreamOrBuilder getStreamsOrBuilder(
         int index) {
       return streams_.get(index);
     }
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;
     public static final int COLUMNS_FIELD_NUMBER = 2;
     private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding> columns_;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+     */
     public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding> getColumnsList() {
       return columns_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+     */
     public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncodingOrBuilder> 
         getColumnsOrBuilderList() {
       return columns_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+     */
     public int getColumnsCount() {
       return columns_.size();
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding getColumns(int index) {
       return columns_.get(index);
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncodingOrBuilder getColumnsOrBuilder(
         int index) {
       return columns_.get(index);
     }
-    
+
     private void initFields() {
       streams_ = java.util.Collections.emptyList();
       columns_ = java.util.Collections.emptyList();
@@ -6240,7 +8064,7 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       for (int i = 0; i < getStreamsCount(); i++) {
         if (!getStreams(i).isInitialized()) {
           memoizedIsInitialized = 0;
@@ -6256,7 +8080,7 @@ public final boolean isInitialized() {
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -6268,12 +8092,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       for (int i = 0; i < streams_.size(); i++) {
         size += com.google.protobuf.CodedOutputStream
@@ -6287,94 +8111,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StripeFooter}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooterOrBuilder {
@@ -6382,18 +8195,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -6406,7 +8222,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         if (streamsBuilder_ == null) {
@@ -6423,20 +8239,20 @@ public Builder clear() {
         }
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter result = buildPartial();
         if (!result.isInitialized()) {
@@ -6444,17 +8260,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter(this);
         int from_bitField0_ = bitField0_;
@@ -6479,7 +8285,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter)other);
@@ -6488,7 +8294,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getDefaultInstance()) return this;
         if (streamsBuilder_ == null) {
@@ -6546,7 +8352,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter 
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         for (int i = 0; i < getStreamsCount(); i++) {
           if (!getStreams(i).isInitialized()) {
@@ -6562,48 +8368,26 @@ public final boolean isInitialized() {
         }
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.newBuilder();
-              input.readMessage(subBuilder, extensionRegistry);
-              addStreams(subBuilder.buildPartial());
-              break;
-            }
-            case 18: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.newBuilder();
-              input.readMessage(subBuilder, extensionRegistry);
-              addColumns(subBuilder.buildPartial());
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;
       private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream> streams_ =
         java.util.Collections.emptyList();
@@ -6613,10 +8397,13 @@ private void ensureStreamsIsMutable() {
           bitField0_ |= 0x00000001;
          }
       }
-      
+
       private com.google.protobuf.RepeatedFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream, org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.StreamOrBuilder> streamsBuilder_;
-      
+
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream> getStreamsList() {
         if (streamsBuilder_ == null) {
           return java.util.Collections.unmodifiableList(streams_);
@@ -6624,6 +8411,9 @@ private void ensureStreamsIsMutable() {
           return streamsBuilder_.getMessageList();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public int getStreamsCount() {
         if (streamsBuilder_ == null) {
           return streams_.size();
@@ -6631,6 +8421,9 @@ public int getStreamsCount() {
           return streamsBuilder_.getCount();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream getStreams(int index) {
         if (streamsBuilder_ == null) {
           return streams_.get(index);
@@ -6638,6 +8431,9 @@ public int getStreamsCount() {
           return streamsBuilder_.getMessage(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public Builder setStreams(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream value) {
         if (streamsBuilder_ == null) {
@@ -6652,6 +8448,9 @@ public Builder setStreams(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public Builder setStreams(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder builderForValue) {
         if (streamsBuilder_ == null) {
@@ -6663,6 +8462,9 @@ public Builder setStreams(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public Builder addStreams(org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream value) {
         if (streamsBuilder_ == null) {
           if (value == null) {
@@ -6676,6 +8478,9 @@ public Builder addStreams(org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream value
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public Builder addStreams(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream value) {
         if (streamsBuilder_ == null) {
@@ -6690,6 +8495,9 @@ public Builder addStreams(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public Builder addStreams(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder builderForValue) {
         if (streamsBuilder_ == null) {
@@ -6701,6 +8509,9 @@ public Builder addStreams(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public Builder addStreams(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder builderForValue) {
         if (streamsBuilder_ == null) {
@@ -6712,6 +8523,9 @@ public Builder addStreams(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public Builder addAllStreams(
           java.lang.Iterable<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream> values) {
         if (streamsBuilder_ == null) {
@@ -6723,6 +8537,9 @@ public Builder addAllStreams(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public Builder clearStreams() {
         if (streamsBuilder_ == null) {
           streams_ = java.util.Collections.emptyList();
@@ -6733,6 +8550,9 @@ public Builder clearStreams() {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public Builder removeStreams(int index) {
         if (streamsBuilder_ == null) {
           ensureStreamsIsMutable();
@@ -6743,10 +8563,16 @@ public Builder removeStreams(int index) {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder getStreamsBuilder(
           int index) {
         return getStreamsFieldBuilder().getBuilder(index);
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StreamOrBuilder getStreamsOrBuilder(
           int index) {
         if (streamsBuilder_ == null) {
@@ -6754,6 +8580,9 @@ public Builder removeStreams(int index) {
           return streamsBuilder_.getMessageOrBuilder(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.StreamOrBuilder> 
            getStreamsOrBuilderList() {
         if (streamsBuilder_ != null) {
@@ -6762,15 +8591,24 @@ public Builder removeStreams(int index) {
           return java.util.Collections.unmodifiableList(streams_);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder addStreamsBuilder() {
         return getStreamsFieldBuilder().addBuilder(
             org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder addStreamsBuilder(
           int index) {
         return getStreamsFieldBuilder().addBuilder(
             index, org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Stream streams = 1;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder> 
            getStreamsBuilderList() {
         return getStreamsFieldBuilder().getBuilderList();
@@ -6789,7 +8627,7 @@ public Builder removeStreams(int index) {
         }
         return streamsBuilder_;
       }
-      
+
       // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;
       private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding> columns_ =
         java.util.Collections.emptyList();
@@ -6799,10 +8637,13 @@ private void ensureColumnsIsMutable() {
           bitField0_ |= 0x00000002;
          }
       }
-      
+
       private com.google.protobuf.RepeatedFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncodingOrBuilder> columnsBuilder_;
-      
+
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding> getColumnsList() {
         if (columnsBuilder_ == null) {
           return java.util.Collections.unmodifiableList(columns_);
@@ -6810,6 +8651,9 @@ private void ensureColumnsIsMutable() {
           return columnsBuilder_.getMessageList();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public int getColumnsCount() {
         if (columnsBuilder_ == null) {
           return columns_.size();
@@ -6817,6 +8661,9 @@ public int getColumnsCount() {
           return columnsBuilder_.getCount();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding getColumns(int index) {
         if (columnsBuilder_ == null) {
           return columns_.get(index);
@@ -6824,6 +8671,9 @@ public int getColumnsCount() {
           return columnsBuilder_.getMessage(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public Builder setColumns(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding value) {
         if (columnsBuilder_ == null) {
@@ -6838,6 +8688,9 @@ public Builder setColumns(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public Builder setColumns(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder builderForValue) {
         if (columnsBuilder_ == null) {
@@ -6849,6 +8702,9 @@ public Builder setColumns(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public Builder addColumns(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding value) {
         if (columnsBuilder_ == null) {
           if (value == null) {
@@ -6862,6 +8718,9 @@ public Builder addColumns(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncodi
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public Builder addColumns(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding value) {
         if (columnsBuilder_ == null) {
@@ -6876,6 +8735,9 @@ public Builder addColumns(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public Builder addColumns(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder builderForValue) {
         if (columnsBuilder_ == null) {
@@ -6887,6 +8749,9 @@ public Builder addColumns(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public Builder addColumns(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder builderForValue) {
         if (columnsBuilder_ == null) {
@@ -6898,6 +8763,9 @@ public Builder addColumns(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public Builder addAllColumns(
           java.lang.Iterable<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding> values) {
         if (columnsBuilder_ == null) {
@@ -6909,6 +8777,9 @@ public Builder addAllColumns(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public Builder clearColumns() {
         if (columnsBuilder_ == null) {
           columns_ = java.util.Collections.emptyList();
@@ -6919,6 +8790,9 @@ public Builder clearColumns() {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public Builder removeColumns(int index) {
         if (columnsBuilder_ == null) {
           ensureColumnsIsMutable();
@@ -6929,10 +8803,16 @@ public Builder removeColumns(int index) {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder getColumnsBuilder(
           int index) {
         return getColumnsFieldBuilder().getBuilder(index);
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncodingOrBuilder getColumnsOrBuilder(
           int index) {
         if (columnsBuilder_ == null) {
@@ -6940,6 +8820,9 @@ public Builder removeColumns(int index) {
           return columnsBuilder_.getMessageOrBuilder(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncodingOrBuilder> 
            getColumnsOrBuilderList() {
         if (columnsBuilder_ != null) {
@@ -6948,15 +8831,24 @@ public Builder removeColumns(int index) {
           return java.util.Collections.unmodifiableList(columns_);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder addColumnsBuilder() {
         return getColumnsFieldBuilder().addBuilder(
             org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder addColumnsBuilder(
           int index) {
         return getColumnsFieldBuilder().addBuilder(
             index, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnEncoding columns = 2;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder> 
            getColumnsBuilderList() {
         return getColumnsFieldBuilder().getBuilderList();
@@ -6975,109 +8867,363 @@ public Builder removeColumns(int index) {
         }
         return columnsBuilder_;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.StripeFooter)
     }
-    
+
     static {
       defaultInstance = new StripeFooter(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.StripeFooter)
   }
-  
+
   public interface TypeOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
+     */
     boolean hasKind();
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind getKind();
-    
+
     // repeated uint32 subtypes = 2 [packed = true];
+    /**
+     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+     */
     java.util.List<java.lang.Integer> getSubtypesList();
+    /**
+     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+     */
     int getSubtypesCount();
+    /**
+     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+     */
     int getSubtypes(int index);
-    
+
     // repeated string fieldNames = 3;
-    java.util.List<String> getFieldNamesList();
+    /**
+     * <code>repeated string fieldNames = 3;</code>
+     */
+    java.util.List<java.lang.String>
+    getFieldNamesList();
+    /**
+     * <code>repeated string fieldNames = 3;</code>
+     */
     int getFieldNamesCount();
-    String getFieldNames(int index);
-    
+    /**
+     * <code>repeated string fieldNames = 3;</code>
+     */
+    java.lang.String getFieldNames(int index);
+    /**
+     * <code>repeated string fieldNames = 3;</code>
+     */
+    com.google.protobuf.ByteString
+        getFieldNamesBytes(int index);
+
     // optional uint32 maximumLength = 4;
+    /**
+     * <code>optional uint32 maximumLength = 4;</code>
+     */
     boolean hasMaximumLength();
+    /**
+     * <code>optional uint32 maximumLength = 4;</code>
+     */
     int getMaximumLength();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Type}
+   */
   public static final class Type extends
       com.google.protobuf.GeneratedMessage
       implements TypeOrBuilder {
     // Use Type.newBuilder() to construct.
-    private Type(Builder builder) {
+    private Type(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private Type(boolean noInit) {}
-    
+    private Type(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final Type defaultInstance;
     public static Type getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public Type getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private Type(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              int rawValue = input.readEnum();
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind value = org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.valueOf(rawValue);
+              if (value == null) {
+                unknownFields.mergeVarintField(1, rawValue);
+              } else {
+                bitField0_ |= 0x00000001;
+                kind_ = value;
+              }
+              break;
+            }
+            case 16: {
+              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
+                subtypes_ = new java.util.ArrayList<java.lang.Integer>();
+                mutable_bitField0_ |= 0x00000002;
+              }
+              subtypes_.add(input.readUInt32());
+              break;
+            }
+            case 18: {
+              int length = input.readRawVarint32();
+              int limit = input.pushLimit(length);
+              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002) && input.getBytesUntilLimit() > 0) {
+                subtypes_ = new java.util.ArrayList<java.lang.Integer>();
+                mutable_bitField0_ |= 0x00000002;
+              }
+              while (input.getBytesUntilLimit() > 0) {
+                subtypes_.add(input.readUInt32());
+              }
+              input.popLimit(limit);
+              break;
+            }
+            case 26: {
+              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
+                fieldNames_ = new com.google.protobuf.LazyStringArrayList();
+                mutable_bitField0_ |= 0x00000004;
+              }
+              fieldNames_.add(input.readBytes());
+              break;
+            }
+            case 32: {
+              bitField0_ |= 0x00000002;
+              maximumLength_ = input.readUInt32();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
+          subtypes_ = java.util.Collections.unmodifiableList(subtypes_);
+        }
+        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
+          fieldNames_ = new com.google.protobuf.UnmodifiableLazyStringList(fieldNames_);
+        }
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<Type> PARSER =
+        new com.google.protobuf.AbstractParser<Type>() {
+      public Type parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new Type(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<Type> getParserForType() {
+      return PARSER;
     }
-    
+
+    /**
+     * Protobuf enum {@code org.apache.hadoop.hive.ql.io.orc.Type.Kind}
+     */
     public enum Kind
         implements com.google.protobuf.ProtocolMessageEnum {
+      /**
+       * <code>BOOLEAN = 0;</code>
+       */
       BOOLEAN(0, 0),
+      /**
+       * <code>BYTE = 1;</code>
+       */
       BYTE(1, 1),
+      /**
+       * <code>SHORT = 2;</code>
+       */
       SHORT(2, 2),
+      /**
+       * <code>INT = 3;</code>
+       */
       INT(3, 3),
+      /**
+       * <code>LONG = 4;</code>
+       */
       LONG(4, 4),
+      /**
+       * <code>FLOAT = 5;</code>
+       */
       FLOAT(5, 5),
+      /**
+       * <code>DOUBLE = 6;</code>
+       */
       DOUBLE(6, 6),
+      /**
+       * <code>STRING = 7;</code>
+       */
       STRING(7, 7),
+      /**
+       * <code>BINARY = 8;</code>
+       */
       BINARY(8, 8),
+      /**
+       * <code>TIMESTAMP = 9;</code>
+       */
       TIMESTAMP(9, 9),
+      /**
+       * <code>LIST = 10;</code>
+       */
       LIST(10, 10),
+      /**
+       * <code>MAP = 11;</code>
+       */
       MAP(11, 11),
+      /**
+       * <code>STRUCT = 12;</code>
+       */
       STRUCT(12, 12),
+      /**
+       * <code>UNION = 13;</code>
+       */
       UNION(13, 13),
+      /**
+       * <code>DECIMAL = 14;</code>
+       */
       DECIMAL(14, 14),
+      /**
+       * <code>DATE = 15;</code>
+       */
       DATE(15, 15),
+      /**
+       * <code>VARCHAR = 16;</code>
+       */
       VARCHAR(16, 16),
       ;
-      
+
+      /**
+       * <code>BOOLEAN = 0;</code>
+       */
       public static final int BOOLEAN_VALUE = 0;
+      /**
+       * <code>BYTE = 1;</code>
+       */
       public static final int BYTE_VALUE = 1;
+      /**
+       * <code>SHORT = 2;</code>
+       */
       public static final int SHORT_VALUE = 2;
+      /**
+       * <code>INT = 3;</code>
+       */
       public static final int INT_VALUE = 3;
+      /**
+       * <code>LONG = 4;</code>
+       */
       public static final int LONG_VALUE = 4;
+      /**
+       * <code>FLOAT = 5;</code>
+       */
       public static final int FLOAT_VALUE = 5;
+      /**
+       * <code>DOUBLE = 6;</code>
+       */
       public static final int DOUBLE_VALUE = 6;
+      /**
+       * <code>STRING = 7;</code>
+       */
       public static final int STRING_VALUE = 7;
+      /**
+       * <code>BINARY = 8;</code>
+       */
       public static final int BINARY_VALUE = 8;
+      /**
+       * <code>TIMESTAMP = 9;</code>
+       */
       public static final int TIMESTAMP_VALUE = 9;
+      /**
+       * <code>LIST = 10;</code>
+       */
       public static final int LIST_VALUE = 10;
+      /**
+       * <code>MAP = 11;</code>
+       */
       public static final int MAP_VALUE = 11;
+      /**
+       * <code>STRUCT = 12;</code>
+       */
       public static final int STRUCT_VALUE = 12;
+      /**
+       * <code>UNION = 13;</code>
+       */
       public static final int UNION_VALUE = 13;
+      /**
+       * <code>DECIMAL = 14;</code>
+       */
       public static final int DECIMAL_VALUE = 14;
+      /**
+       * <code>DATE = 15;</code>
+       */
       public static final int DATE_VALUE = 15;
+      /**
+       * <code>VARCHAR = 16;</code>
+       */
       public static final int VARCHAR_VALUE = 16;
-      
-      
+
+
       public final int getNumber() { return value; }
-      
+
       public static Kind valueOf(int value) {
         switch (value) {
           case 0: return BOOLEAN;
@@ -7100,7 +9246,7 @@ public static Kind valueOf(int value) {
           default: return null;
         }
       }
-      
+
       public static com.google.protobuf.Internal.EnumLiteMap<Kind>
           internalGetValueMap() {
         return internalValueMap;
@@ -7112,7 +9258,7 @@ public Kind findValueByNumber(int number) {
                 return Kind.valueOf(number);
               }
             };
-      
+
       public final com.google.protobuf.Descriptors.EnumValueDescriptor
           getValueDescriptor() {
         return getDescriptor().getValues().get(index);
@@ -7125,11 +9271,9 @@ public Kind findValueByNumber(int number) {
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getDescriptor().getEnumTypes().get(0);
       }
-      
-      private static final Kind[] VALUES = {
-        BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING, BINARY, TIMESTAMP, LIST, MAP, STRUCT, UNION, DECIMAL, DATE, VARCHAR, 
-      };
-      
+
+      private static final Kind[] VALUES = values();
+
       public static Kind valueOf(
           com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
         if (desc.getType() != getDescriptor()) {
@@ -7138,71 +9282,108 @@ public static Kind valueOf(
         }
         return VALUES[desc.getIndex()];
       }
-      
+
       private final int index;
       private final int value;
-      
+
       private Kind(int index, int value) {
         this.index = index;
         this.value = value;
       }
-      
+
       // @@protoc_insertion_point(enum_scope:org.apache.hadoop.hive.ql.io.orc.Type.Kind)
     }
-    
+
     private int bitField0_;
     // required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;
     public static final int KIND_FIELD_NUMBER = 1;
     private org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind kind_;
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
+     */
     public boolean hasKind() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
+    /**
+     * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind getKind() {
       return kind_;
     }
-    
+
     // repeated uint32 subtypes = 2 [packed = true];
     public static final int SUBTYPES_FIELD_NUMBER = 2;
     private java.util.List<java.lang.Integer> subtypes_;
+    /**
+     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+     */
     public java.util.List<java.lang.Integer>
         getSubtypesList() {
       return subtypes_;
     }
+    /**
+     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+     */
     public int getSubtypesCount() {
       return subtypes_.size();
     }
+    /**
+     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+     */
     public int getSubtypes(int index) {
       return subtypes_.get(index);
     }
     private int subtypesMemoizedSerializedSize = -1;
-    
+
     // repeated string fieldNames = 3;
     public static final int FIELDNAMES_FIELD_NUMBER = 3;
     private com.google.protobuf.LazyStringList fieldNames_;
-    public java.util.List<String>
+    /**
+     * <code>repeated string fieldNames = 3;</code>
+     */
+    public java.util.List<java.lang.String>
         getFieldNamesList() {
       return fieldNames_;
     }
+    /**
+     * <code>repeated string fieldNames = 3;</code>
+     */
     public int getFieldNamesCount() {
       return fieldNames_.size();
     }
-    public String getFieldNames(int index) {
+    /**
+     * <code>repeated string fieldNames = 3;</code>
+     */
+    public java.lang.String getFieldNames(int index) {
       return fieldNames_.get(index);
     }
-    
+    /**
+     * <code>repeated string fieldNames = 3;</code>
+     */
+    public com.google.protobuf.ByteString
+        getFieldNamesBytes(int index) {
+      return fieldNames_.getByteString(index);
+    }
+
     // optional uint32 maximumLength = 4;
     public static final int MAXIMUMLENGTH_FIELD_NUMBER = 4;
     private int maximumLength_;
+    /**
+     * <code>optional uint32 maximumLength = 4;</code>
+     */
     public boolean hasMaximumLength() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
+    /**
+     * <code>optional uint32 maximumLength = 4;</code>
+     */
     public int getMaximumLength() {
       return maximumLength_;
     }
-    
+
     private void initFields() {
       kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.BOOLEAN;
-      subtypes_ = java.util.Collections.emptyList();;
+      subtypes_ = java.util.Collections.emptyList();
       fieldNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
       maximumLength_ = 0;
     }
@@ -7210,7 +9391,7 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       if (!hasKind()) {
         memoizedIsInitialized = 0;
         return false;
@@ -7218,7 +9399,7 @@ public final boolean isInitialized() {
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -7240,12 +9421,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -7282,94 +9463,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Type parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Type parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Type parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Type parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Type parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Type parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Type parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Type parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Type parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Type parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.Type prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Type}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.TypeOrBuilder {
@@ -7377,18 +9547,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -7399,12 +9572,12 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.BOOLEAN;
         bitField0_ = (bitField0_ & ~0x00000001);
-        subtypes_ = java.util.Collections.emptyList();;
+        subtypes_ = java.util.Collections.emptyList();
         bitField0_ = (bitField0_ & ~0x00000002);
         fieldNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
         bitField0_ = (bitField0_ & ~0x00000004);
@@ -7412,20 +9585,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000008);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Type getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Type build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.Type result = buildPartial();
         if (!result.isInitialized()) {
@@ -7433,17 +9606,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.Type buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.Type result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Type buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.Type result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.Type(this);
         int from_bitField0_ = bitField0_;
@@ -7471,7 +9634,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.Type) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.Type)other);
@@ -7480,7 +9643,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.Type other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getDefaultInstance()) return this;
         if (other.hasKind()) {
@@ -7512,7 +9675,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.Type other) {
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         if (!hasKind()) {
           
@@ -7520,79 +9683,43 @@ public final boolean isInitialized() {
         }
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              int rawValue = input.readEnum();
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind value = org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.valueOf(rawValue);
-              if (value == null) {
-                unknownFields.mergeVarintField(1, rawValue);
-              } else {
-                bitField0_ |= 0x00000001;
-                kind_ = value;
-              }
-              break;
-            }
-            case 16: {
-              ensureSubtypesIsMutable();
-              subtypes_.add(input.readUInt32());
-              break;
-            }
-            case 18: {
-              int length = input.readRawVarint32();
-              int limit = input.pushLimit(length);
-              while (input.getBytesUntilLimit() > 0) {
-                addSubtypes(input.readUInt32());
-              }
-              input.popLimit(limit);
-              break;
-            }
-            case 26: {
-              ensureFieldNamesIsMutable();
-              fieldNames_.add(input.readBytes());
-              break;
-            }
-            case 32: {
-              bitField0_ |= 0x00000008;
-              maximumLength_ = input.readUInt32();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.Type parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.Type) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;
       private org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.BOOLEAN;
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
+       */
       public boolean hasKind() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind getKind() {
         return kind_;
       }
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
+       */
       public Builder setKind(org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind value) {
         if (value == null) {
           throw new NullPointerException();
@@ -7602,31 +9729,46 @@ public Builder setKind(org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind value
         onChanged();
         return this;
       }
+      /**
+       * <code>required .org.apache.hadoop.hive.ql.io.orc.Type.Kind kind = 1;</code>
+       */
       public Builder clearKind() {
         bitField0_ = (bitField0_ & ~0x00000001);
         kind_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.BOOLEAN;
         onChanged();
         return this;
       }
-      
+
       // repeated uint32 subtypes = 2 [packed = true];
-      private java.util.List<java.lang.Integer> subtypes_ = java.util.Collections.emptyList();;
+      private java.util.List<java.lang.Integer> subtypes_ = java.util.Collections.emptyList();
       private void ensureSubtypesIsMutable() {
         if (!((bitField0_ & 0x00000002) == 0x00000002)) {
           subtypes_ = new java.util.ArrayList<java.lang.Integer>(subtypes_);
           bitField0_ |= 0x00000002;
          }
       }
+      /**
+       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+       */
       public java.util.List<java.lang.Integer>
           getSubtypesList() {
         return java.util.Collections.unmodifiableList(subtypes_);
       }
+      /**
+       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+       */
       public int getSubtypesCount() {
         return subtypes_.size();
       }
+      /**
+       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+       */
       public int getSubtypes(int index) {
         return subtypes_.get(index);
       }
+      /**
+       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+       */
       public Builder setSubtypes(
           int index, int value) {
         ensureSubtypesIsMutable();
@@ -7634,12 +9776,18 @@ public Builder setSubtypes(
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+       */
       public Builder addSubtypes(int value) {
         ensureSubtypesIsMutable();
         subtypes_.add(value);
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+       */
       public Builder addAllSubtypes(
           java.lang.Iterable<? extends java.lang.Integer> values) {
         ensureSubtypesIsMutable();
@@ -7647,13 +9795,16 @@ public Builder addAllSubtypes(
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
+       */
       public Builder clearSubtypes() {
-        subtypes_ = java.util.Collections.emptyList();;
+        subtypes_ = java.util.Collections.emptyList();
         bitField0_ = (bitField0_ & ~0x00000002);
         onChanged();
         return this;
       }
-      
+
       // repeated string fieldNames = 3;
       private com.google.protobuf.LazyStringList fieldNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
       private void ensureFieldNamesIsMutable() {
@@ -7662,18 +9813,37 @@ private void ensureFieldNamesIsMutable() {
           bitField0_ |= 0x00000004;
          }
       }
-      public java.util.List<String>
+      /**
+       * <code>repeated string fieldNames = 3;</code>
+       */
+      public java.util.List<java.lang.String>
           getFieldNamesList() {
         return java.util.Collections.unmodifiableList(fieldNames_);
       }
+      /**
+       * <code>repeated string fieldNames = 3;</code>
+       */
       public int getFieldNamesCount() {
         return fieldNames_.size();
       }
-      public String getFieldNames(int index) {
+      /**
+       * <code>repeated string fieldNames = 3;</code>
+       */
+      public java.lang.String getFieldNames(int index) {
         return fieldNames_.get(index);
       }
+      /**
+       * <code>repeated string fieldNames = 3;</code>
+       */
+      public com.google.protobuf.ByteString
+          getFieldNamesBytes(int index) {
+        return fieldNames_.getByteString(index);
+      }
+      /**
+       * <code>repeated string fieldNames = 3;</code>
+       */
       public Builder setFieldNames(
-          int index, String value) {
+          int index, java.lang.String value) {
         if (value == null) {
     throw new NullPointerException();
   }
@@ -7682,7 +9852,11 @@ public Builder setFieldNames(
         onChanged();
         return this;
       }
-      public Builder addFieldNames(String value) {
+      /**
+       * <code>repeated string fieldNames = 3;</code>
+       */
+      public Builder addFieldNames(
+          java.lang.String value) {
         if (value == null) {
     throw new NullPointerException();
   }
@@ -7691,159 +9865,332 @@ public Builder addFieldNames(String value) {
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated string fieldNames = 3;</code>
+       */
       public Builder addAllFieldNames(
-          java.lang.Iterable<String> values) {
+          java.lang.Iterable<java.lang.String> values) {
         ensureFieldNamesIsMutable();
         super.addAll(values, fieldNames_);
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated string fieldNames = 3;</code>
+       */
       public Builder clearFieldNames() {
         fieldNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
         bitField0_ = (bitField0_ & ~0x00000004);
         onChanged();
         return this;
       }
-      void addFieldNames(com.google.protobuf.ByteString value) {
-        ensureFieldNamesIsMutable();
+      /**
+       * <code>repeated string fieldNames = 3;</code>
+       */
+      public Builder addFieldNamesBytes(
+          com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureFieldNamesIsMutable();
         fieldNames_.add(value);
         onChanged();
+        return this;
       }
-      
+
       // optional uint32 maximumLength = 4;
       private int maximumLength_ ;
+      /**
+       * <code>optional uint32 maximumLength = 4;</code>
+       */
       public boolean hasMaximumLength() {
         return ((bitField0_ & 0x00000008) == 0x00000008);
       }
+      /**
+       * <code>optional uint32 maximumLength = 4;</code>
+       */
       public int getMaximumLength() {
         return maximumLength_;
       }
+      /**
+       * <code>optional uint32 maximumLength = 4;</code>
+       */
       public Builder setMaximumLength(int value) {
         bitField0_ |= 0x00000008;
         maximumLength_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint32 maximumLength = 4;</code>
+       */
       public Builder clearMaximumLength() {
         bitField0_ = (bitField0_ & ~0x00000008);
         maximumLength_ = 0;
         onChanged();
         return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.Type)
     }
-    
+
     static {
       defaultInstance = new Type(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.Type)
   }
-  
+
   public interface StripeInformationOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // optional uint64 offset = 1;
+    /**
+     * <code>optional uint64 offset = 1;</code>
+     */
     boolean hasOffset();
+    /**
+     * <code>optional uint64 offset = 1;</code>
+     */
     long getOffset();
-    
+
     // optional uint64 indexLength = 2;
+    /**
+     * <code>optional uint64 indexLength = 2;</code>
+     */
     boolean hasIndexLength();
+    /**
+     * <code>optional uint64 indexLength = 2;</code>
+     */
     long getIndexLength();
-    
+
     // optional uint64 dataLength = 3;
+    /**
+     * <code>optional uint64 dataLength = 3;</code>
+     */
     boolean hasDataLength();
+    /**
+     * <code>optional uint64 dataLength = 3;</code>
+     */
     long getDataLength();
-    
+
     // optional uint64 footerLength = 4;
+    /**
+     * <code>optional uint64 footerLength = 4;</code>
+     */
     boolean hasFooterLength();
+    /**
+     * <code>optional uint64 footerLength = 4;</code>
+     */
     long getFooterLength();
-    
+
     // optional uint64 numberOfRows = 5;
+    /**
+     * <code>optional uint64 numberOfRows = 5;</code>
+     */
     boolean hasNumberOfRows();
+    /**
+     * <code>optional uint64 numberOfRows = 5;</code>
+     */
     long getNumberOfRows();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StripeInformation}
+   */
   public static final class StripeInformation extends
       com.google.protobuf.GeneratedMessage
       implements StripeInformationOrBuilder {
     // Use StripeInformation.newBuilder() to construct.
-    private StripeInformation(Builder builder) {
+    private StripeInformation(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private StripeInformation(boolean noInit) {}
-    
+    private StripeInformation(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final StripeInformation defaultInstance;
     public static StripeInformation getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public StripeInformation getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private StripeInformation(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              bitField0_ |= 0x00000001;
+              offset_ = input.readUInt64();
+              break;
+            }
+            case 16: {
+              bitField0_ |= 0x00000002;
+              indexLength_ = input.readUInt64();
+              break;
+            }
+            case 24: {
+              bitField0_ |= 0x00000004;
+              dataLength_ = input.readUInt64();
+              break;
+            }
+            case 32: {
+              bitField0_ |= 0x00000008;
+              footerLength_ = input.readUInt64();
+              break;
+            }
+            case 40: {
+              bitField0_ |= 0x00000010;
+              numberOfRows_ = input.readUInt64();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.class);
     }
-    
+
+    public static com.google.protobuf.Parser<StripeInformation> PARSER =
+        new com.google.protobuf.AbstractParser<StripeInformation>() {
+      public StripeInformation parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new StripeInformation(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<StripeInformation> getParserForType() {
+      return PARSER;
+    }
+
     private int bitField0_;
     // optional uint64 offset = 1;
     public static final int OFFSET_FIELD_NUMBER = 1;
     private long offset_;
+    /**
+     * <code>optional uint64 offset = 1;</code>
+     */
     public boolean hasOffset() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
+    /**
+     * <code>optional uint64 offset = 1;</code>
+     */
     public long getOffset() {
       return offset_;
     }
-    
+
     // optional uint64 indexLength = 2;
     public static final int INDEXLENGTH_FIELD_NUMBER = 2;
     private long indexLength_;
+    /**
+     * <code>optional uint64 indexLength = 2;</code>
+     */
     public boolean hasIndexLength() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
+    /**
+     * <code>optional uint64 indexLength = 2;</code>
+     */
     public long getIndexLength() {
       return indexLength_;
     }
-    
+
     // optional uint64 dataLength = 3;
     public static final int DATALENGTH_FIELD_NUMBER = 3;
     private long dataLength_;
+    /**
+     * <code>optional uint64 dataLength = 3;</code>
+     */
     public boolean hasDataLength() {
       return ((bitField0_ & 0x00000004) == 0x00000004);
     }
+    /**
+     * <code>optional uint64 dataLength = 3;</code>
+     */
     public long getDataLength() {
       return dataLength_;
     }
-    
+
     // optional uint64 footerLength = 4;
     public static final int FOOTERLENGTH_FIELD_NUMBER = 4;
     private long footerLength_;
+    /**
+     * <code>optional uint64 footerLength = 4;</code>
+     */
     public boolean hasFooterLength() {
       return ((bitField0_ & 0x00000008) == 0x00000008);
     }
+    /**
+     * <code>optional uint64 footerLength = 4;</code>
+     */
     public long getFooterLength() {
       return footerLength_;
     }
-    
+
     // optional uint64 numberOfRows = 5;
     public static final int NUMBEROFROWS_FIELD_NUMBER = 5;
     private long numberOfRows_;
+    /**
+     * <code>optional uint64 numberOfRows = 5;</code>
+     */
     public boolean hasNumberOfRows() {
       return ((bitField0_ & 0x00000010) == 0x00000010);
     }
+    /**
+     * <code>optional uint64 numberOfRows = 5;</code>
+     */
     public long getNumberOfRows() {
       return numberOfRows_;
     }
-    
+
     private void initFields() {
       offset_ = 0L;
       indexLength_ = 0L;
@@ -7855,11 +10202,11 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -7880,12 +10227,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -7911,94 +10258,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.StripeInformation}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformationOrBuilder {
@@ -8006,18 +10342,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -8028,7 +10367,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         offset_ = 0L;
@@ -8043,20 +10382,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000010);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation result = buildPartial();
         if (!result.isInitialized()) {
@@ -8064,17 +10403,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation(this);
         int from_bitField0_ = bitField0_;
@@ -8103,7 +10432,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation)other);
@@ -8112,7 +10441,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getDefaultInstance()) return this;
         if (other.hasOffset()) {
@@ -8133,263 +10462,394 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInforma
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              offset_ = input.readUInt64();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              indexLength_ = input.readUInt64();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              dataLength_ = input.readUInt64();
-              break;
-            }
-            case 32: {
-              bitField0_ |= 0x00000008;
-              footerLength_ = input.readUInt64();
-              break;
-            }
-            case 40: {
-              bitField0_ |= 0x00000010;
-              numberOfRows_ = input.readUInt64();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // optional uint64 offset = 1;
       private long offset_ ;
+      /**
+       * <code>optional uint64 offset = 1;</code>
+       */
       public boolean hasOffset() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
+      /**
+       * <code>optional uint64 offset = 1;</code>
+       */
       public long getOffset() {
         return offset_;
       }
+      /**
+       * <code>optional uint64 offset = 1;</code>
+       */
       public Builder setOffset(long value) {
         bitField0_ |= 0x00000001;
         offset_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 offset = 1;</code>
+       */
       public Builder clearOffset() {
         bitField0_ = (bitField0_ & ~0x00000001);
         offset_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // optional uint64 indexLength = 2;
       private long indexLength_ ;
+      /**
+       * <code>optional uint64 indexLength = 2;</code>
+       */
       public boolean hasIndexLength() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
+      /**
+       * <code>optional uint64 indexLength = 2;</code>
+       */
       public long getIndexLength() {
         return indexLength_;
       }
+      /**
+       * <code>optional uint64 indexLength = 2;</code>
+       */
       public Builder setIndexLength(long value) {
         bitField0_ |= 0x00000002;
         indexLength_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 indexLength = 2;</code>
+       */
       public Builder clearIndexLength() {
         bitField0_ = (bitField0_ & ~0x00000002);
         indexLength_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // optional uint64 dataLength = 3;
       private long dataLength_ ;
+      /**
+       * <code>optional uint64 dataLength = 3;</code>
+       */
       public boolean hasDataLength() {
         return ((bitField0_ & 0x00000004) == 0x00000004);
       }
+      /**
+       * <code>optional uint64 dataLength = 3;</code>
+       */
       public long getDataLength() {
         return dataLength_;
       }
+      /**
+       * <code>optional uint64 dataLength = 3;</code>
+       */
       public Builder setDataLength(long value) {
         bitField0_ |= 0x00000004;
         dataLength_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 dataLength = 3;</code>
+       */
       public Builder clearDataLength() {
         bitField0_ = (bitField0_ & ~0x00000004);
         dataLength_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // optional uint64 footerLength = 4;
       private long footerLength_ ;
+      /**
+       * <code>optional uint64 footerLength = 4;</code>
+       */
       public boolean hasFooterLength() {
         return ((bitField0_ & 0x00000008) == 0x00000008);
       }
+      /**
+       * <code>optional uint64 footerLength = 4;</code>
+       */
       public long getFooterLength() {
         return footerLength_;
       }
+      /**
+       * <code>optional uint64 footerLength = 4;</code>
+       */
       public Builder setFooterLength(long value) {
         bitField0_ |= 0x00000008;
         footerLength_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 footerLength = 4;</code>
+       */
       public Builder clearFooterLength() {
         bitField0_ = (bitField0_ & ~0x00000008);
         footerLength_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // optional uint64 numberOfRows = 5;
       private long numberOfRows_ ;
+      /**
+       * <code>optional uint64 numberOfRows = 5;</code>
+       */
       public boolean hasNumberOfRows() {
         return ((bitField0_ & 0x00000010) == 0x00000010);
       }
+      /**
+       * <code>optional uint64 numberOfRows = 5;</code>
+       */
       public long getNumberOfRows() {
         return numberOfRows_;
       }
+      /**
+       * <code>optional uint64 numberOfRows = 5;</code>
+       */
       public Builder setNumberOfRows(long value) {
         bitField0_ |= 0x00000010;
         numberOfRows_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 numberOfRows = 5;</code>
+       */
       public Builder clearNumberOfRows() {
         bitField0_ = (bitField0_ & ~0x00000010);
         numberOfRows_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.StripeInformation)
     }
-    
+
     static {
       defaultInstance = new StripeInformation(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.StripeInformation)
   }
-  
+
   public interface UserMetadataItemOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // required string name = 1;
+    /**
+     * <code>required string name = 1;</code>
+     */
     boolean hasName();
-    String getName();
-    
+    /**
+     * <code>required string name = 1;</code>
+     */
+    java.lang.String getName();
+    /**
+     * <code>required string name = 1;</code>
+     */
+    com.google.protobuf.ByteString
+        getNameBytes();
+
     // required bytes value = 2;
+    /**
+     * <code>required bytes value = 2;</code>
+     */
     boolean hasValue();
+    /**
+     * <code>required bytes value = 2;</code>
+     */
     com.google.protobuf.ByteString getValue();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.UserMetadataItem}
+   */
   public static final class UserMetadataItem extends
       com.google.protobuf.GeneratedMessage
       implements UserMetadataItemOrBuilder {
     // Use UserMetadataItem.newBuilder() to construct.
-    private UserMetadataItem(Builder builder) {
+    private UserMetadataItem(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private UserMetadataItem(boolean noInit) {}
-    
+    private UserMetadataItem(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final UserMetadataItem defaultInstance;
     public static UserMetadataItem getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public UserMetadataItem getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private UserMetadataItem(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              name_ = input.readBytes();
+              break;
+            }
+            case 18: {
+              bitField0_ |= 0x00000002;
+              value_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<UserMetadataItem> PARSER =
+        new com.google.protobuf.AbstractParser<UserMetadataItem>() {
+      public UserMetadataItem parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new UserMetadataItem(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<UserMetadataItem> getParserForType() {
+      return PARSER;
     }
-    
+
     private int bitField0_;
     // required string name = 1;
     public static final int NAME_FIELD_NUMBER = 1;
     private java.lang.Object name_;
+    /**
+     * <code>required string name = 1;</code>
+     */
     public boolean hasName() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
-    public String getName() {
+    /**
+     * <code>required string name = 1;</code>
+     */
+    public java.lang.String getName() {
       java.lang.Object ref = name_;
-      if (ref instanceof String) {
-        return (String) ref;
+      if (ref instanceof java.lang.String) {
+        return (java.lang.String) ref;
       } else {
         com.google.protobuf.ByteString bs = 
             (com.google.protobuf.ByteString) ref;
-        String s = bs.toStringUtf8();
-        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+        java.lang.String s = bs.toStringUtf8();
+        if (bs.isValidUtf8()) {
           name_ = s;
         }
         return s;
       }
     }
-    private com.google.protobuf.ByteString getNameBytes() {
+    /**
+     * <code>required string name = 1;</code>
+     */
+    public com.google.protobuf.ByteString
+        getNameBytes() {
       java.lang.Object ref = name_;
-      if (ref instanceof String) {
+      if (ref instanceof java.lang.String) {
         com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+            com.google.protobuf.ByteString.copyFromUtf8(
+                (java.lang.String) ref);
         name_ = b;
         return b;
       } else {
         return (com.google.protobuf.ByteString) ref;
       }
     }
-    
+
     // required bytes value = 2;
     public static final int VALUE_FIELD_NUMBER = 2;
     private com.google.protobuf.ByteString value_;
+    /**
+     * <code>required bytes value = 2;</code>
+     */
     public boolean hasValue() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
+    /**
+     * <code>required bytes value = 2;</code>
+     */
     public com.google.protobuf.ByteString getValue() {
       return value_;
     }
-    
+
     private void initFields() {
       name_ = "";
       value_ = com.google.protobuf.ByteString.EMPTY;
@@ -8398,7 +10858,7 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       if (!hasName()) {
         memoizedIsInitialized = 0;
         return false;
@@ -8410,7 +10870,7 @@ public final boolean isInitialized() {
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -8422,12 +10882,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -8441,94 +10901,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.UserMetadataItem}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItemOrBuilder {
@@ -8536,18 +10985,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -8558,7 +11010,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         name_ = "";
@@ -8567,20 +11019,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem result = buildPartial();
         if (!result.isInitialized()) {
@@ -8588,17 +11040,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem(this);
         int from_bitField0_ = bitField0_;
@@ -8615,7 +11057,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem)other);
@@ -8624,11 +11066,13 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getDefaultInstance()) return this;
         if (other.hasName()) {
-          setName(other.getName());
+          bitField0_ |= 0x00000001;
+          name_ = other.name_;
+          onChanged();
         }
         if (other.hasValue()) {
           setValue(other.getValue());
@@ -8636,7 +11080,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataI
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         if (!hasName()) {
           
@@ -8648,62 +11092,69 @@ public final boolean isInitialized() {
         }
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              name_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              value_ = input.readBytes();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // required string name = 1;
       private java.lang.Object name_ = "";
+      /**
+       * <code>required string name = 1;</code>
+       */
       public boolean hasName() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
-      public String getName() {
+      /**
+       * <code>required string name = 1;</code>
+       */
+      public java.lang.String getName() {
         java.lang.Object ref = name_;
-        if (!(ref instanceof String)) {
-          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+        if (!(ref instanceof java.lang.String)) {
+          java.lang.String s = ((com.google.protobuf.ByteString) ref)
+              .toStringUtf8();
           name_ = s;
           return s;
         } else {
-          return (String) ref;
+          return (java.lang.String) ref;
+        }
+      }
+      /**
+       * <code>required string name = 1;</code>
+       */
+      public com.google.protobuf.ByteString
+          getNameBytes() {
+        java.lang.Object ref = name_;
+        if (ref instanceof String) {
+          com.google.protobuf.ByteString b = 
+              com.google.protobuf.ByteString.copyFromUtf8(
+                  (java.lang.String) ref);
+          name_ = b;
+          return b;
+        } else {
+          return (com.google.protobuf.ByteString) ref;
         }
       }
-      public Builder setName(String value) {
+      /**
+       * <code>required string name = 1;</code>
+       */
+      public Builder setName(
+          java.lang.String value) {
         if (value == null) {
     throw new NullPointerException();
   }
@@ -8712,26 +11163,46 @@ public Builder setName(String value) {
         onChanged();
         return this;
       }
+      /**
+       * <code>required string name = 1;</code>
+       */
       public Builder clearName() {
         bitField0_ = (bitField0_ & ~0x00000001);
         name_ = getDefaultInstance().getName();
         onChanged();
         return this;
       }
-      void setName(com.google.protobuf.ByteString value) {
-        bitField0_ |= 0x00000001;
+      /**
+       * <code>required string name = 1;</code>
+       */
+      public Builder setNameBytes(
+          com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
         name_ = value;
         onChanged();
+        return this;
       }
-      
+
       // required bytes value = 2;
       private com.google.protobuf.ByteString value_ = com.google.protobuf.ByteString.EMPTY;
+      /**
+       * <code>required bytes value = 2;</code>
+       */
       public boolean hasValue() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
+      /**
+       * <code>required bytes value = 2;</code>
+       */
       public com.google.protobuf.ByteString getValue() {
         return value_;
       }
+      /**
+       * <code>required bytes value = 2;</code>
+       */
       public Builder setValue(com.google.protobuf.ByteString value) {
         if (value == null) {
     throw new NullPointerException();
@@ -8741,236 +11212,533 @@ public Builder setValue(com.google.protobuf.ByteString value) {
         onChanged();
         return this;
       }
+      /**
+       * <code>required bytes value = 2;</code>
+       */
       public Builder clearValue() {
         bitField0_ = (bitField0_ & ~0x00000002);
         value_ = getDefaultInstance().getValue();
         onChanged();
         return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.UserMetadataItem)
     }
-    
+
     static {
       defaultInstance = new UserMetadataItem(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.UserMetadataItem)
   }
-  
+
   public interface FooterOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // optional uint64 headerLength = 1;
+    /**
+     * <code>optional uint64 headerLength = 1;</code>
+     */
     boolean hasHeaderLength();
+    /**
+     * <code>optional uint64 headerLength = 1;</code>
+     */
     long getHeaderLength();
-    
+
     // optional uint64 contentLength = 2;
+    /**
+     * <code>optional uint64 contentLength = 2;</code>
+     */
     boolean hasContentLength();
+    /**
+     * <code>optional uint64 contentLength = 2;</code>
+     */
     long getContentLength();
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+     */
     java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation> 
         getStripesList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation getStripes(int index);
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+     */
     int getStripesCount();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+     */
     java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformationOrBuilder> 
         getStripesOrBuilderList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformationOrBuilder getStripesOrBuilder(
         int index);
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+     */
     java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Type> 
         getTypesList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.Type getTypes(int index);
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+     */
     int getTypesCount();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+     */
     java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.TypeOrBuilder> 
         getTypesOrBuilderList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.TypeOrBuilder getTypesOrBuilder(
         int index);
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+     */
     java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem> 
         getMetadataList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem getMetadata(int index);
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+     */
     int getMetadataCount();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+     */
     java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItemOrBuilder> 
         getMetadataOrBuilderList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItemOrBuilder getMetadataOrBuilder(
         int index);
-    
+
     // optional uint64 numberOfRows = 6;
+    /**
+     * <code>optional uint64 numberOfRows = 6;</code>
+     */
     boolean hasNumberOfRows();
+    /**
+     * <code>optional uint64 numberOfRows = 6;</code>
+     */
     long getNumberOfRows();
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+     */
     java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics> 
         getStatisticsList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics getStatistics(int index);
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+     */
     int getStatisticsCount();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+     */
     java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder> 
         getStatisticsOrBuilderList();
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder(
         int index);
-    
+
     // optional uint32 rowIndexStride = 8;
+    /**
+     * <code>optional uint32 rowIndexStride = 8;</code>
+     */
     boolean hasRowIndexStride();
+    /**
+     * <code>optional uint32 rowIndexStride = 8;</code>
+     */
     int getRowIndexStride();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Footer}
+   */
   public static final class Footer extends
       com.google.protobuf.GeneratedMessage
       implements FooterOrBuilder {
     // Use Footer.newBuilder() to construct.
-    private Footer(Builder builder) {
+    private Footer(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private Footer(boolean noInit) {}
-    
+    private Footer(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final Footer defaultInstance;
     public static Footer getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public Footer getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private Footer(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              bitField0_ |= 0x00000001;
+              headerLength_ = input.readUInt64();
+              break;
+            }
+            case 16: {
+              bitField0_ |= 0x00000002;
+              contentLength_ = input.readUInt64();
+              break;
+            }
+            case 26: {
+              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
+                stripes_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation>();
+                mutable_bitField0_ |= 0x00000004;
+              }
+              stripes_.add(input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.PARSER, extensionRegistry));
+              break;
+            }
+            case 34: {
+              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
+                types_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.orc.OrcProto.Type>();
+                mutable_bitField0_ |= 0x00000008;
+              }
+              types_.add(input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.PARSER, extensionRegistry));
+              break;
+            }
+            case 42: {
+              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
+                metadata_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem>();
+                mutable_bitField0_ |= 0x00000010;
+              }
+              metadata_.add(input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.PARSER, extensionRegistry));
+              break;
+            }
+            case 48: {
+              bitField0_ |= 0x00000004;
+              numberOfRows_ = input.readUInt64();
+              break;
+            }
+            case 58: {
+              if (!((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
+                statistics_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics>();
+                mutable_bitField0_ |= 0x00000040;
+              }
+              statistics_.add(input.readMessage(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.PARSER, extensionRegistry));
+              break;
+            }
+            case 64: {
+              bitField0_ |= 0x00000008;
+              rowIndexStride_ = input.readUInt32();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
+          stripes_ = java.util.Collections.unmodifiableList(stripes_);
+        }
+        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
+          types_ = java.util.Collections.unmodifiableList(types_);
+        }
+        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
+          metadata_ = java.util.Collections.unmodifiableList(metadata_);
+        }
+        if (((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
+          statistics_ = java.util.Collections.unmodifiableList(statistics_);
+        }
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.class);
     }
-    
+
+    public static com.google.protobuf.Parser<Footer> PARSER =
+        new com.google.protobuf.AbstractParser<Footer>() {
+      public Footer parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new Footer(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<Footer> getParserForType() {
+      return PARSER;
+    }
+
     private int bitField0_;
     // optional uint64 headerLength = 1;
     public static final int HEADERLENGTH_FIELD_NUMBER = 1;
     private long headerLength_;
+    /**
+     * <code>optional uint64 headerLength = 1;</code>
+     */
     public boolean hasHeaderLength() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
+    /**
+     * <code>optional uint64 headerLength = 1;</code>
+     */
     public long getHeaderLength() {
       return headerLength_;
     }
-    
+
     // optional uint64 contentLength = 2;
     public static final int CONTENTLENGTH_FIELD_NUMBER = 2;
     private long contentLength_;
+    /**
+     * <code>optional uint64 contentLength = 2;</code>
+     */
     public boolean hasContentLength() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
+    /**
+     * <code>optional uint64 contentLength = 2;</code>
+     */
     public long getContentLength() {
       return contentLength_;
     }
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;
     public static final int STRIPES_FIELD_NUMBER = 3;
     private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation> stripes_;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+     */
     public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation> getStripesList() {
       return stripes_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+     */
     public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformationOrBuilder> 
         getStripesOrBuilderList() {
       return stripes_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+     */
     public int getStripesCount() {
       return stripes_.size();
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation getStripes(int index) {
       return stripes_.get(index);
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformationOrBuilder getStripesOrBuilder(
         int index) {
       return stripes_.get(index);
     }
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;
     public static final int TYPES_FIELD_NUMBER = 4;
     private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Type> types_;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+     */
     public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Type> getTypesList() {
       return types_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+     */
     public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.TypeOrBuilder> 
         getTypesOrBuilderList() {
       return types_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+     */
     public int getTypesCount() {
       return types_.size();
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.Type getTypes(int index) {
       return types_.get(index);
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.TypeOrBuilder getTypesOrBuilder(
         int index) {
       return types_.get(index);
     }
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;
     public static final int METADATA_FIELD_NUMBER = 5;
     private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem> metadata_;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+     */
     public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem> getMetadataList() {
       return metadata_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+     */
     public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItemOrBuilder> 
         getMetadataOrBuilderList() {
       return metadata_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+     */
     public int getMetadataCount() {
       return metadata_.size();
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem getMetadata(int index) {
       return metadata_.get(index);
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItemOrBuilder getMetadataOrBuilder(
         int index) {
       return metadata_.get(index);
     }
-    
+
     // optional uint64 numberOfRows = 6;
     public static final int NUMBEROFROWS_FIELD_NUMBER = 6;
     private long numberOfRows_;
+    /**
+     * <code>optional uint64 numberOfRows = 6;</code>
+     */
     public boolean hasNumberOfRows() {
       return ((bitField0_ & 0x00000004) == 0x00000004);
     }
+    /**
+     * <code>optional uint64 numberOfRows = 6;</code>
+     */
     public long getNumberOfRows() {
       return numberOfRows_;
     }
-    
+
     // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;
     public static final int STATISTICS_FIELD_NUMBER = 7;
     private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics> statistics_;
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+     */
     public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics> getStatisticsList() {
       return statistics_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+     */
     public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder> 
         getStatisticsOrBuilderList() {
       return statistics_;
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+     */
     public int getStatisticsCount() {
       return statistics_.size();
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics getStatistics(int index) {
       return statistics_.get(index);
     }
+    /**
+     * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder(
         int index) {
       return statistics_.get(index);
     }
-    
+
     // optional uint32 rowIndexStride = 8;
     public static final int ROWINDEXSTRIDE_FIELD_NUMBER = 8;
     private int rowIndexStride_;
+    /**
+     * <code>optional uint32 rowIndexStride = 8;</code>
+     */
     public boolean hasRowIndexStride() {
       return ((bitField0_ & 0x00000008) == 0x00000008);
     }
+    /**
+     * <code>optional uint32 rowIndexStride = 8;</code>
+     */
     public int getRowIndexStride() {
       return rowIndexStride_;
     }
-    
+
     private void initFields() {
       headerLength_ = 0L;
       contentLength_ = 0L;
@@ -8985,7 +11753,7 @@ private void initFields() {
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       for (int i = 0; i < getTypesCount(); i++) {
         if (!getTypes(i).isInitialized()) {
           memoizedIsInitialized = 0;
@@ -9001,7 +11769,7 @@ public final boolean isInitialized() {
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -9031,12 +11799,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -9074,94 +11842,83 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.Footer}
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.FooterOrBuilder {
@@ -9169,18 +11926,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -9195,7 +11955,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         headerLength_ = 0L;
@@ -9232,20 +11992,20 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000080);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer result = buildPartial();
         if (!result.isInitialized()) {
@@ -9253,17 +12013,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer(this);
         int from_bitField0_ = bitField0_;
@@ -9324,7 +12074,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer)other);
@@ -9333,7 +12083,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getDefaultInstance()) return this;
         if (other.hasHeaderLength()) {
@@ -9455,7 +12205,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer other)
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         for (int i = 0; i < getTypesCount(); i++) {
           if (!getTypes(i).isInitialized()) {
@@ -9471,122 +12221,92 @@ public final boolean isInitialized() {
         }
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              headerLength_ = input.readUInt64();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              contentLength_ = input.readUInt64();
-              break;
-            }
-            case 26: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.newBuilder();
-              input.readMessage(subBuilder, extensionRegistry);
-              addStripes(subBuilder.buildPartial());
-              break;
-            }
-            case 34: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.newBuilder();
-              input.readMessage(subBuilder, extensionRegistry);
-              addTypes(subBuilder.buildPartial());
-              break;
-            }
-            case 42: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.newBuilder();
-              input.readMessage(subBuilder, extensionRegistry);
-              addMetadata(subBuilder.buildPartial());
-              break;
-            }
-            case 48: {
-              bitField0_ |= 0x00000020;
-              numberOfRows_ = input.readUInt64();
-              break;
-            }
-            case 58: {
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder subBuilder = org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.newBuilder();
-              input.readMessage(subBuilder, extensionRegistry);
-              addStatistics(subBuilder.buildPartial());
-              break;
-            }
-            case 64: {
-              bitField0_ |= 0x00000080;
-              rowIndexStride_ = input.readUInt32();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // optional uint64 headerLength = 1;
       private long headerLength_ ;
+      /**
+       * <code>optional uint64 headerLength = 1;</code>
+       */
       public boolean hasHeaderLength() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
+      /**
+       * <code>optional uint64 headerLength = 1;</code>
+       */
       public long getHeaderLength() {
         return headerLength_;
       }
+      /**
+       * <code>optional uint64 headerLength = 1;</code>
+       */
       public Builder setHeaderLength(long value) {
         bitField0_ |= 0x00000001;
         headerLength_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 headerLength = 1;</code>
+       */
       public Builder clearHeaderLength() {
         bitField0_ = (bitField0_ & ~0x00000001);
         headerLength_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // optional uint64 contentLength = 2;
       private long contentLength_ ;
+      /**
+       * <code>optional uint64 contentLength = 2;</code>
+       */
       public boolean hasContentLength() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
+      /**
+       * <code>optional uint64 contentLength = 2;</code>
+       */
       public long getContentLength() {
         return contentLength_;
       }
+      /**
+       * <code>optional uint64 contentLength = 2;</code>
+       */
       public Builder setContentLength(long value) {
         bitField0_ |= 0x00000002;
         contentLength_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 contentLength = 2;</code>
+       */
       public Builder clearContentLength() {
         bitField0_ = (bitField0_ & ~0x00000002);
         contentLength_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;
       private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation> stripes_ =
         java.util.Collections.emptyList();
@@ -9596,10 +12316,13 @@ private void ensureStripesIsMutable() {
           bitField0_ |= 0x00000004;
          }
       }
-      
+
       private com.google.protobuf.RepeatedFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation, org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformationOrBuilder> stripesBuilder_;
-      
+
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation> getStripesList() {
         if (stripesBuilder_ == null) {
           return java.util.Collections.unmodifiableList(stripes_);
@@ -9607,6 +12330,9 @@ private void ensureStripesIsMutable() {
           return stripesBuilder_.getMessageList();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public int getStripesCount() {
         if (stripesBuilder_ == null) {
           return stripes_.size();
@@ -9614,6 +12340,9 @@ public int getStripesCount() {
           return stripesBuilder_.getCount();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation getStripes(int index) {
         if (stripesBuilder_ == null) {
           return stripes_.get(index);
@@ -9621,6 +12350,9 @@ public int getStripesCount() {
           return stripesBuilder_.getMessage(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public Builder setStripes(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation value) {
         if (stripesBuilder_ == null) {
@@ -9635,6 +12367,9 @@ public Builder setStripes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public Builder setStripes(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder builderForValue) {
         if (stripesBuilder_ == null) {
@@ -9646,6 +12381,9 @@ public Builder setStripes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public Builder addStripes(org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation value) {
         if (stripesBuilder_ == null) {
           if (value == null) {
@@ -9659,6 +12397,9 @@ public Builder addStripes(org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInform
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public Builder addStripes(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation value) {
         if (stripesBuilder_ == null) {
@@ -9673,6 +12414,9 @@ public Builder addStripes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public Builder addStripes(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder builderForValue) {
         if (stripesBuilder_ == null) {
@@ -9684,6 +12428,9 @@ public Builder addStripes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public Builder addStripes(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder builderForValue) {
         if (stripesBuilder_ == null) {
@@ -9695,6 +12442,9 @@ public Builder addStripes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public Builder addAllStripes(
           java.lang.Iterable<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation> values) {
         if (stripesBuilder_ == null) {
@@ -9706,6 +12456,9 @@ public Builder addAllStripes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public Builder clearStripes() {
         if (stripesBuilder_ == null) {
           stripes_ = java.util.Collections.emptyList();
@@ -9716,6 +12469,9 @@ public Builder clearStripes() {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public Builder removeStripes(int index) {
         if (stripesBuilder_ == null) {
           ensureStripesIsMutable();
@@ -9726,10 +12482,16 @@ public Builder removeStripes(int index) {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder getStripesBuilder(
           int index) {
         return getStripesFieldBuilder().getBuilder(index);
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformationOrBuilder getStripesOrBuilder(
           int index) {
         if (stripesBuilder_ == null) {
@@ -9737,6 +12499,9 @@ public Builder removeStripes(int index) {
           return stripesBuilder_.getMessageOrBuilder(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformationOrBuilder> 
            getStripesOrBuilderList() {
         if (stripesBuilder_ != null) {
@@ -9745,15 +12510,24 @@ public Builder removeStripes(int index) {
           return java.util.Collections.unmodifiableList(stripes_);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder addStripesBuilder() {
         return getStripesFieldBuilder().addBuilder(
             org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder addStripesBuilder(
           int index) {
         return getStripesFieldBuilder().addBuilder(
             index, org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.StripeInformation stripes = 3;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder> 
            getStripesBuilderList() {
         return getStripesFieldBuilder().getBuilderList();
@@ -9772,7 +12546,7 @@ public Builder removeStripes(int index) {
         }
         return stripesBuilder_;
       }
-      
+
       // repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;
       private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Type> types_ =
         java.util.Collections.emptyList();
@@ -9782,10 +12556,13 @@ private void ensureTypesIsMutable() {
           bitField0_ |= 0x00000008;
          }
       }
-      
+
       private com.google.protobuf.RepeatedFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.Type, org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.TypeOrBuilder> typesBuilder_;
-      
+
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Type> getTypesList() {
         if (typesBuilder_ == null) {
           return java.util.Collections.unmodifiableList(types_);
@@ -9793,6 +12570,9 @@ private void ensureTypesIsMutable() {
           return typesBuilder_.getMessageList();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public int getTypesCount() {
         if (typesBuilder_ == null) {
           return types_.size();
@@ -9800,6 +12580,9 @@ public int getTypesCount() {
           return typesBuilder_.getCount();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Type getTypes(int index) {
         if (typesBuilder_ == null) {
           return types_.get(index);
@@ -9807,6 +12590,9 @@ public int getTypesCount() {
           return typesBuilder_.getMessage(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public Builder setTypes(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.Type value) {
         if (typesBuilder_ == null) {
@@ -9821,6 +12607,9 @@ public Builder setTypes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public Builder setTypes(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder builderForValue) {
         if (typesBuilder_ == null) {
@@ -9832,6 +12621,9 @@ public Builder setTypes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public Builder addTypes(org.apache.hadoop.hive.ql.io.orc.OrcProto.Type value) {
         if (typesBuilder_ == null) {
           if (value == null) {
@@ -9845,6 +12637,9 @@ public Builder addTypes(org.apache.hadoop.hive.ql.io.orc.OrcProto.Type value) {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public Builder addTypes(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.Type value) {
         if (typesBuilder_ == null) {
@@ -9859,6 +12654,9 @@ public Builder addTypes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public Builder addTypes(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder builderForValue) {
         if (typesBuilder_ == null) {
@@ -9870,6 +12668,9 @@ public Builder addTypes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public Builder addTypes(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder builderForValue) {
         if (typesBuilder_ == null) {
@@ -9881,6 +12682,9 @@ public Builder addTypes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public Builder addAllTypes(
           java.lang.Iterable<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.Type> values) {
         if (typesBuilder_ == null) {
@@ -9892,6 +12696,9 @@ public Builder addAllTypes(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public Builder clearTypes() {
         if (typesBuilder_ == null) {
           types_ = java.util.Collections.emptyList();
@@ -9902,6 +12709,9 @@ public Builder clearTypes() {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public Builder removeTypes(int index) {
         if (typesBuilder_ == null) {
           ensureTypesIsMutable();
@@ -9912,10 +12722,16 @@ public Builder removeTypes(int index) {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder getTypesBuilder(
           int index) {
         return getTypesFieldBuilder().getBuilder(index);
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.TypeOrBuilder getTypesOrBuilder(
           int index) {
         if (typesBuilder_ == null) {
@@ -9923,6 +12739,9 @@ public Builder removeTypes(int index) {
           return typesBuilder_.getMessageOrBuilder(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.TypeOrBuilder> 
            getTypesOrBuilderList() {
         if (typesBuilder_ != null) {
@@ -9931,15 +12750,24 @@ public Builder removeTypes(int index) {
           return java.util.Collections.unmodifiableList(types_);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder addTypesBuilder() {
         return getTypesFieldBuilder().addBuilder(
             org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder addTypesBuilder(
           int index) {
         return getTypesFieldBuilder().addBuilder(
             index, org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.Type types = 4;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder> 
            getTypesBuilderList() {
         return getTypesFieldBuilder().getBuilderList();
@@ -9958,7 +12786,7 @@ public Builder removeTypes(int index) {
         }
         return typesBuilder_;
       }
-      
+
       // repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;
       private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem> metadata_ =
         java.util.Collections.emptyList();
@@ -9968,10 +12796,13 @@ private void ensureMetadataIsMutable() {
           bitField0_ |= 0x00000010;
          }
       }
-      
+
       private com.google.protobuf.RepeatedFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem, org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItemOrBuilder> metadataBuilder_;
-      
+
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem> getMetadataList() {
         if (metadataBuilder_ == null) {
           return java.util.Collections.unmodifiableList(metadata_);
@@ -9979,6 +12810,9 @@ private void ensureMetadataIsMutable() {
           return metadataBuilder_.getMessageList();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public int getMetadataCount() {
         if (metadataBuilder_ == null) {
           return metadata_.size();
@@ -9986,6 +12820,9 @@ public int getMetadataCount() {
           return metadataBuilder_.getCount();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem getMetadata(int index) {
         if (metadataBuilder_ == null) {
           return metadata_.get(index);
@@ -9993,6 +12830,9 @@ public int getMetadataCount() {
           return metadataBuilder_.getMessage(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public Builder setMetadata(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem value) {
         if (metadataBuilder_ == null) {
@@ -10007,6 +12847,9 @@ public Builder setMetadata(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public Builder setMetadata(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder builderForValue) {
         if (metadataBuilder_ == null) {
@@ -10018,6 +12861,9 @@ public Builder setMetadata(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public Builder addMetadata(org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem value) {
         if (metadataBuilder_ == null) {
           if (value == null) {
@@ -10031,6 +12877,9 @@ public Builder addMetadata(org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadat
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public Builder addMetadata(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem value) {
         if (metadataBuilder_ == null) {
@@ -10045,6 +12894,9 @@ public Builder addMetadata(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public Builder addMetadata(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder builderForValue) {
         if (metadataBuilder_ == null) {
@@ -10056,6 +12908,9 @@ public Builder addMetadata(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public Builder addMetadata(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder builderForValue) {
         if (metadataBuilder_ == null) {
@@ -10067,6 +12922,9 @@ public Builder addMetadata(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public Builder addAllMetadata(
           java.lang.Iterable<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem> values) {
         if (metadataBuilder_ == null) {
@@ -10078,6 +12936,9 @@ public Builder addAllMetadata(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public Builder clearMetadata() {
         if (metadataBuilder_ == null) {
           metadata_ = java.util.Collections.emptyList();
@@ -10088,6 +12949,9 @@ public Builder clearMetadata() {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public Builder removeMetadata(int index) {
         if (metadataBuilder_ == null) {
           ensureMetadataIsMutable();
@@ -10098,10 +12962,16 @@ public Builder removeMetadata(int index) {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder getMetadataBuilder(
           int index) {
         return getMetadataFieldBuilder().getBuilder(index);
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItemOrBuilder getMetadataOrBuilder(
           int index) {
         if (metadataBuilder_ == null) {
@@ -10109,6 +12979,9 @@ public Builder removeMetadata(int index) {
           return metadataBuilder_.getMessageOrBuilder(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItemOrBuilder> 
            getMetadataOrBuilderList() {
         if (metadataBuilder_ != null) {
@@ -10117,15 +12990,24 @@ public Builder removeMetadata(int index) {
           return java.util.Collections.unmodifiableList(metadata_);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder addMetadataBuilder() {
         return getMetadataFieldBuilder().addBuilder(
             org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder addMetadataBuilder(
           int index) {
         return getMetadataFieldBuilder().addBuilder(
             index, org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.UserMetadataItem metadata = 5;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder> 
            getMetadataBuilderList() {
         return getMetadataFieldBuilder().getBuilderList();
@@ -10144,28 +13026,40 @@ public Builder removeMetadata(int index) {
         }
         return metadataBuilder_;
       }
-      
+
       // optional uint64 numberOfRows = 6;
       private long numberOfRows_ ;
+      /**
+       * <code>optional uint64 numberOfRows = 6;</code>
+       */
       public boolean hasNumberOfRows() {
         return ((bitField0_ & 0x00000020) == 0x00000020);
       }
+      /**
+       * <code>optional uint64 numberOfRows = 6;</code>
+       */
       public long getNumberOfRows() {
         return numberOfRows_;
       }
+      /**
+       * <code>optional uint64 numberOfRows = 6;</code>
+       */
       public Builder setNumberOfRows(long value) {
         bitField0_ |= 0x00000020;
         numberOfRows_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 numberOfRows = 6;</code>
+       */
       public Builder clearNumberOfRows() {
         bitField0_ = (bitField0_ & ~0x00000020);
         numberOfRows_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;
       private java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics> statistics_ =
         java.util.Collections.emptyList();
@@ -10175,10 +13069,13 @@ private void ensureStatisticsIsMutable() {
           bitField0_ |= 0x00000040;
          }
       }
-      
+
       private com.google.protobuf.RepeatedFieldBuilder<
           org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder> statisticsBuilder_;
-      
+
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics> getStatisticsList() {
         if (statisticsBuilder_ == null) {
           return java.util.Collections.unmodifiableList(statistics_);
@@ -10186,6 +13083,9 @@ private void ensureStatisticsIsMutable() {
           return statisticsBuilder_.getMessageList();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public int getStatisticsCount() {
         if (statisticsBuilder_ == null) {
           return statistics_.size();
@@ -10193,6 +13093,9 @@ public int getStatisticsCount() {
           return statisticsBuilder_.getCount();
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics getStatistics(int index) {
         if (statisticsBuilder_ == null) {
           return statistics_.get(index);
@@ -10200,6 +13103,9 @@ public int getStatisticsCount() {
           return statisticsBuilder_.getMessage(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public Builder setStatistics(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics value) {
         if (statisticsBuilder_ == null) {
@@ -10214,6 +13120,9 @@ public Builder setStatistics(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public Builder setStatistics(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder builderForValue) {
         if (statisticsBuilder_ == null) {
@@ -10225,6 +13134,9 @@ public Builder setStatistics(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public Builder addStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics value) {
         if (statisticsBuilder_ == null) {
           if (value == null) {
@@ -10238,6 +13150,9 @@ public Builder addStatistics(org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnSta
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public Builder addStatistics(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics value) {
         if (statisticsBuilder_ == null) {
@@ -10252,6 +13167,9 @@ public Builder addStatistics(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public Builder addStatistics(
           org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder builderForValue) {
         if (statisticsBuilder_ == null) {
@@ -10263,6 +13181,9 @@ public Builder addStatistics(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public Builder addStatistics(
           int index, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder builderForValue) {
         if (statisticsBuilder_ == null) {
@@ -10274,6 +13195,9 @@ public Builder addStatistics(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public Builder addAllStatistics(
           java.lang.Iterable<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics> values) {
         if (statisticsBuilder_ == null) {
@@ -10285,6 +13209,9 @@ public Builder addAllStatistics(
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public Builder clearStatistics() {
         if (statisticsBuilder_ == null) {
           statistics_ = java.util.Collections.emptyList();
@@ -10295,6 +13222,9 @@ public Builder clearStatistics() {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public Builder removeStatistics(int index) {
         if (statisticsBuilder_ == null) {
           ensureStatisticsIsMutable();
@@ -10305,10 +13235,16 @@ public Builder removeStatistics(int index) {
         }
         return this;
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder getStatisticsBuilder(
           int index) {
         return getStatisticsFieldBuilder().getBuilder(index);
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder(
           int index) {
         if (statisticsBuilder_ == null) {
@@ -10316,6 +13252,9 @@ public Builder removeStatistics(int index) {
           return statisticsBuilder_.getMessageOrBuilder(index);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public java.util.List<? extends org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatisticsOrBuilder> 
            getStatisticsOrBuilderList() {
         if (statisticsBuilder_ != null) {
@@ -10324,15 +13263,24 @@ public Builder removeStatistics(int index) {
           return java.util.Collections.unmodifiableList(statistics_);
         }
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder addStatisticsBuilder() {
         return getStatisticsFieldBuilder().addBuilder(
             org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder addStatisticsBuilder(
           int index) {
         return getStatisticsFieldBuilder().addBuilder(
             index, org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDefaultInstance());
       }
+      /**
+       * <code>repeated .org.apache.hadoop.hive.ql.io.orc.ColumnStatistics statistics = 7;</code>
+       */
       public java.util.List<org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder> 
            getStatisticsBuilderList() {
         return getStatisticsFieldBuilder().getBuilderList();
@@ -10351,185 +13299,413 @@ public Builder removeStatistics(int index) {
         }
         return statisticsBuilder_;
       }
-      
+
       // optional uint32 rowIndexStride = 8;
       private int rowIndexStride_ ;
+      /**
+       * <code>optional uint32 rowIndexStride = 8;</code>
+       */
       public boolean hasRowIndexStride() {
         return ((bitField0_ & 0x00000080) == 0x00000080);
       }
+      /**
+       * <code>optional uint32 rowIndexStride = 8;</code>
+       */
       public int getRowIndexStride() {
         return rowIndexStride_;
       }
+      /**
+       * <code>optional uint32 rowIndexStride = 8;</code>
+       */
       public Builder setRowIndexStride(int value) {
         bitField0_ |= 0x00000080;
         rowIndexStride_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint32 rowIndexStride = 8;</code>
+       */
       public Builder clearRowIndexStride() {
         bitField0_ = (bitField0_ & ~0x00000080);
         rowIndexStride_ = 0;
         onChanged();
         return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.Footer)
     }
-    
+
     static {
       defaultInstance = new Footer(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.Footer)
   }
-  
+
   public interface PostScriptOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
-    
+
     // optional uint64 footerLength = 1;
+    /**
+     * <code>optional uint64 footerLength = 1;</code>
+     */
     boolean hasFooterLength();
+    /**
+     * <code>optional uint64 footerLength = 1;</code>
+     */
     long getFooterLength();
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
+     */
     boolean hasCompression();
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
+     */
     org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind getCompression();
-    
+
     // optional uint64 compressionBlockSize = 3;
+    /**
+     * <code>optional uint64 compressionBlockSize = 3;</code>
+     */
     boolean hasCompressionBlockSize();
+    /**
+     * <code>optional uint64 compressionBlockSize = 3;</code>
+     */
     long getCompressionBlockSize();
-    
+
     // repeated uint32 version = 4 [packed = true];
+    /**
+     * <code>repeated uint32 version = 4 [packed = true];</code>
+     */
     java.util.List<java.lang.Integer> getVersionList();
+    /**
+     * <code>repeated uint32 version = 4 [packed = true];</code>
+     */
     int getVersionCount();
+    /**
+     * <code>repeated uint32 version = 4 [packed = true];</code>
+     */
     int getVersion(int index);
-    
+
     // optional string magic = 8000;
+    /**
+     * <code>optional string magic = 8000;</code>
+     *
+     * <pre>
+     * Leave this last in the record
+     * </pre>
+     */
     boolean hasMagic();
-    String getMagic();
+    /**
+     * <code>optional string magic = 8000;</code>
+     *
+     * <pre>
+     * Leave this last in the record
+     * </pre>
+     */
+    java.lang.String getMagic();
+    /**
+     * <code>optional string magic = 8000;</code>
+     *
+     * <pre>
+     * Leave this last in the record
+     * </pre>
+     */
+    com.google.protobuf.ByteString
+        getMagicBytes();
   }
+  /**
+   * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.PostScript}
+   *
+   * <pre>
+   * Serialized length must be less that 255 bytes
+   * </pre>
+   */
   public static final class PostScript extends
       com.google.protobuf.GeneratedMessage
       implements PostScriptOrBuilder {
     // Use PostScript.newBuilder() to construct.
-    private PostScript(Builder builder) {
+    private PostScript(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    private PostScript(boolean noInit) {}
-    
+    private PostScript(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
     private static final PostScript defaultInstance;
     public static PostScript getDefaultInstance() {
       return defaultInstance;
     }
-    
+
     public PostScript getDefaultInstanceForType() {
       return defaultInstance;
     }
-    
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private PostScript(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              bitField0_ |= 0x00000001;
+              footerLength_ = input.readUInt64();
+              break;
+            }
+            case 16: {
+              int rawValue = input.readEnum();
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind value = org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.valueOf(rawValue);
+              if (value == null) {
+                unknownFields.mergeVarintField(2, rawValue);
+              } else {
+                bitField0_ |= 0x00000002;
+                compression_ = value;
+              }
+              break;
+            }
+            case 24: {
+              bitField0_ |= 0x00000004;
+              compressionBlockSize_ = input.readUInt64();
+              break;
+            }
+            case 32: {
+              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
+                version_ = new java.util.ArrayList<java.lang.Integer>();
+                mutable_bitField0_ |= 0x00000008;
+              }
+              version_.add(input.readUInt32());
+              break;
+            }
+            case 34: {
+              int length = input.readRawVarint32();
+              int limit = input.pushLimit(length);
+              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008) && input.getBytesUntilLimit() > 0) {
+                version_ = new java.util.ArrayList<java.lang.Integer>();
+                mutable_bitField0_ |= 0x00000008;
+              }
+              while (input.getBytesUntilLimit() > 0) {
+                version_.add(input.readUInt32());
+              }
+              input.popLimit(limit);
+              break;
+            }
+            case 64002: {
+              bitField0_ |= 0x00000008;
+              magic_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
+          version_ = java.util.Collections.unmodifiableList(version_);
+        }
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor;
     }
-    
+
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable;
+      return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.class);
     }
-    
+
+    public static com.google.protobuf.Parser<PostScript> PARSER =
+        new com.google.protobuf.AbstractParser<PostScript>() {
+      public PostScript parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new PostScript(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<PostScript> getParserForType() {
+      return PARSER;
+    }
+
     private int bitField0_;
     // optional uint64 footerLength = 1;
     public static final int FOOTERLENGTH_FIELD_NUMBER = 1;
     private long footerLength_;
+    /**
+     * <code>optional uint64 footerLength = 1;</code>
+     */
     public boolean hasFooterLength() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
+    /**
+     * <code>optional uint64 footerLength = 1;</code>
+     */
     public long getFooterLength() {
       return footerLength_;
     }
-    
+
     // optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;
     public static final int COMPRESSION_FIELD_NUMBER = 2;
     private org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind compression_;
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
+     */
     public boolean hasCompression() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
+    /**
+     * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
+     */
     public org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind getCompression() {
       return compression_;
     }
-    
+
     // optional uint64 compressionBlockSize = 3;
     public static final int COMPRESSIONBLOCKSIZE_FIELD_NUMBER = 3;
     private long compressionBlockSize_;
+    /**
+     * <code>optional uint64 compressionBlockSize = 3;</code>
+     */
     public boolean hasCompressionBlockSize() {
       return ((bitField0_ & 0x00000004) == 0x00000004);
     }
+    /**
+     * <code>optional uint64 compressionBlockSize = 3;</code>
+     */
     public long getCompressionBlockSize() {
       return compressionBlockSize_;
     }
-    
+
     // repeated uint32 version = 4 [packed = true];
     public static final int VERSION_FIELD_NUMBER = 4;
     private java.util.List<java.lang.Integer> version_;
+    /**
+     * <code>repeated uint32 version = 4 [packed = true];</code>
+     */
     public java.util.List<java.lang.Integer>
         getVersionList() {
       return version_;
     }
+    /**
+     * <code>repeated uint32 version = 4 [packed = true];</code>
+     */
     public int getVersionCount() {
       return version_.size();
     }
+    /**
+     * <code>repeated uint32 version = 4 [packed = true];</code>
+     */
     public int getVersion(int index) {
       return version_.get(index);
     }
     private int versionMemoizedSerializedSize = -1;
-    
+
     // optional string magic = 8000;
     public static final int MAGIC_FIELD_NUMBER = 8000;
     private java.lang.Object magic_;
+    /**
+     * <code>optional string magic = 8000;</code>
+     *
+     * <pre>
+     * Leave this last in the record
+     * </pre>
+     */
     public boolean hasMagic() {
       return ((bitField0_ & 0x00000008) == 0x00000008);
     }
-    public String getMagic() {
+    /**
+     * <code>optional string magic = 8000;</code>
+     *
+     * <pre>
+     * Leave this last in the record
+     * </pre>
+     */
+    public java.lang.String getMagic() {
       java.lang.Object ref = magic_;
-      if (ref instanceof String) {
-        return (String) ref;
+      if (ref instanceof java.lang.String) {
+        return (java.lang.String) ref;
       } else {
         com.google.protobuf.ByteString bs = 
             (com.google.protobuf.ByteString) ref;
-        String s = bs.toStringUtf8();
-        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+        java.lang.String s = bs.toStringUtf8();
+        if (bs.isValidUtf8()) {
           magic_ = s;
         }
         return s;
       }
     }
-    private com.google.protobuf.ByteString getMagicBytes() {
+    /**
+     * <code>optional string magic = 8000;</code>
+     *
+     * <pre>
+     * Leave this last in the record
+     * </pre>
+     */
+    public com.google.protobuf.ByteString
+        getMagicBytes() {
       java.lang.Object ref = magic_;
-      if (ref instanceof String) {
+      if (ref instanceof java.lang.String) {
         com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+            com.google.protobuf.ByteString.copyFromUtf8(
+                (java.lang.String) ref);
         magic_ = b;
         return b;
       } else {
         return (com.google.protobuf.ByteString) ref;
       }
     }
-    
+
     private void initFields() {
       footerLength_ = 0L;
       compression_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.NONE;
       compressionBlockSize_ = 0L;
-      version_ = java.util.Collections.emptyList();;
+      version_ = java.util.Collections.emptyList();
       magic_ = "";
     }
     private byte memoizedIsInitialized = -1;
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
       if (isInitialized != -1) return isInitialized == 1;
-      
+
       memoizedIsInitialized = 1;
       return true;
     }
-    
+
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
@@ -10554,12 +13730,12 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       }
       getUnknownFields().writeTo(output);
     }
-    
+
     private int memoizedSerializedSize = -1;
     public int getSerializedSize() {
       int size = memoizedSerializedSize;
       if (size != -1) return size;
-    
+
       size = 0;
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
         size += com.google.protobuf.CodedOutputStream
@@ -10595,94 +13771,87 @@ public int getSerializedSize() {
       memoizedSerializedSize = size;
       return size;
     }
-    
+
     private static final long serialVersionUID = 0L;
     @java.lang.Override
     protected java.lang.Object writeReplace()
         throws java.io.ObjectStreamException {
       return super.writeReplace();
     }
-    
+
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript parseFrom(
         com.google.protobuf.ByteString data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript parseFrom(byte[] data)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
+      return PARSER.parseFrom(data);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript parseFrom(
         byte[] data,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(data, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
+      return PARSER.parseFrom(input);
     }
     public static org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    
+
     public static Builder newBuilder() { return Builder.create(); }
     public Builder newBuilderForType() { return newBuilder(); }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript prototype) {
       return newBuilder().mergeFrom(prototype);
     }
     public Builder toBuilder() { return newBuilder(this); }
-    
+
     @java.lang.Override
     protected Builder newBuilderForType(
         com.google.protobuf.GeneratedMessage.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
+    /**
+     * Protobuf type {@code org.apache.hadoop.hive.ql.io.orc.PostScript}
+     *
+     * <pre>
+     * Serialized length must be less that 255 bytes
+     * </pre>
+     */
     public static final class Builder extends
         com.google.protobuf.GeneratedMessage.Builder<Builder>
        implements org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScriptOrBuilder {
@@ -10690,18 +13859,21 @@ protected Builder newBuilderForType(
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor;
       }
-      
+
       protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
           internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable;
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.class, org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.class);
       }
-      
+
       // Construct using org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.newBuilder()
       private Builder() {
         maybeForceBuilderInitialization();
       }
-      
-      private Builder(BuilderParent parent) {
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
@@ -10712,7 +13884,7 @@ private void maybeForceBuilderInitialization() {
       private static Builder create() {
         return new Builder();
       }
-      
+
       public Builder clear() {
         super.clear();
         footerLength_ = 0L;
@@ -10721,26 +13893,26 @@ public Builder clear() {
         bitField0_ = (bitField0_ & ~0x00000002);
         compressionBlockSize_ = 0L;
         bitField0_ = (bitField0_ & ~0x00000004);
-        version_ = java.util.Collections.emptyList();;
+        version_ = java.util.Collections.emptyList();
         bitField0_ = (bitField0_ & ~0x00000008);
         magic_ = "";
         bitField0_ = (bitField0_ & ~0x00000010);
         return this;
       }
-      
+
       public Builder clone() {
         return create().mergeFrom(buildPartial());
       }
-      
+
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
-        return org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getDescriptor();
+        return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor;
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getDefaultInstance();
       }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript build() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript result = buildPartial();
         if (!result.isInitialized()) {
@@ -10748,17 +13920,7 @@ public Builder clone() {
         }
         return result;
       }
-      
-      private org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
+
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript buildPartial() {
         org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript result = new org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript(this);
         int from_bitField0_ = bitField0_;
@@ -10788,7 +13950,7 @@ public Builder clone() {
         onBuilt();
         return result;
       }
-      
+
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript)other);
@@ -10797,7 +13959,7 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
           return this;
         }
       }
-      
+
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript other) {
         if (other == org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getDefaultInstance()) return this;
         if (other.hasFooterLength()) {
@@ -10820,114 +13982,87 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript ot
           onChanged();
         }
         if (other.hasMagic()) {
-          setMagic(other.getMagic());
+          bitField0_ |= 0x00000010;
+          magic_ = other.magic_;
+          onChanged();
         }
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-      
+
       public final boolean isInitialized() {
         return true;
       }
-      
+
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              footerLength_ = input.readUInt64();
-              break;
-            }
-            case 16: {
-              int rawValue = input.readEnum();
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind value = org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.valueOf(rawValue);
-              if (value == null) {
-                unknownFields.mergeVarintField(2, rawValue);
-              } else {
-                bitField0_ |= 0x00000002;
-                compression_ = value;
-              }
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              compressionBlockSize_ = input.readUInt64();
-              break;
-            }
-            case 32: {
-              ensureVersionIsMutable();
-              version_.add(input.readUInt32());
-              break;
-            }
-            case 34: {
-              int length = input.readRawVarint32();
-              int limit = input.pushLimit(length);
-              while (input.getBytesUntilLimit() > 0) {
-                addVersion(input.readUInt32());
-              }
-              input.popLimit(limit);
-              break;
-            }
-            case 64002: {
-              bitField0_ |= 0x00000010;
-              magic_ = input.readBytes();
-              break;
-            }
+        org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
           }
         }
+        return this;
       }
-      
       private int bitField0_;
-      
+
       // optional uint64 footerLength = 1;
       private long footerLength_ ;
+      /**
+       * <code>optional uint64 footerLength = 1;</code>
+       */
       public boolean hasFooterLength() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
+      /**
+       * <code>optional uint64 footerLength = 1;</code>
+       */
       public long getFooterLength() {
         return footerLength_;
       }
+      /**
+       * <code>optional uint64 footerLength = 1;</code>
+       */
       public Builder setFooterLength(long value) {
         bitField0_ |= 0x00000001;
         footerLength_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 footerLength = 1;</code>
+       */
       public Builder clearFooterLength() {
         bitField0_ = (bitField0_ & ~0x00000001);
         footerLength_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;
       private org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind compression_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.NONE;
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
+       */
       public boolean hasCompression() {
         return ((bitField0_ & 0x00000002) == 0x00000002);
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
+       */
       public org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind getCompression() {
         return compression_;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
+       */
       public Builder setCompression(org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind value) {
         if (value == null) {
           throw new NullPointerException();
@@ -10937,52 +14072,79 @@ public Builder setCompression(org.apache.hadoop.hive.ql.io.orc.OrcProto.Compress
         onChanged();
         return this;
       }
+      /**
+       * <code>optional .org.apache.hadoop.hive.ql.io.orc.CompressionKind compression = 2;</code>
+       */
       public Builder clearCompression() {
         bitField0_ = (bitField0_ & ~0x00000002);
         compression_ = org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.NONE;
         onChanged();
         return this;
       }
-      
+
       // optional uint64 compressionBlockSize = 3;
       private long compressionBlockSize_ ;
+      /**
+       * <code>optional uint64 compressionBlockSize = 3;</code>
+       */
       public boolean hasCompressionBlockSize() {
         return ((bitField0_ & 0x00000004) == 0x00000004);
       }
+      /**
+       * <code>optional uint64 compressionBlockSize = 3;</code>
+       */
       public long getCompressionBlockSize() {
         return compressionBlockSize_;
       }
+      /**
+       * <code>optional uint64 compressionBlockSize = 3;</code>
+       */
       public Builder setCompressionBlockSize(long value) {
         bitField0_ |= 0x00000004;
         compressionBlockSize_ = value;
         onChanged();
         return this;
       }
+      /**
+       * <code>optional uint64 compressionBlockSize = 3;</code>
+       */
       public Builder clearCompressionBlockSize() {
         bitField0_ = (bitField0_ & ~0x00000004);
         compressionBlockSize_ = 0L;
         onChanged();
         return this;
       }
-      
+
       // repeated uint32 version = 4 [packed = true];
-      private java.util.List<java.lang.Integer> version_ = java.util.Collections.emptyList();;
+      private java.util.List<java.lang.Integer> version_ = java.util.Collections.emptyList();
       private void ensureVersionIsMutable() {
         if (!((bitField0_ & 0x00000008) == 0x00000008)) {
           version_ = new java.util.ArrayList<java.lang.Integer>(version_);
           bitField0_ |= 0x00000008;
          }
       }
+      /**
+       * <code>repeated uint32 version = 4 [packed = true];</code>
+       */
       public java.util.List<java.lang.Integer>
           getVersionList() {
         return java.util.Collections.unmodifiableList(version_);
       }
+      /**
+       * <code>repeated uint32 version = 4 [packed = true];</code>
+       */
       public int getVersionCount() {
         return version_.size();
       }
+      /**
+       * <code>repeated uint32 version = 4 [packed = true];</code>
+       */
       public int getVersion(int index) {
         return version_.get(index);
       }
+      /**
+       * <code>repeated uint32 version = 4 [packed = true];</code>
+       */
       public Builder setVersion(
           int index, int value) {
         ensureVersionIsMutable();
@@ -10990,12 +14152,18 @@ public Builder setVersion(
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint32 version = 4 [packed = true];</code>
+       */
       public Builder addVersion(int value) {
         ensureVersionIsMutable();
         version_.add(value);
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint32 version = 4 [packed = true];</code>
+       */
       public Builder addAllVersion(
           java.lang.Iterable<? extends java.lang.Integer> values) {
         ensureVersionIsMutable();
@@ -11003,29 +14171,75 @@ public Builder addAllVersion(
         onChanged();
         return this;
       }
+      /**
+       * <code>repeated uint32 version = 4 [packed = true];</code>
+       */
       public Builder clearVersion() {
-        version_ = java.util.Collections.emptyList();;
+        version_ = java.util.Collections.emptyList();
         bitField0_ = (bitField0_ & ~0x00000008);
         onChanged();
         return this;
       }
-      
+
       // optional string magic = 8000;
       private java.lang.Object magic_ = "";
+      /**
+       * <code>optional string magic = 8000;</code>
+       *
+       * <pre>
+       * Leave this last in the record
+       * </pre>
+       */
       public boolean hasMagic() {
         return ((bitField0_ & 0x00000010) == 0x00000010);
       }
-      public String getMagic() {
+      /**
+       * <code>optional string magic = 8000;</code>
+       *
+       * <pre>
+       * Leave this last in the record
+       * </pre>
+       */
+      public java.lang.String getMagic() {
         java.lang.Object ref = magic_;
-        if (!(ref instanceof String)) {
-          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+        if (!(ref instanceof java.lang.String)) {
+          java.lang.String s = ((com.google.protobuf.ByteString) ref)
+              .toStringUtf8();
           magic_ = s;
           return s;
         } else {
-          return (String) ref;
+          return (java.lang.String) ref;
         }
       }
-      public Builder setMagic(String value) {
+      /**
+       * <code>optional string magic = 8000;</code>
+       *
+       * <pre>
+       * Leave this last in the record
+       * </pre>
+       */
+      public com.google.protobuf.ByteString
+          getMagicBytes() {
+        java.lang.Object ref = magic_;
+        if (ref instanceof String) {
+          com.google.protobuf.ByteString b = 
+              com.google.protobuf.ByteString.copyFromUtf8(
+                  (java.lang.String) ref);
+          magic_ = b;
+          return b;
+        } else {
+          return (com.google.protobuf.ByteString) ref;
+        }
+      }
+      /**
+       * <code>optional string magic = 8000;</code>
+       *
+       * <pre>
+       * Leave this last in the record
+       * </pre>
+       */
+      public Builder setMagic(
+          java.lang.String value) {
         if (value == null) {
     throw new NullPointerException();
   }
@@ -11034,29 +14248,48 @@ public Builder setMagic(String value) {
         onChanged();
         return this;
       }
+      /**
+       * <code>optional string magic = 8000;</code>
+       *
+       * <pre>
+       * Leave this last in the record
+       * </pre>
+       */
       public Builder clearMagic() {
         bitField0_ = (bitField0_ & ~0x00000010);
         magic_ = getDefaultInstance().getMagic();
         onChanged();
         return this;
       }
-      void setMagic(com.google.protobuf.ByteString value) {
-        bitField0_ |= 0x00000010;
+      /**
+       * <code>optional string magic = 8000;</code>
+       *
+       * <pre>
+       * Leave this last in the record
+       * </pre>
+       */
+      public Builder setMagicBytes(
+          com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000010;
         magic_ = value;
         onChanged();
+        return this;
       }
-      
+
       // @@protoc_insertion_point(builder_scope:org.apache.hadoop.hive.ql.io.orc.PostScript)
     }
-    
+
     static {
       defaultInstance = new PostScript(true);
       defaultInstance.initFields();
     }
-    
+
     // @@protoc_insertion_point(class_scope:org.apache.hadoop.hive.ql.io.orc.PostScript)
   }
-  
+
   private static com.google.protobuf.Descriptors.Descriptor
     internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
   private static
@@ -11142,7 +14375,7 @@ void setMagic(com.google.protobuf.ByteString value) {
   private static
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
       internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable;
-  
+
   public static com.google.protobuf.Descriptors.FileDescriptor
       getDescriptor() {
     return descriptor;
@@ -11231,137 +14464,103 @@ void setMagic(com.google.protobuf.ByteString value) {
           internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor,
-              new java.lang.String[] { "Minimum", "Maximum", "Sum", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.class);
+              new java.lang.String[] { "Minimum", "Maximum", "Sum", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor =
             getDescriptor().getMessageTypes().get(1);
           internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor,
-              new java.lang.String[] { "Minimum", "Maximum", "Sum", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.class);
+              new java.lang.String[] { "Minimum", "Maximum", "Sum", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor =
             getDescriptor().getMessageTypes().get(2);
           internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor,
-              new java.lang.String[] { "Minimum", "Maximum", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.class);
+              new java.lang.String[] { "Minimum", "Maximum", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor =
             getDescriptor().getMessageTypes().get(3);
           internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor,
-              new java.lang.String[] { "Count", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.class);
+              new java.lang.String[] { "Count", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor =
             getDescriptor().getMessageTypes().get(4);
           internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor,
-              new java.lang.String[] { "Minimum", "Maximum", "Sum", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.class);
+              new java.lang.String[] { "Minimum", "Maximum", "Sum", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor =
             getDescriptor().getMessageTypes().get(5);
           internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor,
-              new java.lang.String[] { "Minimum", "Maximum", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.class);
+              new java.lang.String[] { "Minimum", "Maximum", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor =
             getDescriptor().getMessageTypes().get(6);
           internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor,
-              new java.lang.String[] { "NumberOfValues", "IntStatistics", "DoubleStatistics", "StringStatistics", "BucketStatistics", "DecimalStatistics", "DateStatistics", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.class);
+              new java.lang.String[] { "NumberOfValues", "IntStatistics", "DoubleStatistics", "StringStatistics", "BucketStatistics", "DecimalStatistics", "DateStatistics", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor =
             getDescriptor().getMessageTypes().get(7);
           internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor,
-              new java.lang.String[] { "Positions", "Statistics", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.class);
+              new java.lang.String[] { "Positions", "Statistics", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor =
             getDescriptor().getMessageTypes().get(8);
           internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor,
-              new java.lang.String[] { "Entry", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.class);
+              new java.lang.String[] { "Entry", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor =
             getDescriptor().getMessageTypes().get(9);
           internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor,
-              new java.lang.String[] { "Kind", "Column", "Length", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.class);
+              new java.lang.String[] { "Kind", "Column", "Length", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor =
             getDescriptor().getMessageTypes().get(10);
           internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor,
-              new java.lang.String[] { "Kind", "DictionarySize", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.class);
+              new java.lang.String[] { "Kind", "DictionarySize", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor =
             getDescriptor().getMessageTypes().get(11);
           internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor,
-              new java.lang.String[] { "Streams", "Columns", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.class);
+              new java.lang.String[] { "Streams", "Columns", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor =
             getDescriptor().getMessageTypes().get(12);
           internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor,
-              new java.lang.String[] { "Kind", "Subtypes", "FieldNames", "MaximumLength", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.class);
+              new java.lang.String[] { "Kind", "Subtypes", "FieldNames", "MaximumLength", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor =
             getDescriptor().getMessageTypes().get(13);
           internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor,
-              new java.lang.String[] { "Offset", "IndexLength", "DataLength", "FooterLength", "NumberOfRows", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.class);
+              new java.lang.String[] { "Offset", "IndexLength", "DataLength", "FooterLength", "NumberOfRows", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor =
             getDescriptor().getMessageTypes().get(14);
           internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor,
-              new java.lang.String[] { "Name", "Value", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.class);
+              new java.lang.String[] { "Name", "Value", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor =
             getDescriptor().getMessageTypes().get(15);
           internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor,
-              new java.lang.String[] { "HeaderLength", "ContentLength", "Stripes", "Types", "Metadata", "NumberOfRows", "Statistics", "RowIndexStride", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.class);
+              new java.lang.String[] { "HeaderLength", "ContentLength", "Stripes", "Types", "Metadata", "NumberOfRows", "Statistics", "RowIndexStride", });
           internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor =
             getDescriptor().getMessageTypes().get(16);
           internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor,
-              new java.lang.String[] { "FooterLength", "Compression", "CompressionBlockSize", "Version", "Magic", },
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.class,
-              org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.class);
+              new java.lang.String[] { "FooterLength", "Compression", "CompressionBlockSize", "Version", "Magic", });
           return null;
         }
       };
@@ -11370,6 +14569,6 @@ void setMagic(com.google.protobuf.ByteString value) {
         new com.google.protobuf.Descriptors.FileDescriptor[] {
         }, assigner);
   }
-  
+
   // @@protoc_insertion_point(outer_class_scope)
 }
-- 
1.7.0.4

