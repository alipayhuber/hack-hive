From 70ab768c0817dbed6bcfda4712b3b41e353ca6d8 Mon Sep 17 00:00:00 2001
From: Ashutosh Chauhan <hashutosh@apache.org>
Date: Wed, 25 Sep 2013 22:59:23 +0000
Subject: [PATCH 055/375] HIVE-5318 : Import Throws Error when Importing from a table export Hive 0.9 to Hive 0.10 (Xuefu Zhang via Ashutosh Chauhan)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1526325 13f79535-47bb-0310-9956-ffa450edef68
---
 build-common.xml                                   |    2 +-
 data/files/exported_table/_metadata                |    1 +
 data/files/exported_table/data/data                |    2 +
 .../hadoop/hive/ql/plan/CreateTableDesc.java       |   25 +++++++++++--------
 .../queries/clientpositive/import_exported_table.q |   10 ++++++++
 .../clientpositive/import_exported_table.q.out     |   22 +++++++++++++++++
 6 files changed, 50 insertions(+), 12 deletions(-)
 create mode 100644 data/files/exported_table/_metadata
 create mode 100644 data/files/exported_table/data/data
 create mode 100644 ql/src/test/queries/clientpositive/import_exported_table.q
 create mode 100644 ql/src/test/results/clientpositive/import_exported_table.q.out

diff --git a/src/build-common.xml b/src/build-common.xml
index c76b7a4..c6de54e 100644
--- a/src/build-common.xml
+++ b/src/build-common.xml
@@ -59,7 +59,7 @@
   <property name="test.output" value="true"/>
   <property name="test.junit.output.format" value="xml"/>
   <property name="test.junit.output.usefile" value="true"/>
-  <property name="minimr.query.files" value="list_bucket_dml_10.q,input16_cc.q,scriptfile1.q,scriptfile1_win.q,bucket4.q,bucketmapjoin6.q,disable_merge_for_bucketing.q,reduce_deduplicate.q,smb_mapjoin_8.q,join1.q,groupby2.q,bucketizedhiveinputformat.q,bucketmapjoin7.q,optrstat_groupby.q,bucket_num_reducers.q,bucket5.q,load_fs2.q,bucket_num_reducers2.q,infer_bucket_sort_merge.q,infer_bucket_sort_reducers_power_two.q,infer_bucket_sort_dyn_part.q,infer_bucket_sort_bucketed_table.q,infer_bucket_sort_map_operators.q,infer_bucket_sort_num_buckets.q,leftsemijoin_mr.q,schemeAuthority.q,schemeAuthority2.q,truncate_column_buckets.q,remote_script.q,,load_hdfs_file_with_space_in_the_name.q,parallel_orderby.q"/>
+  <property name="minimr.query.files" value="list_bucket_dml_10.q,input16_cc.q,scriptfile1.q,scriptfile1_win.q,bucket4.q,bucketmapjoin6.q,disable_merge_for_bucketing.q,reduce_deduplicate.q,smb_mapjoin_8.q,join1.q,groupby2.q,bucketizedhiveinputformat.q,bucketmapjoin7.q,optrstat_groupby.q,bucket_num_reducers.q,bucket5.q,load_fs2.q,bucket_num_reducers2.q,infer_bucket_sort_merge.q,infer_bucket_sort_reducers_power_two.q,infer_bucket_sort_dyn_part.q,infer_bucket_sort_bucketed_table.q,infer_bucket_sort_map_operators.q,infer_bucket_sort_num_buckets.q,leftsemijoin_mr.q,schemeAuthority.q,schemeAuthority2.q,truncate_column_buckets.q,remote_script.q,,load_hdfs_file_with_space_in_the_name.q,parallel_orderby.q,import_exported_table.q"/>
   <property name="minimr.query.negative.files" value="cluster_tasklog_retrieval.q,minimr_broken_pipe.q,mapreduce_stack_trace.q,mapreduce_stack_trace_turnoff.q,mapreduce_stack_trace_hadoop20.q,mapreduce_stack_trace_turnoff_hadoop20.q" />
   <property name="test.silent" value="true"/>
   <property name="hadoopVersion" value="${hadoop.version.ant-internal}"/>
diff --git a/src/data/files/exported_table/_metadata b/src/data/files/exported_table/_metadata
new file mode 100644
index 0000000..81fbf63
--- /dev/null
+++ b/src/data/files/exported_table/_metadata
@@ -0,0 +1 @@
+{"partitions":[],"table":"{\"1\":{\"str\":\"j1_41\"},\"2\":{\"str\":\"default\"},\"3\":{\"str\":\"johndee\"},\"4\":{\"i32\":1371900915},\"5\":{\"i32\":0},\"6\":{\"i32\":0},\"7\":{\"rec\":{\"1\":{\"lst\":[\"rec\",2,{\"1\":{\"str\":\"a\"},\"2\":{\"str\":\"string\"}},{\"1\":{\"str\":\"b\"},\"2\":{\"str\":\"int\"}}]},\"2\":{\"str\":\"hdfs://hivebase01:8020/user/hive/warehouse/j1_41\"},\"3\":{\"str\":\"org.apache.hadoop.mapred.TextInputFormat\"},\"4\":{\"str\":\"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\"},\"5\":{\"tf\":0},\"6\":{\"i32\":-1},\"7\":{\"rec\":{\"2\":{\"str\":\"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"},\"3\":{\"map\":[\"str\",\"str\",2,{\"serialization.format\":\",\",\"field.delim\":\",\"}]}}},\"8\":{\"lst\":[\"str\",0]},\"9\":{\"lst\":[\"rec\",0]},\"10\":{\"map\":[\"str\",\"str\",0,{}]}}},\"8\":{\"lst\":[\"rec\",0]},\"9\":{\"map\":[\"str\",\"str\",1,{\"transient_lastDdlTime\":\"1371900931\"}]},\"12\":{\"str\":\"MANAGED_TABLE\"}}","version":"0.1"}
\ No newline at end of file
diff --git a/src/data/files/exported_table/data/data b/src/data/files/exported_table/data/data
new file mode 100644
index 0000000..40a75ac
--- /dev/null
+++ b/src/data/files/exported_table/data/data
@@ -0,0 +1,2 @@
+johndee,1
+burks,2
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
index 83e901a..93b4181 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
@@ -31,7 +31,6 @@
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.ql.ErrorMsg;
-import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
 import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
@@ -53,10 +52,10 @@
   String databaseName;
   String tableName;
   boolean isExternal;
-  ArrayList<FieldSchema> cols;
-  ArrayList<FieldSchema> partCols;
-  ArrayList<String> bucketCols;
-  ArrayList<Order> sortCols;
+  List<FieldSchema> cols;
+  List<FieldSchema> partCols;
+  List<String> bucketCols;
+  List<Order> sortCols;
   int numBuckets;
   String fieldDelim;
   String fieldEscape;
@@ -130,8 +129,12 @@ public CreateTableDesc(String tableName, boolean isExternal,
     this.serdeProps = serdeProps;
     this.tblProps = tblProps;
     this.ifNotExists = ifNotExists;
-    this.skewedColNames = new ArrayList<String>(skewedColNames);
-    this.skewedColValues = new ArrayList<List<String>>(skewedColValues);
+    this.skewedColNames = copyList(skewedColNames);
+    this.skewedColValues = copyList(skewedColValues);
+  }
+
+  private static <T> List<T> copyList(List<T> copy) {
+    return copy == null ? null : new ArrayList<T>(copy);
   }
 
   @Explain(displayName = "columns")
@@ -166,7 +169,7 @@ public void setTableName(String tableName) {
     this.tableName = tableName;
   }
 
-  public ArrayList<FieldSchema> getCols() {
+  public List<FieldSchema> getCols() {
     return cols;
   }
 
@@ -174,7 +177,7 @@ public void setCols(ArrayList<FieldSchema> cols) {
     this.cols = cols;
   }
 
-  public ArrayList<FieldSchema> getPartCols() {
+  public List<FieldSchema> getPartCols() {
     return partCols;
   }
 
@@ -183,7 +186,7 @@ public void setPartCols(ArrayList<FieldSchema> partCols) {
   }
 
   @Explain(displayName = "bucket columns")
-  public ArrayList<String> getBucketCols() {
+  public List<String> getBucketCols() {
     return bucketCols;
   }
 
@@ -303,7 +306,7 @@ public void setExternal(boolean isExternal) {
    * @return the sortCols
    */
   @Explain(displayName = "sort columns")
-  public ArrayList<Order> getSortCols() {
+  public List<Order> getSortCols() {
     return sortCols;
   }
 
diff --git a/src/ql/src/test/queries/clientpositive/import_exported_table.q b/src/ql/src/test/queries/clientpositive/import_exported_table.q
new file mode 100644
index 0000000..8496ec1
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/import_exported_table.q
@@ -0,0 +1,10 @@
+dfs ${system:test.dfs.mkdir} hdfs:///tmp/test/;
+
+dfs -copyFromLocal ../data/files/exported_table hdfs:///tmp/test/;
+
+IMPORT FROM '/tmp/test/exported_table';
+DESCRIBE j1_41;
+SELECT * from j1_41;
+
+dfs -rmr hdfs:///tmp/test/exported_table;
+
diff --git a/src/ql/src/test/results/clientpositive/import_exported_table.q.out b/src/ql/src/test/results/clientpositive/import_exported_table.q.out
new file mode 100644
index 0000000..35e5637
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/import_exported_table.q.out
@@ -0,0 +1,22 @@
+#### A masked pattern was here ####
+PREHOOK: type: IMPORT
+#### A masked pattern was here ####
+POSTHOOK: type: IMPORT
+POSTHOOK: Output: default@j1_41
+PREHOOK: query: DESCRIBE j1_41
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: DESCRIBE j1_41
+POSTHOOK: type: DESCTABLE
+a                   	string              	None                
+b                   	int                 	None                
+PREHOOK: query: SELECT * from j1_41
+PREHOOK: type: QUERY
+PREHOOK: Input: default@j1_41
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * from j1_41
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@j1_41
+#### A masked pattern was here ####
+johndee	1
+burks	2
+#### A masked pattern was here ####
-- 
1.7.0.4

