From efcb617b1025f0bee199c54f4dd53590fa005b5a Mon Sep 17 00:00:00 2001
From: Xuefu Zhang <xuefu@apache.org>
Date: Sun, 9 Feb 2014 00:23:49 +0000
Subject: [PATCH 280/375] CDH-16381: HIVE-5783: Native Parquet Support in Hive (Justin via Xuefu)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1566173 13f79535-47bb-0310-9956-ffa450edef68

Conflicts:
	pom.xml
	ql/src/java/org/apache/hadoop/hive/ql/parse/IdentifiersParser.g
	ql/src/java/org/apache/hadoop/hive/ql/parse/authorization/DefaultHiveAuthorizationTaskFactory.java
	ql/src/test/results/clientnegative/authorization_invalid_priv_v1.q.out
---
 data/files/parquet_create.txt                      |    3 +
 data/files/parquet_partitioned.txt                 |    3 +
 pom.xml                                            |   11 +
 ql/pom.xml                                         |   11 +
 .../org/apache/hadoop/hive/ql/io/IOConstants.java  |   28 ++
 .../ql/io/parquet/MapredParquetInputFormat.java    |   56 ++++
 .../ql/io/parquet/MapredParquetOutputFormat.java   |  125 +++++++++
 .../hive/ql/io/parquet/ProjectionPusher.java       |  152 +++++++++++
 .../convert/ArrayWritableGroupConverter.java       |   85 ++++++
 .../convert/DataWritableGroupConverter.java        |  140 ++++++++++
 .../convert/DataWritableRecordConverter.java       |   44 +++
 .../hive/ql/io/parquet/convert/ETypeConverter.java |  160 ++++++++++++
 .../ql/io/parquet/convert/HiveGroupConverter.java  |   46 ++++
 .../ql/io/parquet/convert/HiveSchemaConverter.java |  129 +++++++++
 .../io/parquet/read/DataWritableReadSupport.java   |  127 +++++++++
 .../parquet/read/ParquetRecordReaderWrapper.java   |  236 +++++++++++++++++
 .../parquet/serde/AbstractParquetMapInspector.java |  163 ++++++++++++
 .../serde/ArrayWritableObjectInspector.java        |  222 ++++++++++++++++
 .../parquet/serde/DeepParquetHiveMapInspector.java |   82 ++++++
 .../parquet/serde/ParquetHiveArrayInspector.java   |  185 +++++++++++++
 .../hive/ql/io/parquet/serde/ParquetHiveSerDe.java |  274 ++++++++++++++++++++
 .../serde/StandardParquetHiveMapInspector.java     |   60 +++++
 .../serde/primitive/ParquetByteInspector.java      |   56 ++++
 .../ParquetPrimitiveInspectorFactory.java          |   29 ++
 .../serde/primitive/ParquetShortInspector.java     |   56 ++++
 .../serde/primitive/ParquetStringInspector.java    |   98 +++++++
 .../ql/io/parquet/writable/BigDecimalWritable.java |  143 ++++++++++
 .../ql/io/parquet/writable/BinaryWritable.java     |   93 +++++++
 .../io/parquet/write/DataWritableWriteSupport.java |   61 +++++
 .../ql/io/parquet/write/DataWritableWriter.java    |  154 +++++++++++
 .../parquet/write/ParquetRecordWriterWrapper.java  |   93 +++++++
 .../hadoop/hive/ql/metadata/VirtualColumn.java     |    7 +
 .../hadoop/hive/ql/parse/BaseSemanticAnalyzer.java |   19 ++-
 .../org/apache/hadoop/hive/ql/parse/HiveLexer.g    |    1 +
 .../org/apache/hadoop/hive/ql/parse/HiveParser.g   |    3 +
 .../hadoop/hive/ql/parse/SemanticAnalyzer.java     |    2 +-
 .../parquet/hive/DeprecatedParquetInputFormat.java |   37 +++
 .../hive/DeprecatedParquetOutputFormat.java        |   36 +++
 .../parquet/hive/MapredParquetInputFormat.java     |   36 +++
 .../parquet/hive/MapredParquetOutputFormat.java    |   35 +++
 .../ql/io/parquet/TestHiveSchemaConverter.java     |  114 ++++++++
 .../io/parquet/TestMapredParquetInputFormat.java   |   37 +++
 .../io/parquet/TestMapredParquetOutputFormat.java  |   90 +++++++
 .../hive/ql/io/parquet/TestParquetSerDe.java       |  140 ++++++++++
 .../serde/TestAbstractParquetMapInspector.java     |   98 +++++++
 .../serde/TestDeepParquetHiveMapInspector.java     |   90 +++++++
 .../serde/TestParquetHiveArrayInspector.java       |   80 ++++++
 .../serde/TestStandardParquetHiveMapInspector.java |   88 +++++++
 .../test/queries/clientpositive/parquet_create.q   |   36 +++
 .../queries/clientpositive/parquet_partitioned.q   |   34 +++
 .../results/clientpositive/parquet_create.q.out    |  206 +++++++++++++++
 .../clientpositive/parquet_partitioned.q.out       |  174 +++++++++++++
 52 files changed, 4486 insertions(+), 2 deletions(-)
 create mode 100644 data/files/parquet_create.txt
 create mode 100644 data/files/parquet_partitioned.txt
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/IOConstants.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableGroupConverter.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetByteInspector.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetPrimitiveInspectorFactory.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetShortInspector.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetStringInspector.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BigDecimalWritable.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BinaryWritable.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriteSupport.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java
 create mode 100644 ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java
 create mode 100644 ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java
 create mode 100644 ql/src/java/parquet/hive/MapredParquetInputFormat.java
 create mode 100644 ql/src/java/parquet/hive/MapredParquetOutputFormat.java
 create mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java
 create mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetInputFormat.java
 create mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetOutputFormat.java
 create mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java
 create mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java
 create mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java
 create mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java
 create mode 100644 ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java
 create mode 100644 ql/src/test/queries/clientpositive/parquet_create.q
 create mode 100644 ql/src/test/queries/clientpositive/parquet_partitioned.q
 create mode 100644 ql/src/test/results/clientpositive/parquet_create.q.out
 create mode 100644 ql/src/test/results/clientpositive/parquet_partitioned.q.out

diff --git a/src/data/files/parquet_create.txt b/src/data/files/parquet_create.txt
new file mode 100644
index 0000000..ccd48ee
--- /dev/null
+++ b/src/data/files/parquet_create.txt
@@ -0,0 +1,3 @@
+1|foo line1|key11:value11,key12:value12,key13:value13|a,b,c|one,two
+2|bar line2|key21:value21,key22:value22,key23:value23|d,e,f|three,four
+3|baz line3|key31:value31,key32:value32,key33:value33|g,h,i|five,six
diff --git a/src/data/files/parquet_partitioned.txt b/src/data/files/parquet_partitioned.txt
new file mode 100644
index 0000000..8f322f3
--- /dev/null
+++ b/src/data/files/parquet_partitioned.txt
@@ -0,0 +1,3 @@
+1|foo|part1
+2|bar|part2
+3|baz|part2
diff --git a/src/pom.xml b/src/pom.xml
index 3c86ad0..155aac4 100644
--- a/src/pom.xml
+++ b/src/pom.xml
@@ -244,6 +244,17 @@
         <version>${bonecp.version}</version>
       </dependency>
       <dependency>
+        <groupId>com.twitter</groupId>
+        <artifactId>parquet-hadoop-bundle</artifactId>
+        <version>${parquet.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>com.twitter</groupId>
+        <artifactId>parquet-column</artifactId>
+        <version>${parquet.version}</version>
+        <classifier>tests</classifier>
+      </dependency>
+      <dependency>
         <groupId>com.sun.jersey</groupId>
         <artifactId>jersey-core</artifactId>
         <version>${jersey.version}</version>
diff --git a/src/ql/pom.xml b/src/ql/pom.xml
index 6d79037..ba6a877 100644
--- a/src/ql/pom.xml
+++ b/src/ql/pom.xml
@@ -67,6 +67,10 @@
       <version>${kryo.version}</version>
     </dependency>
     <dependency>
+      <groupId>com.twitter</groupId>
+      <artifactId>parquet-hadoop-bundle</artifactId>
+    </dependency>
+    <dependency>
       <groupId>commons-codec</groupId>
       <artifactId>commons-codec</artifactId>
       <version>${commons-codec.version}</version>
@@ -199,6 +203,12 @@
     <!-- test intra-project -->
     <!-- test inter-project -->
     <dependency>
+      <groupId>com.twitter</groupId>
+      <artifactId>parquet-column</artifactId>
+      <classifier>tests</classifier>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
       <version>${junit.version}</version>
@@ -343,6 +353,7 @@
                   <include>org.apache.hive:hive-exec</include>
                   <include>org.apache.hive:hive-serde</include>
                   <include>com.esotericsoftware.kryo:kryo</include>
+                  <include>com.twiter:parquet-hadoop-bundle</include>
                   <include>org.apache.thrift:libthrift</include>
                   <include>commons-lang:commons-lang</include>
                   <include>org.json:json</include>
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/IOConstants.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/IOConstants.java
new file mode 100644
index 0000000..4131066
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/IOConstants.java
@@ -0,0 +1,28 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io;
+
+public final class IOConstants {
+  public static final String COLUMNS = "columns";
+  public static final String COLUMNS_TYPES = "columns.types";
+  public static final String MAPRED_TASK_ID = "mapred.task.id";
+
+  private IOConstants() {
+    // prevent instantiation
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java
new file mode 100644
index 0000000..d3412df
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet;
+
+import java.io.IOException;
+
+import org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport;
+import org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.RecordReader;
+
+import parquet.hadoop.ParquetInputFormat;
+
+
+/**
+ *
+ * A Parquet InputFormat for Hive (with the deprecated package mapred)
+ *
+ */
+public class MapredParquetInputFormat extends FileInputFormat<Void, ArrayWritable> {
+
+  private final ParquetInputFormat<ArrayWritable> realInput;
+
+  public MapredParquetInputFormat() {
+    this(new ParquetInputFormat<ArrayWritable>(DataWritableReadSupport.class));
+  }
+
+  protected MapredParquetInputFormat(final ParquetInputFormat<ArrayWritable> inputFormat) {
+    this.realInput = inputFormat;
+  }
+
+  @Override
+  public org.apache.hadoop.mapred.RecordReader<Void, ArrayWritable> getRecordReader(
+      final org.apache.hadoop.mapred.InputSplit split,
+      final org.apache.hadoop.mapred.JobConf job,
+      final org.apache.hadoop.mapred.Reporter reporter
+      ) throws IOException {
+    try {
+      return (RecordReader<Void, ArrayWritable>) new ParquetRecordReaderWrapper(realInput, split, job, reporter);
+    } catch (final InterruptedException e) {
+      throw new RuntimeException("Cannot create a RecordReaderWrapper", e);
+    }
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java
new file mode 100644
index 0000000..b87c673
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.java
@@ -0,0 +1,125 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Properties;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.io.FSRecordWriter;
+import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
+import org.apache.hadoop.hive.ql.io.IOConstants;
+import org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter;
+import org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport;
+import org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordWriter;
+import org.apache.hadoop.mapreduce.OutputFormat;
+import org.apache.hadoop.util.Progressable;
+
+import parquet.hadoop.ParquetOutputFormat;
+
+/**
+ *
+ * A Parquet OutputFormat for Hive (with the deprecated package mapred)
+ *
+ */
+public class MapredParquetOutputFormat extends FileOutputFormat<Void, ArrayWritable> implements
+  HiveOutputFormat<Void, ArrayWritable> {
+
+  private static final Log LOG = LogFactory.getLog(MapredParquetOutputFormat.class);
+
+  protected ParquetOutputFormat<ArrayWritable> realOutputFormat;
+
+  public MapredParquetOutputFormat() {
+    realOutputFormat = new ParquetOutputFormat<ArrayWritable>(new DataWritableWriteSupport());
+  }
+
+  public MapredParquetOutputFormat(final OutputFormat<Void, ArrayWritable> mapreduceOutputFormat) {
+    realOutputFormat = (ParquetOutputFormat<ArrayWritable>) mapreduceOutputFormat;
+  }
+
+  @Override
+  public void checkOutputSpecs(final FileSystem ignored, final JobConf job) throws IOException {
+    realOutputFormat.checkOutputSpecs(ShimLoader.getHadoopShims().getHCatShim().createJobContext(job, null));
+  }
+
+  @Override
+  public RecordWriter<Void, ArrayWritable> getRecordWriter(
+      final FileSystem ignored,
+      final JobConf job,
+      final String name,
+      final Progressable progress
+      ) throws IOException {
+    throw new RuntimeException("Should never be used");
+  }
+
+  /**
+   *
+   * Create the parquet schema from the hive schema, and return the RecordWriterWrapper which
+   * contains the real output format
+   */
+  @Override
+  public FSRecordWriter getHiveRecordWriter(
+      final JobConf jobConf,
+      final Path finalOutPath,
+      final Class<? extends Writable> valueClass,
+      final boolean isCompressed,
+      final Properties tableProperties,
+      final Progressable progress) throws IOException {
+
+    LOG.info("creating new record writer..." + this);
+
+    final String columnNameProperty = tableProperties.getProperty(IOConstants.COLUMNS);
+    final String columnTypeProperty = tableProperties.getProperty(IOConstants.COLUMNS_TYPES);
+    List<String> columnNames;
+    List<TypeInfo> columnTypes;
+
+    if (columnNameProperty.length() == 0) {
+      columnNames = new ArrayList<String>();
+    } else {
+      columnNames = Arrays.asList(columnNameProperty.split(","));
+    }
+
+    if (columnTypeProperty.length() == 0) {
+      columnTypes = new ArrayList<TypeInfo>();
+    } else {
+      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);
+    }
+
+    DataWritableWriteSupport.setSchema(HiveSchemaConverter.convert(columnNames, columnTypes), jobConf);
+    return getParquerRecordWriterWrapper(realOutputFormat, jobConf, finalOutPath.toString(), progress);
+  }
+
+  protected ParquetRecordWriterWrapper getParquerRecordWriterWrapper(
+      ParquetOutputFormat<ArrayWritable> realOutputFormat,
+      JobConf jobConf,
+      String finalOutPath,
+      Progressable progress
+      ) throws IOException {
+    return new ParquetRecordWriterWrapper(realOutputFormat, jobConf, finalOutPath.toString(), progress);
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java
new file mode 100644
index 0000000..2f155f6
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java
@@ -0,0 +1,152 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet;
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.plan.MapWork;
+import org.apache.hadoop.hive.ql.plan.PartitionDesc;
+import org.apache.hadoop.hive.ql.plan.TableScanDesc;
+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
+import org.apache.hadoop.mapred.JobConf;
+
+public class ProjectionPusher {
+
+  private static final Log LOG = LogFactory.getLog(ProjectionPusher.class);
+
+  private final Map<String, PartitionDesc> pathToPartitionInfo =
+      new LinkedHashMap<String, PartitionDesc>();
+  /**
+   * MapWork is the Hive object which describes input files,
+   * columns projections, and filters.
+   */
+  private MapWork mapWork;
+
+  /**
+   * Sets the mapWork variable based on the current JobConf in order to get all partitions.
+   *
+   * @param job
+   */
+  private void updateMrWork(final JobConf job) {
+    final String plan = HiveConf.getVar(job, HiveConf.ConfVars.PLAN);
+    if (mapWork == null && plan != null && plan.length() > 0) {
+      mapWork = Utilities.getMapWork(job);
+      pathToPartitionInfo.clear();
+      for (final Map.Entry<String, PartitionDesc> entry : mapWork.getPathToPartitionInfo().entrySet()) {
+        // key contains scheme (such as pfile://) and we want only the path portion fix in HIVE-6366
+        pathToPartitionInfo.put(new Path(entry.getKey()).toUri().getPath(), entry.getValue());
+      }
+    }
+  }
+
+  private void pushProjectionsAndFilters(final JobConf jobConf,
+      final String splitPath, final String splitPathWithNoSchema) {
+
+    if (mapWork == null) {
+      return;
+    } else if (mapWork.getPathToAliases() == null) {
+      return;
+    }
+
+    final ArrayList<String> aliases = new ArrayList<String>();
+    final Iterator<Entry<String, ArrayList<String>>> iterator = mapWork.getPathToAliases().entrySet().iterator();
+
+    while (iterator.hasNext()) {
+      final Entry<String, ArrayList<String>> entry = iterator.next();
+      final String key = new Path(entry.getKey()).toUri().getPath();
+      if (splitPath.equals(key) || splitPathWithNoSchema.equals(key)) {
+        final ArrayList<String> list = entry.getValue();
+        for (final String val : list) {
+          aliases.add(val);
+        }
+      }
+    }
+
+    for (final String alias : aliases) {
+      final Operator<? extends Serializable> op = mapWork.getAliasToWork().get(
+          alias);
+      if (op != null && op instanceof TableScanOperator) {
+        final TableScanOperator tableScan = (TableScanOperator) op;
+
+        // push down projections
+        final List<Integer> list = tableScan.getNeededColumnIDs();
+
+        if (list != null) {
+          ColumnProjectionUtils.appendReadColumnIDs(jobConf, list);
+        } else {
+          ColumnProjectionUtils.setFullyReadColumns(jobConf);
+        }
+
+        pushFilters(jobConf, tableScan);
+      }
+    }
+  }
+
+  private void pushFilters(final JobConf jobConf, final TableScanOperator tableScan) {
+
+    final TableScanDesc scanDesc = tableScan.getConf();
+    if (scanDesc == null) {
+      LOG.debug("Not pushing filters because TableScanDesc is null");
+      return;
+    }
+
+    // construct column name list for reference by filter push down
+    Utilities.setColumnNameList(jobConf, tableScan);
+
+    // push down filters
+    final ExprNodeGenericFuncDesc filterExpr = scanDesc.getFilterExpr();
+    if (filterExpr == null) {
+      LOG.debug("Not pushing filters because FilterExpr is null");
+      return;
+    }
+
+    final String filterText = filterExpr.getExprString();
+    final String filterExprSerialized = Utilities.serializeExpression(filterExpr);
+    jobConf.set(
+        TableScanDesc.FILTER_TEXT_CONF_STR,
+        filterText);
+    jobConf.set(
+        TableScanDesc.FILTER_EXPR_CONF_STR,
+        filterExprSerialized);
+  }
+
+
+  public JobConf pushProjectionsAndFilters(JobConf jobConf, Path path)
+      throws IOException {
+    updateMrWork(jobConf);  // TODO: refactor this in HIVE-6366
+    final JobConf cloneJobConf = new JobConf(jobConf);
+    final PartitionDesc part = pathToPartitionInfo.get(path.toString());
+
+    if ((part != null) && (part.getTableDesc() != null)) {
+      Utilities.copyTableJobPropertiesToConf(part.getTableDesc(), cloneJobConf);
+    }
+    pushProjectionsAndFilters(cloneJobConf, path.toString(), path.toUri().toString());
+    return cloneJobConf;
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java
new file mode 100644
index 0000000..582a5df
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java
@@ -0,0 +1,85 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.convert;
+
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.Writable;
+
+import parquet.io.ParquetDecodingException;
+import parquet.io.api.Converter;
+import parquet.schema.GroupType;
+
+public class ArrayWritableGroupConverter extends HiveGroupConverter {
+
+  private final Converter[] converters;
+  private final HiveGroupConverter parent;
+  private final int index;
+  private final boolean isMap;
+  private Writable currentValue;
+  private Writable[] mapPairContainer;
+
+  public ArrayWritableGroupConverter(final GroupType groupType, final HiveGroupConverter parent,
+      final int index) {
+    this.parent = parent;
+    this.index = index;
+    int count = groupType.getFieldCount();
+    if (count < 1 || count > 2) {
+      throw new IllegalStateException("Field count must be either 1 or 2: " + count);
+    }
+    isMap = count == 2;
+    converters = new Converter[count];
+    for (int i = 0; i < count; i++) {
+      converters[i] = getConverterFromDescription(groupType.getType(i), i, this);
+    }
+  }
+
+  @Override
+  public Converter getConverter(final int fieldIndex) {
+    return converters[fieldIndex];
+  }
+
+  @Override
+  public void start() {
+    if (isMap) {
+      mapPairContainer = new Writable[2];
+    }
+  }
+
+  @Override
+  public void end() {
+    if (isMap) {
+      currentValue = new ArrayWritable(Writable.class, mapPairContainer);
+    }
+    parent.add(index, currentValue);
+  }
+
+  @Override
+  protected void set(final int index, final Writable value) {
+    if (index != 0 && mapPairContainer == null || index > 1) {
+      throw new ParquetDecodingException("Repeated group can only have one or two fields for maps." +
+        " Not allowed to set for the index : " + index);
+    }
+
+    if (isMap) {
+      mapPairContainer[index] = value;
+    } else {
+      currentValue = value;
+    }
+  }
+
+  @Override
+  protected void add(final int index, final Writable value) {
+    set(index, value);
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableGroupConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableGroupConverter.java
new file mode 100644
index 0000000..0e310fb
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableGroupConverter.java
@@ -0,0 +1,140 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.convert;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.Writable;
+
+import parquet.io.api.Converter;
+import parquet.schema.GroupType;
+import parquet.schema.Type;
+
+/**
+ *
+ * A MapWritableGroupConverter, real converter between hive and parquet types recursively for complex types.
+ *
+ */
+public class DataWritableGroupConverter extends HiveGroupConverter {
+
+  private final Converter[] converters;
+  private final HiveGroupConverter parent;
+  private final int index;
+  private final Object[] currentArr;
+  private Writable[] rootMap;
+
+  public DataWritableGroupConverter(final GroupType requestedSchema, final GroupType tableSchema) {
+    this(requestedSchema, null, 0, tableSchema);
+    final int fieldCount = tableSchema.getFieldCount();
+    this.rootMap = new Writable[fieldCount];
+  }
+
+  public DataWritableGroupConverter(final GroupType groupType, final HiveGroupConverter parent,
+      final int index) {
+    this(groupType, parent, index, groupType);
+  }
+
+  public DataWritableGroupConverter(final GroupType selectedGroupType,
+      final HiveGroupConverter parent, final int index, final GroupType containingGroupType) {
+    this.parent = parent;
+    this.index = index;
+    final int totalFieldCount = containingGroupType.getFieldCount();
+    final int selectedFieldCount = selectedGroupType.getFieldCount();
+
+    currentArr = new Object[totalFieldCount];
+    converters = new Converter[selectedFieldCount];
+
+    List<Type> selectedFields = selectedGroupType.getFields();
+    for (int i = 0; i < selectedFieldCount; i++) {
+      Type subtype = selectedFields.get(i);
+      if (containingGroupType.getFields().contains(subtype)) {
+        converters[i] = getConverterFromDescription(subtype,
+            containingGroupType.getFieldIndex(subtype.getName()), this);
+      } else {
+        throw new IllegalStateException("Group type [" + containingGroupType +
+            "] does not contain requested field: " + subtype);
+      }
+    }
+  }
+
+  public final ArrayWritable getCurrentArray() {
+    final Writable[] writableArr;
+    if (this.rootMap != null) { // We're at the root : we can safely re-use the same map to save perf
+      writableArr = this.rootMap;
+    } else {
+      writableArr = new Writable[currentArr.length];
+    }
+
+    for (int i = 0; i < currentArr.length; i++) {
+      final Object obj = currentArr[i];
+      if (obj instanceof List) {
+        final List<?> objList = (List<?>)obj;
+        final ArrayWritable arr = new ArrayWritable(Writable.class,
+            objList.toArray(new Writable[objList.size()]));
+        writableArr[i] = arr;
+      } else {
+        writableArr[i] = (Writable) obj;
+      }
+    }
+    return new ArrayWritable(Writable.class, writableArr);
+  }
+
+  @Override
+  final protected void set(final int index, final Writable value) {
+    currentArr[index] = value;
+  }
+
+  @Override
+  public Converter getConverter(final int fieldIndex) {
+    return converters[fieldIndex];
+  }
+
+  @Override
+  public void start() {
+    for (int i = 0; i < currentArr.length; i++) {
+      currentArr[i] = null;
+    }
+  }
+
+  @Override
+  public void end() {
+    if (parent != null) {
+      parent.set(index, getCurrentArray());
+    }
+  }
+
+  @Override
+  protected void add(final int index, final Writable value) {
+    if (currentArr[index] != null) {
+      final Object obj = currentArr[index];
+      if (obj instanceof List) {
+        final List<Writable> list = (List<Writable>) obj;
+        list.add(value);
+      } else {
+        throw new IllegalStateException("This should be a List: " + obj);
+      }
+    } else {
+      // create a list here because we don't know the final length of the object
+      // and it is more flexible than ArrayWritable.
+      //
+      // converted to ArrayWritable by getCurrentArray().
+      final List<Writable> buffer = new ArrayList<Writable>();
+      buffer.add(value);
+      currentArr[index] = (Object) buffer;
+    }
+
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java
new file mode 100644
index 0000000..7762afe
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.convert;
+
+import org.apache.hadoop.io.ArrayWritable;
+
+import parquet.io.api.GroupConverter;
+import parquet.io.api.RecordMaterializer;
+import parquet.schema.GroupType;
+
+/**
+ *
+ * A MapWritableReadSupport, encapsulates the tuples
+ *
+ */
+public class DataWritableRecordConverter extends RecordMaterializer<ArrayWritable> {
+
+  private final DataWritableGroupConverter root;
+
+  public DataWritableRecordConverter(final GroupType requestedSchema, final GroupType tableSchema) {
+    this.root = new DataWritableGroupConverter(requestedSchema, tableSchema);
+  }
+
+  @Override
+  public ArrayWritable getCurrentRecord() {
+    return root.getCurrentArray();
+  }
+
+  @Override
+  public GroupConverter getRootConverter() {
+    return root;
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
new file mode 100644
index 0000000..f7b9668
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
@@ -0,0 +1,160 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.convert;
+
+import java.math.BigDecimal;
+
+import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable;
+import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable.DicBinaryWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.io.BooleanWritable;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+
+import parquet.column.Dictionary;
+import parquet.io.api.Binary;
+import parquet.io.api.Converter;
+import parquet.io.api.PrimitiveConverter;
+
+/**
+ *
+ * ETypeConverter is an easy way to set the converter for the right type.
+ *
+ */
+public enum ETypeConverter {
+
+  EDOUBLE_CONVERTER(Double.TYPE) {
+    @Override
+    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
+      return new PrimitiveConverter() {
+        @Override
+        public void addDouble(final double value) {
+          parent.set(index, new DoubleWritable(value));
+        }
+      };
+    }
+  },
+  EBOOLEAN_CONVERTER(Boolean.TYPE) {
+    @Override
+    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
+      return new PrimitiveConverter() {
+        @Override
+        public void addBoolean(final boolean value) {
+          parent.set(index, new BooleanWritable(value));
+        }
+      };
+    }
+  },
+  EFLOAT_CONVERTER(Float.TYPE) {
+    @Override
+    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
+      return new PrimitiveConverter() {
+        @Override
+        public void addFloat(final float value) {
+          parent.set(index, new FloatWritable(value));
+        }
+      };
+    }
+  },
+  EINT32_CONVERTER(Integer.TYPE) {
+    @Override
+    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
+      return new PrimitiveConverter() {
+        @Override
+        public void addInt(final int value) {
+          parent.set(index, new IntWritable(value));
+        }
+      };
+    }
+  },
+  EINT64_CONVERTER(Long.TYPE) {
+    @Override
+    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
+      return new PrimitiveConverter() {
+        @Override
+        public void addLong(final long value) {
+          parent.set(index, new LongWritable(value));
+        }
+      };
+    }
+  },
+  EINT96_CONVERTER(BigDecimal.class) {
+    @Override
+    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
+      return new PrimitiveConverter() {
+        // TODO in HIVE-6367 decimal should not be treated as a double
+        @Override
+        public void addDouble(final double value) {
+          parent.set(index, new DoubleWritable(value));
+        }
+      };
+    }
+  },
+  EBINARY_CONVERTER(Binary.class) {
+    @Override
+    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
+      return new PrimitiveConverter() {
+        private Binary[] dictBinary;
+        private String[] dict;
+
+        @Override
+        public boolean hasDictionarySupport() {
+          return true;
+        }
+
+        @Override
+        public void setDictionary(Dictionary dictionary) {
+          dictBinary = new Binary[dictionary.getMaxId() + 1];
+          dict = new String[dictionary.getMaxId() + 1];
+          for (int i = 0; i <= dictionary.getMaxId(); i++) {
+            Binary binary = dictionary.decodeToBinary(i);
+            dictBinary[i] = binary;
+            dict[i] = binary.toStringUsingUTF8();
+          }
+        }
+
+        @Override
+        public void addValueFromDictionary(int dictionaryId) {
+          parent.set(index, new DicBinaryWritable(dictBinary[dictionaryId],  dict[dictionaryId]));
+        }
+
+        @Override
+        public void addBinary(Binary value) {
+          parent.set(index, new BinaryWritable(value));
+        }
+      };
+    }
+  };
+  final Class<?> _type;
+
+  private ETypeConverter(final Class<?> type) {
+    this._type = type;
+  }
+
+  private Class<?> getType() {
+    return _type;
+  }
+
+  abstract Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent);
+
+  public static Converter getNewConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
+    for (final ETypeConverter eConverter : values()) {
+      if (eConverter.getType() == type) {
+        return eConverter.getConverter(type, index, parent);
+      }
+    }
+    throw new IllegalArgumentException("Converter not found ... for type : " + type);
+  }
+}
\ No newline at end of file
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java
new file mode 100644
index 0000000..20c8445
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java
@@ -0,0 +1,46 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.convert;
+
+import org.apache.hadoop.io.Writable;
+
+import parquet.io.api.Converter;
+import parquet.io.api.GroupConverter;
+import parquet.schema.Type;
+import parquet.schema.Type.Repetition;
+
+public abstract class HiveGroupConverter extends GroupConverter {
+
+  protected static Converter getConverterFromDescription(final Type type, final int index,
+      final HiveGroupConverter parent) {
+    if (type == null) {
+      return null;
+    }
+    if (type.isPrimitive()) {
+      return ETypeConverter.getNewConverter(type.asPrimitiveType().getPrimitiveTypeName().javaType,
+          index, parent);
+    } else {
+      if (type.asGroupType().getRepetition() == Repetition.REPEATED) {
+        return new ArrayWritableGroupConverter(type.asGroupType(), parent, index);
+      } else {
+        return new DataWritableGroupConverter(type.asGroupType(), parent, index);
+      }
+    }
+  }
+
+  protected abstract void set(int index, Writable value);
+
+  protected abstract void add(int index, Writable value);
+
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java
new file mode 100644
index 0000000..b5e9c8b
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java
@@ -0,0 +1,129 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.convert;
+
+import java.util.List;
+
+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+
+import parquet.schema.GroupType;
+import parquet.schema.MessageType;
+import parquet.schema.OriginalType;
+import parquet.schema.PrimitiveType;
+import parquet.schema.PrimitiveType.PrimitiveTypeName;
+import parquet.schema.Type;
+import parquet.schema.Type.Repetition;
+
+public class HiveSchemaConverter {
+
+  public static MessageType convert(final List<String> columnNames, final List<TypeInfo> columnTypes) {
+    final MessageType schema = new MessageType("hive_schema", convertTypes(columnNames, columnTypes));
+    return schema;
+  }
+
+  private static Type[] convertTypes(final List<String> columnNames, final List<TypeInfo> columnTypes) {
+    if (columnNames.size() != columnTypes.size()) {
+      throw new IllegalStateException("Mismatched Hive columns and types. Hive columns names" +
+        " found : " + columnNames + " . And Hive types found : " + columnTypes);
+    }
+    final Type[] types = new Type[columnNames.size()];
+    for (int i = 0; i < columnNames.size(); ++i) {
+      types[i] = convertType(columnNames.get(i), columnTypes.get(i));
+    }
+    return types;
+  }
+
+  private static Type convertType(final String name, final TypeInfo typeInfo) {
+    return convertType(name, typeInfo, Repetition.OPTIONAL);
+  }
+
+  private static Type convertType(final String name, final TypeInfo typeInfo, final Repetition repetition) {
+    if (typeInfo.getCategory().equals(Category.PRIMITIVE)) {
+      if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {
+        return new PrimitiveType(repetition, PrimitiveTypeName.BINARY, name);
+      } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo) ||
+          typeInfo.equals(TypeInfoFactory.shortTypeInfo) ||
+          typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {
+        return new PrimitiveType(repetition, PrimitiveTypeName.INT32, name);
+      } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {
+        return new PrimitiveType(repetition, PrimitiveTypeName.INT64, name);
+      } else if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {
+        return new PrimitiveType(repetition, PrimitiveTypeName.DOUBLE, name);
+      } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {
+        return new PrimitiveType(repetition, PrimitiveTypeName.FLOAT, name);
+      } else if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {
+        return new PrimitiveType(repetition, PrimitiveTypeName.BOOLEAN, name);
+      } else if (typeInfo.equals(TypeInfoFactory.binaryTypeInfo)) {
+        // TODO : binaryTypeInfo is a byte array. Need to map it
+        throw new UnsupportedOperationException("Binary type not implemented");
+      } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {
+        throw new UnsupportedOperationException("Timestamp type not implemented");
+      } else if (typeInfo.equals(TypeInfoFactory.voidTypeInfo)) {
+        throw new UnsupportedOperationException("Void type not implemented");
+      } else if (typeInfo.equals(TypeInfoFactory.unknownTypeInfo)) {
+        throw new UnsupportedOperationException("Unknown type not implemented");
+      } else {
+        throw new IllegalArgumentException("Unknown type: " + typeInfo);
+      }
+    } else if (typeInfo.getCategory().equals(Category.LIST)) {
+      return convertArrayType(name, (ListTypeInfo) typeInfo);
+    } else if (typeInfo.getCategory().equals(Category.STRUCT)) {
+      return convertStructType(name, (StructTypeInfo) typeInfo);
+    } else if (typeInfo.getCategory().equals(Category.MAP)) {
+      return convertMapType(name, (MapTypeInfo) typeInfo);
+    } else if (typeInfo.getCategory().equals(Category.UNION)) {
+      throw new UnsupportedOperationException("Union type not implemented");
+    } else {
+      throw new IllegalArgumentException("Unknown type: " + typeInfo);
+    }
+  }
+
+  // An optional group containing a repeated anonymous group "bag", containing
+  // 1 anonymous element "array_element"
+  private static GroupType convertArrayType(final String name, final ListTypeInfo typeInfo) {
+    final TypeInfo subType = typeInfo.getListElementTypeInfo();
+    return listWrapper(name, OriginalType.LIST, new GroupType(Repetition.REPEATED,
+        ParquetHiveSerDe.ARRAY.toString(), convertType("array_element", subType)));
+  }
+
+  // An optional group containing multiple elements
+  private static GroupType convertStructType(final String name, final StructTypeInfo typeInfo) {
+    final List<String> columnNames = typeInfo.getAllStructFieldNames();
+    final List<TypeInfo> columnTypes = typeInfo.getAllStructFieldTypeInfos();
+    return new GroupType(Repetition.OPTIONAL, name, convertTypes(columnNames, columnTypes));
+
+  }
+
+  // An optional group containing a repeated anonymous group "map", containing
+  // 2 elements: "key", "value"
+  private static GroupType convertMapType(final String name, final MapTypeInfo typeInfo) {
+    final Type keyType = convertType(ParquetHiveSerDe.MAP_KEY.toString(),
+        typeInfo.getMapKeyTypeInfo(), Repetition.REQUIRED);
+    final Type valueType = convertType(ParquetHiveSerDe.MAP_VALUE.toString(),
+        typeInfo.getMapValueTypeInfo());
+    return listWrapper(name, OriginalType.MAP_KEY_VALUE,
+        new GroupType(Repetition.REPEATED, ParquetHiveSerDe.MAP.toString(), keyType, valueType));
+  }
+
+  private static GroupType listWrapper(final String name, final OriginalType originalType,
+      final GroupType groupType) {
+    return new GroupType(Repetition.OPTIONAL, name, originalType, groupType);
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
new file mode 100644
index 0000000..9087307
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
@@ -0,0 +1,127 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.read;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.io.IOConstants;
+import org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter;
+import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.util.StringUtils;
+
+import parquet.hadoop.api.ReadSupport;
+import parquet.io.api.RecordMaterializer;
+import parquet.schema.MessageType;
+import parquet.schema.MessageTypeParser;
+import parquet.schema.PrimitiveType;
+import parquet.schema.PrimitiveType.PrimitiveTypeName;
+import parquet.schema.Type;
+import parquet.schema.Type.Repetition;
+
+/**
+ *
+ * A MapWritableReadSupport
+ *
+ * Manages the translation between Hive and Parquet
+ *
+ */
+public class DataWritableReadSupport extends ReadSupport<ArrayWritable> {
+
+  private static final String TABLE_SCHEMA = "table_schema";
+  public static final String HIVE_SCHEMA_KEY = "HIVE_TABLE_SCHEMA";
+
+  /**
+   * From a string which columns names (including hive column), return a list
+   * of string columns
+   *
+   * @param comma separated list of columns
+   * @return list with virtual columns removed
+   */
+  private static List<String> getColumns(final String columns) {
+    return (List<String>) VirtualColumn.
+        removeVirtualColumns(StringUtils.getStringCollection(columns));
+  }
+  /**
+   *
+   * It creates the readContext for Parquet side with the requested schema during the init phase.
+   *
+   * @param configuration needed to get the wanted columns
+   * @param keyValueMetaData // unused
+   * @param fileSchema parquet file schema
+   * @return the parquet ReadContext
+   */
+  @Override
+  public parquet.hadoop.api.ReadSupport.ReadContext init(final Configuration configuration,
+      final Map<String, String> keyValueMetaData, final MessageType fileSchema) {
+    final String columns = configuration.get(IOConstants.COLUMNS);
+    final Map<String, String> contextMetadata = new HashMap<String, String>();
+    if (columns != null) {
+      final List<String> listColumns = getColumns(columns);
+
+      final List<Type> typeListTable = new ArrayList<Type>();
+      for (final String col : listColumns) {
+        // listColumns contains partition columns which are metadata only
+        if (fileSchema.containsField(col)) {
+          typeListTable.add(fileSchema.getType(col));
+        }
+      }
+      MessageType tableSchema = new MessageType(TABLE_SCHEMA, typeListTable);
+      contextMetadata.put(HIVE_SCHEMA_KEY, tableSchema.toString());
+
+      MessageType requestedSchemaByUser = tableSchema;
+      final List<Integer> indexColumnsWanted = ColumnProjectionUtils.getReadColumnIDs(configuration);
+
+      final List<Type> typeListWanted = new ArrayList<Type>();
+      for (final Integer idx : indexColumnsWanted) {
+        typeListWanted.add(tableSchema.getType(listColumns.get(idx)));
+      }
+      requestedSchemaByUser = new MessageType(fileSchema.getName(), typeListWanted);
+
+      return new ReadContext(requestedSchemaByUser, contextMetadata);
+    } else {
+      contextMetadata.put(HIVE_SCHEMA_KEY, fileSchema.toString());
+      return new ReadContext(fileSchema, contextMetadata);
+    }
+  }
+
+  /**
+   *
+   * It creates the hive read support to interpret data from parquet to hive
+   *
+   * @param configuration // unused
+   * @param keyValueMetaData
+   * @param fileSchema // unused
+   * @param readContext containing the requested schema and the schema of the hive table
+   * @return Record Materialize for Hive
+   */
+  @Override
+  public RecordMaterializer<ArrayWritable> prepareForRead(final Configuration configuration,
+      final Map<String, String> keyValueMetaData, final MessageType fileSchema,
+          final parquet.hadoop.api.ReadSupport.ReadContext readContext) {
+    final Map<String, String> metadata = readContext.getReadSupportMetadata();
+    if (metadata == null) {
+      throw new IllegalStateException("ReadContext not initialized properly. " +
+        "Don't know the Hive Schema.");
+    }
+    final MessageType tableSchema = MessageTypeParser.
+        parseMessageType(metadata.get(HIVE_SCHEMA_KEY));
+    return new DataWritableRecordConverter(readContext.getRequestedSchema(), tableSchema);
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
new file mode 100644
index 0000000..e1a7a48
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
@@ -0,0 +1,236 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.read;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.io.IOConstants;
+import org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+
+import parquet.hadoop.ParquetFileReader;
+import parquet.hadoop.ParquetInputFormat;
+import parquet.hadoop.ParquetInputSplit;
+import parquet.hadoop.api.ReadSupport.ReadContext;
+import parquet.hadoop.metadata.BlockMetaData;
+import parquet.hadoop.metadata.FileMetaData;
+import parquet.hadoop.metadata.ParquetMetadata;
+import parquet.hadoop.util.ContextUtil;
+import parquet.schema.MessageTypeParser;
+
+public class ParquetRecordReaderWrapper  implements RecordReader<Void, ArrayWritable> {
+  public static final Log LOG = LogFactory.getLog(ParquetRecordReaderWrapper.class);
+
+  private final long splitLen; // for getPos()
+
+  private org.apache.hadoop.mapreduce.RecordReader<Void, ArrayWritable> realReader;
+  // expect readReader return same Key & Value objects (common case)
+  // this avoids extra serialization & deserialization of these objects
+  private ArrayWritable valueObj = null;
+  private boolean firstRecord = false;
+  private boolean eof = false;
+  private int schemaSize;
+
+  private final ProjectionPusher projectionPusher;
+
+  public ParquetRecordReaderWrapper(
+      final ParquetInputFormat<ArrayWritable> newInputFormat,
+      final InputSplit oldSplit,
+      final JobConf oldJobConf,
+      final Reporter reporter)
+          throws IOException, InterruptedException {
+    this(newInputFormat, oldSplit, oldJobConf, reporter, new ProjectionPusher());
+  }
+
+  public ParquetRecordReaderWrapper(
+      final ParquetInputFormat<ArrayWritable> newInputFormat,
+      final InputSplit oldSplit,
+      final JobConf oldJobConf,
+      final Reporter reporter,
+      final ProjectionPusher pusher)
+          throws IOException, InterruptedException {
+    this.splitLen = oldSplit.getLength();
+    this.projectionPusher = pusher;
+
+    final ParquetInputSplit split = getSplit(oldSplit, oldJobConf);
+
+    TaskAttemptID taskAttemptID = TaskAttemptID.forName(oldJobConf.get(IOConstants.MAPRED_TASK_ID));
+    if (taskAttemptID == null) {
+      taskAttemptID = new TaskAttemptID();
+    }
+
+    // create a TaskInputOutputContext
+    final TaskAttemptContext taskContext = ContextUtil.newTaskAttemptContext(oldJobConf, taskAttemptID);
+
+    if (split != null) {
+      try {
+        realReader = newInputFormat.createRecordReader(split, taskContext);
+        realReader.initialize(split, taskContext);
+
+        // read once to gain access to key and value objects
+        if (realReader.nextKeyValue()) {
+          firstRecord = true;
+          valueObj = realReader.getCurrentValue();
+        } else {
+          eof = true;
+        }
+      } catch (final InterruptedException e) {
+        throw new IOException(e);
+      }
+    } else {
+      realReader = null;
+      eof = true;
+      if (valueObj == null) { // Should initialize the value for createValue
+        valueObj = new ArrayWritable(Writable.class, new Writable[schemaSize]);
+      }
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (realReader != null) {
+      realReader.close();
+    }
+  }
+
+  @Override
+  public Void createKey() {
+    return null;
+  }
+
+  @Override
+  public ArrayWritable createValue() {
+    return valueObj;
+  }
+
+  @Override
+  public long getPos() throws IOException {
+    return (long) (splitLen * getProgress());
+  }
+
+  @Override
+  public float getProgress() throws IOException {
+    if (realReader == null) {
+      return 1f;
+    } else {
+      try {
+        return realReader.getProgress();
+      } catch (final InterruptedException e) {
+        throw new IOException(e);
+      }
+    }
+  }
+
+  @Override
+  public boolean next(final Void key, final ArrayWritable value) throws IOException {
+    if (eof) {
+      return false;
+    }
+    try {
+      if (firstRecord) { // key & value are already read.
+        firstRecord = false;
+      } else if (!realReader.nextKeyValue()) {
+        eof = true; // strictly not required, just for consistency
+        return false;
+      }
+
+      final ArrayWritable tmpCurValue = realReader.getCurrentValue();
+      if (value != tmpCurValue) {
+        final Writable[] arrValue = value.get();
+        final Writable[] arrCurrent = tmpCurValue.get();
+        if (value != null && arrValue.length == arrCurrent.length) {
+          System.arraycopy(arrCurrent, 0, arrValue, 0, arrCurrent.length);
+        } else {
+          if (arrValue.length != arrCurrent.length) {
+            throw new IOException("DeprecatedParquetHiveInput : size of object differs. Value" +
+              " size :  " + arrValue.length + ", Current Object size : " + arrCurrent.length);
+          } else {
+            throw new IOException("DeprecatedParquetHiveInput can not support RecordReaders that" +
+              " don't return same key & value & value is null");
+          }
+        }
+      }
+      return true;
+    } catch (final InterruptedException e) {
+      throw new IOException(e);
+    }
+  }
+
+  /**
+   * gets a ParquetInputSplit corresponding to a split given by Hive
+   *
+   * @param oldSplit The split given by Hive
+   * @param conf The JobConf of the Hive job
+   * @return a ParquetInputSplit corresponding to the oldSplit
+   * @throws IOException if the config cannot be enhanced or if the footer cannot be read from the file
+   */
+  protected ParquetInputSplit getSplit(
+      final InputSplit oldSplit,
+      final JobConf conf
+      ) throws IOException {
+    ParquetInputSplit split;
+    if (oldSplit instanceof FileSplit) {
+      final Path finalPath = ((FileSplit) oldSplit).getPath();
+      final JobConf cloneJob = projectionPusher.pushProjectionsAndFilters(conf, finalPath.getParent());
+
+      final ParquetMetadata parquetMetadata = ParquetFileReader.readFooter(cloneJob, finalPath);
+      final List<BlockMetaData> blocks = parquetMetadata.getBlocks();
+      final FileMetaData fileMetaData = parquetMetadata.getFileMetaData();
+
+      final ReadContext readContext = new DataWritableReadSupport()
+          .init(cloneJob, fileMetaData.getKeyValueMetaData(), fileMetaData.getSchema());
+      schemaSize = MessageTypeParser.parseMessageType(readContext.getReadSupportMetadata()
+          .get(DataWritableReadSupport.HIVE_SCHEMA_KEY)).getFieldCount();
+      final List<BlockMetaData> splitGroup = new ArrayList<BlockMetaData>();
+      final long splitStart = ((FileSplit) oldSplit).getStart();
+      final long splitLength = ((FileSplit) oldSplit).getLength();
+      for (final BlockMetaData block : blocks) {
+        final long firstDataPage = block.getColumns().get(0).getFirstDataPageOffset();
+        if (firstDataPage >= splitStart && firstDataPage < splitStart + splitLength) {
+          splitGroup.add(block);
+        }
+      }
+      if (splitGroup.isEmpty()) {
+        LOG.warn("Skipping split, could not find row group in: " + (FileSplit) oldSplit);
+        split = null;
+      } else {
+        split = new ParquetInputSplit(finalPath,
+                splitStart,
+                splitLength,
+                ((FileSplit) oldSplit).getLocations(),
+                splitGroup,
+                readContext.getRequestedSchema().toString(),
+                fileMetaData.getSchema().toString(),
+                fileMetaData.getKeyValueMetaData(),
+                readContext.getReadSupportMetadata());
+      }
+    } else {
+      throw new IllegalArgumentException("Unknown split type: " + oldSplit);
+    }
+    return split;
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java
new file mode 100644
index 0000000..1d72747
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java
@@ -0,0 +1,163 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.Writable;
+
+public abstract class AbstractParquetMapInspector implements SettableMapObjectInspector {
+
+  protected final ObjectInspector keyInspector;
+  protected final ObjectInspector valueInspector;
+
+  public AbstractParquetMapInspector(final ObjectInspector keyInspector, final ObjectInspector valueInspector) {
+    this.keyInspector = keyInspector;
+    this.valueInspector = valueInspector;
+  }
+
+  @Override
+  public String getTypeName() {
+    return "map<" + keyInspector.getTypeName() + "," + valueInspector.getTypeName() + ">";
+  }
+
+  @Override
+  public Category getCategory() {
+    return Category.MAP;
+  }
+
+  @Override
+  public ObjectInspector getMapKeyObjectInspector() {
+    return keyInspector;
+  }
+
+  @Override
+  public ObjectInspector getMapValueObjectInspector() {
+    return valueInspector;
+  }
+
+  @Override
+  public Map<?, ?> getMap(final Object data) {
+    if (data == null) {
+      return null;
+    }
+
+    if (data instanceof ArrayWritable) {
+      final Writable[] mapContainer = ((ArrayWritable) data).get();
+
+      if (mapContainer == null || mapContainer.length == 0) {
+        return null;
+      }
+
+      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
+      final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
+
+      for (final Writable obj : mapArray) {
+        final ArrayWritable mapObj = (ArrayWritable) obj;
+        final Writable[] arr = mapObj.get();
+        map.put(arr[0], arr[1]);
+      }
+
+      return map;
+    }
+
+    if (data instanceof Map) {
+      return (Map) data;
+    }
+
+    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
+  }
+
+  @Override
+  public int getMapSize(final Object data) {
+    if (data == null) {
+      return -1;
+    }
+
+    if (data instanceof ArrayWritable) {
+      final Writable[] mapContainer = ((ArrayWritable) data).get();
+
+      if (mapContainer == null || mapContainer.length == 0) {
+        return -1;
+      } else {
+        return ((ArrayWritable) mapContainer[0]).get().length;
+      }
+    }
+
+    if (data instanceof Map) {
+      return ((Map) data).size();
+    }
+
+    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
+  }
+
+  @Override
+  public Object create() {
+    Map<Object, Object> m = new HashMap<Object, Object>();
+    return m;
+  }
+
+  @Override
+  public Object put(Object map, Object key, Object value) {
+    Map<Object, Object> m = (HashMap<Object, Object>) map;
+    m.put(key, value);
+    return m;
+  }
+
+  @Override
+  public Object remove(Object map, Object key) {
+    Map<Object, Object> m = (HashMap<Object, Object>) map;
+    m.remove(key);
+    return m;
+  }
+
+  @Override
+  public Object clear(Object map) {
+    Map<Object, Object> m = (HashMap<Object, Object>) map;
+    m.clear();
+    return m;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (obj == null) {
+      return false;
+    }
+    if (getClass() != obj.getClass()) {
+      return false;
+    }
+    final StandardParquetHiveMapInspector other = (StandardParquetHiveMapInspector) obj;
+    if (this.keyInspector != other.keyInspector &&
+        (this.keyInspector == null || !this.keyInspector.equals(other.keyInspector))) {
+      return false;
+    }
+    if (this.valueInspector != other.valueInspector &&
+        (this.valueInspector == null || !this.valueInspector.equals(other.valueInspector))) {
+      return false;
+    }
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    int hash = 7;
+    hash = 59 * hash + (this.keyInspector != null ? this.keyInspector.hashCode() : 0);
+    hash = 59 * hash + (this.valueInspector != null ? this.valueInspector.hashCode() : 0);
+    return hash;
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java
new file mode 100644
index 0000000..a2c7fe0
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java
@@ -0,0 +1,222 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+
+import org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetPrimitiveInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.SettableStructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.StructField;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.io.ArrayWritable;
+
+/**
+ *
+ * The ArrayWritableObjectInspector will inspect an ArrayWritable, considering it as a Hive struct.<br />
+ * It can also inspect a List if Hive decides to inspect the result of an inspection.
+ *
+ */
+public class ArrayWritableObjectInspector extends SettableStructObjectInspector {
+
+  private final TypeInfo typeInfo;
+  private final List<TypeInfo> fieldInfos;
+  private final List<String> fieldNames;
+  private final List<StructField> fields;
+  private final HashMap<String, StructFieldImpl> fieldsByName;
+
+  public ArrayWritableObjectInspector(final StructTypeInfo rowTypeInfo) {
+
+    typeInfo = rowTypeInfo;
+    fieldNames = rowTypeInfo.getAllStructFieldNames();
+    fieldInfos = rowTypeInfo.getAllStructFieldTypeInfos();
+    fields = new ArrayList<StructField>(fieldNames.size());
+    fieldsByName = new HashMap<String, StructFieldImpl>();
+
+    for (int i = 0; i < fieldNames.size(); ++i) {
+      final String name = fieldNames.get(i);
+      final TypeInfo fieldInfo = fieldInfos.get(i);
+
+      final StructFieldImpl field = new StructFieldImpl(name, getObjectInspector(fieldInfo), i);
+      fields.add(field);
+      fieldsByName.put(name, field);
+    }
+  }
+
+  private ObjectInspector getObjectInspector(final TypeInfo typeInfo) {
+    if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {
+      return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
+    } else if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {
+      return PrimitiveObjectInspectorFactory.writableBooleanObjectInspector;
+    } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {
+      return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
+    } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo)) {
+      return PrimitiveObjectInspectorFactory.writableIntObjectInspector;
+    } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {
+      return PrimitiveObjectInspectorFactory.writableLongObjectInspector;
+    } else if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {
+      return ParquetPrimitiveInspectorFactory.parquetStringInspector;
+    } else if (typeInfo.getCategory().equals(Category.STRUCT)) {
+      return new ArrayWritableObjectInspector((StructTypeInfo) typeInfo);
+    } else if (typeInfo.getCategory().equals(Category.LIST)) {
+      final TypeInfo subTypeInfo = ((ListTypeInfo) typeInfo).getListElementTypeInfo();
+      return new ParquetHiveArrayInspector(getObjectInspector(subTypeInfo));
+    } else if (typeInfo.getCategory().equals(Category.MAP)) {
+      final TypeInfo keyTypeInfo = ((MapTypeInfo) typeInfo).getMapKeyTypeInfo();
+      final TypeInfo valueTypeInfo = ((MapTypeInfo) typeInfo).getMapValueTypeInfo();
+      if (keyTypeInfo.equals(TypeInfoFactory.stringTypeInfo) || keyTypeInfo.equals(TypeInfoFactory.byteTypeInfo)
+              || keyTypeInfo.equals(TypeInfoFactory.shortTypeInfo)) {
+        return new DeepParquetHiveMapInspector(getObjectInspector(keyTypeInfo), getObjectInspector(valueTypeInfo));
+      } else {
+        return new StandardParquetHiveMapInspector(getObjectInspector(keyTypeInfo), getObjectInspector(valueTypeInfo));
+      }
+    } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {
+      throw new UnsupportedOperationException("timestamp not implemented yet");
+    } else if (typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {
+      return ParquetPrimitiveInspectorFactory.parquetByteInspector;
+    } else if (typeInfo.equals(TypeInfoFactory.shortTypeInfo)) {
+      return ParquetPrimitiveInspectorFactory.parquetShortInspector;
+    } else {
+      throw new IllegalArgumentException("Unknown field info: " + typeInfo);
+    }
+
+  }
+
+  @Override
+  public Category getCategory() {
+    return Category.STRUCT;
+  }
+
+  @Override
+  public String getTypeName() {
+    return typeInfo.getTypeName();
+  }
+
+  @Override
+  public List<? extends StructField> getAllStructFieldRefs() {
+    return fields;
+  }
+
+  @Override
+  public Object getStructFieldData(final Object data, final StructField fieldRef) {
+    if (data == null) {
+      return null;
+    }
+
+    if (data instanceof ArrayWritable) {
+      final ArrayWritable arr = (ArrayWritable) data;
+      return arr.get()[((StructFieldImpl) fieldRef).getIndex()];
+    }
+
+    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
+  }
+
+  @Override
+  public StructField getStructFieldRef(final String name) {
+    return fieldsByName.get(name);
+  }
+
+  @Override
+  public List<Object> getStructFieldsDataAsList(final Object data) {
+    if (data == null) {
+      return null;
+    }
+
+    if (data instanceof ArrayWritable) {
+      final ArrayWritable arr = (ArrayWritable) data;
+      final Object[] arrWritable = arr.get();
+      return new ArrayList<Object>(Arrays.asList(arrWritable));
+    }
+
+    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
+  }
+
+  @Override
+  public Object create() {
+    final ArrayList<Object> list = new ArrayList<Object>(fields.size());
+    for (int i = 0; i < fields.size(); ++i) {
+      list.add(null);
+    }
+    return list;
+  }
+
+  @Override
+  public Object setStructFieldData(Object struct, StructField field, Object fieldValue) {
+    final ArrayList<Object> list = (ArrayList<Object>) struct;
+    list.set(((StructFieldImpl) field).getIndex(), fieldValue);
+    return list;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (obj == null) {
+      return false;
+    }
+    if (getClass() != obj.getClass()) {
+      return false;
+    }
+    final ArrayWritableObjectInspector other = (ArrayWritableObjectInspector) obj;
+    if (this.typeInfo != other.typeInfo && (this.typeInfo == null || !this.typeInfo.equals(other.typeInfo))) {
+      return false;
+    }
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    int hash = 5;
+    hash = 29 * hash + (this.typeInfo != null ? this.typeInfo.hashCode() : 0);
+    return hash;
+  }
+
+  class StructFieldImpl implements StructField {
+
+    private final String name;
+    private final ObjectInspector inspector;
+    private final int index;
+
+    public StructFieldImpl(final String name, final ObjectInspector inspector, final int index) {
+      this.name = name;
+      this.inspector = inspector;
+      this.index = index;
+    }
+
+    @Override
+    public String getFieldComment() {
+      return "";
+    }
+
+    @Override
+    public String getFieldName() {
+      return name;
+    }
+
+    public int getIndex() {
+      return index;
+    }
+
+    @Override
+    public ObjectInspector getFieldObjectInspector() {
+      return inspector;
+    }
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java
new file mode 100644
index 0000000..d38c641
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java
@@ -0,0 +1,82 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde;
+
+import java.util.Map;
+
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.Writable;
+
+/**
+ * The DeepParquetHiveMapInspector will inspect an ArrayWritable, considering it as a Hive map.<br />
+ * It can also inspect a Map if Hive decides to inspect the result of an inspection.<br />
+ * When trying to access elements from the map it will iterate over all keys, inspecting them and comparing them to the
+ * desired key.
+ *
+ */
+public class DeepParquetHiveMapInspector extends AbstractParquetMapInspector {
+
+  public DeepParquetHiveMapInspector(final ObjectInspector keyInspector, final ObjectInspector valueInspector) {
+    super(keyInspector, valueInspector);
+  }
+
+  @Override
+  public Object getMapValueElement(final Object data, final Object key) {
+    if (data == null || key == null) {
+      return null;
+    }
+
+    if (data instanceof ArrayWritable) {
+      final Writable[] mapContainer = ((ArrayWritable) data).get();
+
+      if (mapContainer == null || mapContainer.length == 0) {
+        return null;
+      }
+
+      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
+
+      for (final Writable obj : mapArray) {
+        final ArrayWritable mapObj = (ArrayWritable) obj;
+        final Writable[] arr = mapObj.get();
+        if (key.equals(arr[0]) || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveJavaObject(arr[0]))
+                || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveWritableObject(arr[0]))) {
+          return arr[1];
+        }
+      }
+
+      return null;
+    }
+
+    if (data instanceof Map) {
+      final Map<?, ?> map = (Map<?, ?>) data;
+
+      if (map.containsKey(key)) {
+        return map.get(key);
+      }
+
+      for (final Map.Entry<?, ?> entry : map.entrySet()) {
+        if (key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveJavaObject(entry.getKey()))
+                || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveWritableObject(entry.getKey()))) {
+          return entry.getValue();
+        }
+      }
+
+      return null;
+    }
+
+    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java
new file mode 100644
index 0000000..53ca31d
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java
@@ -0,0 +1,185 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.SettableListObjectInspector;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.Writable;
+
+/**
+ * The ParquetHiveArrayInspector will inspect an ArrayWritable, considering it as an Hive array.<br />
+ * It can also inspect a List if Hive decides to inspect the result of an inspection.
+ *
+ */
+public class ParquetHiveArrayInspector implements SettableListObjectInspector {
+
+  ObjectInspector arrayElementInspector;
+
+  public ParquetHiveArrayInspector(final ObjectInspector arrayElementInspector) {
+    this.arrayElementInspector = arrayElementInspector;
+  }
+
+  @Override
+  public String getTypeName() {
+    return "array<" + arrayElementInspector.getTypeName() + ">";
+  }
+
+  @Override
+  public Category getCategory() {
+    return Category.LIST;
+  }
+
+  @Override
+  public ObjectInspector getListElementObjectInspector() {
+    return arrayElementInspector;
+  }
+
+  @Override
+  public Object getListElement(final Object data, final int index) {
+    if (data == null) {
+      return null;
+    }
+
+    if (data instanceof ArrayWritable) {
+      final Writable[] listContainer = ((ArrayWritable) data).get();
+
+      if (listContainer == null || listContainer.length == 0) {
+        return null;
+      }
+
+      final Writable subObj = listContainer[0];
+
+      if (subObj == null) {
+        return null;
+      }
+
+      if (index >= 0 && index < ((ArrayWritable) subObj).get().length) {
+        return ((ArrayWritable) subObj).get()[index];
+      } else {
+        return null;
+      }
+    }
+
+    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
+  }
+
+  @Override
+  public int getListLength(final Object data) {
+    if (data == null) {
+      return -1;
+    }
+
+    if (data instanceof ArrayWritable) {
+      final Writable[] listContainer = ((ArrayWritable) data).get();
+
+      if (listContainer == null || listContainer.length == 0) {
+        return -1;
+      }
+
+      final Writable subObj = listContainer[0];
+
+      if (subObj == null) {
+        return 0;
+      }
+
+      return ((ArrayWritable) subObj).get().length;
+    }
+
+    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
+  }
+
+  @Override
+  public List<?> getList(final Object data) {
+    if (data == null) {
+      return null;
+    }
+
+    if (data instanceof ArrayWritable) {
+      final Writable[] listContainer = ((ArrayWritable) data).get();
+
+      if (listContainer == null || listContainer.length == 0) {
+        return null;
+      }
+
+      final Writable subObj = listContainer[0];
+
+      if (subObj == null) {
+        return null;
+      }
+
+      final Writable[] array = ((ArrayWritable) subObj).get();
+      final List<Writable> list = new ArrayList<Writable>();
+
+      for (final Writable obj : array) {
+        list.add(obj);
+      }
+
+      return list;
+    }
+
+    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
+  }
+
+  @Override
+  public Object create(final int size) {
+    final ArrayList<Object> result = new ArrayList<Object>(size);
+    for (int i = 0; i < size; ++i) {
+      result.add(null);
+    }
+    return result;
+  }
+
+  @Override
+  public Object set(final Object list, final int index, final Object element) {
+    final ArrayList l = (ArrayList) list;
+    l.set(index, element);
+    return list;
+  }
+
+  @Override
+  public Object resize(final Object list, final int newSize) {
+    final ArrayList l = (ArrayList) list;
+    l.ensureCapacity(newSize);
+    while (l.size() < newSize) {
+      l.add(null);
+    }
+    while (l.size() > newSize) {
+      l.remove(l.size() - 1);
+    }
+    return list;
+  }
+
+  @Override
+  public boolean equals(final Object o) {
+    if (o == null || o.getClass() != getClass()) {
+      return false;
+    } else if (o == this) {
+      return true;
+    } else {
+      final ObjectInspector other = ((ParquetHiveArrayInspector) o).arrayElementInspector;
+      return other.equals(arrayElementInspector);
+    }
+  }
+
+  @Override
+  public int hashCode() {
+    int hash = 3;
+    hash = 29 * hash + (this.arrayElementInspector != null ? this.arrayElementInspector.hashCode() : 0);
+    return hash;
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
new file mode 100644
index 0000000..b689336
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
@@ -0,0 +1,274 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Properties;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.io.IOConstants;
+import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
+import org.apache.hadoop.hive.serde2.SerDeException;
+import org.apache.hadoop.hive.serde2.SerDeStats;
+import org.apache.hadoop.hive.serde2.io.ByteWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.StructField;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.BooleanWritable;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+
+import parquet.io.api.Binary;
+
+/**
+ *
+ * A ParquetHiveSerDe for Hive (with the deprecated package mapred)
+ *
+ */
+public class ParquetHiveSerDe extends AbstractSerDe {
+
+  public static final Text MAP_KEY = new Text("key");
+  public static final Text MAP_VALUE = new Text("value");
+  public static final Text MAP = new Text("map");
+  public static final Text ARRAY = new Text("bag");
+
+  private SerDeStats stats;
+  private ObjectInspector objInspector;
+
+  private enum LAST_OPERATION {
+    SERIALIZE,
+    DESERIALIZE,
+    UNKNOWN
+  }
+
+  private LAST_OPERATION status;
+  private long serializedSize;
+  private long deserializedSize;
+
+  @Override
+  public final void initialize(final Configuration conf, final Properties tbl) throws SerDeException {
+
+    final TypeInfo rowTypeInfo;
+    final List<String> columnNames;
+    final List<TypeInfo> columnTypes;
+    // Get column names and sort order
+    final String columnNameProperty = tbl.getProperty(IOConstants.COLUMNS);
+    final String columnTypeProperty = tbl.getProperty(IOConstants.COLUMNS_TYPES);
+
+    if (columnNameProperty.length() == 0) {
+      columnNames = new ArrayList<String>();
+    } else {
+      columnNames = Arrays.asList(columnNameProperty.split(","));
+    }
+    if (columnTypeProperty.length() == 0) {
+      columnTypes = new ArrayList<TypeInfo>();
+    } else {
+      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);
+    }
+    if (columnNames.size() != columnTypes.size()) {
+      throw new IllegalArgumentException("ParquetHiveSerde initialization failed. Number of column " +
+        "name and column type differs. columnNames = " + columnNames + ", columnTypes = " +
+        columnTypes);
+    }
+    // Create row related objects
+    rowTypeInfo = TypeInfoFactory.getStructTypeInfo(columnNames, columnTypes);
+    this.objInspector = new ArrayWritableObjectInspector((StructTypeInfo) rowTypeInfo);
+
+    // Stats part
+    stats = new SerDeStats();
+    serializedSize = 0;
+    deserializedSize = 0;
+    status = LAST_OPERATION.UNKNOWN;
+  }
+
+  @Override
+  public Object deserialize(final Writable blob) throws SerDeException {
+    status = LAST_OPERATION.DESERIALIZE;
+    deserializedSize = 0;
+    if (blob instanceof ArrayWritable) {
+      deserializedSize = ((ArrayWritable) blob).get().length;
+      return blob;
+    } else {
+      return null;
+    }
+  }
+
+  @Override
+  public ObjectInspector getObjectInspector() throws SerDeException {
+    return objInspector;
+  }
+
+  @Override
+  public Class<? extends Writable> getSerializedClass() {
+    return ArrayWritable.class;
+  }
+
+  @Override
+  public Writable serialize(final Object obj, final ObjectInspector objInspector)
+      throws SerDeException {
+    if (!objInspector.getCategory().equals(Category.STRUCT)) {
+      throw new SerDeException("Cannot serialize " + objInspector.getCategory() + ". Can only serialize a struct");
+    }
+    final ArrayWritable serializeData = createStruct(obj, (StructObjectInspector) objInspector);
+    serializedSize = serializeData.get().length;
+    status = LAST_OPERATION.SERIALIZE;
+    return serializeData;
+  }
+
+  private ArrayWritable createStruct(final Object obj, final StructObjectInspector inspector)
+      throws SerDeException {
+    final List<? extends StructField> fields = inspector.getAllStructFieldRefs();
+    final Writable[] arr = new Writable[fields.size()];
+    for (int i = 0; i < fields.size(); i++) {
+      final StructField field = fields.get(i);
+      final Object subObj = inspector.getStructFieldData(obj, field);
+      final ObjectInspector subInspector = field.getFieldObjectInspector();
+      arr[i] = createObject(subObj, subInspector);
+    }
+    return new ArrayWritable(Writable.class, arr);
+  }
+
+  private Writable createMap(final Object obj, final MapObjectInspector inspector)
+      throws SerDeException {
+    final Map<?, ?> sourceMap = inspector.getMap(obj);
+    final ObjectInspector keyInspector = inspector.getMapKeyObjectInspector();
+    final ObjectInspector valueInspector = inspector.getMapValueObjectInspector();
+    final List<ArrayWritable> array = new ArrayList<ArrayWritable>();
+
+    if (sourceMap != null) {
+      for (final Entry<?, ?> keyValue : sourceMap.entrySet()) {
+        final Writable key = createObject(keyValue.getKey(), keyInspector);
+        final Writable value = createObject(keyValue.getValue(), valueInspector);
+        if (key != null) {
+          Writable[] arr = new Writable[2];
+          arr[0] = key;
+          arr[1] = value;
+          array.add(new ArrayWritable(Writable.class, arr));
+        }
+      }
+    }
+    if (array.size() > 0) {
+      final ArrayWritable subArray = new ArrayWritable(ArrayWritable.class,
+          array.toArray(new ArrayWritable[array.size()]));
+      return new ArrayWritable(Writable.class, new Writable[] {subArray});
+    } else {
+      return null;
+    }
+  }
+
+  private ArrayWritable createArray(final Object obj, final ListObjectInspector inspector)
+      throws SerDeException {
+    final List<?> sourceArray = inspector.getList(obj);
+    final ObjectInspector subInspector = inspector.getListElementObjectInspector();
+    final List<Writable> array = new ArrayList<Writable>();
+    if (sourceArray != null) {
+      for (final Object curObj : sourceArray) {
+        final Writable newObj = createObject(curObj, subInspector);
+        if (newObj != null) {
+          array.add(newObj);
+        }
+      }
+    }
+    if (array.size() > 0) {
+      final ArrayWritable subArray = new ArrayWritable(array.get(0).getClass(),
+          array.toArray(new Writable[array.size()]));
+      return new ArrayWritable(Writable.class, new Writable[] {subArray});
+    } else {
+      return null;
+    }
+  }
+
+  private Writable createPrimitive(final Object obj, final PrimitiveObjectInspector inspector)
+      throws SerDeException {
+    if (obj == null) {
+      return null;
+    }
+    switch (inspector.getPrimitiveCategory()) {
+    case VOID:
+      return null;
+    case BOOLEAN:
+      return new BooleanWritable(((BooleanObjectInspector) inspector).get(obj) ? Boolean.TRUE : Boolean.FALSE);
+    case BYTE:
+      return new ByteWritable((byte) ((ByteObjectInspector) inspector).get(obj));
+    case DOUBLE:
+      return new DoubleWritable(((DoubleObjectInspector) inspector).get(obj));
+    case FLOAT:
+      return new FloatWritable(((FloatObjectInspector) inspector).get(obj));
+    case INT:
+      return new IntWritable(((IntObjectInspector) inspector).get(obj));
+    case LONG:
+      return new LongWritable(((LongObjectInspector) inspector).get(obj));
+    case SHORT:
+      return new ShortWritable((short) ((ShortObjectInspector) inspector).get(obj));
+    case STRING:
+      return new BinaryWritable(Binary.fromString(((StringObjectInspector) inspector).getPrimitiveJavaObject(obj)));
+    default:
+      throw new SerDeException("Unknown primitive : " + inspector.getPrimitiveCategory());
+    }
+  }
+
+  private Writable createObject(final Object obj, final ObjectInspector inspector) throws SerDeException {
+    switch (inspector.getCategory()) {
+    case STRUCT:
+      return createStruct(obj, (StructObjectInspector) inspector);
+    case LIST:
+      return createArray(obj, (ListObjectInspector) inspector);
+    case MAP:
+      return createMap(obj, (MapObjectInspector) inspector);
+    case PRIMITIVE:
+      return createPrimitive(obj, (PrimitiveObjectInspector) inspector);
+    default:
+      throw new SerDeException("Unknown data type" + inspector.getCategory());
+    }
+  }
+
+  @Override
+  public SerDeStats getSerDeStats() {
+    // must be different
+    assert (status != LAST_OPERATION.UNKNOWN);
+    if (status == LAST_OPERATION.SERIALIZE) {
+      stats.setRawDataSize(serializedSize);
+    } else {
+      stats.setRawDataSize(deserializedSize);
+    }
+    return stats;
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java
new file mode 100644
index 0000000..5aa1448
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java
@@ -0,0 +1,60 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde;
+
+import java.util.Map;
+
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.Writable;
+
+/**
+ * The StandardParquetHiveMapInspector will inspect an ArrayWritable, considering it as a Hive map.<br />
+ * It can also inspect a Map if Hive decides to inspect the result of an inspection.
+ *
+ */
+public class StandardParquetHiveMapInspector extends AbstractParquetMapInspector {
+
+  public StandardParquetHiveMapInspector(final ObjectInspector keyInspector,
+      final ObjectInspector valueInspector) {
+    super(keyInspector, valueInspector);
+  }
+
+  @Override
+  public Object getMapValueElement(final Object data, final Object key) {
+    if (data == null || key == null) {
+      return null;
+    }
+    if (data instanceof ArrayWritable) {
+      final Writable[] mapContainer = ((ArrayWritable) data).get();
+
+      if (mapContainer == null || mapContainer.length == 0) {
+        return null;
+      }
+      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
+      for (final Writable obj : mapArray) {
+        final ArrayWritable mapObj = (ArrayWritable) obj;
+        final Writable[] arr = mapObj.get();
+        if (key.equals(arr[0])) {
+          return arr[1];
+        }
+      }
+      return null;
+    }
+    if (data instanceof Map) {
+      return ((Map) data).get(key);
+    }
+    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
+  }
+}
\ No newline at end of file
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetByteInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetByteInspector.java
new file mode 100644
index 0000000..d5d1bf1
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetByteInspector.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde.primitive;
+
+import org.apache.hadoop.hive.serde2.io.ByteWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableByteObjectInspector;
+import org.apache.hadoop.io.IntWritable;
+
+/**
+ * The ParquetByteInspector can inspect both ByteWritables and IntWritables into bytes.
+ *
+ */
+public class ParquetByteInspector extends AbstractPrimitiveJavaObjectInspector implements SettableByteObjectInspector {
+
+  ParquetByteInspector() {
+    super(TypeInfoFactory.byteTypeInfo);
+  }
+
+  @Override
+  public Object getPrimitiveWritableObject(final Object o) {
+    return o == null ? null : new ByteWritable(get(o));
+  }
+
+  @Override
+  public Object create(final byte val) {
+    return new ByteWritable(val);
+  }
+
+  @Override
+  public Object set(final Object o, final byte val) {
+    ((ByteWritable) o).set(val);
+    return o;
+  }
+
+  @Override
+  public byte get(Object o) {
+    // Accept int writables and convert them.
+    if (o instanceof IntWritable) {
+      return (byte) ((IntWritable) o).get();
+    }
+    return ((ByteWritable) o).get();
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetPrimitiveInspectorFactory.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetPrimitiveInspectorFactory.java
new file mode 100644
index 0000000..79d88ce
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetPrimitiveInspectorFactory.java
@@ -0,0 +1,29 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde.primitive;
+
+/**
+ * The ParquetPrimitiveInspectorFactory allows us to be sure that the same object is inspected by the same inspector.
+ *
+ */
+public class ParquetPrimitiveInspectorFactory {
+
+  public static final ParquetByteInspector parquetByteInspector = new ParquetByteInspector();
+  public static final ParquetShortInspector parquetShortInspector = new ParquetShortInspector();
+  public static final ParquetStringInspector parquetStringInspector = new ParquetStringInspector();
+
+  private ParquetPrimitiveInspectorFactory() {
+    // prevent instantiation
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetShortInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetShortInspector.java
new file mode 100644
index 0000000..94f2813
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetShortInspector.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde.primitive;
+
+import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableShortObjectInspector;
+import org.apache.hadoop.io.IntWritable;
+
+/**
+ * The ParquetShortInspector can inspect both ShortWritables and IntWritables into shorts.
+ *
+ */
+public class ParquetShortInspector extends AbstractPrimitiveJavaObjectInspector implements SettableShortObjectInspector {
+
+  ParquetShortInspector() {
+    super(TypeInfoFactory.shortTypeInfo);
+  }
+
+  @Override
+  public Object getPrimitiveWritableObject(final Object o) {
+    return o == null ? null : new ShortWritable(get(o));
+  }
+
+  @Override
+  public Object create(final short val) {
+    return new ShortWritable(val);
+  }
+
+  @Override
+  public Object set(final Object o, final short val) {
+    ((ShortWritable) o).set(val);
+    return o;
+  }
+
+  @Override
+  public short get(Object o) {
+    // Accept int writables and convert them.
+    if (o instanceof IntWritable) {
+      return (short) ((IntWritable) o).get();
+    }
+    return ((ShortWritable) o).get();
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetStringInspector.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetStringInspector.java
new file mode 100644
index 0000000..03e8369
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetStringInspector.java
@@ -0,0 +1,98 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde.primitive;
+
+import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableStringObjectInspector;
+import org.apache.hadoop.io.Text;
+
+import parquet.io.api.Binary;
+
+/**
+ * The ParquetStringInspector inspects a BinaryWritable to give a Text or String.
+ *
+ */
+public class ParquetStringInspector extends AbstractPrimitiveJavaObjectInspector implements SettableStringObjectInspector {
+
+  ParquetStringInspector() {
+    super(TypeInfoFactory.stringTypeInfo);
+  }
+
+  @Override
+  public Text getPrimitiveWritableObject(final Object o) {
+    if (o == null) {
+      return null;
+    }
+
+    if (o instanceof BinaryWritable) {
+      return new Text(((BinaryWritable) o).getBytes());
+    }
+
+    if (o instanceof Text) {
+      return (Text) o;
+    }
+
+    if (o instanceof String) {
+      return new Text((String) o);
+    }
+
+    throw new UnsupportedOperationException("Cannot inspect " + o.getClass().getCanonicalName());
+  }
+
+  @Override
+  public String getPrimitiveJavaObject(final Object o) {
+    if (o == null) {
+      return null;
+    }
+
+    if (o instanceof BinaryWritable) {
+      return ((BinaryWritable) o).getString();
+    }
+
+    if (o instanceof Text) {
+      return ((Text) o).toString();
+    }
+
+    if (o instanceof String) {
+      return (String) o;
+    }
+
+    throw new UnsupportedOperationException("Cannot inspect " + o.getClass().getCanonicalName());
+  }
+
+  @Override
+  public Object set(final Object o, final Text text) {
+    return new BinaryWritable(text == null ? null : Binary.fromByteArray(text.getBytes()));
+  }
+
+  @Override
+  public Object set(final Object o, final String string) {
+    return new BinaryWritable(string == null ? null : Binary.fromString(string));
+  }
+
+  @Override
+  public Object create(final Text text) {
+    if (text == null) {
+      return null;
+    }
+    return text.toString();
+  }
+
+  @Override
+  public Object create(final String string) {
+    return string;
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BigDecimalWritable.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BigDecimalWritable.java
new file mode 100644
index 0000000..c5d6394
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BigDecimalWritable.java
@@ -0,0 +1,143 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.writable;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import org.apache.hadoop.hive.serde2.ByteStream.Output;
+import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils;
+import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.VInt;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.io.WritableUtils;
+
+/**
+ * This file is taken from a patch to hive 0.11
+ * Issue : https://issues.apache.org/jira/browse/HIVE-2693
+ *
+ */
+public class BigDecimalWritable implements WritableComparable<BigDecimalWritable> {
+
+  private byte[] internalStorage = new byte[0];
+  private int scale;
+
+  private final VInt vInt = new VInt(); // reusable integer
+
+  public BigDecimalWritable() {
+  }
+
+  public BigDecimalWritable(final byte[] bytes, final int scale) {
+    set(bytes, scale);
+  }
+
+  public BigDecimalWritable(final BigDecimalWritable writable) {
+    set(writable.getBigDecimal());
+  }
+
+  public BigDecimalWritable(final BigDecimal value) {
+    set(value);
+  }
+
+  public void set(BigDecimal value) {
+    value = value.stripTrailingZeros();
+    if (value.compareTo(BigDecimal.ZERO) == 0) {
+      // Special case for 0, because java doesn't strip zeros correctly on
+      // that number.
+      value = BigDecimal.ZERO;
+    }
+    set(value.unscaledValue().toByteArray(), value.scale());
+  }
+
+  public void set(final BigDecimalWritable writable) {
+    set(writable.getBigDecimal());
+  }
+
+  public void set(final byte[] bytes, final int scale) {
+    this.internalStorage = bytes;
+    this.scale = scale;
+  }
+
+  public void setFromBytes(final byte[] bytes, int offset, final int length) {
+    LazyBinaryUtils.readVInt(bytes, offset, vInt);
+    scale = vInt.value;
+    offset += vInt.length;
+    LazyBinaryUtils.readVInt(bytes, offset, vInt);
+    offset += vInt.length;
+    if (internalStorage.length != vInt.value) {
+      internalStorage = new byte[vInt.value];
+    }
+    System.arraycopy(bytes, offset, internalStorage, 0, vInt.value);
+  }
+
+  public BigDecimal getBigDecimal() {
+    return new BigDecimal(new BigInteger(internalStorage), scale);
+  }
+
+  @Override
+  public void readFields(final DataInput in) throws IOException {
+    scale = WritableUtils.readVInt(in);
+    final int byteArrayLen = WritableUtils.readVInt(in);
+    if (internalStorage.length != byteArrayLen) {
+      internalStorage = new byte[byteArrayLen];
+    }
+    in.readFully(internalStorage);
+  }
+
+  @Override
+  public void write(final DataOutput out) throws IOException {
+    WritableUtils.writeVInt(out, scale);
+    WritableUtils.writeVInt(out, internalStorage.length);
+    out.write(internalStorage);
+  }
+
+  @Override
+  public int compareTo(final BigDecimalWritable that) {
+    return getBigDecimal().compareTo(that.getBigDecimal());
+  }
+
+  public void writeToByteStream(final Output byteStream) {
+    LazyBinaryUtils.writeVInt(byteStream, scale);
+    LazyBinaryUtils.writeVInt(byteStream, internalStorage.length);
+    byteStream.write(internalStorage, 0, internalStorage.length);
+  }
+
+  @Override
+  public String toString() {
+    return getBigDecimal().toString();
+  }
+
+  @Override
+  public boolean equals(final Object other) {
+    if (other == null || !(other instanceof BigDecimalWritable)) {
+      return false;
+    }
+    final BigDecimalWritable bdw = (BigDecimalWritable) other;
+
+    // 'equals' and 'compareTo' are not compatible with BigDecimals. We want
+    // compareTo which returns true iff the numbers are equal (e.g.: 3.14 is
+        // the same as 3.140). 'Equals' returns true iff equal and the same
+    // scale
+    // is set in the decimals (e.g.: 3.14 is not the same as 3.140)
+    return getBigDecimal().compareTo(bdw.getBigDecimal()) == 0;
+  }
+
+  @Override
+  public int hashCode() {
+    return getBigDecimal().hashCode();
+  }
+
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BinaryWritable.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BinaryWritable.java
new file mode 100644
index 0000000..11ab576
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/writable/BinaryWritable.java
@@ -0,0 +1,93 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.writable;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import parquet.io.api.Binary;
+
+/**
+ *
+ * A Wrapper to support constructor with Binary and String
+ *
+ * TODO : remove it, and call BytesWritable with the getBytes() in HIVE-6366
+ *
+ */
+public class BinaryWritable implements Writable {
+
+  private Binary binary;
+
+  public BinaryWritable(final Binary binary) {
+    this.binary = binary;
+  }
+
+  public Binary getBinary() {
+    return binary;
+  }
+
+  public byte[] getBytes() {
+    return binary.getBytes();
+  }
+
+  public String getString() {
+    return binary.toStringUsingUTF8();
+  }
+
+  @Override
+  public void readFields(DataInput input) throws IOException {
+    byte[] bytes = new byte[input.readInt()];
+    input.readFully(bytes);
+    binary = Binary.fromByteArray(bytes);
+  }
+
+  @Override
+  public void write(DataOutput output) throws IOException {
+    output.writeInt(binary.length());
+    binary.writeTo(output);
+  }
+
+  @Override
+  public int hashCode() {
+    return binary == null ? 0 : binary.hashCode();
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (obj instanceof BinaryWritable) {
+      final BinaryWritable other = (BinaryWritable)obj;
+      return binary.equals(other.binary);
+    }
+    return false;
+  }
+
+  public static class DicBinaryWritable extends BinaryWritable {
+
+    private final String string;
+
+    public DicBinaryWritable(Binary binary, String string) {
+      super(binary);
+      this.string = string;
+    }
+
+    @Override
+    public String getString() {
+      return string;
+    }
+  }
+
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriteSupport.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriteSupport.java
new file mode 100644
index 0000000..060b1b7
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriteSupport.java
@@ -0,0 +1,61 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.write;
+
+import java.util.HashMap;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.ArrayWritable;
+
+import parquet.hadoop.api.WriteSupport;
+import parquet.io.api.RecordConsumer;
+import parquet.schema.MessageType;
+import parquet.schema.MessageTypeParser;
+
+/**
+ *
+ * DataWritableWriteSupport is a WriteSupport for the DataWritableWriter
+ *
+ */
+public class DataWritableWriteSupport extends WriteSupport<ArrayWritable> {
+
+  public static final String PARQUET_HIVE_SCHEMA = "parquet.hive.schema";
+
+  private DataWritableWriter writer;
+  private MessageType schema;
+
+  public static void setSchema(final MessageType schema, final Configuration configuration) {
+    configuration.set(PARQUET_HIVE_SCHEMA, schema.toString());
+  }
+
+  public static MessageType getSchema(final Configuration configuration) {
+    return MessageTypeParser.parseMessageType(configuration.get(PARQUET_HIVE_SCHEMA));
+  }
+
+  @Override
+  public WriteContext init(final Configuration configuration) {
+    schema = getSchema(configuration);
+    return new WriteContext(schema, new HashMap<String, String>());
+  }
+
+  @Override
+  public void prepareForWrite(final RecordConsumer recordConsumer) {
+    writer = new DataWritableWriter(recordConsumer, schema);
+  }
+
+  @Override
+  public void write(final ArrayWritable record) {
+    writer.write(record);
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java
new file mode 100644
index 0000000..a98f6be
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java
@@ -0,0 +1,154 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.write;
+
+import org.apache.hadoop.hive.ql.io.parquet.writable.BigDecimalWritable;
+import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable;
+import org.apache.hadoop.hive.serde2.io.ByteWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.BooleanWritable;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Writable;
+
+import parquet.io.ParquetEncodingException;
+import parquet.io.api.RecordConsumer;
+import parquet.schema.GroupType;
+import parquet.schema.Type;
+
+/**
+ *
+ * DataWritableWriter is a writer,
+ * that will read an ArrayWritable and give the data to parquet
+ * with the expected schema
+ *
+ */
+public class DataWritableWriter {
+
+  private final RecordConsumer recordConsumer;
+  private final GroupType schema;
+
+  public DataWritableWriter(final RecordConsumer recordConsumer, final GroupType schema) {
+    this.recordConsumer = recordConsumer;
+    this.schema = schema;
+  }
+
+  public void write(final ArrayWritable arr) {
+    if (arr == null) {
+      return;
+    }
+    recordConsumer.startMessage();
+    writeData(arr, schema);
+    recordConsumer.endMessage();
+  }
+
+  private void writeData(final ArrayWritable arr, final GroupType type) {
+    if (arr == null) {
+      return;
+    }
+    final int fieldCount = type.getFieldCount();
+    Writable[] values = arr.get();
+    for (int field = 0; field < fieldCount; ++field) {
+      final Type fieldType = type.getType(field);
+      final String fieldName = fieldType.getName();
+      final Writable value = values[field];
+      if (value == null) {
+        continue;
+      }
+      recordConsumer.startField(fieldName, field);
+
+      if (fieldType.isPrimitive()) {
+        writePrimitive(value);
+      } else {
+        recordConsumer.startGroup();
+        if (value instanceof ArrayWritable) {
+          if (fieldType.asGroupType().getRepetition().equals(Type.Repetition.REPEATED)) {
+            writeArray((ArrayWritable) value, fieldType.asGroupType());
+          } else {
+            writeData((ArrayWritable) value, fieldType.asGroupType());
+          }
+        } else if (value != null) {
+          throw new ParquetEncodingException("This should be an ArrayWritable or MapWritable: " + value);
+        }
+
+        recordConsumer.endGroup();
+      }
+
+      recordConsumer.endField(fieldName, field);
+    }
+  }
+
+  private void writeArray(final ArrayWritable array, final GroupType type) {
+    if (array == null) {
+      return;
+    }
+    final Writable[] subValues = array.get();
+    final int fieldCount = type.getFieldCount();
+    for (int field = 0; field < fieldCount; ++field) {
+      final Type subType = type.getType(field);
+      recordConsumer.startField(subType.getName(), field);
+      for (int i = 0; i < subValues.length; ++i) {
+        final Writable subValue = subValues[i];
+        if (subValue != null) {
+          if (subType.isPrimitive()) {
+            if (subValue instanceof ArrayWritable) {
+              writePrimitive(((ArrayWritable) subValue).get()[field]);// 0 ?
+            } else {
+              writePrimitive(subValue);
+            }
+          } else {
+            if (!(subValue instanceof ArrayWritable)) {
+              throw new RuntimeException("This should be a ArrayWritable: " + subValue);
+            } else {
+              recordConsumer.startGroup();
+              writeData((ArrayWritable) subValue, subType.asGroupType());
+              recordConsumer.endGroup();
+            }
+          }
+        }
+      }
+      recordConsumer.endField(subType.getName(), field);
+    }
+  }
+
+  private void writePrimitive(final Writable value) {
+    if (value == null) {
+      return;
+    }
+    if (value instanceof DoubleWritable) {
+      recordConsumer.addDouble(((DoubleWritable) value).get());
+    } else if (value instanceof BooleanWritable) {
+      recordConsumer.addBoolean(((BooleanWritable) value).get());
+    } else if (value instanceof FloatWritable) {
+      recordConsumer.addFloat(((FloatWritable) value).get());
+    } else if (value instanceof IntWritable) {
+      recordConsumer.addInteger(((IntWritable) value).get());
+    } else if (value instanceof LongWritable) {
+      recordConsumer.addLong(((LongWritable) value).get());
+    } else if (value instanceof ShortWritable) {
+      recordConsumer.addInteger(((ShortWritable) value).get());
+    } else if (value instanceof ByteWritable) {
+      recordConsumer.addInteger(((ByteWritable) value).get());
+    } else if (value instanceof BigDecimalWritable) {
+      throw new UnsupportedOperationException("BigDecimal writing not implemented");
+    } else if (value instanceof BinaryWritable) {
+      recordConsumer.addBinary(((BinaryWritable) value).getBinary());
+    } else {
+      throw new IllegalArgumentException("Unknown value type: " + value + " " + value.getClass());
+    }
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java
new file mode 100644
index 0000000..cd603c2
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java
@@ -0,0 +1,93 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.write;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordWriter;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapreduce.OutputFormat;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.hive.ql.io.FSRecordWriter;
+
+import parquet.hadoop.ParquetOutputFormat;
+import parquet.hadoop.util.ContextUtil;
+
+public class ParquetRecordWriterWrapper implements RecordWriter<Void, ArrayWritable>,
+  FSRecordWriter {
+
+  public static final Log LOG = LogFactory.getLog(ParquetRecordWriterWrapper.class);
+
+  private final org.apache.hadoop.mapreduce.RecordWriter<Void, ArrayWritable> realWriter;
+  private final TaskAttemptContext taskContext;
+
+  public ParquetRecordWriterWrapper(
+      final OutputFormat<Void, ArrayWritable> realOutputFormat,
+      final JobConf jobConf,
+      final String name,
+      final Progressable progress) throws IOException {
+    try {
+      // create a TaskInputOutputContext
+      TaskAttemptID taskAttemptID = TaskAttemptID.forName(jobConf.get("mapred.task.id"));
+      if (taskAttemptID == null) {
+        taskAttemptID = new TaskAttemptID();
+      }
+      taskContext = ContextUtil.newTaskAttemptContext(jobConf, taskAttemptID);
+
+      LOG.info("creating real writer to write at " + name);
+      realWriter = (org.apache.hadoop.mapreduce.RecordWriter<Void, ArrayWritable>)
+          ((ParquetOutputFormat) realOutputFormat).getRecordWriter(taskContext, new Path(name));
+      LOG.info("real writer: " + realWriter);
+    } catch (final InterruptedException e) {
+      throw new IOException(e);
+    }
+  }
+
+  @Override
+  public void close(final Reporter reporter) throws IOException {
+    try {
+      realWriter.close(taskContext);
+    } catch (final InterruptedException e) {
+      throw new IOException(e);
+    }
+  }
+
+  @Override
+  public void write(final Void key, final ArrayWritable value) throws IOException {
+    try {
+      realWriter.write(key, value);
+    } catch (final InterruptedException e) {
+      throw new IOException(e);
+    }
+  }
+
+  @Override
+  public void close(final boolean abort) throws IOException {
+    close(null);
+  }
+
+  @Override
+  public void write(final Writable w) throws IOException {
+    write(null, (ArrayWritable) w);
+  }
+
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/VirtualColumn.java b/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/VirtualColumn.java
index 2bc7e86..0637d46 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/VirtualColumn.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/metadata/VirtualColumn.java
@@ -20,6 +20,7 @@
 
 import java.io.Serializable;
 import java.util.ArrayList;
+import java.util.Collection;
 import java.util.List;
 
 import org.apache.hadoop.conf.Configuration;
@@ -131,6 +132,12 @@ public boolean equals(Object o) {
         && this.typeInfo.getTypeName().equals(c.getTypeInfo().getTypeName());
   }
 
+  public static Collection<String> removeVirtualColumns(final Collection<String> columns) {
+    for(VirtualColumn vcol : VIRTUAL_COLUMNS) {
+      columns.remove(vcol.getName());
+    }
+    return columns;
+  }
 
   public static StructObjectInspector getVCSObjectInspector(List<VirtualColumn> vcs) {
     List<String> names = new ArrayList<String>(vcs.size());
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index 61fa04b..fd35a51 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -54,6 +54,9 @@
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
 import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;
 import org.apache.hadoop.hive.ql.io.orc.OrcSerde;
+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;
+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;
+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;
 import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -128,6 +131,10 @@
   protected static final String ORCFILE_SERDE = OrcSerde.class
       .getName();
 
+  protected static final String PARQUETFILE_INPUT = MapredParquetInputFormat.class.getName();
+  protected static final String PARQUETFILE_OUTPUT = MapredParquetOutputFormat.class.getName();
+  protected static final String PARQUETFILE_SERDE = ParquetHiveSerDe.class.getName();
+
   class RowFormatParams {
     String fieldDelim = null;
     String fieldEscape = null;
@@ -215,6 +222,12 @@ protected boolean fillStorageFormat(ASTNode child, AnalyzeCreateCommonVars share
         shared.serde = ORCFILE_SERDE;
         storageFormat = true;
         break;
+      case HiveParser.TOK_TBLPARQUETFILE:
+        inputFormat = PARQUETFILE_INPUT;
+        outputFormat = PARQUETFILE_OUTPUT;
+        shared.serde = PARQUETFILE_SERDE;
+        storageFormat = true;
+        break;
       case HiveParser.TOK_TABLEFILEFORMAT:
         inputFormat = unescapeSQLString(child.getChild(0).getText());
         outputFormat = unescapeSQLString(child.getChild(1).getText());
@@ -246,6 +259,10 @@ protected void fillDefaultStorageFormat(AnalyzeCreateCommonVars shared) {
           inputFormat = ORCFILE_INPUT;
           outputFormat = ORCFILE_OUTPUT;
           shared.serde = ORCFILE_SERDE;
+        } else if ("PARQUET".equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.HIVEDEFAULTFILEFORMAT))) {
+          inputFormat = PARQUETFILE_INPUT;
+          outputFormat = PARQUETFILE_OUTPUT;
+          shared.serde = PARQUETFILE_SERDE;
         } else {
           inputFormat = TEXTFILE_INPUT;
           outputFormat = TEXTFILE_OUTPUT;
@@ -934,7 +951,7 @@ public void setColumnAccessInfo(ColumnAccessInfo columnAccessInfo) {
    * @return true if the specification is prefix; never returns false, but throws
    * @throws HiveException
    */
-  final public boolean isValidPrefixSpec(Table tTable, Map<String, String> spec)
+  public final boolean isValidPrefixSpec(Table tTable, Map<String, String> spec)
  throws HiveException {
 
     // TODO - types need to be checked.
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
index ed9917d..923b93c 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
@@ -135,6 +135,7 @@ KW_SEQUENCEFILE: 'SEQUENCEFILE';
 KW_TEXTFILE: 'TEXTFILE';
 KW_RCFILE: 'RCFILE';
 KW_ORCFILE: 'ORC';
+KW_PARQUETFILE: 'PARQUET';
 KW_INPUTFORMAT: 'INPUTFORMAT';
 KW_OUTPUTFORMAT: 'OUTPUTFORMAT';
 KW_INPUTDRIVER: 'INPUTDRIVER';
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
index 0875c23..c1a36fb 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
@@ -180,6 +180,7 @@ TOK_TABLEROWFORMATMAPKEYS;
 TOK_TABLEROWFORMATLINES;
 TOK_TABLEROWFORMATNULL;
 TOK_TBLORCFILE;
+TOK_TBLPARQUETFILE;
 TOK_TBLSEQUENCEFILE;
 TOK_TBLTEXTFILE;
 TOK_TBLRCFILE;
@@ -1176,6 +1177,7 @@ fileFormat
     | KW_TEXTFILE  -> ^(TOK_TBLTEXTFILE)
     | KW_RCFILE  -> ^(TOK_TBLRCFILE)
     | KW_ORCFILE -> ^(TOK_TBLORCFILE)
+    | KW_PARQUETFILE -> ^(TOK_TBLPARQUETFILE)
     | KW_INPUTFORMAT inFmt=StringLiteral KW_OUTPUTFORMAT outFmt=StringLiteral (KW_INPUTDRIVER inDriver=StringLiteral KW_OUTPUTDRIVER outDriver=StringLiteral)?
       -> ^(TOK_TABLEFILEFORMAT $inFmt $outFmt $inDriver? $outDriver?)
     | genericSpec=identifier -> ^(TOK_FILEFORMAT_GENERIC $genericSpec)
@@ -1616,6 +1618,7 @@ tableFileFormat
       | KW_STORED KW_AS KW_TEXTFILE  -> TOK_TBLTEXTFILE
       | KW_STORED KW_AS KW_RCFILE  -> TOK_TBLRCFILE
       | KW_STORED KW_AS KW_ORCFILE -> TOK_TBLORCFILE
+      | KW_STORED KW_AS KW_PARQUETFILE -> TOK_TBLPARQUETFILE
       | KW_STORED KW_AS KW_INPUTFORMAT inFmt=StringLiteral KW_OUTPUTFORMAT outFmt=StringLiteral (KW_INPUTDRIVER inDriver=StringLiteral KW_OUTPUTDRIVER outDriver=StringLiteral)?
       -> ^(TOK_TABLEFILEFORMAT $inFmt $outFmt $inDriver? $outDriver?)
       | KW_STORED KW_BY storageHandler=StringLiteral
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index d50945e..ace0d34 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -9001,7 +9001,7 @@ private ASTNode analyzeCreateTable(ASTNode ast, QB qb)
 
         break;
       default:
-        assert false;
+        throw new AssertionError("Unknown token: " + child.getToken());
       }
     }
 
diff --git a/src/ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java b/src/ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java
new file mode 100644
index 0000000..ec0ebc0
--- /dev/null
+++ b/src/ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java
@@ -0,0 +1,37 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package parquet.hive;
+
+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;
+import org.apache.hadoop.io.ArrayWritable;
+
+import parquet.hadoop.ParquetInputFormat;
+
+/**
+ * Deprecated name of the parquet-hive input format. This class exists
+ * simply to provide backwards compatibility with users who specified
+ * this name in the Hive metastore. All users should now use
+ * STORED AS PARQUET
+ */
+@Deprecated
+public class DeprecatedParquetInputFormat extends MapredParquetInputFormat {
+
+  public DeprecatedParquetInputFormat() {
+    super();
+  }
+
+  public DeprecatedParquetInputFormat(final ParquetInputFormat<ArrayWritable> realInputFormat) {
+    super(realInputFormat);
+  }
+}
diff --git a/src/ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java b/src/ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java
new file mode 100644
index 0000000..a0bdd75
--- /dev/null
+++ b/src/ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package parquet.hive;
+
+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.mapreduce.OutputFormat;
+
+/**
+ * Deprecated name of the parquet-hive output format. This class exists
+ * simply to provide backwards compatibility with users who specified
+ * this name in the Hive metastore. All users should now use
+ * STORED AS PARQUET
+ */
+@Deprecated
+public class DeprecatedParquetOutputFormat extends MapredParquetOutputFormat {
+
+  public DeprecatedParquetOutputFormat() {
+    super();
+  }
+
+  public DeprecatedParquetOutputFormat(final OutputFormat<Void, ArrayWritable> mapreduceOutputFormat) {
+    super(mapreduceOutputFormat);
+  }
+}
diff --git a/src/ql/src/java/parquet/hive/MapredParquetInputFormat.java b/src/ql/src/java/parquet/hive/MapredParquetInputFormat.java
new file mode 100644
index 0000000..9b3d453
--- /dev/null
+++ b/src/ql/src/java/parquet/hive/MapredParquetInputFormat.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package parquet.hive;
+
+import org.apache.hadoop.io.ArrayWritable;
+
+import parquet.hadoop.ParquetInputFormat;
+
+/**
+ * Deprecated name of the parquet-hive input format. This class exists
+ * simply to provide backwards compatibility with users who specified
+ * this name in the Hive metastore. All users should now use
+ * STORED AS PARQUET
+ */
+@Deprecated
+public class MapredParquetInputFormat extends org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat {
+
+  public MapredParquetInputFormat() {
+    super();
+  }
+
+  public MapredParquetInputFormat(final ParquetInputFormat<ArrayWritable> realInputFormat) {
+   super(realInputFormat);
+  }
+}
diff --git a/src/ql/src/java/parquet/hive/MapredParquetOutputFormat.java b/src/ql/src/java/parquet/hive/MapredParquetOutputFormat.java
new file mode 100644
index 0000000..dc6ea3e
--- /dev/null
+++ b/src/ql/src/java/parquet/hive/MapredParquetOutputFormat.java
@@ -0,0 +1,35 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package parquet.hive;
+
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.mapreduce.OutputFormat;
+
+/**
+ * Deprecated name of the parquet-hive output format. This class exists
+ * simply to provide backwards compatibility with users who specified
+ * this name in the Hive metastore. All users should now use
+ * STORED AS PARQUET
+ */
+@Deprecated
+public class MapredParquetOutputFormat extends org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat {
+
+  public MapredParquetOutputFormat () {
+    super();
+  }
+
+  public MapredParquetOutputFormat(final OutputFormat<Void, ArrayWritable> mapreduceOutputFormat) {
+    super(mapreduceOutputFormat);
+  }
+}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java
new file mode 100644
index 0000000..0b25f6e
--- /dev/null
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java
@@ -0,0 +1,114 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet;
+
+import static org.junit.Assert.assertEquals;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+import org.junit.Test;
+
+import parquet.schema.MessageType;
+import parquet.schema.MessageTypeParser;
+
+public class TestHiveSchemaConverter {
+
+  private List<String> createHiveColumnsFrom(final String columnNamesStr) {
+    List<String> columnNames;
+    if (columnNamesStr.length() == 0) {
+      columnNames = new ArrayList<String>();
+    } else {
+      columnNames = Arrays.asList(columnNamesStr.split(","));
+    }
+
+    return columnNames;
+  }
+
+  private List<TypeInfo> createHiveTypeInfoFrom(final String columnsTypeStr) {
+    List<TypeInfo> columnTypes;
+
+    if (columnsTypeStr.length() == 0) {
+      columnTypes = new ArrayList<TypeInfo>();
+    } else {
+      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnsTypeStr);
+    }
+
+    return columnTypes;
+  }
+
+  private void testConversion(final String columnNamesStr, final String columnsTypeStr, final String expectedSchema) throws Exception {
+    final List<String> columnNames = createHiveColumnsFrom(columnNamesStr);
+    final List<TypeInfo> columnTypes = createHiveTypeInfoFrom(columnsTypeStr);
+    final MessageType messageTypeFound = HiveSchemaConverter.convert(columnNames, columnTypes);
+    final MessageType expectedMT = MessageTypeParser.parseMessageType(expectedSchema);
+    assertEquals("converting " + columnNamesStr + ": " + columnsTypeStr + " to " + expectedSchema, expectedMT, messageTypeFound);
+  }
+
+  @Test
+  public void testSimpleType() throws Exception {
+    testConversion(
+            "a,b,c",
+            "int,double,boolean",
+            "message hive_schema {\n"
+            + "  optional int32 a;\n"
+            + "  optional double b;\n"
+            + "  optional boolean c;\n"
+            + "}\n");
+  }
+
+  @Test
+  public void testArray() throws Exception {
+    testConversion("arrayCol",
+            "array<int>",
+            "message hive_schema {\n"
+            + "  optional group arrayCol (LIST) {\n"
+            + "    repeated group bag {\n"
+            + "      optional int32 array_element;\n"
+            + "    }\n"
+            + "  }\n"
+            + "}\n");
+  }
+
+  @Test
+  public void testStruct() throws Exception {
+    testConversion("structCol",
+            "struct<a:int,b:double,c:boolean>",
+            "message hive_schema {\n"
+            + "  optional group structCol {\n"
+            + "    optional int32 a;\n"
+            + "    optional double b;\n"
+            + "    optional boolean c;\n"
+            + "  }\n"
+            + "}\n");
+  }
+
+  @Test
+  public void testMap() throws Exception {
+    testConversion("mapCol",
+            "map<string,string>",
+            "message hive_schema {\n"
+            + "  optional group mapCol (MAP) {\n"
+            + "    repeated group map (MAP_KEY_VALUE) {\n"
+            + "      required binary key;\n"
+            + "      optional binary value;\n"
+            + "    }\n"
+            + "  }\n"
+            + "}\n");
+  }
+}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetInputFormat.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetInputFormat.java
new file mode 100644
index 0000000..1a54bf5
--- /dev/null
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetInputFormat.java
@@ -0,0 +1,37 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet;
+
+import static org.mockito.Mockito.mock;
+
+import org.apache.hadoop.io.ArrayWritable;
+import org.junit.Test;
+
+import parquet.hadoop.ParquetInputFormat;
+
+public class TestMapredParquetInputFormat {
+  @Test
+  public void testDefaultConstructor() {
+    new MapredParquetInputFormat();
+  }
+
+  @SuppressWarnings("unchecked")
+  @Test
+  public void testConstructorWithParquetInputFormat() {
+    new MapredParquetInputFormat(
+        (ParquetInputFormat<ArrayWritable>) mock(ParquetInputFormat.class)
+        );
+  }
+
+}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetOutputFormat.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetOutputFormat.java
new file mode 100644
index 0000000..417676d
--- /dev/null
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapredParquetOutputFormat.java
@@ -0,0 +1,90 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.fail;
+import static org.mockito.Mockito.mock;
+
+import java.io.IOException;
+import java.util.Properties;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport;
+import org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.util.Progressable;
+import org.junit.Test;
+
+import parquet.hadoop.ParquetOutputFormat;
+
+public class TestMapredParquetOutputFormat {
+
+  @Test
+  public void testConstructor() {
+    new MapredParquetOutputFormat();
+  }
+
+  @SuppressWarnings("unchecked")
+  @Test
+  public void testConstructorWithFormat() {
+    new MapredParquetOutputFormat((ParquetOutputFormat<ArrayWritable>) mock(ParquetOutputFormat.class));
+  }
+
+  @Test
+  public void testGetRecordWriterThrowsException() {
+    try {
+      new MapredParquetOutputFormat().getRecordWriter(null, null, null, null);
+      fail("should throw runtime exception.");
+    } catch (Exception e) {
+      assertEquals("Should never be used", e.getMessage());
+    }
+  }
+
+  @SuppressWarnings("unchecked")
+  @Test
+  public void testGetHiveRecordWriter() throws IOException {
+    Properties tableProps = new Properties();
+    tableProps.setProperty("columns", "foo,bar");
+    tableProps.setProperty("columns.types", "int:int");
+
+    final Progressable mockProgress = mock(Progressable.class);
+    final ParquetOutputFormat<ArrayWritable> outputFormat = (ParquetOutputFormat<ArrayWritable>) mock(ParquetOutputFormat.class);
+
+    JobConf jobConf = new JobConf();
+
+    try {
+      new MapredParquetOutputFormat(outputFormat) {
+        @Override
+        protected ParquetRecordWriterWrapper getParquerRecordWriterWrapper(
+            ParquetOutputFormat<ArrayWritable> realOutputFormat,
+            JobConf jobConf,
+            String finalOutPath,
+            Progressable progress
+            ) throws IOException {
+          assertEquals(outputFormat, realOutputFormat);
+          assertNotNull(jobConf.get(DataWritableWriteSupport.PARQUET_HIVE_SCHEMA));
+          assertEquals("/foo", finalOutPath.toString());
+          assertEquals(mockProgress, progress);
+          throw new RuntimeException("passed tests");
+        }
+      }.getHiveRecordWriter(jobConf, new Path("/foo"), null, false, tableProps, mockProgress);
+      fail("should throw runtime exception.");
+    } catch (RuntimeException e) {
+      assertEquals("passed tests", e.getMessage());
+    }
+  }
+}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java
new file mode 100644
index 0000000..be518b9
--- /dev/null
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java
@@ -0,0 +1,140 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet;
+
+import java.util.Properties;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;
+import org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable;
+import org.apache.hadoop.hive.serde2.SerDeException;
+import org.apache.hadoop.hive.serde2.io.ByteWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Writable;
+
+import parquet.io.api.Binary;
+
+public class TestParquetSerDe extends TestCase {
+
+  public void testParquetHiveSerDe() throws Throwable {
+    try {
+      // Create the SerDe
+      System.out.println("test: testParquetHiveSerDe");
+
+      final ParquetHiveSerDe serDe = new ParquetHiveSerDe();
+      final Configuration conf = new Configuration();
+      final Properties tbl = createProperties();
+      serDe.initialize(conf, tbl);
+
+      // Data
+      final Writable[] arr = new Writable[8];
+
+      arr[0] = new ByteWritable((byte) 123);
+      arr[1] = new ShortWritable((short) 456);
+      arr[2] = new IntWritable(789);
+      arr[3] = new LongWritable(1000l);
+      arr[4] = new DoubleWritable((double) 5.3);
+      arr[5] = new BinaryWritable(Binary.fromString("hive and hadoop and parquet. Big family."));
+
+      final Writable[] mapContainer = new Writable[1];
+      final Writable[] map = new Writable[3];
+      for (int i = 0; i < 3; ++i) {
+        final Writable[] pair = new Writable[2];
+        pair[0] = new BinaryWritable(Binary.fromString("key_" + i));
+        pair[1] = new IntWritable(i);
+        map[i] = new ArrayWritable(Writable.class, pair);
+      }
+      mapContainer[0] = new ArrayWritable(Writable.class, map);
+      arr[6] = new ArrayWritable(Writable.class, mapContainer);
+
+      final Writable[] arrayContainer = new Writable[1];
+      final Writable[] array = new Writable[5];
+      for (int i = 0; i < 5; ++i) {
+        array[i] = new BinaryWritable(Binary.fromString("elem_" + i));
+      }
+      arrayContainer[0] = new ArrayWritable(Writable.class, array);
+      arr[7] = new ArrayWritable(Writable.class, arrayContainer);
+
+      final ArrayWritable arrWritable = new ArrayWritable(Writable.class, arr);
+      // Test
+      deserializeAndSerializeLazySimple(serDe, arrWritable);
+      System.out.println("test: testParquetHiveSerDe - OK");
+
+    } catch (final Throwable e) {
+      e.printStackTrace();
+      throw e;
+    }
+  }
+
+  private void deserializeAndSerializeLazySimple(final ParquetHiveSerDe serDe, final ArrayWritable t) throws SerDeException {
+
+    // Get the row structure
+    final StructObjectInspector oi = (StructObjectInspector) serDe.getObjectInspector();
+
+    // Deserialize
+    final Object row = serDe.deserialize(t);
+    assertEquals("deserialization gives the wrong object class", row.getClass(), ArrayWritable.class);
+    assertEquals("size correct after deserialization", serDe.getSerDeStats().getRawDataSize(), t.get().length);
+    assertEquals("deserialization gives the wrong object", t, row);
+
+    // Serialize
+    final ArrayWritable serializedArr = (ArrayWritable) serDe.serialize(row, oi);
+    assertEquals("size correct after serialization", serDe.getSerDeStats().getRawDataSize(), serializedArr.get().length);
+    assertTrue("serialized object should be equal to starting object", arrayWritableEquals(t, serializedArr));
+  }
+
+  private Properties createProperties() {
+    final Properties tbl = new Properties();
+
+    // Set the configuration parameters
+    tbl.setProperty("columns", "abyte,ashort,aint,along,adouble,astring,amap,alist");
+    tbl.setProperty("columns.types", "tinyint:smallint:int:bigint:double:string:map<string,int>:array<string>");
+    tbl.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_FORMAT, "NULL");
+    return tbl;
+  }
+
+  public static boolean arrayWritableEquals(final ArrayWritable a1, final ArrayWritable a2) {
+    final Writable[] a1Arr = a1.get();
+    final Writable[] a2Arr = a2.get();
+
+    if (a1Arr.length != a2Arr.length) {
+      return false;
+    }
+
+    for (int i = 0; i < a1Arr.length; ++i) {
+      if (a1Arr[i] instanceof ArrayWritable) {
+        if (!(a2Arr[i] instanceof ArrayWritable)) {
+          return false;
+        }
+        if (!arrayWritableEquals((ArrayWritable) a1Arr[i], (ArrayWritable) a2Arr[i])) {
+          return false;
+        }
+      } else {
+        if (!a1Arr[i].equals(a2Arr[i])) {
+          return false;
+        }
+      }
+
+    }
+    return true;
+  }
+
+}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java
new file mode 100644
index 0000000..ef05150
--- /dev/null
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java
@@ -0,0 +1,98 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Writable;
+import org.junit.Test;
+
+public class TestAbstractParquetMapInspector extends TestCase {
+
+  class TestableAbstractParquetMapInspector extends AbstractParquetMapInspector {
+
+    public TestableAbstractParquetMapInspector(ObjectInspector keyInspector, ObjectInspector valueInspector) {
+      super(keyInspector, valueInspector);
+    }
+
+    @Override
+    public Object getMapValueElement(Object o, Object o1) {
+      throw new UnsupportedOperationException("Should not be called");
+    }
+  }
+  private TestableAbstractParquetMapInspector inspector;
+
+  @Override
+  public void setUp() {
+    inspector = new TestableAbstractParquetMapInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector,
+            PrimitiveObjectInspectorFactory.javaIntObjectInspector);
+  }
+
+  @Test
+  public void testNullMap() {
+    assertEquals("Wrong size", -1, inspector.getMapSize(null));
+    assertNull("Should be null", inspector.getMap(null));
+  }
+
+  @Test
+  public void testNullContainer() {
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);
+    assertEquals("Wrong size", -1, inspector.getMapSize(map));
+    assertNull("Should be null", inspector.getMap(map));
+  }
+
+  @Test
+  public void testEmptyContainer() {
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
+    assertEquals("Wrong size", -1, inspector.getMapSize(map));
+    assertNull("Should be null", inspector.getMap(map));
+  }
+
+  @Test
+  public void testRegularMap() {
+    final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
+    final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
+
+    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
+      new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
+
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
+
+    final Map<Writable, Writable> expected = new HashMap<Writable, Writable>();
+    expected.put(new IntWritable(0), new IntWritable(1));
+    expected.put(new IntWritable(2), new IntWritable(3));
+
+    assertEquals("Wrong size", 2, inspector.getMapSize(map));
+    assertEquals("Wrong result of inspection", expected, inspector.getMap(map));
+  }
+
+  @Test
+  public void testHashMap() {
+    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
+    map.put(new IntWritable(0), new IntWritable(1));
+    map.put(new IntWritable(2), new IntWritable(3));
+    map.put(new IntWritable(4), new IntWritable(5));
+    map.put(new IntWritable(6), new IntWritable(7));
+
+    assertEquals("Wrong size", 4, inspector.getMapSize(map));
+    assertEquals("Wrong result of inspection", map, inspector.getMap(map));
+  }
+}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java
new file mode 100644
index 0000000..8646ff4
--- /dev/null
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java
@@ -0,0 +1,90 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetPrimitiveInspectorFactory;
+import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Writable;
+import org.junit.Test;
+
+public class TestDeepParquetHiveMapInspector extends TestCase {
+
+  private DeepParquetHiveMapInspector inspector;
+
+  @Override
+  public void setUp() {
+    inspector = new DeepParquetHiveMapInspector(ParquetPrimitiveInspectorFactory.parquetShortInspector,
+            PrimitiveObjectInspectorFactory.javaIntObjectInspector);
+  }
+
+  @Test
+  public void testNullMap() {
+    assertNull("Should be null", inspector.getMapValueElement(null, new ShortWritable((short) 0)));
+  }
+
+  @Test
+  public void testNullContainer() {
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);
+    assertNull("Should be null", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
+  }
+
+  @Test
+  public void testEmptyContainer() {
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
+    assertNull("Should be null", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
+  }
+
+  @Test
+  public void testRegularMap() {
+    final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
+    final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
+
+    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
+      new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
+
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
+
+    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
+    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
+    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new ShortWritable((short) 0)));
+    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new ShortWritable((short) 2)));
+  }
+
+  @Test
+  public void testHashMap() {
+    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
+    map.put(new IntWritable(0), new IntWritable(1));
+    map.put(new IntWritable(2), new IntWritable(3));
+    map.put(new IntWritable(4), new IntWritable(5));
+    map.put(new IntWritable(6), new IntWritable(7));
+
+
+    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
+    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
+    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new IntWritable(4)));
+    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new IntWritable(6)));
+    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new ShortWritable((short) 0)));
+    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new ShortWritable((short) 2)));
+    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new ShortWritable((short) 4)));
+    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new ShortWritable((short) 6)));
+  }
+}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java
new file mode 100644
index 0000000..f3a24af
--- /dev/null
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java
@@ -0,0 +1,80 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Writable;
+import org.junit.Test;
+
+public class TestParquetHiveArrayInspector extends TestCase {
+
+  private ParquetHiveArrayInspector inspector;
+
+  @Override
+  public void setUp() {
+    inspector = new ParquetHiveArrayInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector);
+  }
+
+  @Test
+  public void testNullArray() {
+    assertEquals("Wrong size", -1, inspector.getListLength(null));
+    assertNull("Should be null", inspector.getList(null));
+    assertNull("Should be null", inspector.getListElement(null, 0));
+  }
+
+  @Test
+  public void testNullContainer() {
+    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, null);
+    assertEquals("Wrong size", -1, inspector.getListLength(list));
+    assertNull("Should be null", inspector.getList(list));
+    assertNull("Should be null", inspector.getListElement(list, 0));
+  }
+
+  @Test
+  public void testEmptyContainer() {
+    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
+    assertEquals("Wrong size", -1, inspector.getListLength(list));
+    assertNull("Should be null", inspector.getList(list));
+    assertNull("Should be null", inspector.getListElement(list, 0));
+  }
+
+  @Test
+  public void testRegularList() {
+    final ArrayWritable internalList = new ArrayWritable(Writable.class,
+            new Writable[]{new IntWritable(3), new IntWritable(5), new IntWritable(1)});
+    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, new ArrayWritable[]{internalList});
+
+    final List<Writable> expected = new ArrayList<Writable>();
+    expected.add(new IntWritable(3));
+    expected.add(new IntWritable(5));
+    expected.add(new IntWritable(1));
+
+    assertEquals("Wrong size", 3, inspector.getListLength(list));
+    assertEquals("Wrong result of inspection", expected, inspector.getList(list));
+
+    for (int i = 0; i < expected.size(); ++i) {
+      assertEquals("Wrong result of inspection", expected.get(i), inspector.getListElement(list, i));
+
+    }
+
+    assertNull("Should be null", inspector.getListElement(list, 3));
+  }
+}
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java
new file mode 100644
index 0000000..278419f
--- /dev/null
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java
@@ -0,0 +1,88 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.parquet.serde;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.io.ArrayWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Writable;
+import org.junit.Test;
+
+public class TestStandardParquetHiveMapInspector extends TestCase {
+
+  private StandardParquetHiveMapInspector inspector;
+
+  @Override
+  public void setUp() {
+    inspector = new StandardParquetHiveMapInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector,
+            PrimitiveObjectInspectorFactory.javaIntObjectInspector);
+  }
+
+  @Test
+  public void testNullMap() {
+    assertNull("Should be null", inspector.getMapValueElement(null, new IntWritable(0)));
+  }
+
+  @Test
+  public void testNullContainer() {
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);
+    assertNull("Should be null", inspector.getMapValueElement(map, new IntWritable(0)));
+  }
+
+  @Test
+  public void testEmptyContainer() {
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
+    assertNull("Should be null", inspector.getMapValueElement(map, new IntWritable(0)));
+  }
+
+  @Test
+  public void testRegularMap() {
+    final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
+    final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
+
+    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
+      new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
+
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
+
+    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
+    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
+    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
+    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 2)));
+  }
+
+  @Test
+  public void testHashMap() {
+    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
+    map.put(new IntWritable(0), new IntWritable(1));
+    map.put(new IntWritable(2), new IntWritable(3));
+    map.put(new IntWritable(4), new IntWritable(5));
+    map.put(new IntWritable(6), new IntWritable(7));
+
+    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
+    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
+    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new IntWritable(4)));
+    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new IntWritable(6)));
+    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
+    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 2)));
+    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 4)));
+    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 6)));
+  }
+}
diff --git a/src/ql/src/test/queries/clientpositive/parquet_create.q b/src/ql/src/test/queries/clientpositive/parquet_create.q
new file mode 100644
index 0000000..0b976bd
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/parquet_create.q
@@ -0,0 +1,36 @@
+DROP TABLE parquet_create_staging;
+DROP TABLE parquet_create;
+
+CREATE TABLE parquet_create_staging (
+    id int,
+    str string,
+    mp  MAP<STRING,STRING>,
+    lst ARRAY<STRING>,
+    strct STRUCT<A:STRING,B:STRING>
+) ROW FORMAT DELIMITED
+FIELDS TERMINATED BY '|'
+COLLECTION ITEMS TERMINATED BY ','
+MAP KEYS TERMINATED BY ':';
+
+CREATE TABLE parquet_create (
+    id int,
+    str string,
+    mp  MAP<STRING,STRING>,
+    lst ARRAY<STRING>,
+    strct STRUCT<A:STRING,B:STRING>
+) STORED AS PARQUET;
+
+DESCRIBE FORMATTED parquet_create;
+
+LOAD DATA LOCAL INPATH '../../data/files/parquet_create.txt' OVERWRITE INTO TABLE parquet_create_staging;
+
+SELECT * FROM parquet_create_staging;
+
+INSERT OVERWRITE TABLE parquet_create SELECT * FROM parquet_create_staging;
+
+SELECT * FROM parquet_create group by id;
+SELECT id, count(0) FROM parquet_create group by id;
+SELECT str from parquet_create;
+SELECT mp from parquet_create;
+SELECT lst from parquet_create;
+SELECT strct from parquet_create;
diff --git a/src/ql/src/test/queries/clientpositive/parquet_partitioned.q b/src/ql/src/test/queries/clientpositive/parquet_partitioned.q
new file mode 100644
index 0000000..103d26f
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/parquet_partitioned.q
@@ -0,0 +1,34 @@
+set hive.exec.dynamic.partition.mode=nonstrict;
+set hive.exec.dynamic.partition=true;
+
+DROP TABLE parquet_partitioned_staging;
+DROP TABLE parquet_partitioned;
+
+CREATE TABLE parquet_partitioned_staging (
+    id int,
+    str string,
+    part string
+) ROW FORMAT DELIMITED
+FIELDS TERMINATED BY '|';
+
+CREATE TABLE parquet_partitioned (
+    id int,
+    str string
+) PARTITIONED BY (part string)
+STORED AS PARQUET;
+
+DESCRIBE FORMATTED parquet_partitioned;
+
+LOAD DATA LOCAL INPATH '../../data/files/parquet_partitioned.txt' OVERWRITE INTO TABLE parquet_partitioned_staging;
+
+SELECT * FROM parquet_partitioned_staging;
+
+INSERT OVERWRITE TABLE parquet_partitioned PARTITION (part) SELECT * FROM parquet_partitioned_staging;
+
+set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
+SELECT * FROM parquet_partitioned;
+SELECT part, COUNT(0) FROM parquet_partitioned GROUP BY part;
+
+set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
+SELECT * FROM parquet_partitioned;
+SELECT part, COUNT(0) FROM parquet_partitioned GROUP BY part;
diff --git a/src/ql/src/test/results/clientpositive/parquet_create.q.out b/src/ql/src/test/results/clientpositive/parquet_create.q.out
new file mode 100644
index 0000000..34fdea2
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/parquet_create.q.out
@@ -0,0 +1,206 @@
+PREHOOK: query: DROP TABLE parquet_create_staging
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE parquet_create_staging
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: DROP TABLE parquet_create
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE parquet_create
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE parquet_create_staging (
+    id int,
+    str string,
+    mp  MAP<STRING,STRING>,
+    lst ARRAY<STRING>,
+    strct STRUCT<A:STRING,B:STRING>
+) ROW FORMAT DELIMITED
+FIELDS TERMINATED BY '|'
+COLLECTION ITEMS TERMINATED BY ','
+MAP KEYS TERMINATED BY ':'
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE parquet_create_staging (
+    id int,
+    str string,
+    mp  MAP<STRING,STRING>,
+    lst ARRAY<STRING>,
+    strct STRUCT<A:STRING,B:STRING>
+) ROW FORMAT DELIMITED
+FIELDS TERMINATED BY '|'
+COLLECTION ITEMS TERMINATED BY ','
+MAP KEYS TERMINATED BY ':'
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@parquet_create_staging
+PREHOOK: query: CREATE TABLE parquet_create (
+    id int,
+    str string,
+    mp  MAP<STRING,STRING>,
+    lst ARRAY<STRING>,
+    strct STRUCT<A:STRING,B:STRING>
+) STORED AS PARQUET
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE parquet_create (
+    id int,
+    str string,
+    mp  MAP<STRING,STRING>,
+    lst ARRAY<STRING>,
+    strct STRUCT<A:STRING,B:STRING>
+) STORED AS PARQUET
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@parquet_create
+PREHOOK: query: DESCRIBE FORMATTED parquet_create
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: DESCRIBE FORMATTED parquet_create
+POSTHOOK: type: DESCTABLE
+# col_name            	data_type           	comment             
+	 	 
+id                  	int                 	from deserializer   
+str                 	string              	from deserializer   
+mp                  	map<string,string>  	from deserializer   
+lst                 	array<string>       	from deserializer   
+strct               	struct<A:string,B:string>	from deserializer   
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe	 
+InputFormat:        	org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/parquet_create.txt' OVERWRITE INTO TABLE parquet_create_staging
+PREHOOK: type: LOAD
+PREHOOK: Output: default@parquet_create_staging
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/parquet_create.txt' OVERWRITE INTO TABLE parquet_create_staging
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@parquet_create_staging
+PREHOOK: query: SELECT * FROM parquet_create_staging
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_create_staging
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM parquet_create_staging
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_create_staging
+#### A masked pattern was here ####
+1	foo line1	{"key11":"value11","key12":"value12","key13":"value13"}	["a","b","c"]	{"a":"one","b":"two"}
+2	bar line2	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]	{"a":"three","b":"four"}
+3	baz line3	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]	{"a":"five","b":"six"}
+PREHOOK: query: INSERT OVERWRITE TABLE parquet_create SELECT * FROM parquet_create_staging
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_create_staging
+PREHOOK: Output: default@parquet_create
+POSTHOOK: query: INSERT OVERWRITE TABLE parquet_create SELECT * FROM parquet_create_staging
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_create_staging
+POSTHOOK: Output: default@parquet_create
+POSTHOOK: Lineage: parquet_create.id SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_create.lst SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.mp SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.str SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_create.strct SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
+PREHOOK: query: SELECT * FROM parquet_create group by id
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM parquet_create group by id
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: Lineage: parquet_create.id SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_create.lst SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.mp SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.str SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_create.strct SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
+1
+2
+3
+PREHOOK: query: SELECT id, count(0) FROM parquet_create group by id
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT id, count(0) FROM parquet_create group by id
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: Lineage: parquet_create.id SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_create.lst SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.mp SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.str SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_create.strct SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
+1	1
+2	1
+3	1
+PREHOOK: query: SELECT str from parquet_create
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT str from parquet_create
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: Lineage: parquet_create.id SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_create.lst SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.mp SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.str SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_create.strct SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
+foo line1
+bar line2
+baz line3
+PREHOOK: query: SELECT mp from parquet_create
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT mp from parquet_create
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: Lineage: parquet_create.id SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_create.lst SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.mp SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.str SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_create.strct SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
+{"key12":"value12","key11":"value11","key13":"value13"}
+{"key21":"value21","key23":"value23","key22":"value22"}
+{"key33":"value33","key31":"value31","key32":"value32"}
+PREHOOK: query: SELECT lst from parquet_create
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT lst from parquet_create
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: Lineage: parquet_create.id SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_create.lst SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.mp SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.str SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_create.strct SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
+["a","b","c"]
+["d","e","f"]
+["g","h","i"]
+PREHOOK: query: SELECT strct from parquet_create
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT strct from parquet_create
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_create
+#### A masked pattern was here ####
+POSTHOOK: Lineage: parquet_create.id SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_create.lst SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.mp SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]
+POSTHOOK: Lineage: parquet_create.str SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_create.strct SIMPLE [(parquet_create_staging)parquet_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
+{"a":"one","b":"two"}
+{"a":"three","b":"four"}
+{"a":"five","b":"six"}
diff --git a/src/ql/src/test/results/clientpositive/parquet_partitioned.q.out b/src/ql/src/test/results/clientpositive/parquet_partitioned.q.out
new file mode 100644
index 0000000..ecba6ce
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/parquet_partitioned.q.out
@@ -0,0 +1,174 @@
+PREHOOK: query: DROP TABLE parquet_partitioned_staging
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE parquet_partitioned_staging
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: DROP TABLE parquet_partitioned
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE parquet_partitioned
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE parquet_partitioned_staging (
+    id int,
+    str string,
+    part string
+) ROW FORMAT DELIMITED
+FIELDS TERMINATED BY '|'
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE parquet_partitioned_staging (
+    id int,
+    str string,
+    part string
+) ROW FORMAT DELIMITED
+FIELDS TERMINATED BY '|'
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@parquet_partitioned_staging
+PREHOOK: query: CREATE TABLE parquet_partitioned (
+    id int,
+    str string
+) PARTITIONED BY (part string)
+STORED AS PARQUET
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE parquet_partitioned (
+    id int,
+    str string
+) PARTITIONED BY (part string)
+STORED AS PARQUET
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@parquet_partitioned
+PREHOOK: query: DESCRIBE FORMATTED parquet_partitioned
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: DESCRIBE FORMATTED parquet_partitioned
+POSTHOOK: type: DESCTABLE
+# col_name            	data_type           	comment             
+	 	 
+id                  	int                 	from deserializer   
+str                 	string              	from deserializer   
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+part                	string              	None                
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe	 
+InputFormat:        	org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/parquet_partitioned.txt' OVERWRITE INTO TABLE parquet_partitioned_staging
+PREHOOK: type: LOAD
+PREHOOK: Output: default@parquet_partitioned_staging
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/parquet_partitioned.txt' OVERWRITE INTO TABLE parquet_partitioned_staging
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@parquet_partitioned_staging
+PREHOOK: query: SELECT * FROM parquet_partitioned_staging
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_partitioned_staging
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM parquet_partitioned_staging
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_partitioned_staging
+#### A masked pattern was here ####
+1	foo	part1
+2	bar	part2
+3	baz	part2
+PREHOOK: query: INSERT OVERWRITE TABLE parquet_partitioned PARTITION (part) SELECT * FROM parquet_partitioned_staging
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_partitioned_staging
+PREHOOK: Output: default@parquet_partitioned
+POSTHOOK: query: INSERT OVERWRITE TABLE parquet_partitioned PARTITION (part) SELECT * FROM parquet_partitioned_staging
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_partitioned_staging
+POSTHOOK: Output: default@parquet_partitioned@part=part1
+POSTHOOK: Output: default@parquet_partitioned@part=part2
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part1).id SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part1).str SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part2).id SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part2).str SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:str, type:string, comment:null), ]
+PREHOOK: query: SELECT * FROM parquet_partitioned
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_partitioned
+PREHOOK: Input: default@parquet_partitioned@part=part1
+PREHOOK: Input: default@parquet_partitioned@part=part2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM parquet_partitioned
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_partitioned
+POSTHOOK: Input: default@parquet_partitioned@part=part1
+POSTHOOK: Input: default@parquet_partitioned@part=part2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part1).id SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part1).str SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part2).id SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part2).str SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:str, type:string, comment:null), ]
+1	foo	part1
+2	bar	part2
+3	baz	part2
+PREHOOK: query: SELECT part, COUNT(0) FROM parquet_partitioned GROUP BY part
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_partitioned
+PREHOOK: Input: default@parquet_partitioned@part=part1
+PREHOOK: Input: default@parquet_partitioned@part=part2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT part, COUNT(0) FROM parquet_partitioned GROUP BY part
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_partitioned
+POSTHOOK: Input: default@parquet_partitioned@part=part1
+POSTHOOK: Input: default@parquet_partitioned@part=part2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part1).id SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part1).str SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part2).id SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part2).str SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:str, type:string, comment:null), ]
+part1	1
+part2	2
+PREHOOK: query: SELECT * FROM parquet_partitioned
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_partitioned
+PREHOOK: Input: default@parquet_partitioned@part=part1
+PREHOOK: Input: default@parquet_partitioned@part=part2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM parquet_partitioned
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_partitioned
+POSTHOOK: Input: default@parquet_partitioned@part=part1
+POSTHOOK: Input: default@parquet_partitioned@part=part2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part1).id SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part1).str SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part2).id SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part2).str SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:str, type:string, comment:null), ]
+1	foo	part1
+2	bar	part2
+3	baz	part2
+PREHOOK: query: SELECT part, COUNT(0) FROM parquet_partitioned GROUP BY part
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_partitioned
+PREHOOK: Input: default@parquet_partitioned@part=part1
+PREHOOK: Input: default@parquet_partitioned@part=part2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT part, COUNT(0) FROM parquet_partitioned GROUP BY part
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_partitioned
+POSTHOOK: Input: default@parquet_partitioned@part=part1
+POSTHOOK: Input: default@parquet_partitioned@part=part2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part1).id SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part1).str SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part2).id SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: parquet_partitioned PARTITION(part=part2).str SIMPLE [(parquet_partitioned_staging)parquet_partitioned_staging.FieldSchema(name:str, type:string, comment:null), ]
+part1	1
+part2	2
-- 
1.7.0.4

