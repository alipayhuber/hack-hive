From 234932e4463bb74ecfd8fd0c3dbc9282fa311492 Mon Sep 17 00:00:00 2001
From: Szehon Ho <szehon@cloudera.com>
Date: Mon, 27 Jan 2014 14:04:22 -0800
Subject: [PATCH 252/375] Revert "CDH-16069 - Rebase and backport HIVE-5783: Native Parquet Support in Hive"

This reverts commit 7ada82789357adc2edc9868154ac2307dbb7c5ef.
---
 pom.xml                                            |   11 -
 ql/pom.xml                                         |   13 -
 .../hadoop/hive/ql/parse/BaseSemanticAnalyzer.java |   18 -
 .../org/apache/hadoop/hive/ql/parse/HiveLexer.g    |    1 -
 .../org/apache/hadoop/hive/ql/parse/HiveParser.g   |    3 -
 .../hadoop/hive/ql/parse/IdentifiersParser.g       |    2 +-
 .../parquet/hive/DeprecatedParquetInputFormat.java |   35 -
 .../hive/DeprecatedParquetOutputFormat.java        |   35 -
 ql/src/java/parquet/hive/HiveBinding.java          |   52 -
 ql/src/java/parquet/hive/HiveBindingFactory.java   |   31 -
 .../parquet/hive/MapredParquetInputFormat.java     |  379 ------
 .../parquet/hive/MapredParquetOutputFormat.java    |  167 ---
 .../hive/convert/ArrayWritableGroupConverter.java  |   98 --
 .../hive/convert/DataWritableGroupConverter.java   |  138 ---
 .../hive/convert/DataWritableRecordConverter.java  |   44 -
 .../java/parquet/hive/convert/ETypeConverter.java  |  165 ---
 .../parquet/hive/convert/HiveGroupConverter.java   |   47 -
 .../parquet/hive/convert/HiveSchemaConverter.java  |  138 ---
 .../parquet/hive/internal/AbstractHiveBinding.java |   36 -
 .../java/parquet/hive/internal/Hive012Binding.java |  152 ---
 .../parquet/hive/read/DataWritableReadSupport.java |  111 --
 .../hive/serde/AbstractParquetMapInspector.java    |  163 ---
 .../hive/serde/ArrayWritableObjectInspector.java   |  224 ----
 .../hive/serde/DeepParquetHiveMapInspector.java    |   82 --
 .../hive/serde/ParquetHiveArrayInspector.java      |  185 ---
 .../java/parquet/hive/serde/ParquetHiveSerDe.java  |  291 -----
 .../serde/StandardParquetHiveMapInspector.java     |   65 --
 .../hive/serde/primitive/ParquetByteInspector.java |   59 -
 .../ParquetPrimitiveInspectorFactory.java          |   32 -
 .../serde/primitive/ParquetShortInspector.java     |   59 -
 .../serde/primitive/ParquetStringInspector.java    |  100 --
 .../parquet/hive/writable/BigDecimalWritable.java  |  152 ---
 .../java/parquet/hive/writable/BinaryWritable.java |   98 --
 .../hive/write/DataWritableWriteSupport.java       |   61 -
 .../parquet/hive/write/DataWritableWriter.java     |  161 ---
 .../test/parquet/hive/TestHiveSchemaConverter.java |  124 --
 .../parquet/hive/TestMapredParquetInputFormat.java |  387 -------
 .../parquet/hive/TestMapredParquetOuputFormat.java |  235 ----
 ql/src/test/parquet/hive/TestParquetSerDe.java     |  119 --
 ql/src/test/parquet/hive/UtilitiesTestMethods.java |  245 ----
 .../serde/TestAbstractParquetMapInspector.java     |  102 --
 .../serde/TestDeepParquetHiveMapInspector.java     |   94 --
 .../hive/serde/TestParquetHiveArrayInspector.java  |   84 --
 .../serde/TestStandardParquetHiveMapInspector.java |   92 --
 .../queries/clientpositive/cdh_parquet_basic.q     |   19 -
 .../results/clientpositive/cdh_parquet_basic.q.out | 1202 --------------------
 46 files changed, 1 insertions(+), 6110 deletions(-)
 delete mode 100644 ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java
 delete mode 100644 ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java
 delete mode 100644 ql/src/java/parquet/hive/HiveBinding.java
 delete mode 100644 ql/src/java/parquet/hive/HiveBindingFactory.java
 delete mode 100644 ql/src/java/parquet/hive/MapredParquetInputFormat.java
 delete mode 100644 ql/src/java/parquet/hive/MapredParquetOutputFormat.java
 delete mode 100644 ql/src/java/parquet/hive/convert/ArrayWritableGroupConverter.java
 delete mode 100644 ql/src/java/parquet/hive/convert/DataWritableGroupConverter.java
 delete mode 100644 ql/src/java/parquet/hive/convert/DataWritableRecordConverter.java
 delete mode 100644 ql/src/java/parquet/hive/convert/ETypeConverter.java
 delete mode 100644 ql/src/java/parquet/hive/convert/HiveGroupConverter.java
 delete mode 100644 ql/src/java/parquet/hive/convert/HiveSchemaConverter.java
 delete mode 100644 ql/src/java/parquet/hive/internal/AbstractHiveBinding.java
 delete mode 100644 ql/src/java/parquet/hive/internal/Hive012Binding.java
 delete mode 100644 ql/src/java/parquet/hive/read/DataWritableReadSupport.java
 delete mode 100644 ql/src/java/parquet/hive/serde/AbstractParquetMapInspector.java
 delete mode 100644 ql/src/java/parquet/hive/serde/ArrayWritableObjectInspector.java
 delete mode 100644 ql/src/java/parquet/hive/serde/DeepParquetHiveMapInspector.java
 delete mode 100644 ql/src/java/parquet/hive/serde/ParquetHiveArrayInspector.java
 delete mode 100644 ql/src/java/parquet/hive/serde/ParquetHiveSerDe.java
 delete mode 100644 ql/src/java/parquet/hive/serde/StandardParquetHiveMapInspector.java
 delete mode 100644 ql/src/java/parquet/hive/serde/primitive/ParquetByteInspector.java
 delete mode 100644 ql/src/java/parquet/hive/serde/primitive/ParquetPrimitiveInspectorFactory.java
 delete mode 100644 ql/src/java/parquet/hive/serde/primitive/ParquetShortInspector.java
 delete mode 100644 ql/src/java/parquet/hive/serde/primitive/ParquetStringInspector.java
 delete mode 100644 ql/src/java/parquet/hive/writable/BigDecimalWritable.java
 delete mode 100644 ql/src/java/parquet/hive/writable/BinaryWritable.java
 delete mode 100644 ql/src/java/parquet/hive/write/DataWritableWriteSupport.java
 delete mode 100644 ql/src/java/parquet/hive/write/DataWritableWriter.java
 delete mode 100644 ql/src/test/parquet/hive/TestHiveSchemaConverter.java
 delete mode 100644 ql/src/test/parquet/hive/TestMapredParquetInputFormat.java
 delete mode 100644 ql/src/test/parquet/hive/TestMapredParquetOuputFormat.java
 delete mode 100644 ql/src/test/parquet/hive/TestParquetSerDe.java
 delete mode 100644 ql/src/test/parquet/hive/UtilitiesTestMethods.java
 delete mode 100644 ql/src/test/parquet/hive/serde/TestAbstractParquetMapInspector.java
 delete mode 100644 ql/src/test/parquet/hive/serde/TestDeepParquetHiveMapInspector.java
 delete mode 100644 ql/src/test/parquet/hive/serde/TestParquetHiveArrayInspector.java
 delete mode 100644 ql/src/test/parquet/hive/serde/TestStandardParquetHiveMapInspector.java
 delete mode 100644 ql/src/test/queries/clientpositive/cdh_parquet_basic.q
 delete mode 100644 ql/src/test/results/clientpositive/cdh_parquet_basic.q.out

diff --git a/src/pom.xml b/src/pom.xml
index a8fa6f5..5495cd1 100644
--- a/src/pom.xml
+++ b/src/pom.xml
@@ -133,7 +133,6 @@
         requires netty < 3.6.0 we force hadoops version
       -->
     <netty.version>3.4.0.Final</netty.version>
-    <parquet.version>${cdh.parquet.version}</parquet.version>
     <pig.version>${cdh.pig.version}</pig.version>
     <protobuf.version>${cdh.protobuf.version}</protobuf.version>
     <rat.version>0.8</rat.version>
@@ -526,11 +525,6 @@
         <artifactId>xercesImpl</artifactId>
         <version>${xerces.version}</version>
       </dependency>
-      <dependency>
-        <groupId>com.twitter</groupId>
-        <artifactId>parquet-hive-bundle</artifactId>
-        <version>${parquet.version}</version>
-      </dependency>
     </dependencies>
   </dependencyManagement>
 
@@ -926,11 +920,6 @@
         <dependencies>
           <dependency>
             <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-client</artifactId>
-            <version>${hadoop-23.version}</version>
-          </dependency>
-          <dependency>
-            <groupId>org.apache.hadoop</groupId>
             <artifactId>hadoop-common</artifactId>
             <version>${hadoop-23.version}</version>
           </dependency>
diff --git a/src/ql/pom.xml b/src/ql/pom.xml
index 5e772e9..6d79037 100644
--- a/src/ql/pom.xml
+++ b/src/ql/pom.xml
@@ -67,11 +67,6 @@
       <version>${kryo.version}</version>
     </dependency>
     <dependency>
-      <groupId>com.twitter</groupId>
-      <artifactId>parquet-hive-bundle</artifactId>
-      <version>${parquet.version}</version>
-    </dependency>
-    <dependency>
       <groupId>commons-codec</groupId>
       <artifactId>commons-codec</artifactId>
       <version>${commons-codec.version}</version>
@@ -204,13 +199,6 @@
     <!-- test intra-project -->
     <!-- test inter-project -->
     <dependency>
-      <groupId>com.twitter</groupId>
-      <artifactId>parquet-column</artifactId>
-      <version>${parquet.version}</version>
-      <type>test-jar</type>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
       <version>${junit.version}</version>
@@ -354,7 +342,6 @@
                   <include>org.apache.hive:hive-common</include>
                   <include>org.apache.hive:hive-exec</include>
                   <include>org.apache.hive:hive-serde</include>
-                  <include>org.apache.hive:parquet-hive-bundle</include>
                   <include>com.esotericsoftware.kryo:kryo</include>
                   <include>org.apache.thrift:libthrift</include>
                   <include>commons-lang:commons-lang</include>
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index 4b4cc1c..953910d 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -74,10 +74,6 @@
 import org.apache.hadoop.mapred.SequenceFileOutputFormat;
 import org.apache.hadoop.mapred.TextInputFormat;
 
-import parquet.hive.MapredParquetInputFormat;
-import parquet.hive.MapredParquetOutputFormat;
-import parquet.hive.serde.ParquetHiveSerDe;
-
 /**
  * BaseSemanticAnalyzer.
  *
@@ -132,10 +128,6 @@
   protected static final String ORCFILE_SERDE = OrcSerde.class
       .getName();
 
-  protected static final String PARQUETFILE_INPUT = MapredParquetInputFormat.class.getName();
-  protected static final String PARQUETFILE_OUTPUT = MapredParquetOutputFormat.class.getName();
-  protected static final String PARQUETFILE_SERDE = ParquetHiveSerDe.class.getName();
-
   class RowFormatParams {
     String fieldDelim = null;
     String fieldEscape = null;
@@ -223,12 +215,6 @@ protected boolean fillStorageFormat(ASTNode child, AnalyzeCreateCommonVars share
         shared.serde = ORCFILE_SERDE;
         storageFormat = true;
         break;
-      case HiveParser.TOK_TBLPARQUETFILE:
-        inputFormat = PARQUETFILE_INPUT;
-        outputFormat = PARQUETFILE_OUTPUT;
-        shared.serde = PARQUETFILE_SERDE;
-        storageFormat = true;
-        break;
       case HiveParser.TOK_TABLEFILEFORMAT:
         inputFormat = unescapeSQLString(child.getChild(0).getText());
         outputFormat = unescapeSQLString(child.getChild(1).getText());
@@ -260,10 +246,6 @@ protected void fillDefaultStorageFormat(AnalyzeCreateCommonVars shared) {
           inputFormat = ORCFILE_INPUT;
           outputFormat = ORCFILE_OUTPUT;
           shared.serde = ORCFILE_SERDE;
-        } else if ("PARQUET".equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.HIVEDEFAULTFILEFORMAT))) {
-          inputFormat = PARQUETFILE_INPUT;
-          outputFormat = PARQUETFILE_OUTPUT;
-          shared.serde = PARQUETFILE_SERDE;
         } else {
           inputFormat = TEXTFILE_INPUT;
           outputFormat = TEXTFILE_OUTPUT;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
index 923b93c..ed9917d 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
@@ -135,7 +135,6 @@ KW_SEQUENCEFILE: 'SEQUENCEFILE';
 KW_TEXTFILE: 'TEXTFILE';
 KW_RCFILE: 'RCFILE';
 KW_ORCFILE: 'ORC';
-KW_PARQUETFILE: 'PARQUET';
 KW_INPUTFORMAT: 'INPUTFORMAT';
 KW_OUTPUTFORMAT: 'OUTPUTFORMAT';
 KW_INPUTDRIVER: 'INPUTDRIVER';
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
index c1a36fb..0875c23 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
@@ -180,7 +180,6 @@ TOK_TABLEROWFORMATMAPKEYS;
 TOK_TABLEROWFORMATLINES;
 TOK_TABLEROWFORMATNULL;
 TOK_TBLORCFILE;
-TOK_TBLPARQUETFILE;
 TOK_TBLSEQUENCEFILE;
 TOK_TBLTEXTFILE;
 TOK_TBLRCFILE;
@@ -1177,7 +1176,6 @@ fileFormat
     | KW_TEXTFILE  -> ^(TOK_TBLTEXTFILE)
     | KW_RCFILE  -> ^(TOK_TBLRCFILE)
     | KW_ORCFILE -> ^(TOK_TBLORCFILE)
-    | KW_PARQUETFILE -> ^(TOK_TBLPARQUETFILE)
     | KW_INPUTFORMAT inFmt=StringLiteral KW_OUTPUTFORMAT outFmt=StringLiteral (KW_INPUTDRIVER inDriver=StringLiteral KW_OUTPUTDRIVER outDriver=StringLiteral)?
       -> ^(TOK_TABLEFILEFORMAT $inFmt $outFmt $inDriver? $outDriver?)
     | genericSpec=identifier -> ^(TOK_FILEFORMAT_GENERIC $genericSpec)
@@ -1618,7 +1616,6 @@ tableFileFormat
       | KW_STORED KW_AS KW_TEXTFILE  -> TOK_TBLTEXTFILE
       | KW_STORED KW_AS KW_RCFILE  -> TOK_TBLRCFILE
       | KW_STORED KW_AS KW_ORCFILE -> TOK_TBLORCFILE
-      | KW_STORED KW_AS KW_PARQUETFILE -> TOK_TBLPARQUETFILE
       | KW_STORED KW_AS KW_INPUTFORMAT inFmt=StringLiteral KW_OUTPUTFORMAT outFmt=StringLiteral (KW_INPUTDRIVER inDriver=StringLiteral KW_OUTPUTDRIVER outDriver=StringLiteral)?
       -> ^(TOK_TABLEFILEFORMAT $inFmt $outFmt $inDriver? $outDriver?)
       | KW_STORED KW_BY storageHandler=StringLiteral
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/IdentifiersParser.g b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/IdentifiersParser.g
index 82e2d6c..67aa49a 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/IdentifiersParser.g
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/IdentifiersParser.g
@@ -525,5 +525,5 @@ identifier
     
 nonReserved
     :
-    KW_TRUE | KW_FALSE | KW_LIKE | KW_EXISTS | KW_ASC | KW_DESC | KW_ORDER | KW_GROUP | KW_BY | KW_AS | KW_INSERT | KW_OVERWRITE | KW_OUTER | KW_LEFT | KW_RIGHT | KW_FULL | KW_PARTITION | KW_PARTITIONS | KW_TABLE | KW_TABLES | KW_COLUMNS | KW_INDEX | KW_INDEXES | KW_REBUILD | KW_FUNCTIONS | KW_SHOW | KW_MSCK | KW_REPAIR | KW_DIRECTORY | KW_LOCAL | KW_USING | KW_CLUSTER | KW_DISTRIBUTE | KW_SORT | KW_UNION | KW_LOAD | KW_EXPORT | KW_IMPORT | KW_DATA | KW_INPATH | KW_IS | KW_NULL | KW_CREATE | KW_EXTERNAL | KW_ALTER | KW_CHANGE | KW_FIRST | KW_AFTER | KW_DESCRIBE | KW_DROP | KW_RENAME | KW_IGNORE | KW_PROTECTION | KW_TO | KW_COMMENT | KW_BOOLEAN | KW_TINYINT | KW_SMALLINT | KW_INT | KW_BIGINT | KW_FLOAT | KW_DOUBLE | KW_DATE | KW_DATETIME | KW_TIMESTAMP | KW_DECIMAL | KW_STRING | KW_ARRAY | KW_STRUCT | KW_UNIONTYPE | KW_PARTITIONED | KW_CLUSTERED | KW_SORTED | KW_INTO | KW_BUCKETS | KW_ROW | KW_ROWS | KW_FORMAT | KW_DELIMITED | KW_FIELDS | KW_TERMINATED | KW_ESCAPED | KW_COLLECTION | KW_ITEMS | KW_KEYS | KW_KEY_TYPE | KW_LINES | KW_STORED | KW_FILEFORMAT | KW_SEQUENCEFILE | KW_TEXTFILE | KW_RCFILE | KW_ORCFILE | KW_PARQUETFILE | KW_INPUTFORMAT | KW_OUTPUTFORMAT | KW_INPUTDRIVER | KW_OUTPUTDRIVER | KW_OFFLINE | KW_ENABLE | KW_DISABLE | KW_READONLY | KW_NO_DROP | KW_LOCATION | KW_BUCKET | KW_OUT | KW_OF | KW_PERCENT | KW_ADD | KW_REPLACE | KW_RLIKE | KW_REGEXP | KW_TEMPORARY | KW_EXPLAIN | KW_FORMATTED | KW_PRETTY | KW_DEPENDENCY | KW_LOGICAL | KW_SERDE | KW_WITH | KW_DEFERRED | KW_SERDEPROPERTIES | KW_DBPROPERTIES | KW_LIMIT | KW_SET | KW_UNSET | KW_TBLPROPERTIES | KW_IDXPROPERTIES | KW_VALUE_TYPE | KW_ELEM_TYPE | KW_MAPJOIN | KW_STREAMTABLE | KW_HOLD_DDLTIME | KW_CLUSTERSTATUS | KW_UTC | KW_UTCTIMESTAMP | KW_LONG | KW_DELETE | KW_PLUS | KW_MINUS | KW_FETCH | KW_INTERSECT | KW_VIEW | KW_IN | KW_DATABASES | KW_MATERIALIZED | KW_SCHEMA | KW_SCHEMAS | KW_GRANT | KW_REVOKE | KW_SSL | KW_UNDO | KW_LOCK | KW_LOCKS | KW_UNLOCK | KW_SHARED | KW_EXCLUSIVE | KW_PROCEDURE | KW_UNSIGNED | KW_WHILE | KW_READ | KW_READS | KW_PURGE | KW_RANGE | KW_ANALYZE | KW_BEFORE | KW_BETWEEN | KW_BOTH | KW_BINARY | KW_CONTINUE | KW_CURSOR | KW_TRIGGER | KW_RECORDREADER | KW_RECORDWRITER | KW_SEMI | KW_LATERAL | KW_TOUCH | KW_ARCHIVE | KW_UNARCHIVE | KW_COMPUTE | KW_STATISTICS | KW_USE | KW_OPTION | KW_CONCATENATE | KW_SHOW_DATABASE | KW_UPDATE | KW_RESTRICT | KW_CASCADE | KW_SKEWED | KW_ROLLUP | KW_CUBE | KW_DIRECTORIES | KW_FOR | KW_GROUPING | KW_SETS | KW_TRUNCATE | KW_NOSCAN | KW_USER | KW_ROLE | KW_INNER | KW_DEFINED
+    KW_TRUE | KW_FALSE | KW_LIKE | KW_EXISTS | KW_ASC | KW_DESC | KW_ORDER | KW_GROUP | KW_BY | KW_AS | KW_INSERT | KW_OVERWRITE | KW_OUTER | KW_LEFT | KW_RIGHT | KW_FULL | KW_PARTITION | KW_PARTITIONS | KW_TABLE | KW_TABLES | KW_COLUMNS | KW_INDEX | KW_INDEXES | KW_REBUILD | KW_FUNCTIONS | KW_SHOW | KW_MSCK | KW_REPAIR | KW_DIRECTORY | KW_LOCAL | KW_USING | KW_CLUSTER | KW_DISTRIBUTE | KW_SORT | KW_UNION | KW_LOAD | KW_EXPORT | KW_IMPORT | KW_DATA | KW_INPATH | KW_IS | KW_NULL | KW_CREATE | KW_EXTERNAL | KW_ALTER | KW_CHANGE | KW_FIRST | KW_AFTER | KW_DESCRIBE | KW_DROP | KW_RENAME | KW_IGNORE | KW_PROTECTION | KW_TO | KW_COMMENT | KW_BOOLEAN | KW_TINYINT | KW_SMALLINT | KW_INT | KW_BIGINT | KW_FLOAT | KW_DOUBLE | KW_DATE | KW_DATETIME | KW_TIMESTAMP | KW_DECIMAL | KW_STRING | KW_ARRAY | KW_STRUCT | KW_UNIONTYPE | KW_PARTITIONED | KW_CLUSTERED | KW_SORTED | KW_INTO | KW_BUCKETS | KW_ROW | KW_ROWS | KW_FORMAT | KW_DELIMITED | KW_FIELDS | KW_TERMINATED | KW_ESCAPED | KW_COLLECTION | KW_ITEMS | KW_KEYS | KW_KEY_TYPE | KW_LINES | KW_STORED | KW_FILEFORMAT | KW_SEQUENCEFILE | KW_TEXTFILE | KW_RCFILE | KW_ORCFILE | KW_INPUTFORMAT | KW_OUTPUTFORMAT | KW_INPUTDRIVER | KW_OUTPUTDRIVER | KW_OFFLINE | KW_ENABLE | KW_DISABLE | KW_READONLY | KW_NO_DROP | KW_LOCATION | KW_BUCKET | KW_OUT | KW_OF | KW_PERCENT | KW_ADD | KW_REPLACE | KW_RLIKE | KW_REGEXP | KW_TEMPORARY | KW_EXPLAIN | KW_FORMATTED | KW_PRETTY | KW_DEPENDENCY | KW_LOGICAL | KW_SERDE | KW_WITH | KW_DEFERRED | KW_SERDEPROPERTIES | KW_DBPROPERTIES | KW_LIMIT | KW_SET | KW_UNSET | KW_TBLPROPERTIES | KW_IDXPROPERTIES | KW_VALUE_TYPE | KW_ELEM_TYPE | KW_MAPJOIN | KW_STREAMTABLE | KW_HOLD_DDLTIME | KW_CLUSTERSTATUS | KW_UTC | KW_UTCTIMESTAMP | KW_LONG | KW_DELETE | KW_PLUS | KW_MINUS | KW_FETCH | KW_INTERSECT | KW_VIEW | KW_IN | KW_DATABASES | KW_MATERIALIZED | KW_SCHEMA | KW_SCHEMAS | KW_GRANT | KW_REVOKE | KW_SSL | KW_UNDO | KW_LOCK | KW_LOCKS | KW_UNLOCK | KW_SHARED | KW_EXCLUSIVE | KW_PROCEDURE | KW_UNSIGNED | KW_WHILE | KW_READ | KW_READS | KW_PURGE | KW_RANGE | KW_ANALYZE | KW_BEFORE | KW_BETWEEN | KW_BOTH | KW_BINARY | KW_CONTINUE | KW_CURSOR | KW_TRIGGER | KW_RECORDREADER | KW_RECORDWRITER | KW_SEMI | KW_LATERAL | KW_TOUCH | KW_ARCHIVE | KW_UNARCHIVE | KW_COMPUTE | KW_STATISTICS | KW_USE | KW_OPTION | KW_CONCATENATE | KW_SHOW_DATABASE | KW_UPDATE | KW_RESTRICT | KW_CASCADE | KW_SKEWED | KW_ROLLUP | KW_CUBE | KW_DIRECTORIES | KW_FOR | KW_GROUPING | KW_SETS | KW_TRUNCATE | KW_NOSCAN | KW_USER | KW_ROLE | KW_INNER | KW_DEFINED
     ;
diff --git a/src/ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java b/src/ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java
deleted file mode 100644
index fd9e0d5..0000000
--- a/src/ql/src/java/parquet/hive/DeprecatedParquetInputFormat.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.mapreduce.InputFormat;
-
-/**
- * Deprecated name of the parquet-hive input format. This class exists
- * simply to provide backwards compatibility with users who specified
- * this name in the Hive metastore. All users should now use
- * {@link MapredParquetInputFormat MapredParquetInputFormat}
- */
-@Deprecated
-public class DeprecatedParquetInputFormat extends MapredParquetInputFormat {
-
-  public DeprecatedParquetInputFormat() {
-    super();
-  }
-
-  public DeprecatedParquetInputFormat(final InputFormat<Void, ArrayWritable> realInputFormat) {
-    super(realInputFormat);
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java b/src/ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java
deleted file mode 100644
index 07c3f6c..0000000
--- a/src/ql/src/java/parquet/hive/DeprecatedParquetOutputFormat.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.mapreduce.OutputFormat;
-
-/**
- * Deprecated name of the parquet-hive output format. This class exists
- * simply to provide backwards compatibility with users who specified
- * this name in the Hive metastore. All users should now use
- * {@link MapredParquetOutputFormat MapredParquetOutputFormat}
- */
-@Deprecated
-public class DeprecatedParquetOutputFormat extends MapredParquetOutputFormat {
-
-  public DeprecatedParquetOutputFormat() {
-    super();
-  }
-
-  public DeprecatedParquetOutputFormat(final OutputFormat<Void, ArrayWritable> mapreduceOutputFormat) {
-    super(mapreduceOutputFormat);
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/HiveBinding.java b/src/ql/src/java/parquet/hive/HiveBinding.java
deleted file mode 100644
index 858e93b..0000000
--- a/src/ql/src/java/parquet/hive/HiveBinding.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.JobConf;
-
-public interface HiveBinding {
-
-  /**
-   * From a string which columns names (including hive column), return a list
-   * of string columns
-   *
-   * @param comma separated list of columns
-   * @return list with virtual columns removed
-   */
-  public List<String> getColumns(final String columns);
-  
-  /**
-   * Processes the JobConf object pushing down projections and filters.
-   *
-   * We are going to get the Table from a partition in order to get all the
-   * aliases from it. Once we have them, we take a look at the different
-   * columns needed for each of them, and we update the job by appending
-   * these columns.
-   *
-   * The JobConf is modified and therefore is cloned first to ensure
-   * other owners are not impacted by the changes here. This is a standard
-   * practice when modifying JobConf objects in InputFormats, for example
-   * HCatalog does this.
-   *
-   * @param jobConf 
-   * @param path
-   * @return cloned jobConf which can be used to read Parquet files
-   * @throws IOException
-   */
-  public JobConf pushProjectionsAndFilters(final JobConf jobConf, final Path path) throws IOException;
-}
diff --git a/src/ql/src/java/parquet/hive/HiveBindingFactory.java b/src/ql/src/java/parquet/hive/HiveBindingFactory.java
deleted file mode 100644
index 81b6ea9..0000000
--- a/src/ql/src/java/parquet/hive/HiveBindingFactory.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import parquet.hive.internal.Hive012Binding;
-
-/**
- * Factory for creating HiveBinding objects based on the version of Hive
- * available in the classpath. This class does not provide static methods
- * to enable mocking.
- */
-public class HiveBindingFactory {
-
-  /**
-   * @return HiveBinding based on the Hive version in the classpath
-   */
-  public HiveBinding create() {
-    return new Hive012Binding();
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/MapredParquetInputFormat.java b/src/ql/src/java/parquet/hive/MapredParquetInputFormat.java
deleted file mode 100644
index 7f2dae0..0000000
--- a/src/ql/src/java/parquet/hive/MapredParquetInputFormat.java
+++ /dev/null
@@ -1,379 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.FileSplit;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapreduce.InputFormat;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hadoop.util.ReflectionUtils;
-
-import parquet.Log;
-import parquet.hadoop.ParquetFileReader;
-import parquet.hadoop.ParquetInputFormat;
-import parquet.hadoop.ParquetInputSplit;
-import parquet.hadoop.api.ReadSupport.ReadContext;
-import parquet.hadoop.metadata.BlockMetaData;
-import parquet.hadoop.metadata.FileMetaData;
-import parquet.hadoop.metadata.ParquetMetadata;
-import parquet.hadoop.util.ContextUtil;
-import parquet.hive.read.DataWritableReadSupport;
-import parquet.schema.MessageTypeParser;
-
-/**
- *
- * A Parquet InputFormat for Hive (with the deprecated package mapred)
- *
- * TODO : Refactor all of the wrappers here Talk about it on : https://github.com/Parquet/parquet-mr/pull/28s
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- *
- */
-public class MapredParquetInputFormat extends FileInputFormat<Void, ArrayWritable> {
-
-  private static final Log LOG = Log.getLog(MapredParquetInputFormat.class);
-  private final ParquetInputFormat<ArrayWritable> realInput;
-  private final HiveBinding hiveBinding;
-
-  public MapredParquetInputFormat() {
-    this(new ParquetInputFormat<ArrayWritable>(DataWritableReadSupport.class),
-        new HiveBindingFactory().create());
-  }
-
-  public MapredParquetInputFormat(final InputFormat<Void, ArrayWritable> realInputFormat) {
-    this((ParquetInputFormat<ArrayWritable>) realInputFormat, new HiveBindingFactory().create());
-  }
-
-  private MapredParquetInputFormat(final ParquetInputFormat<ArrayWritable> inputFormat,
-      final HiveBinding hiveBinding) {
-    this.realInput = inputFormat;
-    this.hiveBinding = hiveBinding;
-  }
-
-
-  @Override
-  public org.apache.hadoop.mapred.InputSplit[] getSplits(final org.apache.hadoop.mapred.JobConf job, 
-      final int numSplits) throws IOException {
-    final Path[] dirs = FileInputFormat.getInputPaths(job);
-    if (dirs.length == 0) {
-      throw new IOException("No input paths specified in job");
-    }
-
-    final Path tmpPath = new Path((dirs[dirs.length - 1]).makeQualified(FileSystem.get(job)).toUri().getPath());
-    final JobConf cloneJobConf = hiveBinding.pushProjectionsAndFilters(job, tmpPath);
-    final List<org.apache.hadoop.mapreduce.InputSplit> splits = realInput.
-        getSplits(ContextUtil.newJobContext(cloneJobConf, null));
-
-    if (splits == null) {
-      return null;
-    }
-
-    final InputSplit[] resultSplits = new InputSplit[splits.size()];
-    int i = 0;
-
-    for (final org.apache.hadoop.mapreduce.InputSplit split : splits) {
-      try {
-        resultSplits[i++] = new InputSplitWrapper((ParquetInputSplit) split);
-      } catch (final InterruptedException e) {
-        throw new RuntimeException("Cannot create an InputSplitWrapper", e);
-      }
-    }
-
-    return resultSplits;
-  }
-
-  @Override
-  public org.apache.hadoop.mapred.RecordReader<Void, ArrayWritable> getRecordReader(final org.apache.hadoop.mapred.InputSplit split,
-          final org.apache.hadoop.mapred.JobConf job, final org.apache.hadoop.mapred.Reporter reporter) throws IOException {
-    try {
-      return (RecordReader<Void, ArrayWritable>) new RecordReaderWrapper(realInput, split, job, reporter);
-    } catch (final InterruptedException e) {
-      throw new RuntimeException("Cannot create a RecordReaderWrapper", e);
-    }
-  }
-
-  static class InputSplitWrapper extends FileSplit implements InputSplit {
-
-    private ParquetInputSplit realSplit;
-
-    public ParquetInputSplit getRealSplit() {
-      return realSplit;
-    }
-
-    // MapReduce instantiates this.
-    public InputSplitWrapper() {
-      super((Path) null, 0, 0, (String[]) null);
-    }
-
-    public InputSplitWrapper(final ParquetInputSplit realSplit) throws IOException, InterruptedException {
-      super(realSplit.getPath(), realSplit.getStart(), realSplit.getLength(), realSplit.getLocations());
-      this.realSplit = realSplit;
-    }
-
-    @Override
-    public long getLength() {
-      if (realSplit == null) {
-        return 0;
-      } else {
-        try {
-          return realSplit.getLength();
-        } catch (IOException ex) {
-          throw new RuntimeException("Cannot get the length of the ParquetInputSplit: " + realSplit, ex);
-        } catch (InterruptedException ex) {
-          throw new RuntimeException("Cannot get the length of the ParquetInputSplit: " + realSplit, ex);
-        }
-      }
-    }
-
-    @Override
-    public String[] getLocations() throws IOException {
-      try {
-        return realSplit.getLocations();
-      } catch (final InterruptedException e) {
-        throw new IOException(e);
-      }
-    }
-
-    @Override
-    public void readFields(final DataInput in) throws IOException {
-      final String className = WritableUtils.readString(in);
-      Class<?> splitClass;
-
-      try {
-        splitClass = Class.forName(className);
-      } catch (final ClassNotFoundException e) {
-        throw new IOException(e);
-      }
-
-      realSplit = (ParquetInputSplit) ReflectionUtils.newInstance(splitClass, null);
-      ((Writable) realSplit).readFields(in);
-    }
-
-    @Override
-    public void write(final DataOutput out) throws IOException {
-      WritableUtils.writeString(out, realSplit.getClass().getName());
-      ((Writable) realSplit).write(out);
-    }
-
-    @Override
-    public Path getPath() {
-      return realSplit.getPath();
-    }
-
-    @Override
-    public long getStart() {
-      return realSplit.getStart();
-    }
-  }
-
-  protected static class RecordReaderWrapper implements RecordReader<Void, ArrayWritable> {
-
-    private final HiveBinding hiveBinding;
-    private final long splitLen; // for getPos()
-
-    private org.apache.hadoop.mapreduce.RecordReader<Void, ArrayWritable> realReader;
-    // expect readReader return same Key & Value objects (common case)
-    // this avoids extra serialization & deserialization of these objects
-    private ArrayWritable valueObj = null;
-    private boolean firstRecord = false;
-    private boolean eof = false;
-    private int schemaSize;
-
-    public RecordReaderWrapper(final ParquetInputFormat<ArrayWritable> newInputFormat, 
-        final InputSplit oldSplit, final JobConf oldJobConf, final Reporter reporter)
-            throws IOException, InterruptedException {
-      hiveBinding = new HiveBindingFactory().create();
-      splitLen = oldSplit.getLength();
-      final ParquetInputSplit split = getSplit(oldSplit, oldJobConf);
-
-      TaskAttemptID taskAttemptID = TaskAttemptID.forName(oldJobConf.get("mapred.task.id"));
-      if (taskAttemptID == null) {
-        taskAttemptID = new TaskAttemptID();
-      }
-
-      // create a TaskInputOutputContext
-      final TaskAttemptContext taskContext = ContextUtil.newTaskAttemptContext(oldJobConf, taskAttemptID);
-
-      if (split != null) {
-        try {
-          realReader = newInputFormat.createRecordReader(split, taskContext);
-          realReader.initialize(split, taskContext);
-
-          // read once to gain access to key and value objects
-          if (realReader.nextKeyValue()) {
-            firstRecord = true;
-            valueObj = realReader.getCurrentValue();
-          } else {
-            eof = true;
-          }
-        } catch (final InterruptedException e) {
-          throw new IOException(e);
-        }
-      } else {
-        realReader = null;
-        eof = true;
-        if (valueObj == null) { // Should initialize the value for createValue
-          valueObj = new ArrayWritable(Writable.class, new Writable[schemaSize]);
-        }
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      if (realReader != null) {
-        realReader.close();
-      }
-    }
-
-    @Override
-    public Void createKey() {
-      return null;
-    }
-
-    @Override
-    public ArrayWritable createValue() {
-      return valueObj;
-    }
-
-    @Override
-    public long getPos() throws IOException {
-      return (long) (splitLen * getProgress());
-    }
-
-    @Override
-    public float getProgress() throws IOException {
-      if (realReader == null) {
-        return 1f;
-      } else {
-        try {
-          return realReader.getProgress();
-        } catch (final InterruptedException e) {
-          throw new IOException(e);
-        }
-      }
-    }
-
-    @Override
-    public boolean next(final Void key, final ArrayWritable value) throws IOException {
-      if (eof) {
-        return false;
-      }
-
-      try {
-        if (firstRecord) { // key & value are already read.
-          firstRecord = false;
-        } else if (!realReader.nextKeyValue()) {
-          eof = true; // strictly not required, just for consistency
-          return false;
-        }
-
-        final ArrayWritable tmpCurValue = realReader.getCurrentValue();
-
-        if (value != tmpCurValue) {
-          final Writable[] arrValue = value.get();
-          final Writable[] arrCurrent = tmpCurValue.get();
-          if (value != null && arrValue.length == arrCurrent.length) {
-            System.arraycopy(arrCurrent, 0, arrValue, 0, arrCurrent.length);
-          } else {
-            if (arrValue.length != arrCurrent.length) {
-              throw new IOException("DeprecatedParquetHiveInput : size of object differs. Value size :  " + arrValue.length + ", Current Object size : "
-                      + arrCurrent.length);
-            } else {
-              throw new IOException("DeprecatedParquetHiveInput can not support RecordReaders that don't return same key & value & value is null");
-            }
-          }
-        }
-        return true;
-
-      } catch (final InterruptedException e) {
-        throw new IOException(e);
-      }
-    }
-
-    /**
-     * gets a ParquetInputSplit corresponding to a split given by Hive
-     *
-     * @param oldSplit The split given by Hive
-     * @param conf The JobConf of the Hive job
-     * @return a ParquetInputSplit corresponding to the oldSplit
-     * @throws IOException if the config cannot be enhanced or if the footer cannot be read from the file
-     */
-    protected ParquetInputSplit getSplit(final InputSplit oldSplit, final JobConf conf) throws IOException {
-      ParquetInputSplit split;
-
-      if (oldSplit instanceof InputSplitWrapper) {
-        split = ((InputSplitWrapper) oldSplit).getRealSplit();
-      } else if (oldSplit instanceof FileSplit) {
-        final Path finalPath = ((FileSplit) oldSplit).getPath();
-        final JobConf cloneJob = hiveBinding.pushProjectionsAndFilters(conf, finalPath.getParent());
-
-        final ParquetMetadata parquetMetadata = ParquetFileReader.readFooter(cloneJob, finalPath);
-        final List<BlockMetaData> blocks = parquetMetadata.getBlocks();
-        final FileMetaData fileMetaData = parquetMetadata.getFileMetaData();
-
-        final ReadContext readContext = new DataWritableReadSupport().init(cloneJob, fileMetaData.getKeyValueMetaData(), fileMetaData.getSchema());
-        schemaSize = MessageTypeParser.parseMessageType(readContext.getReadSupportMetadata().get(DataWritableReadSupport.HIVE_SCHEMA_KEY)).getFieldCount();
-
-        final List<BlockMetaData> splitGroup = new ArrayList<BlockMetaData>();
-        final long splitStart = ((FileSplit) oldSplit).getStart();
-        final long splitLength = ((FileSplit) oldSplit).getLength();
-        for (final BlockMetaData block : blocks) {
-          final long firstDataPage = block.getColumns().get(0).getFirstDataPageOffset();
-          if (firstDataPage >= splitStart && firstDataPage < splitStart + splitLength) {
-            splitGroup.add(block);
-          }
-        }
-
-        if (splitGroup.isEmpty()) {
-          LOG.warn("Skipping split, could not find row group in: " + (FileSplit) oldSplit);
-          split = null;
-        } else {
-          split = new ParquetInputSplit(finalPath,
-                  splitStart,
-                  splitLength,
-                  ((FileSplit) oldSplit).getLocations(),
-                  splitGroup,
-                  readContext.getRequestedSchema().toString(),
-                  fileMetaData.getSchema().toString(),
-                  fileMetaData.getKeyValueMetaData(),
-                  readContext.getReadSupportMetadata());
-        }
-
-      } else {
-        throw new IllegalArgumentException("Unknown split type: " + oldSplit);
-      }
-
-      return split;
-    }
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/MapredParquetOutputFormat.java b/src/ql/src/java/parquet/hive/MapredParquetOutputFormat.java
deleted file mode 100644
index 4cd811a..0000000
--- a/src/ql/src/java/parquet/hive/MapredParquetOutputFormat.java
+++ /dev/null
@@ -1,167 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Properties;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.FileOutputFormat;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordWriter;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.OutputFormat;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hadoop.util.Progressable;
-
-import parquet.hadoop.ParquetOutputFormat;
-import parquet.hadoop.util.ContextUtil;
-import parquet.hive.convert.HiveSchemaConverter;
-import parquet.hive.write.DataWritableWriteSupport;
-
-/**
- *
- * A Parquet OutputFormat for Hive (with the deprecated package mapred)
- *
- * TODO : Refactor all of the wrappers here
- * Talk about it on : https://github.com/Parquet/parquet-mr/pull/28s
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- *
- */
-@SuppressWarnings({"unchecked", "rawtypes"})
-public class MapredParquetOutputFormat extends FileOutputFormat<Void, ArrayWritable> implements HiveOutputFormat<Void, ArrayWritable> {
-
-  protected ParquetOutputFormat<ArrayWritable> realOutputFormat;
-
-  public MapredParquetOutputFormat() {
-    realOutputFormat = new ParquetOutputFormat<ArrayWritable>(new DataWritableWriteSupport());
-  }
-
-  public MapredParquetOutputFormat(final OutputFormat<Void, ArrayWritable> mapreduceOutputFormat) {
-    realOutputFormat = (ParquetOutputFormat<ArrayWritable>) mapreduceOutputFormat;
-  }
-
-  @Override
-  public void checkOutputSpecs(final FileSystem ignored, final JobConf job) throws IOException {
-    realOutputFormat.checkOutputSpecs(ContextUtil.newJobContext(job, null));
-  }
-
-  @Override
-  public RecordWriter<Void, ArrayWritable> getRecordWriter(final FileSystem ignored, 
-      final JobConf job, final String name, final Progressable progress) throws IOException {
-    throw new RuntimeException("Should never be used");
-  }
-
-  /**
-   *
-   * Create the parquet schema from the hive schema, and return the RecordWriterWrapper which contains the real output format
-   */
-  @Override
-  public org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter getHiveRecordWriter(final JobConf jc, final Path finalOutPath, final Class<? extends Writable> valueClass,
-          final boolean isCompressed, final Properties tableProperties, final Progressable progress) throws IOException {
-    // TODO find out if we can overwrite any _col0 column names here
-    final String columnNameProperty = tableProperties.getProperty("columns");
-    final String columnTypeProperty = tableProperties.getProperty("columns.types");
-    List<String> columnNames;
-    List<TypeInfo> columnTypes;
-
-    if (columnNameProperty.length() == 0) {
-      columnNames = new ArrayList<String>();
-    } else {
-      columnNames = Arrays.asList(columnNameProperty.split(","));
-    }
-
-    if (columnTypeProperty.length() == 0) {
-      columnTypes = new ArrayList<TypeInfo>();
-    } else {
-      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);
-    }
-
-    DataWritableWriteSupport.setSchema(HiveSchemaConverter.convert(columnNames, columnTypes), jc);
-    return new RecordWriterWrapper(realOutputFormat, jc, finalOutPath.toString(), progress);
-  }
-
-  private static class RecordWriterWrapper implements RecordWriter<Void, ArrayWritable>, org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter {
-
-    private final org.apache.hadoop.mapreduce.RecordWriter<Void, ArrayWritable> realWriter;
-    private TaskAttemptContext taskContext;
-
-    RecordWriterWrapper(final OutputFormat<Void, ArrayWritable> realOutputFormat, final JobConf jobConf, final String name, final Progressable progress) throws IOException {
-      try {
-        // create a TaskInputOutputContext
-        TaskAttemptID taskAttemptID = TaskAttemptID.forName(jobConf.get("mapred.task.id"));
-        if (taskAttemptID == null) {
-          taskAttemptID = new TaskAttemptID();
-        }
-        taskContext = ContextUtil.newTaskAttemptContext(jobConf, taskAttemptID);
-
-        realWriter = (org.apache.hadoop.mapreduce.RecordWriter<Void, ArrayWritable>) ((ParquetOutputFormat) realOutputFormat).getRecordWriter(taskContext, new Path(name));
-      } catch (final InterruptedException e) {
-        throw new IOException(e);
-      }
-    }
-
-    @Override
-    public void close(final Reporter reporter) throws IOException {
-      try {
-        // create a context just to pass reporter
-        realWriter.close(taskContext);
-      } catch (final InterruptedException e) {
-        throw new IOException(e);
-      }
-    }
-
-    @Override
-    public void write(final Void key, final ArrayWritable value) throws IOException {
-      try {
-        realWriter.write(key, value);
-      } catch (final InterruptedException e) {
-        throw new IOException(e);
-      }
-    }
-
-    @Override
-    public void close(final boolean abort) throws IOException {
-      try {
-        realWriter.close(taskContext);
-      } catch (final InterruptedException e) {
-        throw new IOException(e);
-      }
-    }
-
-    @Override
-    public void write(final Writable w) throws IOException {
-      try {
-        realWriter.write(null, (ArrayWritable) w);
-      } catch (final InterruptedException e) {
-        throw new IOException(e);
-      }
-    }
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/convert/ArrayWritableGroupConverter.java b/src/ql/src/java/parquet/hive/convert/ArrayWritableGroupConverter.java
deleted file mode 100644
index b7a56b4..0000000
--- a/src/ql/src/java/parquet/hive/convert/ArrayWritableGroupConverter.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.convert;
-
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-import parquet.io.ParquetDecodingException;
-import parquet.io.api.Converter;
-import parquet.schema.GroupType;
-
-/**
- *
- * A ArrayWritableGroupConverter
- *
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- *
- */
-public class ArrayWritableGroupConverter extends HiveGroupConverter {
-
-  private final Converter[] converters;
-  private final HiveGroupConverter parent;
-  private final int index;
-  private final boolean isMap;
-  private Writable currentValue;
-  private Writable[] mapPairContainer;
-
-  public ArrayWritableGroupConverter(final GroupType groupType, final HiveGroupConverter parent, final int index) {
-    this.parent = parent;
-    this.index = index;
-
-    if (groupType.getFieldCount() == 2) {
-      converters = new Converter[2];
-      converters[0] = getConverterFromDescription(groupType.getType(0), 0, this);
-      converters[1] = getConverterFromDescription(groupType.getType(1), 1, this);
-      isMap = true;
-    } else if (groupType.getFieldCount() == 1) {
-      converters = new Converter[1];
-      converters[0] = getConverterFromDescription(groupType.getType(0), 0, this);
-      isMap = false;
-    } else {
-      throw new RuntimeException("Invalid parquet hive schema: " + groupType);
-    }
-
-  }
-
-  @Override
-  public Converter getConverter(final int fieldIndex) {
-    return converters[fieldIndex];
-  }
-
-  @Override
-  public void start() {
-    if (isMap) {
-      mapPairContainer = new Writable[2];
-    }
-  }
-
-  @Override
-  public void end() {
-    if (isMap) {
-      currentValue = new ArrayWritable(Writable.class, mapPairContainer);
-    }
-    parent.add(index, currentValue);
-  }
-
-  @Override
-  protected void set(final int index, final Writable value) {
-    if (index != 0 && mapPairContainer == null || index > 1) {
-      throw new ParquetDecodingException("Repeated group can only have one or two fields for maps. Not allowed to set for the index : " + index);
-    }
-
-    if (isMap) {
-      mapPairContainer[index] = value;
-    } else {
-      currentValue = value;
-    }
-  }
-
-  @Override
-  protected void add(final int index, final Writable value) {
-    set(index, value);
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/convert/DataWritableGroupConverter.java b/src/ql/src/java/parquet/hive/convert/DataWritableGroupConverter.java
deleted file mode 100644
index f21e18e..0000000
--- a/src/ql/src/java/parquet/hive/convert/DataWritableGroupConverter.java
+++ /dev/null
@@ -1,138 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License
- * at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS
- * OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
- */
-package parquet.hive.convert;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-import parquet.io.api.Converter;
-import parquet.schema.GroupType;
-import parquet.schema.Type;
-
-/**
- *
- * A MapWritableGroupConverter, real converter between hive and parquet types recursively for complex types.
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- *
- */
-public class DataWritableGroupConverter extends HiveGroupConverter {
-
-  private final Converter[] converters;
-  private final HiveGroupConverter parent;
-  private final int index;
-  private final Object[] currentArr;
-  private Writable[] rootMap;
-
-  public DataWritableGroupConverter(final GroupType requestedSchema, final GroupType tableSchema) {
-    this(requestedSchema, null, 0, tableSchema);
-    final int fieldCount = tableSchema.getFieldCount();
-    this.rootMap = new Writable[fieldCount];
-  }
-
-  public DataWritableGroupConverter(final GroupType groupType, final HiveGroupConverter parent, final int index) {
-    this(groupType, parent, index, groupType);
-  }
-
-  public DataWritableGroupConverter(final GroupType selectedGroupType, final HiveGroupConverter parent, final int index, final GroupType containingGroupType) {
-    this.parent = parent;
-    this.index = index;
-    final int totalFieldCount = containingGroupType.getFieldCount();
-    final int selectedFieldCount = selectedGroupType.getFieldCount();
-
-    currentArr = new Object[totalFieldCount];
-    converters = new Converter[selectedFieldCount];
-
-    int i = 0;
-    for (final Type subtype : selectedGroupType.getFields()) {
-      if (containingGroupType.getFields().contains(subtype)) {
-        converters[i] = getConverterFromDescription(subtype, containingGroupType.getFieldIndex(subtype.getName()), this);
-      } else {
-        throw new RuntimeException("Group type [" + containingGroupType + "] does not contain requested field: " + subtype);
-      }
-      ++i;
-    }
-  }
-
-  final public ArrayWritable getCurrentArray() {
-    final Writable[] writableArr;
-    if (this.rootMap != null) { // We're at the root : we can safely re-use the same map to save perf
-      writableArr = this.rootMap;
-    } else {
-      writableArr = new Writable[currentArr.length];
-    }
-
-    for (int i = 0; i < currentArr.length; i++) {
-      final Object obj = currentArr[i];
-      if (obj instanceof List) {
-        final List<?> objList = (List<?>)obj;
-        final ArrayWritable arr = new ArrayWritable(Writable.class, objList.toArray(new Writable[objList.size()]));
-        writableArr[i] = arr;
-      } else {
-        writableArr[i] = (Writable) obj;
-      }
-    }
-    return new ArrayWritable(Writable.class, writableArr);
-  }
-
-  @Override
-  final protected void set(final int index, final Writable value) {
-    currentArr[index] = value;
-  }
-
-  @Override
-  public Converter getConverter(final int fieldIndex) {
-    return converters[fieldIndex];
-  }
-
-  @Override
-  public void start() {
-    for (int i = 0; i < currentArr.length; i++) {
-      currentArr[i] = null;
-    }
-  }
-
-  @Override
-  public void end() {
-    if (parent != null) {
-      parent.set(index, getCurrentArray());
-    }
-  }
-
-  @Override
-  protected void add(final int index, final Writable value) {
-
-    if (currentArr[index] != null) {
-
-      final Object obj = currentArr[index];
-      if (obj instanceof List) {
-        final List<Writable> list = (List<Writable>) obj;
-        list.add(value);
-      } else {
-        throw new RuntimeException("This should be a List: " + obj);
-      }
-
-    } else {
-      // create a list here because we don't know the final length of the object
-      // and it is more flexible than ArrayWritable.
-      //
-      // converted to ArrayWritable by getCurrentArray().
-      final List<Writable> buffer = new ArrayList<Writable>();
-      buffer.add(value);
-      currentArr[index] = (Object) buffer;
-    }
-
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/convert/DataWritableRecordConverter.java b/src/ql/src/java/parquet/hive/convert/DataWritableRecordConverter.java
deleted file mode 100644
index 2c048f3..0000000
--- a/src/ql/src/java/parquet/hive/convert/DataWritableRecordConverter.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License
- * at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS
- * OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
- */
-package parquet.hive.convert;
-
-import org.apache.hadoop.io.ArrayWritable;
-
-import parquet.io.api.GroupConverter;
-import parquet.io.api.RecordMaterializer;
-import parquet.schema.GroupType;
-
-/**
- *
- * A MapWritableReadSupport, encapsulates the tuples
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- *
- */
-public class DataWritableRecordConverter extends RecordMaterializer<ArrayWritable> {
-
-  private final DataWritableGroupConverter root;
-
-  public DataWritableRecordConverter(final GroupType requestedSchema, final GroupType tableSchema) {
-    this.root = new DataWritableGroupConverter(requestedSchema, tableSchema);
-  }
-
-  @Override
-  public ArrayWritable getCurrentRecord() {
-    return root.getCurrentArray();
-  }
-
-  @Override
-  public GroupConverter getRootConverter() {
-    return root;
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/convert/ETypeConverter.java b/src/ql/src/java/parquet/hive/convert/ETypeConverter.java
deleted file mode 100644
index 81e8938..0000000
--- a/src/ql/src/java/parquet/hive/convert/ETypeConverter.java
+++ /dev/null
@@ -1,165 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.convert;
-
-import java.math.BigDecimal;
-
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.io.BooleanWritable;
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-
-import parquet.column.Dictionary;
-import parquet.hive.writable.BinaryWritable;
-import parquet.hive.writable.BinaryWritable.DicBinaryWritable;
-import parquet.io.api.Binary;
-import parquet.io.api.Converter;
-import parquet.io.api.PrimitiveConverter;
-
-/**
- *
- * ETypeConverter is an easy way to set the converter for the right type.
- *
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- *
- */
-public enum ETypeConverter {
-
-  EDOUBLE_CONVERTER(Double.TYPE) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addDouble(final double value) {
-          parent.set(index, new DoubleWritable(value));
-        }
-      };
-    }
-  },
-  EBOOLEAN_CONVERTER(Boolean.TYPE) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addBoolean(final boolean value) {
-          parent.set(index, new BooleanWritable(value));
-        }
-      };
-    }
-  },
-  EFLOAT_CONVERTER(Float.TYPE) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addFloat(final float value) {
-          parent.set(index, new FloatWritable(value));
-        }
-      };
-    }
-  },
-  EINT32_CONVERTER(Integer.TYPE) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addInt(final int value) {
-          parent.set(index, new IntWritable(value));
-        }
-      };
-    }
-  },
-  EINT64_CONVERTER(Long.TYPE) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addLong(final long value) {
-          parent.set(index, new LongWritable(value));
-        }
-      };
-    }
-  },
-  EINT96_CONVERTER(BigDecimal.class) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        @Override
-        final public void addDouble(final double value) {
-          parent.set(index, new DoubleWritable(value));
-        }
-      };
-    }
-  },
-  EBINARY_CONVERTER(Binary.class) {
-    @Override
-    Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-      return new PrimitiveConverter() {
-        private Binary[] dictBinary;
-        private String[] dict;
-
-        @Override
-        public boolean hasDictionarySupport() {
-          return true;
-        }
-
-        @Override
-        public void setDictionary(Dictionary dictionary) {
-          dictBinary = new Binary[dictionary.getMaxId() + 1];
-          dict = new String[dictionary.getMaxId() + 1];
-          for (int i = 0; i <= dictionary.getMaxId(); i++) {
-            Binary binary = dictionary.decodeToBinary(i);
-            dictBinary[i] = binary;
-            dict[i] = binary.toStringUsingUTF8();
-          }
-        }
-
-        @Override
-        public void addValueFromDictionary(int dictionaryId) {
-          parent.set(index, new DicBinaryWritable(dictBinary[dictionaryId],  dict[dictionaryId]));
-        }
-
-        @Override
-        final public void addBinary(Binary value) {
-          parent.set(index, new BinaryWritable(value));
-        }
-      };
-    }
-  };
-  final Class<?> _type;
-
-  private ETypeConverter(final Class<?> type) {
-    this._type = type;
-  }
-
-  private Class<?> getType() {
-    return _type;
-  }
-
-  abstract Converter getConverter(final Class<?> type, final int index, final HiveGroupConverter parent);
-
-  static public Converter getNewConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {
-    for (final ETypeConverter eConverter : values()) {
-      if (eConverter.getType() == type) {
-        return eConverter.getConverter(type, index, parent);
-      }
-    }
-    throw new RuntimeException("Converter not found ... for type : " + type);
-  }
-
-}
diff --git a/src/ql/src/java/parquet/hive/convert/HiveGroupConverter.java b/src/ql/src/java/parquet/hive/convert/HiveGroupConverter.java
deleted file mode 100644
index 7546012..0000000
--- a/src/ql/src/java/parquet/hive/convert/HiveGroupConverter.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.convert;
-
-import org.apache.hadoop.io.Writable;
-
-import parquet.io.api.Converter;
-import parquet.io.api.GroupConverter;
-import parquet.schema.Type;
-import parquet.schema.Type.Repetition;
-
-public abstract class HiveGroupConverter extends GroupConverter {
-
-  static protected Converter getConverterFromDescription(final Type type, final int index, final HiveGroupConverter parent) {
-    if (type == null) {
-      return null;
-    }
-
-    if (type.isPrimitive()) {
-      return ETypeConverter.getNewConverter(type.asPrimitiveType().getPrimitiveTypeName().javaType, index, parent);
-    } else {
-      if (type.asGroupType().getRepetition() == Repetition.REPEATED) {
-        return new ArrayWritableGroupConverter(type.asGroupType(), parent, index);
-      } else {
-        return new DataWritableGroupConverter(type.asGroupType(), parent, index);
-      }
-    }
-  }
-
-  abstract protected void set(int index, Writable value);
-
-  abstract protected void add(int index, Writable value);
-
-}
diff --git a/src/ql/src/java/parquet/hive/convert/HiveSchemaConverter.java b/src/ql/src/java/parquet/hive/convert/HiveSchemaConverter.java
deleted file mode 100644
index dd70469..0000000
--- a/src/ql/src/java/parquet/hive/convert/HiveSchemaConverter.java
+++ /dev/null
@@ -1,138 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.convert;
-
-import java.util.List;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
-import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-
-import parquet.Log;
-import parquet.hive.serde.ParquetHiveSerDe;
-import parquet.schema.GroupType;
-import parquet.schema.MessageType;
-import parquet.schema.OriginalType;
-import parquet.schema.PrimitiveType;
-import parquet.schema.PrimitiveType.PrimitiveTypeName;
-import parquet.schema.Type;
-import parquet.schema.Type.Repetition;
-
-/**
- *
- * A HiveSchemaConverter
- *
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- *
- */
-public class HiveSchemaConverter {
-
-  private static final Log LOG = Log.getLog(HiveSchemaConverter.class);
-
-  static public MessageType convert(final List<String> columnNames, final List<TypeInfo> columnTypes) {
-    final MessageType schema = new MessageType("hive_schema", convertTypes(columnNames, columnTypes));
-    return schema;
-  }
-
-  static private Type[] convertTypes(final List<String> columnNames, final List<TypeInfo> columnTypes) {
-    if (columnNames.size() != columnTypes.size()) {
-      throw new RuntimeException("Mismatched Hive columns and types. Hive columns names found : " + columnNames
-              + " . And Hive types found : " + columnTypes);
-    }
-
-    final Type[] types = new Type[columnNames.size()];
-
-    for (int i = 0; i < columnNames.size(); ++i) {
-      types[i] = convertType(columnNames.get(i), columnTypes.get(i));
-    }
-
-    return types;
-  }
-
-  static private Type convertType(final String name, final TypeInfo typeInfo) {
-    return convertType(name, typeInfo, Repetition.OPTIONAL);
-  }
-
-  static private Type convertType(final String name, final TypeInfo typeInfo, final Repetition repetition) {
-    if (typeInfo.getCategory().equals(Category.PRIMITIVE)) {
-      if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.BINARY, name);
-      } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo) || typeInfo.equals(TypeInfoFactory.shortTypeInfo) || typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.INT32, name);
-      } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.INT64, name);
-      } else if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.DOUBLE, name);
-      } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.FLOAT, name);
-      } else if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.BOOLEAN, name);
-      } else if (typeInfo.equals(TypeInfoFactory.binaryTypeInfo)) {
-        // TODO : binaryTypeInfo is a byte array. Need to map it
-        throw new UnsupportedOperationException("Binary type not implemented");
-      } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {
-        throw new UnsupportedOperationException("Timestamp type not implemented");
-      } else if (typeInfo.equals(TypeInfoFactory.voidTypeInfo)) {
-        throw new UnsupportedOperationException("Void type not implemented");
-      } else if (typeInfo.equals(TypeInfoFactory.unknownTypeInfo)) {
-        throw new UnsupportedOperationException("Unknown type not implemented");
-      } else {
-        throw new RuntimeException("Unknown type: " + typeInfo);
-      }
-    } else if (typeInfo.getCategory().equals(Category.LIST)) {
-      return convertArrayType(name, (ListTypeInfo) typeInfo);
-    } else if (typeInfo.getCategory().equals(Category.STRUCT)) {
-      return convertStructType(name, (StructTypeInfo) typeInfo);
-    } else if (typeInfo.getCategory().equals(Category.MAP)) {
-      return convertMapType(name, (MapTypeInfo) typeInfo);
-    } else if (typeInfo.getCategory().equals(Category.UNION)) {
-      throw new UnsupportedOperationException("Union type not implemented");
-    } else {
-      throw new RuntimeException("Unknown type: " + typeInfo);
-    }
-  }
-
-  // An optional group containing a repeated anonymous group "bag", containing
-  // 1 anonymous element "array_element"
-  static private GroupType convertArrayType(final String name, final ListTypeInfo typeInfo) {
-    final TypeInfo subType = typeInfo.getListElementTypeInfo();
-    return listWrapper(name, OriginalType.LIST, new GroupType(Repetition.REPEATED, ParquetHiveSerDe.ARRAY.toString(), convertType("array_element", subType)));
-  }
-
-  // An optional group containing multiple elements
-  static private GroupType convertStructType(final String name, final StructTypeInfo typeInfo) {
-    final List<String> columnNames = typeInfo.getAllStructFieldNames();
-    final List<TypeInfo> columnTypes = typeInfo.getAllStructFieldTypeInfos();
-    return new GroupType(Repetition.OPTIONAL, name, convertTypes(columnNames, columnTypes));
-
-  }
-
-  // An optional group containing a repeated anonymous group "map", containing
-  // 2 elements: "key", "value"
-  static private GroupType convertMapType(final String name, final MapTypeInfo typeInfo) {
-    final Type keyType = convertType(ParquetHiveSerDe.MAP_KEY.toString(), typeInfo.getMapKeyTypeInfo(), Repetition.REQUIRED);
-    final Type valueType = convertType(ParquetHiveSerDe.MAP_VALUE.toString(), typeInfo.getMapValueTypeInfo());
-    return listWrapper(name, OriginalType.MAP_KEY_VALUE, new GroupType(Repetition.REPEATED, ParquetHiveSerDe.MAP.toString(), keyType, valueType));
-  }
-
-  static private GroupType listWrapper(final String name, final OriginalType originalType, final GroupType groupType) {
-    return new GroupType(Repetition.OPTIONAL, name, originalType, groupType);
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/internal/AbstractHiveBinding.java b/src/ql/src/java/parquet/hive/internal/AbstractHiveBinding.java
deleted file mode 100644
index 649ece0..0000000
--- a/src/ql/src/java/parquet/hive/internal/AbstractHiveBinding.java
+++ /dev/null
@@ -1,36 +0,0 @@
-package parquet.hive.internal;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.hadoop.util.StringUtils;
-
-import parquet.hive.HiveBinding;
-
-/**
- * Common code among implementations of {@link parquet.hive.HiveBinding HiveBinding}.
- */
-public abstract class AbstractHiveBinding implements HiveBinding {
-  private static final List<String> virtualColumns;
-  
-  static {
-    List<String> vcols =  new ArrayList<String>();
-    vcols.add("INPUT__FILE__NAME");
-    vcols.add("BLOCK__OFFSET__INSIDE__FILE");
-    vcols.add("ROW__OFFSET__INSIDE__BLOCK");
-    vcols.add("RAW__DATA__SIZE");
-    virtualColumns = Collections.unmodifiableList(vcols);
-  }
-  
-  /**
-   * {@inheritDoc}
-   */
-  @Override
-  public List<String> getColumns(final String columns) {
-    final List<String> result = (List<String>) StringUtils.getStringCollection(columns);
-    result.removeAll(virtualColumns);
-    return result;
-  }
-
-}
diff --git a/src/ql/src/java/parquet/hive/internal/Hive012Binding.java b/src/ql/src/java/parquet/hive/internal/Hive012Binding.java
deleted file mode 100644
index ebb0987..0000000
--- a/src/ql/src/java/parquet/hive/internal/Hive012Binding.java
+++ /dev/null
@@ -1,152 +0,0 @@
-package parquet.hive.internal;
-
-
-import java.io.IOException;
-import java.io.Serializable;
-import java.util.List;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.exec.Operator;
-import org.apache.hadoop.hive.ql.exec.TableScanOperator;
-import org.apache.hadoop.hive.ql.exec.Utilities;
-import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
-import org.apache.hadoop.hive.ql.plan.MapWork;
-import org.apache.hadoop.hive.ql.plan.PartitionDesc;
-import org.apache.hadoop.hive.ql.plan.TableScanDesc;
-import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
-import org.apache.hadoop.mapred.JobConf;
-
-import parquet.Log;
-
-/**
- * Hive 0.12 implementation of {@link parquet.hive.HiveBinding HiveBinding}.
- * This class is a copied and slightly modified version of
- * <a href="http://bit.ly/1a4tcrb">ManageJobConfig</a> class.
- */
-public class Hive012Binding extends AbstractHiveBinding {
-  private static final Log LOG = Log.getLog(Hive012Binding.class);
-  private final Map<String, PartitionDesc> pathToPartitionInfo =
-      new LinkedHashMap<String, PartitionDesc>();
-  /**
-   * MapWork is the Hive object which describes input files,
-   * columns projections, and filters.
-   */
-  private MapWork mapWork;
-
-
-  /**
-   * Initialize the mapWork variable in order to get all the partition and start to update the jobconf
-   *
-   * @param job
-   */
-  private void init(final JobConf job) {
-    final String plan = HiveConf.getVar(job, HiveConf.ConfVars.PLAN);
-    if (mapWork == null && plan != null && plan.length() > 0) {
-      mapWork = Utilities.getMapWork(job);
-      pathToPartitionInfo.clear();
-      for (final Map.Entry<String, PartitionDesc> entry : mapWork.getPathToPartitionInfo().entrySet()) {
-        pathToPartitionInfo.put(new Path(entry.getKey()).toUri().getPath().toString(), entry.getValue());
-      }
-    }
-  }
-
-  private void pushProjectionsAndFilters(final JobConf jobConf,
-      final String splitPath, final String splitPathWithNoSchema) {
-
-    if (mapWork == null) {
-      LOG.debug("Not pushing projections and filters because MapWork is null");
-      return;
-    } else if (mapWork.getPathToAliases() == null) {
-      LOG.debug("Not pushing projections and filters because pathToAliases is null");
-      return;
-    }
-
-    final ArrayList<String> aliases = new ArrayList<String>();
-    final Iterator<Entry<String, ArrayList<String>>> iterator = mapWork.getPathToAliases().entrySet().iterator();
-
-    while (iterator.hasNext()) {
-      final Entry<String, ArrayList<String>> entry = iterator.next();
-      final String key = new Path(entry.getKey()).toUri().getPath();
-
-      if (splitPath.equals(key) || splitPathWithNoSchema.equals(key)) {
-        final ArrayList<String> list = entry.getValue();
-        for (final String val : list) {
-          aliases.add(val);
-        }
-      }
-    }
-
-    for (final String alias : aliases) {
-      final Operator<? extends Serializable> op = mapWork.getAliasToWork().get(
-              alias);
-      if (op != null && op instanceof TableScanOperator) {
-        final TableScanOperator tableScan = (TableScanOperator) op;
-
-        // push down projections
-        final List<Integer> list = tableScan.getNeededColumnIDs();
-
-        if (list != null) {
-          ColumnProjectionUtils.appendReadColumnIDs(jobConf, list);
-        } else {
-          ColumnProjectionUtils.setFullyReadColumns(jobConf);
-        }
-
-        pushFilters(jobConf, tableScan);
-      }
-    }
-  }
-
-  private void pushFilters(final JobConf jobConf, final TableScanOperator tableScan) {
-
-    final TableScanDesc scanDesc = tableScan.getConf();
-    if (scanDesc == null) {
-      LOG.debug("Not pushing filters because TableScanDesc is null");
-      return;
-    }
-
-    // construct column name list for reference by filter push down
-    Utilities.setColumnNameList(jobConf, tableScan);
-
-    // push down filters
-    final ExprNodeDesc filterExpr = scanDesc.getFilterExpr();
-    if (filterExpr == null) {
-      LOG.debug("Not pushing filters because FilterExpr is null");
-      return;
-    }
-
-    final String filterText = filterExpr.getExprString();
-    /*
-    final String filterExprSerialized = Utilities.serializeExpression(filterExpr);
-    jobConf.set(
-            TableScanDesc.FILTER_TEXT_CONF_STR,
-            filterText);
-    jobConf.set(
-            TableScanDesc.FILTER_EXPR_CONF_STR,
-            filterExprSerialized);
-    */
-  }
-
-  /**
-   * {@inheritDoc}
-   */
-  @Override
-  public JobConf pushProjectionsAndFilters(JobConf jobConf, Path path)
-      throws IOException {
-    init(jobConf);
-    final JobConf cloneJobConf = new JobConf(jobConf);
-    final PartitionDesc part = pathToPartitionInfo.get(path.toString());
-
-    if ((part != null) && (part.getTableDesc() != null)) {
-      Utilities.copyTableJobPropertiesToConf(part.getTableDesc(), cloneJobConf);
-    }
-
-    pushProjectionsAndFilters(cloneJobConf, path.toString(), path.toUri().toString());
-    return cloneJobConf;
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/read/DataWritableReadSupport.java b/src/ql/src/java/parquet/hive/read/DataWritableReadSupport.java
deleted file mode 100644
index 77ecd00..0000000
--- a/src/ql/src/java/parquet/hive/read/DataWritableReadSupport.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
- * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
- * specific language governing permissions and limitations under the License.
- */
-package parquet.hive.read;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
-import org.apache.hadoop.io.ArrayWritable;
-
-import parquet.hadoop.api.ReadSupport;
-import parquet.hive.HiveBindingFactory;
-import parquet.hive.convert.DataWritableRecordConverter;
-import parquet.io.api.RecordMaterializer;
-import parquet.schema.MessageType;
-import parquet.schema.MessageTypeParser;
-import parquet.schema.PrimitiveType;
-import parquet.schema.PrimitiveType.PrimitiveTypeName;
-import parquet.schema.Type;
-import parquet.schema.Type.Repetition;
-
-/**
- *
- * A MapWritableReadSupport
- *
- * Manages the translation between Hive and Parquet
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- *
- */
-public class DataWritableReadSupport extends ReadSupport<ArrayWritable> {
-
-  public static final String HIVE_SCHEMA_KEY = "HIVE_TABLE_SCHEMA";
-
-  /**
-   *
-   * It creates the readContext for Parquet side with the requested schema during the init phase.
-   *
-   * @param configuration needed to get the wanted columns
-   * @param keyValueMetaData // unused
-   * @param fileSchema parquet file schema
-   * @return the parquet ReadContext
-   */
-  @Override
-  public parquet.hadoop.api.ReadSupport.ReadContext init(final Configuration configuration, final Map<String, String> keyValueMetaData, final MessageType fileSchema) {
-    final String columns = configuration.get("columns");
-    final Map<String, String> contextMetadata = new HashMap<String, String>();
-    if (columns != null) {
-      final List<String> listColumns = (new HiveBindingFactory()).create().getColumns(columns);
-
-      final List<Type> typeListTable = new ArrayList<Type>();
-      for (final String col : listColumns) {
-        if (fileSchema.containsField(col)) {
-          typeListTable.add(fileSchema.getType(col));
-        } else { // dummy type, should not be called
-          typeListTable.add(new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, col));
-        }
-      }
-      MessageType tableSchema = new MessageType("table_schema", typeListTable);
-      contextMetadata.put(HIVE_SCHEMA_KEY, tableSchema.toString());
-
-      MessageType requestedSchemaByUser = tableSchema;
-      final List<Integer> indexColumnsWanted = ColumnProjectionUtils.getReadColumnIDs(configuration);
-
-      final List<Type> typeListWanted = new ArrayList<Type>();
-      for (final Integer idx : indexColumnsWanted) {
-        typeListWanted.add(tableSchema.getType(listColumns.get(idx)));
-      }
-      requestedSchemaByUser = new MessageType(fileSchema.getName(), typeListWanted);
-
-      return new ReadContext(requestedSchemaByUser, contextMetadata);
-    } else {
-      contextMetadata.put(HIVE_SCHEMA_KEY, fileSchema.toString());
-      return new ReadContext(fileSchema, contextMetadata);
-    }
-  }
-
-  /**
-   *
-   * It creates the hive read support to interpret data from parquet to hive
-   *
-   * @param configuration // unused
-   * @param keyValueMetaData
-   * @param fileSchema // unused
-   * @param readContext containing the requested schema and the schema of the hive table
-   * @return Record Materialize for Hive
-   */
-  @Override
-  public RecordMaterializer<ArrayWritable> prepareForRead(final Configuration configuration, final Map<String, String> keyValueMetaData, final MessageType fileSchema,
-          final parquet.hadoop.api.ReadSupport.ReadContext readContext) {
-    final Map<String, String> metadata = readContext.getReadSupportMetadata();
-    if (metadata == null) {
-      throw new RuntimeException("ReadContext not initialized properly. Don't know the Hive Schema.");
-    }
-    final MessageType tableSchema = MessageTypeParser.parseMessageType(metadata.get(HIVE_SCHEMA_KEY));
-    return new DataWritableRecordConverter(readContext.getRequestedSchema(), tableSchema);
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/serde/AbstractParquetMapInspector.java b/src/ql/src/java/parquet/hive/serde/AbstractParquetMapInspector.java
deleted file mode 100644
index 83bf00f..0000000
--- a/src/ql/src/java/parquet/hive/serde/AbstractParquetMapInspector.java
+++ /dev/null
@@ -1,163 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
- * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
- * specific language governing permissions and limitations under the License.
- */
-package parquet.hive.serde;
-
-import java.util.HashMap;
-import java.util.Map;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-/**
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public abstract class AbstractParquetMapInspector implements SettableMapObjectInspector {
-
-  protected final ObjectInspector keyInspector;
-  protected final ObjectInspector valueInspector;
-
-  public AbstractParquetMapInspector(final ObjectInspector keyInspector, final ObjectInspector valueInspector) {
-    this.keyInspector = keyInspector;
-    this.valueInspector = valueInspector;
-  }
-
-  @Override
-  public String getTypeName() {
-    return "map<" + keyInspector.getTypeName() + "," + valueInspector.getTypeName() + ">";
-  }
-
-  @Override
-  public Category getCategory() {
-    return Category.MAP;
-  }
-
-  @Override
-  public ObjectInspector getMapKeyObjectInspector() {
-    return keyInspector;
-  }
-
-  @Override
-  public ObjectInspector getMapValueObjectInspector() {
-    return valueInspector;
-  }
-
-  @Override
-  public Map<?, ?> getMap(final Object data) {
-    if (data == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
-
-      if (mapContainer == null || mapContainer.length == 0) {
-        return null;
-      }
-
-      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
-      final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
-
-      for (final Writable obj : mapArray) {
-        final ArrayWritable mapObj = (ArrayWritable) obj;
-        final Writable[] arr = mapObj.get();
-        map.put(arr[0], arr[1]);
-      }
-
-      return map;
-    }
-
-    if (data instanceof Map) {
-      return (Map) data;
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public int getMapSize(final Object data) {
-    if (data == null) {
-      return -1;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
-
-      if (mapContainer == null || mapContainer.length == 0) {
-        return -1;
-      } else {
-        return ((ArrayWritable) mapContainer[0]).get().length;
-      }
-    }
-
-    if (data instanceof Map) {
-      return ((Map) data).size();
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public Object create() {
-    Map<Object, Object> m = new HashMap<Object, Object>();
-    return m;
-  }
-
-  @Override
-  public Object put(Object map, Object key, Object value) {
-    Map<Object, Object> m = (HashMap<Object, Object>) map;
-    m.put(key, value);
-    return m;
-  }
-
-  @Override
-  public Object remove(Object map, Object key) {
-    Map<Object, Object> m = (HashMap<Object, Object>) map;
-    m.remove(key);
-    return m;
-  }
-
-  @Override
-  public Object clear(Object map) {
-    Map<Object, Object> m = (HashMap<Object, Object>) map;
-    m.clear();
-    return m;
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null) {
-      return false;
-    }
-    if (getClass() != obj.getClass()) {
-      return false;
-    }
-    final StandardParquetHiveMapInspector other = (StandardParquetHiveMapInspector) obj;
-    if (this.keyInspector != other.keyInspector && (this.keyInspector == null || !this.keyInspector.equals(other.keyInspector))) {
-      return false;
-    }
-    if (this.valueInspector != other.valueInspector && (this.valueInspector == null || !this.valueInspector.equals(other.valueInspector))) {
-      return false;
-    }
-    return true;
-  }
-
-  @Override
-  public int hashCode() {
-    int hash = 7;
-    hash = 59 * hash + (this.keyInspector != null ? this.keyInspector.hashCode() : 0);
-    hash = 59 * hash + (this.valueInspector != null ? this.valueInspector.hashCode() : 0);
-    return hash;
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/serde/ArrayWritableObjectInspector.java b/src/ql/src/java/parquet/hive/serde/ArrayWritableObjectInspector.java
deleted file mode 100644
index 2f93383..0000000
--- a/src/ql/src/java/parquet/hive/serde/ArrayWritableObjectInspector.java
+++ /dev/null
@@ -1,224 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
- * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
- * specific language governing permissions and limitations under the License.
- */
-package parquet.hive.serde;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.SettableStructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.StructField;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.io.ArrayWritable;
-import parquet.hive.serde.primitive.ParquetPrimitiveInspectorFactory;
-
-/**
- *
- * The ArrayWritableObjectInspector will inspect an ArrayWritable, considering it as a Hive struct.<br />
- * It can also inspect a List if Hive decides to inspect the result of an inspection.
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- *
- */
-public class ArrayWritableObjectInspector extends SettableStructObjectInspector {
-
-  private final TypeInfo typeInfo;
-  private final List<TypeInfo> fieldInfos;
-  private final List<String> fieldNames;
-  private final List<StructField> fields;
-  private final HashMap<String, StructFieldImpl> fieldsByName;
-
-  public ArrayWritableObjectInspector(final StructTypeInfo rowTypeInfo) {
-
-    typeInfo = rowTypeInfo;
-    fieldNames = rowTypeInfo.getAllStructFieldNames();
-    fieldInfos = rowTypeInfo.getAllStructFieldTypeInfos();
-    fields = new ArrayList<StructField>(fieldNames.size());
-    fieldsByName = new HashMap<String, StructFieldImpl>();
-
-    for (int i = 0; i < fieldNames.size(); ++i) {
-      final String name = fieldNames.get(i);
-      final TypeInfo fieldInfo = fieldInfos.get(i);
-
-      final StructFieldImpl field = new StructFieldImpl(name, getObjectInspector(fieldInfo), i);
-      fields.add(field);
-      fieldsByName.put(name, field);
-    }
-  }
-
-  private ObjectInspector getObjectInspector(final TypeInfo typeInfo) {
-    if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {
-      return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {
-      return PrimitiveObjectInspectorFactory.writableBooleanObjectInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {
-      return PrimitiveObjectInspectorFactory.writableFloatObjectInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo)) {
-      return PrimitiveObjectInspectorFactory.writableIntObjectInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {
-      return PrimitiveObjectInspectorFactory.writableLongObjectInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {
-      return ParquetPrimitiveInspectorFactory.parquetStringInspector;
-    } else if (typeInfo.getCategory().equals(Category.STRUCT)) {
-      return new ArrayWritableObjectInspector((StructTypeInfo) typeInfo);
-    } else if (typeInfo.getCategory().equals(Category.LIST)) {
-      final TypeInfo subTypeInfo = ((ListTypeInfo) typeInfo).getListElementTypeInfo();
-      return new ParquetHiveArrayInspector(getObjectInspector(subTypeInfo));
-    } else if (typeInfo.getCategory().equals(Category.MAP)) {
-      final TypeInfo keyTypeInfo = ((MapTypeInfo) typeInfo).getMapKeyTypeInfo();
-      final TypeInfo valueTypeInfo = ((MapTypeInfo) typeInfo).getMapValueTypeInfo();
-      if (keyTypeInfo.equals(TypeInfoFactory.stringTypeInfo) || keyTypeInfo.equals(TypeInfoFactory.byteTypeInfo)
-              || keyTypeInfo.equals(TypeInfoFactory.shortTypeInfo)) {
-        return new DeepParquetHiveMapInspector(getObjectInspector(keyTypeInfo), getObjectInspector(valueTypeInfo));
-      } else {
-        return new StandardParquetHiveMapInspector(getObjectInspector(keyTypeInfo), getObjectInspector(valueTypeInfo));
-      }
-    } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {
-      throw new UnsupportedOperationException("timestamp not implemented yet");
-    } else if (typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {
-      return ParquetPrimitiveInspectorFactory.parquetByteInspector;
-    } else if (typeInfo.equals(TypeInfoFactory.shortTypeInfo)) {
-      return ParquetPrimitiveInspectorFactory.parquetShortInspector;
-    } else {
-      throw new RuntimeException("Unknown field info: " + typeInfo);
-    }
-
-  }
-
-  @Override
-  public Category getCategory() {
-    return Category.STRUCT;
-  }
-
-  @Override
-  public String getTypeName() {
-    return typeInfo.getTypeName();
-  }
-
-  @Override
-  public List<? extends StructField> getAllStructFieldRefs() {
-    return fields;
-  }
-
-  @Override
-  public Object getStructFieldData(final Object data, final StructField fieldRef) {
-    if (data == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final ArrayWritable arr = (ArrayWritable) data;
-      return arr.get()[((StructFieldImpl) fieldRef).getIndex()];
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public StructField getStructFieldRef(final String name) {
-    return fieldsByName.get(name);
-  }
-
-  @Override
-  public List<Object> getStructFieldsDataAsList(final Object data) {
-    if (data == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final ArrayWritable arr = (ArrayWritable) data;
-      final Object[] arrWritable = arr.get();
-      return new ArrayList<Object>(Arrays.asList(arrWritable));
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public Object create() {
-    final ArrayList<Object> list = new ArrayList<Object>(fields.size());
-    for (int i = 0; i < fields.size(); ++i) {
-      list.add(null);
-    }
-    return list;
-  }
-
-  @Override
-  public Object setStructFieldData(Object struct, StructField field, Object fieldValue) {
-    final ArrayList<Object> list = (ArrayList<Object>) struct;
-    list.set(((StructFieldImpl) field).getIndex(), fieldValue);
-    return list;
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null) {
-      return false;
-    }
-    if (getClass() != obj.getClass()) {
-      return false;
-    }
-    final ArrayWritableObjectInspector other = (ArrayWritableObjectInspector) obj;
-    if (this.typeInfo != other.typeInfo && (this.typeInfo == null || !this.typeInfo.equals(other.typeInfo))) {
-      return false;
-    }
-    return true;
-  }
-
-  @Override
-  public int hashCode() {
-    int hash = 5;
-    hash = 29 * hash + (this.typeInfo != null ? this.typeInfo.hashCode() : 0);
-    return hash;
-  }
-
-  class StructFieldImpl implements StructField {
-
-    private final String name;
-    private final ObjectInspector inspector;
-    private final int index;
-
-    public StructFieldImpl(final String name, final ObjectInspector inspector, final int index) {
-      this.name = name;
-      this.inspector = inspector;
-      this.index = index;
-    }
-
-    @Override
-    public String getFieldComment() {
-      return "";
-    }
-
-    @Override
-    public String getFieldName() {
-      return name;
-    }
-
-    public int getIndex() {
-      return index;
-    }
-
-    @Override
-    public ObjectInspector getFieldObjectInspector() {
-      return inspector;
-    }
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/serde/DeepParquetHiveMapInspector.java b/src/ql/src/java/parquet/hive/serde/DeepParquetHiveMapInspector.java
deleted file mode 100644
index 2a38949..0000000
--- a/src/ql/src/java/parquet/hive/serde/DeepParquetHiveMapInspector.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
- * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
- * specific language governing permissions and limitations under the License.
- */
-package parquet.hive.serde;
-
-import java.util.Map;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-/**
- * The DeepParquetHiveMapInspector will inspect an ArrayWritable, considering it as a Hive map.<br />
- * It can also inspect a Map if Hive decides to inspect the result of an inspection.<br />
- * When trying to access elements from the map it will iterate over all keys, inspecting them and comparing them to the
- * desired key.
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class DeepParquetHiveMapInspector extends AbstractParquetMapInspector {
-
-  public DeepParquetHiveMapInspector(final ObjectInspector keyInspector, final ObjectInspector valueInspector) {
-    super(keyInspector, valueInspector);
-  }
-
-  @Override
-  public Object getMapValueElement(final Object data, final Object key) {
-    if (data == null || key == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
-
-      if (mapContainer == null || mapContainer.length == 0) {
-        return null;
-      }
-
-      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
-
-      for (final Writable obj : mapArray) {
-        final ArrayWritable mapObj = (ArrayWritable) obj;
-        final Writable[] arr = mapObj.get();
-        if (key.equals(arr[0]) || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveJavaObject(arr[0]))
-                || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveWritableObject(arr[0]))) {
-          return arr[1];
-        }
-      }
-
-      return null;
-    }
-
-    if (data instanceof Map) {
-      final Map<?, ?> map = (Map<?, ?>) data;
-
-      if (map.containsKey(key)) {
-        return map.get(key);
-      }
-
-      for (final Map.Entry<?, ?> entry : map.entrySet()) {
-        if (key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveJavaObject(entry.getKey()))
-                || key.equals(((PrimitiveObjectInspector) keyInspector).getPrimitiveWritableObject(entry.getKey()))) {
-          return entry.getValue();
-        }
-      }
-
-      return null;
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/serde/ParquetHiveArrayInspector.java b/src/ql/src/java/parquet/hive/serde/ParquetHiveArrayInspector.java
deleted file mode 100644
index 1b0094f..0000000
--- a/src/ql/src/java/parquet/hive/serde/ParquetHiveArrayInspector.java
+++ /dev/null
@@ -1,185 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
- * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
- * specific language governing permissions and limitations under the License.
- */
-package parquet.hive.serde;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.SettableListObjectInspector;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-/**
- * The ParquetHiveArrayInspector will inspect an ArrayWritable, considering it as an Hive array.<br />
- * It can also inspect a List if Hive decides to inspect the result of an inspection.
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class ParquetHiveArrayInspector implements SettableListObjectInspector {
-
-  ObjectInspector arrayElementInspector;
-
-  public ParquetHiveArrayInspector(final ObjectInspector arrayElementInspector) {
-    this.arrayElementInspector = arrayElementInspector;
-  }
-
-  @Override
-  public String getTypeName() {
-    return "array<" + arrayElementInspector.getTypeName() + ">";
-  }
-
-  @Override
-  public Category getCategory() {
-    return Category.LIST;
-  }
-
-  @Override
-  public ObjectInspector getListElementObjectInspector() {
-    return arrayElementInspector;
-  }
-
-  @Override
-  public Object getListElement(final Object data, final int index) {
-    if (data == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] listContainer = ((ArrayWritable) data).get();
-
-      if (listContainer == null || listContainer.length == 0) {
-        return null;
-      }
-
-      final Writable subObj = listContainer[0];
-
-      if (subObj == null) {
-        return null;
-      }
-
-      if (index >= 0 && index < ((ArrayWritable) subObj).get().length) {
-        return ((ArrayWritable) subObj).get()[index];
-      } else {
-        return null;
-      }
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public int getListLength(final Object data) {
-    if (data == null) {
-      return -1;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] listContainer = ((ArrayWritable) data).get();
-
-      if (listContainer == null || listContainer.length == 0) {
-        return -1;
-      }
-
-      final Writable subObj = listContainer[0];
-
-      if (subObj == null) {
-        return 0;
-      }
-
-      return ((ArrayWritable) subObj).get().length;
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public List<?> getList(final Object data) {
-    if (data == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] listContainer = ((ArrayWritable) data).get();
-
-      if (listContainer == null || listContainer.length == 0) {
-        return null;
-      }
-
-      final Writable subObj = listContainer[0];
-
-      if (subObj == null) {
-        return null;
-      }
-
-      final Writable[] array = ((ArrayWritable) subObj).get();
-      final List<Writable> list = new ArrayList<Writable>();
-
-      for (final Writable obj : array) {
-        list.add(obj);
-      }
-
-      return list;
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-
-  @Override
-  public Object create(final int size) {
-    final ArrayList<Object> result = new ArrayList<Object>(size);
-    for (int i = 0; i < size; ++i) {
-      result.add(null);
-    }
-    return result;
-  }
-
-  @Override
-  public Object set(final Object list, final int index, final Object element) {
-    final ArrayList l = (ArrayList) list;
-    l.set(index, element);
-    return list;
-  }
-
-  @Override
-  public Object resize(final Object list, final int newSize) {
-    final ArrayList l = (ArrayList) list;
-    l.ensureCapacity(newSize);
-    while (l.size() < newSize) {
-      l.add(null);
-    }
-    while (l.size() > newSize) {
-      l.remove(l.size() - 1);
-    }
-    return list;
-  }
-
-  @Override
-  public boolean equals(final Object o) {
-    if (o == null || o.getClass() != getClass()) {
-      return false;
-    } else if (o == this) {
-      return true;
-    } else {
-      final ObjectInspector other = ((ParquetHiveArrayInspector) o).arrayElementInspector;
-      return other.equals(arrayElementInspector);
-    }
-  }
-
-  @Override
-  public int hashCode() {
-    int hash = 3;
-    hash = 29 * hash + (this.arrayElementInspector != null ? this.arrayElementInspector.hashCode() : 0);
-    return hash;
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/serde/ParquetHiveSerDe.java b/src/ql/src/java/parquet/hive/serde/ParquetHiveSerDe.java
deleted file mode 100644
index f216e5e..0000000
--- a/src/ql/src/java/parquet/hive/serde/ParquetHiveSerDe.java
+++ /dev/null
@@ -1,291 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.serde;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Properties;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.serde2.SerDe;
-import org.apache.hadoop.hive.serde2.SerDeException;
-import org.apache.hadoop.hive.serde2.SerDeStats;
-import org.apache.hadoop.hive.serde2.io.ByteWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.StructField;
-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
-import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.BooleanWritable;
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-
-import parquet.hive.writable.BinaryWritable;
-import parquet.io.api.Binary;
-
-/**
- *
- * A ParquetHiveSerDe for Hive (with the deprecated package mapred)
- *
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- *
- */
-public class ParquetHiveSerDe implements SerDe {
-
-  public static Text MAP_KEY = new Text("key");
-  public static Text MAP_VALUE = new Text("value");
-  public static Text MAP = new Text("map");
-  public static Text ARRAY = new Text("bag");
-  private SerDeStats stats;
-  ObjectInspector objInspector;
-
-  private enum LAST_OPERATION {
-
-    SERIALIZE,
-    DESERIALIZE,
-    UNKNOWN
-  }
-  LAST_OPERATION status;
-  private long serializedSize;
-  private long deserializedSize;
-
-  @Override
-  final public void initialize(final Configuration conf, final Properties tbl) throws SerDeException {
-
-    final TypeInfo rowTypeInfo;
-    final List<String> columnNames;
-    final List<TypeInfo> columnTypes;
-    // Get column names and sort order
-    final String columnNameProperty = tbl.getProperty("columns");
-    final String columnTypeProperty = tbl.getProperty("columns.types");
-
-    if (columnNameProperty.length() == 0) {
-      columnNames = new ArrayList<String>();
-    } else {
-      columnNames = Arrays.asList(columnNameProperty.split(","));
-    }
-
-    if (columnTypeProperty.length() == 0) {
-      columnTypes = new ArrayList<TypeInfo>();
-    } else {
-      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);
-    }
-
-    if (columnNames.size() != columnTypes.size()) {
-      throw new RuntimeException("ParquetHiveSerde initialization failed. Number of column name and column type differs.");
-    }
-
-    // Create row related objects
-    rowTypeInfo = TypeInfoFactory.getStructTypeInfo(columnNames, columnTypes);
-    this.objInspector = new ArrayWritableObjectInspector((StructTypeInfo) rowTypeInfo);
-
-    // Stats part
-    stats = new SerDeStats();
-    serializedSize = 0;
-    deserializedSize = 0;
-    status = LAST_OPERATION.UNKNOWN;
-  }
-
-  @Override
-  public Object deserialize(final Writable blob) throws SerDeException {
-    status = LAST_OPERATION.DESERIALIZE;
-    deserializedSize = 0;
-    if (blob instanceof ArrayWritable) {
-      deserializedSize = ((ArrayWritable) blob).get().length;
-      return blob;
-    } else {
-      return null;
-    }
-  }
-
-  @Override
-  public ObjectInspector getObjectInspector() throws SerDeException {
-    return objInspector;
-  }
-
-  @Override
-  public Class<? extends Writable> getSerializedClass() {
-    return ArrayWritable.class;
-  }
-
-  @Override
-  public Writable serialize(final Object obj, final ObjectInspector objInspector) throws SerDeException {
-    if (!objInspector.getCategory().equals(Category.STRUCT)) {
-      throw new SerDeException("Cannot serialize " + objInspector.getCategory() + ". Can only serialize a struct");
-    }
-
-    final ArrayWritable serializeData = createStruct(obj, (StructObjectInspector) objInspector);
-
-    serializedSize = serializeData.get().length;
-    status = LAST_OPERATION.SERIALIZE;
-
-    return serializeData;
-  }
-
-  private ArrayWritable createStruct(final Object obj, final StructObjectInspector inspector) throws SerDeException {
-
-    final List<? extends StructField> fields = inspector.getAllStructFieldRefs();
-    final Writable[] arr = new Writable[fields.size()];
-
-    int i = 0;
-
-    for (final StructField field : fields) {
-      final Object subObj = inspector.getStructFieldData(obj, field);
-      final ObjectInspector subInspector = field.getFieldObjectInspector();
-
-      arr[i] = createObject(subObj, subInspector);
-      ++i;
-    }
-
-    return new ArrayWritable(Writable.class, arr);
-
-  }
-
-  private Writable createMap(final Object obj, final MapObjectInspector inspector) throws SerDeException {
-    final Map<?, ?> sourceMap = inspector.getMap(obj);
-    final ObjectInspector keyInspector = inspector.getMapKeyObjectInspector();
-    final ObjectInspector valueInspector = inspector.getMapValueObjectInspector();
-    final List<ArrayWritable> array = new ArrayList<ArrayWritable>();
-
-    if (sourceMap != null) {
-      for (final Entry<?, ?> keyValue : sourceMap.entrySet()) {
-        final Writable key = createObject(keyValue.getKey(), keyInspector);
-        final Writable value = createObject(keyValue.getValue(), valueInspector);
-
-        if (key != null) {
-          Writable[] arr = new Writable[2];
-          arr[0] = key;
-          arr[1] = value;
-          array.add(new ArrayWritable(Writable.class, arr));
-        }
-
-      }
-    }
-
-    if (array.size() > 0) {
-      final ArrayWritable subArray = new ArrayWritable(ArrayWritable.class, array.toArray(new ArrayWritable[array.size()]));
-      return new ArrayWritable(Writable.class, new Writable[] {subArray});
-    } else {
-      return null;
-    }
-  }
-
-  private ArrayWritable createArray(final Object obj, final ListObjectInspector inspector) throws SerDeException {
-    final List<?> sourceArray = inspector.getList(obj);
-    final ObjectInspector subInspector = inspector.getListElementObjectInspector();
-    final List<Writable> array = new ArrayList<Writable>();
-
-    if (sourceArray != null) {
-      for (final Object curObj : sourceArray) {
-        final Writable newObj = createObject(curObj, subInspector);
-        if (newObj != null) {
-          array.add(newObj);
-        }
-      }
-    }
-
-    if (array.size() > 0) {
-      final ArrayWritable subArray = new ArrayWritable(array.get(0).getClass(), array.toArray(new Writable[array.size()]));
-      return new ArrayWritable(Writable.class, new Writable[] {subArray});
-    } else {
-      return null;
-    }
-  }
-
-  private Writable createPrimitive(final Object obj, final PrimitiveObjectInspector inspector) throws SerDeException {
-
-    if (obj == null) {
-      return null;
-    }
-
-    switch (inspector.getPrimitiveCategory()) {
-    case VOID:
-      return null;
-    case BOOLEAN:
-      return new BooleanWritable(((BooleanObjectInspector) inspector).get(obj) ? Boolean.TRUE : Boolean.FALSE);
-    case BYTE:
-      return new ByteWritable((byte) ((ByteObjectInspector) inspector).get(obj));
-    case DOUBLE:
-      return new DoubleWritable(((DoubleObjectInspector) inspector).get(obj));
-    case FLOAT:
-      return new FloatWritable(((FloatObjectInspector) inspector).get(obj));
-    case INT:
-      return new IntWritable(((IntObjectInspector) inspector).get(obj));
-    case LONG:
-      return new LongWritable(((LongObjectInspector) inspector).get(obj));
-    case SHORT:
-      return new ShortWritable((short) ((ShortObjectInspector) inspector).get(obj));
-    case STRING:
-      return new BinaryWritable(Binary.fromString(((StringObjectInspector) inspector).getPrimitiveJavaObject(obj)));
-    default:
-      throw new SerDeException("Unknown primitive : " + inspector.getPrimitiveCategory());
-    }
-  }
-
-  private Writable createObject(final Object obj, final ObjectInspector inspector) throws SerDeException {
-    switch (inspector.getCategory()) {
-    case STRUCT:
-      return createStruct(obj, (StructObjectInspector) inspector);
-    case LIST:
-      return createArray(obj, (ListObjectInspector) inspector);
-    case MAP:
-      return createMap(obj, (MapObjectInspector) inspector);
-    case PRIMITIVE:
-      return createPrimitive(obj, (PrimitiveObjectInspector) inspector);
-    default:
-      throw new SerDeException("Unknown data type" + inspector.getCategory());
-    }
-  }
-
-  //
-  @Override
-  public SerDeStats getSerDeStats() {
-    // must be different
-    assert (status != LAST_OPERATION.UNKNOWN);
-
-    if (status == LAST_OPERATION.SERIALIZE) {
-      stats.setRawDataSize(serializedSize);
-    } else {
-      stats.setRawDataSize(deserializedSize);
-    }
-    return stats;
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/serde/StandardParquetHiveMapInspector.java b/src/ql/src/java/parquet/hive/serde/StandardParquetHiveMapInspector.java
deleted file mode 100644
index be703df..0000000
--- a/src/ql/src/java/parquet/hive/serde/StandardParquetHiveMapInspector.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
- * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
- * specific language governing permissions and limitations under the License.
- */
-package parquet.hive.serde;
-
-import java.util.Map;
-
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.Writable;
-
-/**
- * The StandardParquetHiveMapInspector will inspect an ArrayWritable, considering it as a Hive map.<br />
- * It can also inspect a Map if Hive decides to inspect the result of an inspection.
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class StandardParquetHiveMapInspector extends AbstractParquetMapInspector {
-
-  public StandardParquetHiveMapInspector(final ObjectInspector keyInspector, final ObjectInspector valueInspector) {
-    super(keyInspector, valueInspector);
-  }
-
-  @Override
-  public Object getMapValueElement(final Object data, final Object key) {
-    if (data == null || key == null) {
-      return null;
-    }
-
-    if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
-
-      if (mapContainer == null || mapContainer.length == 0) {
-        return null;
-      }
-
-      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
-
-      for (final Writable obj : mapArray) {
-        final ArrayWritable mapObj = (ArrayWritable) obj;
-        final Writable[] arr = mapObj.get();
-        if (key.equals(arr[0])) {
-          return arr[1];
-        }
-      }
-
-      return null;
-    }
-
-    if (data instanceof Map) {
-      return ((Map) data).get(key);
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + data.getClass().getCanonicalName());
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/serde/primitive/ParquetByteInspector.java b/src/ql/src/java/parquet/hive/serde/primitive/ParquetByteInspector.java
deleted file mode 100644
index 7ba43b5..0000000
--- a/src/ql/src/java/parquet/hive/serde/primitive/ParquetByteInspector.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.serde.primitive;
-
-import org.apache.hadoop.hive.serde2.io.ByteWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableByteObjectInspector;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.io.IntWritable;
-
-/**
- * The ParquetByteInspector can inspect both ByteWritables and IntWritables into bytes.
- *
- * @author Nong Li
- */
-public class ParquetByteInspector extends AbstractPrimitiveJavaObjectInspector implements SettableByteObjectInspector {
-
-  ParquetByteInspector() {
-    super(TypeInfoFactory.byteTypeInfo);
-  }
-
-  @Override
-  public Object getPrimitiveWritableObject(final Object o) {
-    return o == null ? null : new ByteWritable(get(o));
-  }
-
-  @Override
-  public Object create(final byte val) {
-    return new ByteWritable(val);
-  }
-
-  @Override
-  public Object set(final Object o, final byte val) {
-    ((ByteWritable) o).set(val);
-    return o;
-  }
-
-  @Override
-  public byte get(Object o) {
-    // Accept int writables and convert them.
-    if (o instanceof IntWritable) {
-      return (byte) ((IntWritable) o).get();
-    }
-    return ((ByteWritable) o).get();
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/serde/primitive/ParquetPrimitiveInspectorFactory.java b/src/ql/src/java/parquet/hive/serde/primitive/ParquetPrimitiveInspectorFactory.java
deleted file mode 100644
index fd4e007..0000000
--- a/src/ql/src/java/parquet/hive/serde/primitive/ParquetPrimitiveInspectorFactory.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.serde.primitive;
-
-/**
- * The ParquetPrimitiveInspectorFactory allows us to be sure that the same object is inspected by the same inspector.
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class ParquetPrimitiveInspectorFactory {
-
-  public static final ParquetByteInspector parquetByteInspector = new ParquetByteInspector();
-  public static final ParquetShortInspector parquetShortInspector = new ParquetShortInspector();
-  public static final ParquetStringInspector parquetStringInspector = new ParquetStringInspector();
-
-  private ParquetPrimitiveInspectorFactory() {
-    // prevent instantiation
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/serde/primitive/ParquetShortInspector.java b/src/ql/src/java/parquet/hive/serde/primitive/ParquetShortInspector.java
deleted file mode 100644
index abf19a2..0000000
--- a/src/ql/src/java/parquet/hive/serde/primitive/ParquetShortInspector.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.serde.primitive;
-
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableShortObjectInspector;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.io.IntWritable;
-
-/**
- * The ParquetShortInspector can inspect both ShortWritables and IntWritables into shorts.
- *
- * @author Nong Li
- */
-public class ParquetShortInspector extends AbstractPrimitiveJavaObjectInspector implements SettableShortObjectInspector {
-
-  ParquetShortInspector() {
-    super(TypeInfoFactory.shortTypeInfo);
-  }
-
-  @Override
-  public Object getPrimitiveWritableObject(final Object o) {
-    return o == null ? null : new ShortWritable(get(o));
-  }
-
-  @Override
-  public Object create(final short val) {
-    return new ShortWritable(val);
-  }
-
-  @Override
-  public Object set(final Object o, final short val) {
-    ((ShortWritable) o).set(val);
-    return o;
-  }
-
-  @Override
-  public short get(Object o) {
-    // Accept int writables and convert them.
-    if (o instanceof IntWritable) {
-      return (short) ((IntWritable) o).get();
-    }
-    return ((ShortWritable) o).get();
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/serde/primitive/ParquetStringInspector.java b/src/ql/src/java/parquet/hive/serde/primitive/ParquetStringInspector.java
deleted file mode 100644
index d1b7273..0000000
--- a/src/ql/src/java/parquet/hive/serde/primitive/ParquetStringInspector.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/*
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.serde.primitive;
-
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableStringObjectInspector;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.io.Text;
-import parquet.hive.writable.BinaryWritable;
-import parquet.io.api.Binary;
-
-/**
- * The ParquetStringInspector inspects a BinaryWritable to give a Text or String.
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class ParquetStringInspector extends AbstractPrimitiveJavaObjectInspector implements SettableStringObjectInspector {
-
-  ParquetStringInspector() {
-    super(TypeInfoFactory.stringTypeInfo);
-  }
-
-  @Override
-  public Text getPrimitiveWritableObject(final Object o) {
-    if (o == null) {
-      return null;
-    }
-
-    if (o instanceof BinaryWritable) {
-      return new Text(((BinaryWritable) o).getBytes());
-    }
-
-    if (o instanceof Text) {
-      return (Text) o;
-    }
-
-    if (o instanceof String) {
-      return new Text((String) o);
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + o.getClass().getCanonicalName());
-  }
-
-  @Override
-  public String getPrimitiveJavaObject(final Object o) {
-    if (o == null) {
-      return null;
-    }
-
-    if (o instanceof BinaryWritable) {
-      return ((BinaryWritable) o).getString();
-    }
-
-    if (o instanceof Text) {
-      return ((Text) o).toString();
-    }
-
-    if (o instanceof String) {
-      return (String) o;
-    }
-
-    throw new UnsupportedOperationException("Cannot inspect " + o.getClass().getCanonicalName());
-  }
-
-  @Override
-  public Object set(final Object o, final Text text) {
-    return new BinaryWritable(text == null ? null : Binary.fromByteArray(text.getBytes()));
-  }
-
-  @Override
-  public Object set(final Object o, final String string) {
-    return new BinaryWritable(string == null ? null : Binary.fromString(string));
-  }
-
-  @Override
-  public Object create(final Text text) {
-    if (text == null) {
-      return null;
-    }
-    return text.toString();
-  }
-
-  @Override
-  public Object create(final String string) {
-    return string;
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/writable/BigDecimalWritable.java b/src/ql/src/java/parquet/hive/writable/BigDecimalWritable.java
deleted file mode 100644
index 9ba4f49..0000000
--- a/src/ql/src/java/parquet/hive/writable/BigDecimalWritable.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package parquet.hive.writable;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.math.BigDecimal;
-import java.math.BigInteger;
-
-import org.apache.hadoop.hive.serde2.ByteStream.Output;
-import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils;
-import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.VInt;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.io.WritableUtils;
-
-/**
- * This file is taken from hive 0.11
- * website : http://hive.apache.org/
- * github : https://github.com/apache/hive
- * branch : branch-0.11
- * Issue : https://issues.apache.org/jira/browse/HIVE-2693
- * License : Same as the header of this file
- *
- */
-public class BigDecimalWritable implements WritableComparable<BigDecimalWritable> {
-
-  private byte[] internalStorage = new byte[0];
-  private int scale;
-
-  private final VInt vInt = new VInt(); // reusable integer
-
-  public BigDecimalWritable() {
-  }
-
-  public BigDecimalWritable(final byte[] bytes, final int scale) {
-    set(bytes, scale);
-  }
-
-  public BigDecimalWritable(final BigDecimalWritable writable) {
-    set(writable.getBigDecimal());
-  }
-
-  public BigDecimalWritable(final BigDecimal value) {
-    set(value);
-  }
-
-  public void set(BigDecimal value) {
-    value = value.stripTrailingZeros();
-    if (value.compareTo(BigDecimal.ZERO) == 0) {
-      // Special case for 0, because java doesn't strip zeros correctly on
-      // that number.
-      value = BigDecimal.ZERO;
-    }
-    set(value.unscaledValue().toByteArray(), value.scale());
-  }
-
-  public void set(final BigDecimalWritable writable) {
-    set(writable.getBigDecimal());
-  }
-
-  public void set(final byte[] bytes, final int scale) {
-    this.internalStorage = bytes;
-    this.scale = scale;
-  }
-
-  public void setFromBytes(final byte[] bytes, int offset, final int length) {
-    LazyBinaryUtils.readVInt(bytes, offset, vInt);
-    scale = vInt.value;
-    offset += vInt.length;
-    LazyBinaryUtils.readVInt(bytes, offset, vInt);
-    offset += vInt.length;
-    if (internalStorage.length != vInt.value) {
-      internalStorage = new byte[vInt.value];
-    }
-    System.arraycopy(bytes, offset, internalStorage, 0, vInt.value);
-  }
-
-  public BigDecimal getBigDecimal() {
-    return new BigDecimal(new BigInteger(internalStorage), scale);
-  }
-
-  @Override
-  public void readFields(final DataInput in) throws IOException {
-    scale = WritableUtils.readVInt(in);
-    final int byteArrayLen = WritableUtils.readVInt(in);
-    if (internalStorage.length != byteArrayLen) {
-      internalStorage = new byte[byteArrayLen];
-    }
-    in.readFully(internalStorage);
-  }
-
-  @Override
-  public void write(final DataOutput out) throws IOException {
-    WritableUtils.writeVInt(out, scale);
-    WritableUtils.writeVInt(out, internalStorage.length);
-    out.write(internalStorage);
-  }
-
-  @Override
-  public int compareTo(final BigDecimalWritable that) {
-    return getBigDecimal().compareTo(that.getBigDecimal());
-  }
-
-  public void writeToByteStream(final Output byteStream) {
-    LazyBinaryUtils.writeVInt(byteStream, scale);
-    LazyBinaryUtils.writeVInt(byteStream, internalStorage.length);
-    byteStream.write(internalStorage, 0, internalStorage.length);
-  }
-
-  @Override
-  public String toString() {
-    return getBigDecimal().toString();
-  }
-
-  @Override
-  public boolean equals(final Object other) {
-    if (other == null || !(other instanceof BigDecimalWritable)) {
-      return false;
-    }
-    final BigDecimalWritable bdw = (BigDecimalWritable) other;
-
-    // 'equals' and 'compareTo' are not compatible with BigDecimals. We want
-    // compareTo which returns true iff the numbers are equal (e.g.: 3.14 is
-        // the same as 3.140). 'Equals' returns true iff equal and the same
-    // scale
-    // is set in the decimals (e.g.: 3.14 is not the same as 3.140)
-    return getBigDecimal().compareTo(bdw.getBigDecimal()) == 0;
-  }
-
-  @Override
-  public int hashCode() {
-    return getBigDecimal().hashCode();
-  }
-
-}
diff --git a/src/ql/src/java/parquet/hive/writable/BinaryWritable.java b/src/ql/src/java/parquet/hive/writable/BinaryWritable.java
deleted file mode 100644
index f187e31..0000000
--- a/src/ql/src/java/parquet/hive/writable/BinaryWritable.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.writable;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.Writable;
-
-import parquet.io.api.Binary;
-
-/**
- *
- * A Wrapper to support constructor with Binary and String
- *
- * TODO : remove it, and call BytesWritable with the getBytes()
- *
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- *
- */
-public class BinaryWritable implements Writable {
-
-  private Binary binary;
-
-  public BinaryWritable(final Binary binary) {
-    this.binary = binary;
-  }
-
-  public Binary getBinary() {
-    return binary;
-  }
-
-  public byte[] getBytes() {
-    return binary.getBytes();
-  }
-
-  public String getString() {
-    return binary.toStringUsingUTF8();
-  }
-
-  @Override
-  public void readFields(DataInput input) throws IOException {
-    byte[] bytes = new byte[input.readInt()];
-    input.readFully(bytes);
-    binary = Binary.fromByteArray(bytes);
-  }
-
-  @Override
-  public void write(DataOutput output) throws IOException {
-    output.writeInt(binary.length());
-    binary.writeTo(output);
-  }
-
-  @Override
-  public int hashCode() {
-    return binary == null ? 0 : binary.hashCode();
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (obj instanceof BinaryWritable) {
-      final BinaryWritable other = (BinaryWritable)obj;
-      return binary.equals(other.binary);
-    }
-    return false;
-  }
-
-  public static class DicBinaryWritable extends BinaryWritable {
-
-    private String string;
-
-    public DicBinaryWritable(Binary binary, String string) {
-      super(binary);
-      this.string = string;
-    }
-
-    public String getString() {
-      return string;
-    }
-  }
-
-}
diff --git a/src/ql/src/java/parquet/hive/write/DataWritableWriteSupport.java b/src/ql/src/java/parquet/hive/write/DataWritableWriteSupport.java
deleted file mode 100644
index 06cf328..0000000
--- a/src/ql/src/java/parquet/hive/write/DataWritableWriteSupport.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License
- * at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS
- * OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
- */
-package parquet.hive.write;
-
-import java.util.HashMap;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.ArrayWritable;
-
-import parquet.hadoop.api.WriteSupport;
-import parquet.io.api.RecordConsumer;
-import parquet.schema.MessageType;
-import parquet.schema.MessageTypeParser;
-
-/**
- *
- * DataWritableWriteSupport is a WriteSupport for the DataWritableWriter
- *
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- *
- */
-public class DataWritableWriteSupport extends WriteSupport<ArrayWritable> {
-
-  public static final String PARQUET_HIVE_SCHEMA = "parquet.hive.schema";
-
-  public static void setSchema(final MessageType schema, final Configuration configuration) {
-    configuration.set(PARQUET_HIVE_SCHEMA, schema.toString());
-  }
-
-  public static MessageType getSchema(final Configuration configuration) {
-    return MessageTypeParser.parseMessageType(configuration.get(PARQUET_HIVE_SCHEMA));
-  }
-  private DataWritableWriter writer;
-  private MessageType schema;
-
-  @Override
-  public WriteContext init(final Configuration configuration) {
-    schema = getSchema(configuration);
-    return new WriteContext(schema, new HashMap<String, String>());
-  }
-
-  @Override
-  public void prepareForWrite(final RecordConsumer recordConsumer) {
-    writer = new DataWritableWriter(recordConsumer, schema);
-  }
-
-  @Override
-  public void write(final ArrayWritable record) {
-    writer.write(record);
-  }
-}
diff --git a/src/ql/src/java/parquet/hive/write/DataWritableWriter.java b/src/ql/src/java/parquet/hive/write/DataWritableWriter.java
deleted file mode 100644
index 32873b7..0000000
--- a/src/ql/src/java/parquet/hive/write/DataWritableWriter.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License
- * at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS
- * OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
- */
-package parquet.hive.write;
-
-import org.apache.hadoop.hive.serde2.io.ByteWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.BooleanWritable;
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Writable;
-import parquet.hive.writable.BigDecimalWritable;
-
-import parquet.hive.writable.BinaryWritable;
-import parquet.io.ParquetEncodingException;
-import parquet.io.api.Binary;
-import parquet.io.api.RecordConsumer;
-import parquet.schema.GroupType;
-import parquet.schema.Type;
-
-/**
- *
- * DataWritableWriter is a writer,
- * that will read an ArrayWritable and give the data to parquet
- * with the expected schema
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- *
- */
-public class DataWritableWriter {
-
-  private final RecordConsumer recordConsumer;
-  private final GroupType schema;
-
-  public DataWritableWriter(final RecordConsumer recordConsumer, final GroupType schema) {
-    this.recordConsumer = recordConsumer;
-    this.schema = schema;
-  }
-
-  public void write(final ArrayWritable arr) {
-
-    if (arr == null) {
-      return;
-    }
-    recordConsumer.startMessage();
-    writeData(arr, schema);
-    recordConsumer.endMessage();
-  }
-
-  private void writeData(final ArrayWritable arr, final GroupType type) {
-
-    if (arr == null) {
-      return;
-    }
-
-    final int fieldCount = type.getFieldCount();
-    Writable[] values = arr.get();
-    for (int field = 0; field < fieldCount; ++field) {
-      final Type fieldType = type.getType(field);
-      final String fieldName = fieldType.getName();
-      final Writable value = values[field];
-      if (value == null) {
-        continue;
-      }
-      recordConsumer.startField(fieldName, field);
-
-      if (fieldType.isPrimitive()) {
-        writePrimitive(value);
-      } else {
-        recordConsumer.startGroup();
-        if (value instanceof ArrayWritable) {
-          if (fieldType.asGroupType().getRepetition().equals(Type.Repetition.REPEATED)) {
-            writeArray((ArrayWritable) value, fieldType.asGroupType());
-          } else {
-            writeData((ArrayWritable) value, fieldType.asGroupType());
-          }
-        } else if (value != null) {
-          throw new ParquetEncodingException("This should be an ArrayWritable or MapWritable: " + value);
-        }
-
-        recordConsumer.endGroup();
-      }
-
-      recordConsumer.endField(fieldName, field);
-    }
-  }
-
-  private void writeArray(final ArrayWritable array, final GroupType type) {
-    if (array == null) {
-      return;
-    }
-
-    final Writable[] subValues = array.get();
-
-    final int fieldCount = type.getFieldCount();
-    for (int field = 0; field < fieldCount; ++field) {
-      final Type subType = type.getType(field);
-      recordConsumer.startField(subType.getName(), field);
-      for (int i = 0; i < subValues.length; ++i) {
-        final Writable subValue = subValues[i];
-        if (subValue != null) {
-          if (subType.isPrimitive()) {
-            if (subValue instanceof ArrayWritable) {
-              writePrimitive(((ArrayWritable) subValue).get()[field]);// 0 ?
-            } else {
-              writePrimitive(subValue);
-            }
-          } else {
-            if (!(subValue instanceof ArrayWritable)) {
-              throw new RuntimeException("This should be a ArrayWritable: " + subValue);
-            } else {
-              recordConsumer.startGroup();
-              writeData((ArrayWritable) subValue, subType.asGroupType());
-              recordConsumer.endGroup();
-            }
-          }
-        }
-      }
-      recordConsumer.endField(subType.getName(), field);
-    }
-
-  }
-
-  private void writePrimitive(final Writable value) {
-    if (value == null) {
-      return;
-    }
-    if (value instanceof DoubleWritable) {
-      recordConsumer.addDouble(((DoubleWritable) value).get());
-    } else if (value instanceof BooleanWritable) {
-      recordConsumer.addBoolean(((BooleanWritable) value).get());
-    } else if (value instanceof FloatWritable) {
-      recordConsumer.addFloat(((FloatWritable) value).get());
-    } else if (value instanceof IntWritable) {
-      recordConsumer.addInteger(((IntWritable) value).get());
-    } else if (value instanceof LongWritable) {
-      recordConsumer.addLong(((LongWritable) value).get());
-    } else if (value instanceof ShortWritable) {
-      recordConsumer.addInteger(((ShortWritable) value).get());
-    } else if (value instanceof ByteWritable) {
-      recordConsumer.addInteger(((ByteWritable) value).get());
-    } else if (value instanceof BigDecimalWritable) {
-      throw new UnsupportedOperationException("BigDecimal writing not implemented");
-    } else if (value instanceof BinaryWritable) {
-      recordConsumer.addBinary(((BinaryWritable) value).getBinary());
-    } else {
-      throw new RuntimeException("Unknown value type: " + value + " " + value.getClass());
-    }
-  }
-}
diff --git a/src/ql/src/test/parquet/hive/TestHiveSchemaConverter.java b/src/ql/src/test/parquet/hive/TestHiveSchemaConverter.java
deleted file mode 100644
index aa60173..0000000
--- a/src/ql/src/test/parquet/hive/TestHiveSchemaConverter.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import static org.junit.Assert.assertEquals;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-import org.junit.Test;
-
-import parquet.hive.convert.HiveSchemaConverter;
-import parquet.schema.MessageType;
-import parquet.schema.MessageTypeParser;
-
-/**
- *
- * TestHiveSchemaConverter
- *
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- *
- */
-public class TestHiveSchemaConverter {
-
-  private List<String> createHiveColumnsFrom(final String columnNamesStr) {
-    List<String> columnNames;
-    if (columnNamesStr.length() == 0) {
-      columnNames = new ArrayList<String>();
-    } else {
-      columnNames = Arrays.asList(columnNamesStr.split(","));
-    }
-
-    return columnNames;
-  }
-
-  private List<TypeInfo> createHiveTypeInfoFrom(final String columnsTypeStr) {
-    List<TypeInfo> columnTypes;
-
-    if (columnsTypeStr.length() == 0) {
-      columnTypes = new ArrayList<TypeInfo>();
-    } else {
-      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnsTypeStr);
-    }
-
-    return columnTypes;
-  }
-
-  private void testConversion(final String columnNamesStr, final String columnsTypeStr, final String expectedSchema) throws Exception {
-    final List<String> columnNames = createHiveColumnsFrom(columnNamesStr);
-    final List<TypeInfo> columnTypes = createHiveTypeInfoFrom(columnsTypeStr);
-    final MessageType messageTypeFound = HiveSchemaConverter.convert(columnNames, columnTypes);
-    final MessageType expectedMT = MessageTypeParser.parseMessageType(expectedSchema);
-    assertEquals("converting " + columnNamesStr + ": " + columnsTypeStr + " to " + expectedSchema, expectedMT, messageTypeFound);
-  }
-
-  @Test
-  public void testSimpleType() throws Exception {
-    testConversion(
-            "a,b,c",
-            "int,double,boolean",
-            "message hive_schema {\n"
-            + "  optional int32 a;\n"
-            + "  optional double b;\n"
-            + "  optional boolean c;\n"
-            + "}\n");
-  }
-
-  @Test
-  public void testArray() throws Exception {
-    testConversion("arrayCol",
-            "array<int>",
-            "message hive_schema {\n"
-            + "  optional group arrayCol (LIST) {\n"
-            + "    repeated group bag {\n"
-            + "      optional int32 array_element;\n"
-            + "    }\n"
-            + "  }\n"
-            + "}\n");
-  }
-
-  @Test
-  public void testStruct() throws Exception {
-    testConversion("structCol",
-            "struct<a:int,b:double,c:boolean>",
-            "message hive_schema {\n"
-            + "  optional group structCol {\n"
-            + "    optional int32 a;\n"
-            + "    optional double b;\n"
-            + "    optional boolean c;\n"
-            + "  }\n"
-            + "}\n");
-  }
-
-  @Test
-  public void testMap() throws Exception {
-    testConversion("mapCol",
-            "map<string,string>",
-            "message hive_schema {\n"
-            + "  optional group mapCol (MAP) {\n"
-            + "    repeated group map (MAP_KEY_VALUE) {\n"
-            + "      required binary key;\n"
-            + "      optional binary value;\n"
-            + "    }\n"
-            + "  }\n"
-            + "}\n");
-  }
-}
diff --git a/src/ql/src/test/parquet/hive/TestMapredParquetInputFormat.java b/src/ql/src/test/parquet/hive/TestMapredParquetInputFormat.java
deleted file mode 100644
index 47a65a2..0000000
--- a/src/ql/src/test/parquet/hive/TestMapredParquetInputFormat.java
+++ /dev/null
@@ -1,387 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import static org.junit.Assert.*;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.commons.lang.StringUtils;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.FileSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.junit.Before;
-import org.junit.Test;
-
-import parquet.column.impl.ColumnWriteStoreImpl;
-import parquet.column.page.mem.MemPageStore;
-import parquet.hadoop.ParquetFileReader;
-import parquet.hadoop.ParquetInputSplit;
-import parquet.hadoop.metadata.BlockMetaData;
-import parquet.hadoop.metadata.ParquetMetadata;
-import parquet.hive.MapredParquetInputFormat.InputSplitWrapper;
-import parquet.hive.read.DataWritableReadSupport;
-import parquet.io.ColumnIOFactory;
-import parquet.io.MessageColumnIO;
-import parquet.io.api.RecordConsumer;
-import parquet.schema.GroupType;
-import parquet.schema.MessageType;
-import parquet.schema.MessageTypeParser;
-import parquet.schema.OriginalType;
-import parquet.schema.PrimitiveType;
-import parquet.schema.PrimitiveType.PrimitiveTypeName;
-import parquet.schema.Type.Repetition;
-
-/**
- *
- * TestHiveInputFormat
- *
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- *
- */
-public class TestMapredParquetInputFormat {
-
-  private Configuration conf;
-  private JobConf job;
-  private FileSystem fs;
-  private Path dir;
-  private File testFile;
-  private Reporter reporter;
-  private Map<Integer, ArrayWritable> mapData;
-
-  @Test
-  public void testParquetHiveInputFormatWithoutSpecificSchema() throws Exception {
-    final String schemaRequested = "message customer {\n"
-            + "  optional int32 c_custkey;\n"
-            + "  optional binary c_name;\n"
-            + "  optional binary c_address;\n"
-            + "  optional int32 c_nationkey;\n"
-            + "  optional binary c_phone;\n"
-            + "  optional double c_acctbal;\n"
-            + "  optional binary c_mktsegment;\n"
-            + "  optional binary c_comment;\n"
-            + "  optional group c_map (MAP_KEY_VALUE) {\n"
-            + "    repeated group map {\n"
-            + "      required binary key;\n"
-            + "      optional binary value;\n"
-            + "    }\n"
-            + "  }\n"
-            + "  optional group c_list (LIST) {\n"
-            + "    repeated group bag {\n"
-            + "      optional int32 array_element;\n"
-            + "    }\n"
-            + "  }\n"
-            + "}";
-    readParquetHiveInputFormat(schemaRequested, new Integer[] {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10});
-  }
-
-  @Test
-  public void testParquetHiveInputFormatWithSpecificSchema() throws Exception {
-    final String schemaRequested = "message customer {\n"
-            + "  optional int32 c_custkey;\n"
-            + "  optional binary c_name;\n"
-            + "  optional double c_acctbal;\n"
-            + "  optional binary c_mktsegment;\n"
-            + "  optional binary c_comment;\n"
-            + "}";
-    readParquetHiveInputFormat(schemaRequested, new Integer[] {0, 1, 5, 6, 7});
-  }
-
-  @Test
-  public void testParquetHiveInputFormatWithSpecificSchemaRandomColumn() throws Exception {
-    final String schemaRequested = "message customer {\n"
-            + "  optional int32 c_custkey;\n"
-            + "  optional binary c_mktsegment;\n"
-            + "}";
-    readParquetHiveInputFormat(schemaRequested, new Integer[] {0, 6});
-  }
-
-  @Test
-  public void testParquetHiveInputFormatWithSpecificSchemaFirstColumn() throws Exception {
-    final String schemaRequested = "message customer {\n"
-            + "  optional int32 c_custkey;\n"
-            + "}";
-    readParquetHiveInputFormat(schemaRequested, new Integer[] {0});
-  }
-
-  @Test
-  public void testParquetHiveInputFormatWithSpecificSchemaUnknownColumn() throws Exception {
-    final String schemaRequested = "message customer {\n"
-            + "  optional int32 c_custkey;\n"
-            + "  optional int32 unknown;\n"
-            + "}";
-    readParquetHiveInputFormat(schemaRequested, new Integer[] {0, Integer.MIN_VALUE});
-  }
-
-  @Test
-  public void testGetSplit() throws Exception {
-    final ParquetMetadata readFooter = ParquetFileReader.readFooter(conf, new Path(testFile.getAbsolutePath()));
-
-    final MessageType fileSchema = readFooter.getFileMetaData().getSchema();
-    final MessageType requestedSchema = MessageTypeParser.parseMessageType("message customer {\n"
-            + "  optional int32 c_custkey;\n"
-            + "  optional binary c_name;\n"
-            + "  optional double c_acctbal;\n"
-            + "  optional binary c_mktsegment;\n"
-            + "  optional binary c_comment;\n"
-            + "}");
-    final MessageType hiveSchema = MessageTypeParser.parseMessageType("message customer {\n"
-            + "  optional int32 c_custkey;\n"
-            + "  optional binary c_name;\n"
-            + "  optional binary c_address;\n"
-            + "  optional int32 c_nationkey;\n"
-            + "  optional binary c_phone;\n"
-            + "  optional double c_acctbal;\n"
-            + "  optional binary c_mktsegment;\n"
-            + "  optional binary c_comment;\n"
-            + "  optional group c_map (MAP_KEY_VALUE) {\n"
-            + "    repeated group map {\n"
-            + "      required binary key;\n"
-            + "      optional binary value;\n"
-            + "    }\n"
-            + "  }\n"
-            + "  optional group c_list (LIST) {\n"
-            + "    repeated group bag {\n"
-            + "      optional int32 array_element;\n"
-            + "    }\n"
-            + "  }\n"
-            + "  optional binary unknown;\n"
-            + "}");
-
-    // Put columns and projection info in the conf
-    List<String> columns = new ArrayList<String>();
-    List<Integer> readColumns = new ArrayList<Integer>();
-    for (int i = 0; i < hiveSchema.getFieldCount(); ++i) {
-      final String name = hiveSchema.getType(i).getName();
-      columns.add(name);
-      if (requestedSchema.containsField(name)) {
-        readColumns.add(i);
-      }
-    }
-    job.set("columns", StringUtils.join(columns, ","));
-    ColumnProjectionUtils.setReadColumnIDs(job, readColumns);
-
-    long size = 0;
-    final List<BlockMetaData> blocks = readFooter.getBlocks();
-    for (final BlockMetaData block : blocks) {
-      size += block.getTotalByteSize();
-    }
-
-    final FileInputFormat<Void, ArrayWritable> format = new MapredParquetInputFormat();
-    final String[] locations = new String[] {"localhost"};
-
-    final Map<String, String> readSupportMetaData = new HashMap<String, String>();
-    readSupportMetaData.put(DataWritableReadSupport.HIVE_SCHEMA_KEY, hiveSchema.toString());
-    final ParquetInputSplit realSplit = new ParquetInputSplit(new Path(testFile.getAbsolutePath()), 0, size, locations, blocks,
-            fileSchema.toString(), requestedSchema.toString(), readFooter.getFileMetaData().getKeyValueMetaData(), readSupportMetaData);
-
-    final MapredParquetInputFormat.InputSplitWrapper splitWrapper = new InputSplitWrapper(realSplit);
-
-    // construct the record reader
-    final RecordReader<Void, ArrayWritable> reader = format.getRecordReader(splitWrapper, job, reporter);
-
-    assertEquals("Wrong real split inside wrapper", realSplit,
-            ((MapredParquetInputFormat.RecordReaderWrapper) reader).getSplit(splitWrapper, job));
-
-    // Recreate the split using getSplit, as Hive would
-    final FileSplit fileSplit = new FileSplit(splitWrapper.getPath(), splitWrapper.getStart(), splitWrapper.getLength(), splitWrapper.getLocations());
-    final ParquetInputSplit recreatedSplit = ((MapredParquetInputFormat.RecordReaderWrapper) reader).getSplit(fileSplit, job);
-    assertTrue("Wrong file schema", UtilitiesTestMethods.smartCheckSchema(fileSchema,
-            MessageTypeParser.parseMessageType(recreatedSplit.getFileSchema())));
-    assertTrue("Wrong requested schema", UtilitiesTestMethods.smartCheckSchema(requestedSchema,
-            MessageTypeParser.parseMessageType(recreatedSplit.getRequestedSchema())));
-    assertTrue("Wrong hive schema", UtilitiesTestMethods.smartCheckSchema(hiveSchema,
-            MessageTypeParser.parseMessageType(recreatedSplit.getReadSupportMetadata().get(DataWritableReadSupport.HIVE_SCHEMA_KEY))));
-  }
-
-  @Before
-  public void setUp() throws Exception {
-    //
-    // create job and filesystem and reporter and such.
-    //
-    mapData = new HashMap<Integer, ArrayWritable>();
-    conf = new Configuration();
-    job = new JobConf(conf);
-    fs = FileSystem.getLocal(conf);
-    dir = new Path("target/tests/from_java/deprecatedoutputformat/");
-    testFile = new File(dir.toString(), "customer");
-    reporter = Reporter.NULL;
-    if (testFile.exists()) {
-      if (!testFile.delete()) {
-        throw new RuntimeException("can not remove existing file " + testFile.getAbsolutePath());
-      }
-    }
-    fs.delete(dir, true);
-    FileInputFormat.setInputPaths(job, dir);
-
-    // Write data
-    writeFile();
-
-  }
-
-  private void writeFile() throws IOException {
-    final MessageType schema = new MessageType("customer",
-            new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.INT32, "c_custkey"),
-            new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, "c_name"),
-            new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, "c_address"),
-            new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.INT32, "c_nationkey"),
-            new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, "c_phone"),
-            new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.DOUBLE, "c_acctbal"),
-            new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, "c_mktsegment"),
-            new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, "c_comment"),
-            new GroupType(Repetition.OPTIONAL, "c_map", OriginalType.MAP_KEY_VALUE, new GroupType(Repetition.REPEATED, "map", new PrimitiveType(Repetition.REQUIRED, PrimitiveTypeName.BINARY, "key"), new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, "value"))),
-            new GroupType(Repetition.OPTIONAL, "c_list", OriginalType.LIST, new GroupType(Repetition.REPEATED, "bag", new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.INT32, "array_element"))));
-
-    final MemPageStore pageStore = new MemPageStore(1000);
-    final ColumnWriteStoreImpl store = new ColumnWriteStoreImpl(pageStore, 8 * 1024, 8 * 1024, 8 * 1024, false);
-    final MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);
-
-    final RecordConsumer recordWriter = columnIO.getRecordWriter(store);
-
-    Map<String, String> map = new HashMap<String, String>();
-    map.put("testkey", "testvalue");
-    map.put("foo", "bar");
-
-    List<Integer> list = new ArrayList<Integer>();
-    list.add(0);
-    list.add(12);
-    list.add(17);
-
-    int recordCount = 0;
-    mapData.clear();
-    for (int i = 0; i < 1000; i++) {
-      recordWriter.startMessage();
-      mapData.put(i, UtilitiesTestMethods.createArrayWritable(i, i % 11 == 0 ? null : "name_" + i, i % 12 == 0 ? null : "add_" + i,
-              i % 13 == 0 ? null : i, i % 14 == 0 ? null : "phone_" + i, i % 15 == 0 ? null : 1.2d * i, i % 16 == 0 ? null : "mktsegment_" + i,
-              i % 17 == 0 ? null : "comment_" + i, i % 18 == 0 ? null : map, i % 19 == 0 ? null : list));
-      saveData(recordWriter, i, i % 11 == 0 ? null : "name_" + i, i % 12 == 0 ? null : "add_" + i,
-              i % 13 == 0 ? null : i, i % 14 == 0 ? null : "phone_" + i, i % 15 == 0 ? null : 1.2d * i, i % 16 == 0 ? null : "mktsegment_" + i,
-              i % 17 == 0 ? null : "comment_" + i, i % 18 == 0 ? null : map, i % 19 == 0 ? null : list);
-      recordWriter.endMessage();
-      ++recordCount;
-    }
-    store.flush();
-
-    UtilitiesTestMethods.writeToFile(new Path(testFile.getAbsolutePath()), conf, schema, pageStore, recordCount);
-  }
-
-  private void saveData(final RecordConsumer recordWriter, final Integer custkey, final String name, final String address, final Integer nationkey, final String phone,
-          final Double acctbal, final String mktsegment, final String comment, final Map<String, String> map, final List<Integer> list) {
-    UtilitiesTestMethods.writeField(recordWriter, 0, "c_custkey", custkey);
-    UtilitiesTestMethods.writeField(recordWriter, 1, "c_name", name);
-    UtilitiesTestMethods.writeField(recordWriter, 2, "c_address", address);
-    UtilitiesTestMethods.writeField(recordWriter, 3, "c_nationkey", nationkey);
-    UtilitiesTestMethods.writeField(recordWriter, 4, "c_phone", phone);
-    UtilitiesTestMethods.writeField(recordWriter, 5, "c_acctbal", acctbal);
-    UtilitiesTestMethods.writeField(recordWriter, 6, "c_mktsegment", mktsegment);
-    UtilitiesTestMethods.writeField(recordWriter, 7, "c_comment", comment);
-    UtilitiesTestMethods.writeField(recordWriter, 8, "c_map", map);
-    UtilitiesTestMethods.writeField(recordWriter, 9, "c_list", list);
-  }
-
-  private void readParquetHiveInputFormat(final String schemaRequested, final Integer[] arrCheckIndexValues) throws Exception {
-    final ParquetMetadata readFooter = ParquetFileReader.readFooter(conf, new Path(testFile.getAbsolutePath()));
-    final MessageType schema = readFooter.getFileMetaData().getSchema();
-
-    long size = 0;
-    final List<BlockMetaData> blocks = readFooter.getBlocks();
-    for (final BlockMetaData block : blocks) {
-      size += block.getTotalByteSize();
-    }
-
-    final FileInputFormat<Void, ArrayWritable> format = new MapredParquetInputFormat();
-    final String[] locations = new String[] {"localhost"};
-    final String schemaToString = schema.toString();
-    System.out.println(schemaToString);
-
-    final String specificSchema = schemaRequested == null ? schemaToString : schemaRequested;
-
-    // Set the configuration parameters
-    final String columnsStr = "message customer {\n"
-            + "  optional int32 c_custkey;\n"
-            + "  optional binary c_name;\n"
-            + "  optional binary c_address;\n"
-            + "  optional int32 c_nationkey;\n"
-            + "  optional binary c_phone;\n"
-            + "  optional double c_acctbal;\n"
-            + "  optional binary c_mktsegment;\n"
-            + "  optional binary c_comment;\n"
-            + "  optional group c_map (MAP_KEY_VALUE) {\n"
-            + "    repeated group map {\n"
-            + "      required binary key;\n"
-            + "      optional binary value;\n"
-            + "    }\n"
-            + "  }\n"
-            + "  optional group c_list (LIST) {\n"
-            + "    repeated group bag {\n"
-            + "      optional int32 array_element;\n"
-            + "    }\n"
-            + "  }\n"
-            + "  optional int32 unknown;\n"
-            + "}";
-
-
-    final Map<String, String> readSupportMetaData = new HashMap<String, String>();
-    readSupportMetaData.put(DataWritableReadSupport.HIVE_SCHEMA_KEY, columnsStr);
-    final ParquetInputSplit realSplit = new ParquetInputSplit(new Path(testFile.getAbsolutePath()), 0, size, locations, blocks,
-            schemaToString, specificSchema, readFooter.getFileMetaData().getKeyValueMetaData(), readSupportMetaData);
-
-    final MapredParquetInputFormat.InputSplitWrapper splitWrapper = new InputSplitWrapper(realSplit);
-
-    // construct the record reader
-    final RecordReader<Void, ArrayWritable> reader = format.getRecordReader(splitWrapper, job, reporter);
-
-    // create key/value
-    final Void key = reader.createKey();
-    final ArrayWritable value = reader.createValue();
-
-    int count = 0;
-    final int sizeExpected = mapData.size();
-    while (reader.next(key, value)) {
-      assertTrue(count < sizeExpected);
-      assertTrue(key == null);
-      final Writable[] arrValue = value.get();
-      final ArrayWritable expected = mapData.get(((IntWritable) arrValue[0]).get());
-      final Writable[] arrExpected = expected.get();
-      assertEquals(arrValue.length, arrExpected.length);
-
-      final boolean deepEquals = UtilitiesTestMethods.smartCheckArray(arrValue, arrExpected, arrCheckIndexValues);
-
-      assertTrue(deepEquals);
-      count++;
-    }
-    System.out.println("nb lines " + count);
-    reader.close();
-
-    assertEquals("Number of lines found and data written don't match", count, sizeExpected);
-  }
-}
diff --git a/src/ql/src/test/parquet/hive/TestMapredParquetOuputFormat.java b/src/ql/src/test/parquet/hive/TestMapredParquetOuputFormat.java
deleted file mode 100644
index e5938f8..0000000
--- a/src/ql/src/test/parquet/hive/TestMapredParquetOuputFormat.java
+++ /dev/null
@@ -1,235 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive;
-
-import static org.junit.Assert.*;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapreduce.Counter;
-import org.apache.hadoop.util.Progressable;
-import org.junit.Before;
-import org.junit.Test;
-
-import parquet.hadoop.ParquetFileReader;
-import parquet.hadoop.ParquetInputSplit;
-import parquet.hadoop.metadata.BlockMetaData;
-import parquet.hadoop.metadata.ParquetMetadata;
-import parquet.hive.read.DataWritableReadSupport;
-import parquet.schema.MessageType;
-
-/**
- *
- * TestHiveOuputFormat
- *
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- *
- */
-public class TestMapredParquetOuputFormat {
-
-  private Map<Integer, ArrayWritable> mapData;
-  private Configuration conf;
-  private JobConf job;
-  private Path dir;
-  private File testFile;
-  private Reporter reporter;
-
-  @Before
-  public void setUp() throws Exception {
-    conf = new Configuration();
-    job = new JobConf(conf);
-    dir = new Path("target/tests/from_java/deprecatedoutputformat/");
-    testFile = new File(dir.toString(), "customer");
-    if (testFile.exists()) {
-      if (!testFile.delete()) {
-        throw new RuntimeException("can not remove existing file " + testFile.getAbsolutePath());
-      }
-    }
-    reporter = Reporter.NULL;
-
-    Map<String, String> map = new HashMap<String, String>();
-    map.put("testkey", "testvalue");
-    map.put("foo", "bar");
-
-    List<Integer> list = new ArrayList<Integer>();
-    list.add(0);
-    list.add(12);
-    list.add(17);
-
-    mapData = new HashMap<Integer, ArrayWritable>();
-    mapData.clear();
-    for (int i = 0; i < 1000; i++) {
-      mapData.put(i, UtilitiesTestMethods.createArrayWritable(i, i % 11 == 0 ? null : "name_" + i, i % 12 == 0 ? null : "add_" + i,
-              i % 13 == 0 ? null : i, i % 14 == 0 ? null : "phone_" + i, i % 15 == 0 ? null : 1.2d * i, i % 16 == 0 ? null : "mktsegment_" + i,
-              i % 17 == 0 ? null : "comment_" + i, i % 18 == 0 ? null : map, i % 19 == 0 ? null : list));
-    }
-  }
-
-  @Test
-  public void testParquetHiveOutputFormat() throws Exception {
-    final HiveOutputFormat<Void, ArrayWritable> format = new MapredParquetOutputFormat();
-    final Properties tableProperties = new Properties();
-
-    // Set the configuration parameters
-    tableProperties.setProperty("columns",
-            "c_custkey,c_name,c_address,c_nationkey,c_phone,c_acctbal,c_mktsegment,c_comment,c_map,c_list");
-    tableProperties.setProperty("columns.types",
-            "int:string:string:int:string:double:string:string:map<string,string>:array<int>");
-    tableProperties.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_FORMAT, "NULL");
-
-    System.out.println("First part, write the data");
-
-    job.set("mapred.task.id", "attempt_201304241759_32973_m_000002_0"); // FAKE ID
-    final FakeStatus reporter = new FakeStatus();
-    final org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter recordWriter = format.getHiveRecordWriter(
-            job,
-            new Path(testFile.getAbsolutePath()),
-            NullWritable.class,
-            false,
-            tableProperties,
-            reporter);
-    // create key/value
-    for (final Map.Entry<Integer, ArrayWritable> entry : mapData.entrySet()) {
-      recordWriter.write(entry.getValue());
-    }
-    recordWriter.close(false);
-    assertTrue("File not created", testFile.exists());
-
-    System.out.println("Second part, check if everything is ok");
-
-    checkWrite();
-  }
-
-  private void checkWrite() throws IOException, InterruptedException {
-    final ParquetMetadata readFooter = ParquetFileReader.readFooter(conf, new Path(testFile.getAbsolutePath()));
-    final MessageType schema = readFooter.getFileMetaData().getSchema();
-
-    long size = 0;
-    final List<BlockMetaData> blocks = readFooter.getBlocks();
-    for (final BlockMetaData block : blocks) {
-      size += block.getTotalByteSize();
-    }
-
-
-    final FileInputFormat<Void, ArrayWritable> format = new MapredParquetInputFormat();
-    final String[] locations = new String[] {"localhost"};
-    final String schemaToString = schema.toString();
-    final String columnsStr = "message customer {\n"
-            + "  optional int32 c_custkey;\n"
-            + "  optional binary c_name;\n"
-            + "  optional binary c_address;\n"
-            + "  optional int32 c_nationkey;\n"
-            + "  optional binary c_phone;\n"
-            + "  optional double c_acctbal;\n"
-            + "  optional binary c_mktsegment;\n"
-            + "  optional binary c_comment;\n"
-            + "  optional group c_map (MAP_KEY_VALUE) {\n"
-            + "    repeated group map {\n"
-            + "      required binary key;\n"
-            + "      optional binary value;\n"
-            + "    }\n"
-            + "  }\n"
-            + "  optional group c_list (LIST) {\n"
-            + "    repeated group bag {\n"
-            + "      optional int32 array_element;\n"
-            + "    }\n"
-            + "  }\n"
-            + "}";
-
-
-    final Map<String, String> readSupportMetaData = new HashMap<String, String>();
-    readSupportMetaData.put(DataWritableReadSupport.HIVE_SCHEMA_KEY, columnsStr);
-    final ParquetInputSplit realSplit = new ParquetInputSplit(new Path(testFile.getAbsolutePath()), 0, size, locations, blocks,
-            schemaToString, schemaToString, readFooter.getFileMetaData().getKeyValueMetaData(), readSupportMetaData);
-
-    final MapredParquetInputFormat.InputSplitWrapper splitWrapper = new MapredParquetInputFormat.InputSplitWrapper(realSplit);
-
-    // construct the record reader
-    final RecordReader<Void, ArrayWritable> reader = format.getRecordReader(splitWrapper, job, reporter);
-
-    // create key/value
-    final Void key = reader.createKey();
-    final ArrayWritable value = reader.createValue();
-
-
-    int count = 0;
-    while (reader.next(key, value)) {
-      assertTrue(count < mapData.size());
-      assertTrue(key == null);
-      final Writable[] arrValue = value.get();
-      final Writable[] writableArr = arrValue;
-      final ArrayWritable expected = mapData.get(((IntWritable) writableArr[0]).get());
-      final Writable[] arrExpected = expected.get();
-      assertEquals(arrValue.length, 10);
-
-      final boolean deepEquals = UtilitiesTestMethods.smartCheckArray(arrValue, arrExpected, new Integer[] {0, 1, 2, 3, 4, 5, 6, 7, 8, 9});
-
-      assertTrue(deepEquals);
-      count++;
-    }
-    reader.close();
-
-    assertEquals("Number of lines found and data written don't match", count, mapData.size());
-
-  }
-
-  // FAKE Class in order to compile
-  private class FakeStatus extends org.apache.hadoop.mapreduce.StatusReporter implements Progressable {
-
-    @Override
-    public Counter getCounter(final Enum<?> e) {
-      throw new UnsupportedOperationException("Not supported yet.");
-    }
-
-    @Override
-    public Counter getCounter(final String string, final String string1) {
-      throw new UnsupportedOperationException("Not supported yet.");
-    }
-
-    @Override
-    public void progress() {
-      throw new UnsupportedOperationException("Not supported yet.");
-    }
-
-    @Override
-    public void setStatus(final String string) {
-      throw new UnsupportedOperationException("Not supported yet.");
-    }
-
-    @Override
-    public float getProgress() {
-      throw new UnsupportedOperationException("Not supported yet."); //To change body of generated methods, choose Tools | Templates.
-    }
-  }
-}
diff --git a/src/ql/src/test/parquet/hive/TestParquetSerDe.java b/src/ql/src/test/parquet/hive/TestParquetSerDe.java
deleted file mode 100644
index 0522b6b..0000000
--- a/src/ql/src/test/parquet/hive/TestParquetSerDe.java
+++ /dev/null
@@ -1,119 +0,0 @@
-/**
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License
- * at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS
- * OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
- */
-package parquet.hive;
-
-import java.util.Properties;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.serde2.SerDeException;
-import org.apache.hadoop.hive.serde2.io.ByteWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Writable;
-
-import parquet.hive.serde.ParquetHiveSerDe;
-import parquet.hive.writable.BinaryWritable;
-import parquet.io.api.Binary;
-
-/**
- *
- * testParquetHiveSerDe
- *
- *
- * @author Mickaël Lacour <m.lacour@criteo.com>
- *
- */
-public class TestParquetSerDe extends TestCase {
-
-  public void testParquetHiveSerDe() throws Throwable {
-    try {
-      // Create the SerDe
-      System.out.println("test: testParquetHiveSerDe");
-
-      final ParquetHiveSerDe serDe = new ParquetHiveSerDe();
-      final Configuration conf = new Configuration();
-      final Properties tbl = createProperties();
-      serDe.initialize(conf, tbl);
-
-      // Data
-      final Writable[] arr = new Writable[8];
-
-      arr[0] = new ByteWritable((byte) 123);
-      arr[1] = new ShortWritable((short) 456);
-      arr[2] = new IntWritable(789);
-      arr[3] = new LongWritable(1000l);
-      arr[4] = new DoubleWritable((double) 5.3);
-      arr[5] = new BinaryWritable(Binary.fromString("hive and hadoop and parquet. Big family."));
-
-      final Writable[] mapContainer = new Writable[1];
-      final Writable[] map = new Writable[3];
-      for (int i = 0; i < 3; ++i) {
-        final Writable[] pair = new Writable[2];
-        pair[0] = new BinaryWritable(Binary.fromString("key_" + i));
-        pair[1] = new IntWritable(i);
-        map[i] = new ArrayWritable(Writable.class, pair);
-      }
-      mapContainer[0] = new ArrayWritable(Writable.class, map);
-      arr[6] = new ArrayWritable(Writable.class, mapContainer);
-
-      final Writable[] arrayContainer = new Writable[1];
-      final Writable[] array = new Writable[5];
-      for (int i = 0; i < 5; ++i) {
-        array[i] = new BinaryWritable(Binary.fromString("elem_" + i));
-      }
-      arrayContainer[0] = new ArrayWritable(Writable.class, array);
-      arr[7] = new ArrayWritable(Writable.class, arrayContainer);
-
-      final ArrayWritable arrWritable = new ArrayWritable(Writable.class, arr);
-      // Test
-      deserializeAndSerializeLazySimple(serDe, arrWritable);
-      System.out.println("test: testParquetHiveSerDe - OK");
-
-    } catch (final Throwable e) {
-      e.printStackTrace();
-      throw e;
-    }
-  }
-
-  private void deserializeAndSerializeLazySimple(final ParquetHiveSerDe serDe, final ArrayWritable t) throws SerDeException {
-
-    // Get the row structure
-    final StructObjectInspector oi = (StructObjectInspector) serDe.getObjectInspector();
-
-    // Deserialize
-    final Object row = serDe.deserialize(t);
-    assertEquals("deserialization gives the wrong object class", row.getClass(), ArrayWritable.class);
-    assertEquals("size correct after deserialization", serDe.getSerDeStats().getRawDataSize(), t.get().length);
-    assertEquals("deserialization gives the wrong object", t, row);
-
-    // Serialize
-    final ArrayWritable serializedArr = (ArrayWritable) serDe.serialize(row, oi);
-    assertEquals("size correct after serialization", serDe.getSerDeStats().getRawDataSize(), serializedArr.get().length);
-    assertTrue("serialized object should be equal to starting object", UtilitiesTestMethods.arrayWritableEquals(t, serializedArr));
-  }
-
-  private Properties createProperties() {
-    final Properties tbl = new Properties();
-
-    // Set the configuration parameters
-    tbl.setProperty("columns", "abyte,ashort,aint,along,adouble,astring,amap,alist");
-    tbl.setProperty("columns.types", "tinyint:smallint:int:bigint:double:string:map<string,int>:array<string>");
-    tbl.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_FORMAT, "NULL");
-    return tbl;
-  }
-}
diff --git a/src/ql/src/test/parquet/hive/UtilitiesTestMethods.java b/src/ql/src/test/parquet/hive/UtilitiesTestMethods.java
deleted file mode 100644
index d1d40ad..0000000
--- a/src/ql/src/test/parquet/hive/UtilitiesTestMethods.java
+++ /dev/null
@@ -1,245 +0,0 @@
-package parquet.hive;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Writable;
-
-import parquet.bytes.BytesInput;
-import parquet.column.ColumnDescriptor;
-import parquet.column.page.Page;
-import parquet.column.page.PageReader;
-import parquet.column.page.mem.MemPageStore;
-import parquet.hadoop.ParquetFileWriter;
-import parquet.hadoop.metadata.CompressionCodecName;
-import parquet.hive.writable.BinaryWritable;
-import parquet.io.api.Binary;
-import parquet.io.api.RecordConsumer;
-import parquet.schema.GroupType;
-import parquet.schema.MessageType;
-import parquet.schema.Type;
-
-public class UtilitiesTestMethods {
-
-  public static void writeToFile(final Path file, final Configuration configuration, final MessageType schema, final MemPageStore pageStore, final int recordCount)
-          throws IOException {
-    final ParquetFileWriter w = startFile(file, configuration, schema);
-    writeBlock(schema, pageStore, recordCount, w);
-    endFile(w);
-  }
-
-  public static void endFile(final ParquetFileWriter w) throws IOException {
-    w.end(new HashMap<String, String>());
-  }
-
-  public static boolean smartCheckSchema(final GroupType expectedSchema, final GroupType actualSchema) {
-    if (expectedSchema.getFieldCount() != actualSchema.getFieldCount()) {
-      return false;
-    }
-
-    for (int i = 0; i < expectedSchema.getFieldCount(); ++i) {
-      Type expectedType = expectedSchema.getType(i);
-      Type actualType = actualSchema.getType(i);
-
-      if (!expectedType.getName().equals(actualType.getName())
-              || expectedType.getRepetition() != actualType.getRepetition()
-              || expectedType.isPrimitive() != actualType.isPrimitive()) {
-        return false;
-      }
-
-      if (expectedType.isPrimitive()) {
-        if (expectedType.asPrimitiveType().getPrimitiveTypeName() != actualType.asPrimitiveType().getPrimitiveTypeName()
-                || expectedType.asPrimitiveType().getTypeLength() != actualType.asPrimitiveType().getTypeLength()) {
-          return false;
-        }
-      } else {
-        if (!smartCheckSchema(expectedType.asGroupType(), actualType.asGroupType())) {
-          return false;
-        }
-      }
-    }
-
-    return true;
-  }
-
-  public static boolean smartCheckArray(final Writable[] arrValue, final Writable[] arrExpected, final Integer[] arrCheckIndexValues) {
-
-    int i = 0;
-    for (final Integer index : arrCheckIndexValues) {
-      if (index != Integer.MIN_VALUE) {
-        final Writable value = arrValue[index];
-        final Writable expectedValue = arrExpected[index];
-
-        if (((value == null && expectedValue == null)
-                || (((value != null && expectedValue != null) && (value.equals(expectedValue))))
-                || (value != null && expectedValue != null && value instanceof ArrayWritable && expectedValue instanceof ArrayWritable && arrayWritableEquals((ArrayWritable) value, (ArrayWritable) expectedValue))) == false) {
-          return false;
-        }
-      } else {
-        final Writable value = arrValue[i];
-        if (value != null) {
-          return false;
-        }
-      }
-      ++i;
-    }
-
-    return true;
-  }
-
-  public static boolean arrayWritableEquals(final ArrayWritable a1, final ArrayWritable a2) {
-    final Writable[] a1Arr = a1.get();
-    final Writable[] a2Arr = a2.get();
-
-    if (a1Arr.length != a2Arr.length) {
-      return false;
-    }
-
-    for (int i = 0; i < a1Arr.length; ++i) {
-      if (a1Arr[i] instanceof ArrayWritable) {
-        if (!(a2Arr[i] instanceof ArrayWritable)) {
-          return false;
-        }
-        if (!arrayWritableEquals((ArrayWritable) a1Arr[i], (ArrayWritable) a2Arr[i])) {
-          return false;
-        }
-      } else {
-        if (!a1Arr[i].equals(a2Arr[i])) {
-          return false;
-        }
-      }
-
-    }
-    return true;
-  }
-
-  static public ArrayWritable createArrayWritable(final Integer custkey, final String name, final String address, final Integer nationkey, final String phone, final Double acctbal, final String mktsegment, final String comment, final Map<String, String> map, final List<Integer> list) {
-
-    final Writable[] arr = new Writable[11]; // The last one is for the unknown column
-    arr[0] = new IntWritable(custkey);
-    if (name != null) {
-      arr[1] = new BinaryWritable(Binary.fromString(name));
-    }
-    if (address != null) {
-      arr[2] = new BinaryWritable(Binary.fromString(address));
-    }
-    if (nationkey != null) {
-      arr[3] = new IntWritable(nationkey);
-    }
-    if (phone != null) {
-      arr[4] = new BinaryWritable(Binary.fromString(phone));
-    }
-    if (acctbal != null) {
-      arr[5] = new DoubleWritable(acctbal);
-    }
-    if (mktsegment != null) {
-      arr[6] = new BinaryWritable(Binary.fromString(mktsegment));
-    }
-    if (comment != null) {
-      arr[7] = new BinaryWritable(Binary.fromString(comment));
-    }
-    if (map != null) {
-      final Writable[] mapContainer = new Writable[1];
-      final Writable[] mapArr = new Writable[map.size()];
-      int i = 0;
-      for (Map.Entry<String, String> entry : map.entrySet()) {
-        final Writable[] pair = new Writable[2];
-        pair[0] = new BinaryWritable(Binary.fromString(entry.getKey()));
-        pair[1] = new BinaryWritable(Binary.fromString(entry.getValue()));
-        mapArr[i] = new ArrayWritable(Writable.class, pair);
-        ++i;
-      }
-      mapContainer[0] = new ArrayWritable(Writable.class, mapArr);
-      arr[8] = new ArrayWritable(Writable.class, mapContainer);
-    }
-    if (list != null) {
-      final Writable[] listContainer = new Writable[1];
-      final Writable[] listArr = new Writable[list.size()];
-      for (int i = 0; i < list.size(); ++i) {
-        listArr[i] = new IntWritable(list.get(i));
-      }
-      listContainer[0] = new ArrayWritable(Writable.class, listArr);
-      arr[9] = new ArrayWritable(Writable.class, listContainer);
-    }
-
-    return new ArrayWritable(Writable.class, arr);
-  }
-
-  public static void writeBlock(final MessageType schema, final MemPageStore pageStore,
-          final int recordCount, final ParquetFileWriter w) throws IOException {
-    w.startBlock(recordCount);
-    final List<ColumnDescriptor> columns = schema.getColumns();
-    for (final ColumnDescriptor columnDescriptor : columns) {
-      final PageReader pageReader = pageStore.getPageReader(columnDescriptor);
-      final long totalValueCount = pageReader.getTotalValueCount();
-      w.startColumn(columnDescriptor, totalValueCount, CompressionCodecName.UNCOMPRESSED);
-      int n = 0;
-      do {
-        final Page page = pageReader.readPage();
-        n += page.getValueCount();
-        // TODO: change INTFC
-        w.writeDataPage(
-                page.getValueCount(),
-                (int) page.getBytes().size(),
-                BytesInput.from(page.getBytes().toByteArray()),
-                page.getRlEncoding(),
-                page.getDlEncoding(),
-                page.getValueEncoding());
-      } while (n < totalValueCount);
-      w.endColumn();
-    }
-    w.endBlock();
-  }
-
-  public static ParquetFileWriter startFile(final Path file,
-          final Configuration configuration, final MessageType schema) throws IOException {
-    final ParquetFileWriter w = new ParquetFileWriter(configuration, schema, file);
-    w.start();
-    return w;
-  }
-
-  public static void writeField(final RecordConsumer recordWriter, final int index, final String name, final Object value) {
-    if (value != null) {
-      recordWriter.startField(name, index);
-      if (value instanceof Integer) {
-        recordWriter.addInteger((Integer) value);
-      } else if (value instanceof String) {
-        recordWriter.addBinary(Binary.fromString((String) value));
-      } else if (value instanceof Double) {
-        recordWriter.addDouble((Double) value);
-      } else if (value instanceof Map) {
-        recordWriter.startGroup();
-        recordWriter.startField("map", 0);
-        for (Map.Entry<?, ?> entry : ((Map<?, ?>) value).entrySet()) {
-          recordWriter.startGroup();
-          writeField(recordWriter, 0, "key", entry.getKey());
-          writeField(recordWriter, 1, "value", entry.getValue());
-          recordWriter.endGroup();
-        }
-        recordWriter.endField("map", 0);
-        recordWriter.endGroup();
-      } else if (value instanceof List) {
-        recordWriter.startGroup();
-        recordWriter.startField("bag", 0);
-        for (Object element : (List<?>) value) {
-          recordWriter.startGroup();
-          writeField(recordWriter, 0, "array_element", element);
-          recordWriter.endGroup();
-        }
-        recordWriter.endField("bag", 0);
-        recordWriter.endGroup();
-      } else {
-        throw new IllegalArgumentException(value.getClass().getName() + " not supported");
-      }
-
-      recordWriter.endField(name, index);
-    }
-  }
-}
diff --git a/src/ql/src/test/parquet/hive/serde/TestAbstractParquetMapInspector.java b/src/ql/src/test/parquet/hive/serde/TestAbstractParquetMapInspector.java
deleted file mode 100644
index f0edd8d..0000000
--- a/src/ql/src/test/parquet/hive/serde/TestAbstractParquetMapInspector.java
+++ /dev/null
@@ -1,102 +0,0 @@
-/*
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.serde;
-
-import java.util.HashMap;
-import java.util.Map;
-import junit.framework.TestCase;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Writable;
-import org.junit.Test;
-
-/**
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class TestAbstractParquetMapInspector extends TestCase {
-
-  class TestableAbstractParquetMapInspector extends AbstractParquetMapInspector {
-
-    public TestableAbstractParquetMapInspector(ObjectInspector keyInspector, ObjectInspector valueInspector) {
-      super(keyInspector, valueInspector);
-    }
-
-    @Override
-    public Object getMapValueElement(Object o, Object o1) {
-      throw new UnsupportedOperationException("Should not be called");
-    }
-  }
-  private TestableAbstractParquetMapInspector inspector;
-
-  @Override
-  public void setUp() {
-    inspector = new TestableAbstractParquetMapInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector,
-            PrimitiveObjectInspectorFactory.javaIntObjectInspector);
-  }
-
-  @Test
-  public void testNullMap() {
-    assertEquals("Wrong size", -1, inspector.getMapSize(null));
-    assertNull("Should be null", inspector.getMap(null));
-  }
-
-  @Test
-  public void testNullContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);
-    assertEquals("Wrong size", -1, inspector.getMapSize(map));
-    assertNull("Should be null", inspector.getMap(map));
-  }
-
-  @Test
-  public void testEmptyContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
-    assertEquals("Wrong size", -1, inspector.getMapSize(map));
-    assertNull("Should be null", inspector.getMap(map));
-  }
-
-  @Test
-  public void testRegularMap() {
-    final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
-    final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
-
-    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
-      new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
-
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
-
-    final Map<Writable, Writable> expected = new HashMap<Writable, Writable>();
-    expected.put(new IntWritable(0), new IntWritable(1));
-    expected.put(new IntWritable(2), new IntWritable(3));
-
-    assertEquals("Wrong size", 2, inspector.getMapSize(map));
-    assertEquals("Wrong result of inspection", expected, inspector.getMap(map));
-  }
-
-  @Test
-  public void testHashMap() {
-    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
-    map.put(new IntWritable(0), new IntWritable(1));
-    map.put(new IntWritable(2), new IntWritable(3));
-    map.put(new IntWritable(4), new IntWritable(5));
-    map.put(new IntWritable(6), new IntWritable(7));
-
-    assertEquals("Wrong size", 4, inspector.getMapSize(map));
-    assertEquals("Wrong result of inspection", map, inspector.getMap(map));
-  }
-}
diff --git a/src/ql/src/test/parquet/hive/serde/TestDeepParquetHiveMapInspector.java b/src/ql/src/test/parquet/hive/serde/TestDeepParquetHiveMapInspector.java
deleted file mode 100644
index 838f264..0000000
--- a/src/ql/src/test/parquet/hive/serde/TestDeepParquetHiveMapInspector.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.serde;
-
-import java.util.HashMap;
-import java.util.Map;
-import junit.framework.TestCase;
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Writable;
-import org.junit.Test;
-import parquet.hive.serde.primitive.ParquetPrimitiveInspectorFactory;
-
-/**
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class TestDeepParquetHiveMapInspector extends TestCase {
-
-  private DeepParquetHiveMapInspector inspector;
-
-  @Override
-  public void setUp() {
-    inspector = new DeepParquetHiveMapInspector(ParquetPrimitiveInspectorFactory.parquetShortInspector,
-            PrimitiveObjectInspectorFactory.javaIntObjectInspector);
-  }
-
-  @Test
-  public void testNullMap() {
-    assertNull("Should be null", inspector.getMapValueElement(null, new ShortWritable((short) 0)));
-  }
-
-  @Test
-  public void testNullContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);
-    assertNull("Should be null", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-  }
-
-  @Test
-  public void testEmptyContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
-    assertNull("Should be null", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-  }
-
-  @Test
-  public void testRegularMap() {
-    final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
-    final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
-
-    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
-      new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
-
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
-
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new ShortWritable((short) 2)));
-  }
-
-  @Test
-  public void testHashMap() {
-    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
-    map.put(new IntWritable(0), new IntWritable(1));
-    map.put(new IntWritable(2), new IntWritable(3));
-    map.put(new IntWritable(4), new IntWritable(5));
-    map.put(new IntWritable(6), new IntWritable(7));
-
-
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
-    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new IntWritable(4)));
-    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new IntWritable(6)));
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new ShortWritable((short) 2)));
-    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new ShortWritable((short) 4)));
-    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new ShortWritable((short) 6)));
-  }
-}
diff --git a/src/ql/src/test/parquet/hive/serde/TestParquetHiveArrayInspector.java b/src/ql/src/test/parquet/hive/serde/TestParquetHiveArrayInspector.java
deleted file mode 100644
index a55071c..0000000
--- a/src/ql/src/test/parquet/hive/serde/TestParquetHiveArrayInspector.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/*
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.serde;
-
-import java.util.ArrayList;
-import java.util.List;
-import junit.framework.TestCase;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Writable;
-import org.junit.Test;
-
-/**
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class TestParquetHiveArrayInspector extends TestCase {
-
-  private ParquetHiveArrayInspector inspector;
-
-  @Override
-  public void setUp() {
-    inspector = new ParquetHiveArrayInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector);
-  }
-
-  @Test
-  public void testNullArray() {
-    assertEquals("Wrong size", -1, inspector.getListLength(null));
-    assertNull("Should be null", inspector.getList(null));
-    assertNull("Should be null", inspector.getListElement(null, 0));
-  }
-
-  @Test
-  public void testNullContainer() {
-    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, null);
-    assertEquals("Wrong size", -1, inspector.getListLength(list));
-    assertNull("Should be null", inspector.getList(list));
-    assertNull("Should be null", inspector.getListElement(list, 0));
-  }
-
-  @Test
-  public void testEmptyContainer() {
-    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
-    assertEquals("Wrong size", -1, inspector.getListLength(list));
-    assertNull("Should be null", inspector.getList(list));
-    assertNull("Should be null", inspector.getListElement(list, 0));
-  }
-
-  @Test
-  public void testRegularList() {
-    final ArrayWritable internalList = new ArrayWritable(Writable.class,
-            new Writable[]{new IntWritable(3), new IntWritable(5), new IntWritable(1)});
-    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, new ArrayWritable[]{internalList});
-
-    final List<Writable> expected = new ArrayList<Writable>();
-    expected.add(new IntWritable(3));
-    expected.add(new IntWritable(5));
-    expected.add(new IntWritable(1));
-
-    assertEquals("Wrong size", 3, inspector.getListLength(list));
-    assertEquals("Wrong result of inspection", expected, inspector.getList(list));
-
-    for (int i = 0; i < expected.size(); ++i) {
-      assertEquals("Wrong result of inspection", expected.get(i), inspector.getListElement(list, i));
-
-    }
-
-    assertNull("Should be null", inspector.getListElement(list, 3));
-  }
-}
diff --git a/src/ql/src/test/parquet/hive/serde/TestStandardParquetHiveMapInspector.java b/src/ql/src/test/parquet/hive/serde/TestStandardParquetHiveMapInspector.java
deleted file mode 100644
index c5c131c..0000000
--- a/src/ql/src/test/parquet/hive/serde/TestStandardParquetHiveMapInspector.java
+++ /dev/null
@@ -1,92 +0,0 @@
-/*
- * Copyright 2013 Criteo.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package parquet.hive.serde;
-
-import java.util.HashMap;
-import java.util.Map;
-import junit.framework.TestCase;
-import org.apache.hadoop.hive.serde2.io.ShortWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Writable;
-import org.junit.Test;
-
-/**
- *
- * @author Rémy Pecqueur <r.pecqueur@criteo.com>
- */
-public class TestStandardParquetHiveMapInspector extends TestCase {
-
-  private StandardParquetHiveMapInspector inspector;
-
-  @Override
-  public void setUp() {
-    inspector = new StandardParquetHiveMapInspector(PrimitiveObjectInspectorFactory.javaIntObjectInspector,
-            PrimitiveObjectInspectorFactory.javaIntObjectInspector);
-  }
-
-  @Test
-  public void testNullMap() {
-    assertNull("Should be null", inspector.getMapValueElement(null, new IntWritable(0)));
-  }
-
-  @Test
-  public void testNullContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, null);
-    assertNull("Should be null", inspector.getMapValueElement(map, new IntWritable(0)));
-  }
-
-  @Test
-  public void testEmptyContainer() {
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new ArrayWritable[0]);
-    assertNull("Should be null", inspector.getMapValueElement(map, new IntWritable(0)));
-  }
-
-  @Test
-  public void testRegularMap() {
-    final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
-    final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
-
-    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
-      new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
-
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
-
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 2)));
-  }
-
-  @Test
-  public void testHashMap() {
-    final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
-    map.put(new IntWritable(0), new IntWritable(1));
-    map.put(new IntWritable(2), new IntWritable(3));
-    map.put(new IntWritable(4), new IntWritable(5));
-    map.put(new IntWritable(6), new IntWritable(7));
-
-    assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
-    assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
-    assertEquals("Wrong result of inspection", new IntWritable(5), inspector.getMapValueElement(map, new IntWritable(4)));
-    assertEquals("Wrong result of inspection", new IntWritable(7), inspector.getMapValueElement(map, new IntWritable(6)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 2)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 4)));
-    assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 6)));
-  }
-}
diff --git a/src/ql/src/test/queries/clientpositive/cdh_parquet_basic.q b/src/ql/src/test/queries/clientpositive/cdh_parquet_basic.q
deleted file mode 100644
index 56c8086..0000000
--- a/src/ql/src/test/queries/clientpositive/cdh_parquet_basic.q
+++ /dev/null
@@ -1,19 +0,0 @@
-
-
-create table parquet_tab1(key string, value string) stored as PARQUET;
-create table parquet_tab2(key string, value string) stored as PARQUET;
-insert overwrite table parquet_tab1 select * from src;
-insert overwrite table parquet_tab2 select * from src;
-
-select * from parquet_tab1 limit 10;
-select * from parquet_tab2 limit 10;
-
-explain
-select parquet_tab1.key, parquet_tab2.value
-FROM parquet_tab1 JOIN parquet_tab2 ON parquet_tab1.key = parquet_tab2.key;
-
-select parquet_tab1.key, parquet_tab2.value
-FROM parquet_tab1 JOIN parquet_tab2 ON parquet_tab1.key = parquet_tab2.key;
-
-
-
diff --git a/src/ql/src/test/results/clientpositive/cdh_parquet_basic.q.out b/src/ql/src/test/results/clientpositive/cdh_parquet_basic.q.out
deleted file mode 100644
index 072d987..0000000
--- a/src/ql/src/test/results/clientpositive/cdh_parquet_basic.q.out
+++ /dev/null
@@ -1,1202 +0,0 @@
-PREHOOK: query: create table parquet_tab1(key string, value string) stored as PARQUET
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table parquet_tab1(key string, value string) stored as PARQUET
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@parquet_tab1
-PREHOOK: query: create table parquet_tab2(key string, value string) stored as PARQUET
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table parquet_tab2(key string, value string) stored as PARQUET
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@parquet_tab2
-PREHOOK: query: insert overwrite table parquet_tab1 select * from src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@parquet_tab1
-POSTHOOK: query: insert overwrite table parquet_tab1 select * from src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@parquet_tab1
-POSTHOOK: Lineage: parquet_tab1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: insert overwrite table parquet_tab2 select * from src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@parquet_tab2
-POSTHOOK: query: insert overwrite table parquet_tab2 select * from src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@parquet_tab2
-POSTHOOK: Lineage: parquet_tab1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: select * from parquet_tab1 limit 10
-PREHOOK: type: QUERY
-PREHOOK: Input: default@parquet_tab1
-#### A masked pattern was here ####
-POSTHOOK: query: select * from parquet_tab1 limit 10
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@parquet_tab1
-#### A masked pattern was here ####
-POSTHOOK: Lineage: parquet_tab1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-238	val_238
-86	val_86
-311	val_311
-27	val_27
-165	val_165
-409	val_409
-255	val_255
-278	val_278
-98	val_98
-484	val_484
-PREHOOK: query: select * from parquet_tab2 limit 10
-PREHOOK: type: QUERY
-PREHOOK: Input: default@parquet_tab2
-#### A masked pattern was here ####
-POSTHOOK: query: select * from parquet_tab2 limit 10
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@parquet_tab2
-#### A masked pattern was here ####
-POSTHOOK: Lineage: parquet_tab1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-238	val_238
-86	val_86
-311	val_311
-27	val_27
-165	val_165
-409	val_409
-255	val_255
-278	val_278
-98	val_98
-484	val_484
-PREHOOK: query: explain
-select parquet_tab1.key, parquet_tab2.value
-FROM parquet_tab1 JOIN parquet_tab2 ON parquet_tab1.key = parquet_tab2.key
-PREHOOK: type: QUERY
-POSTHOOK: query: explain
-select parquet_tab1.key, parquet_tab2.value
-FROM parquet_tab1 JOIN parquet_tab2 ON parquet_tab1.key = parquet_tab2.key
-POSTHOOK: type: QUERY
-POSTHOOK: Lineage: parquet_tab1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME parquet_tab1)) (TOK_TABREF (TOK_TABNAME parquet_tab2)) (= (. (TOK_TABLE_OR_COL parquet_tab1) key) (. (TOK_TABLE_OR_COL parquet_tab2) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL parquet_tab1) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL parquet_tab2) value)))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        parquet_tab1 
-          TableScan
-            alias: parquet_tab1
-            Reduce Output Operator
-              key expressions:
-                    expr: key
-                    type: string
-              sort order: +
-              Map-reduce partition columns:
-                    expr: key
-                    type: string
-              tag: 0
-              value expressions:
-                    expr: key
-                    type: string
-        parquet_tab2 
-          TableScan
-            alias: parquet_tab2
-            Reduce Output Operator
-              key expressions:
-                    expr: key
-                    type: string
-              sort order: +
-              Map-reduce partition columns:
-                    expr: key
-                    type: string
-              tag: 1
-              value expressions:
-                    expr: value
-                    type: string
-      Reduce Operator Tree:
-        Join Operator
-          condition map:
-               Inner Join 0 to 1
-          condition expressions:
-            0 {VALUE._col0}
-            1 {VALUE._col1}
-          handleSkewJoin: false
-          outputColumnNames: _col0, _col5
-          Select Operator
-            expressions:
-                  expr: _col0
-                  type: string
-                  expr: _col5
-                  type: string
-            outputColumnNames: _col0, _col1
-            File Output Operator
-              compressed: false
-              GlobalTableId: 0
-              table:
-                  input format: org.apache.hadoop.mapred.TextInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select parquet_tab1.key, parquet_tab2.value
-FROM parquet_tab1 JOIN parquet_tab2 ON parquet_tab1.key = parquet_tab2.key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@parquet_tab1
-PREHOOK: Input: default@parquet_tab2
-#### A masked pattern was here ####
-POSTHOOK: query: select parquet_tab1.key, parquet_tab2.value
-FROM parquet_tab1 JOIN parquet_tab2 ON parquet_tab1.key = parquet_tab2.key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@parquet_tab1
-POSTHOOK: Input: default@parquet_tab2
-#### A masked pattern was here ####
-POSTHOOK: Lineage: parquet_tab1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: parquet_tab2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-0	val_0
-0	val_0
-0	val_0
-0	val_0
-0	val_0
-0	val_0
-0	val_0
-0	val_0
-0	val_0
-10	val_10
-100	val_100
-100	val_100
-100	val_100
-100	val_100
-103	val_103
-103	val_103
-103	val_103
-103	val_103
-104	val_104
-104	val_104
-104	val_104
-104	val_104
-105	val_105
-11	val_11
-111	val_111
-113	val_113
-113	val_113
-113	val_113
-113	val_113
-114	val_114
-116	val_116
-118	val_118
-118	val_118
-118	val_118
-118	val_118
-119	val_119
-119	val_119
-119	val_119
-119	val_119
-119	val_119
-119	val_119
-119	val_119
-119	val_119
-119	val_119
-12	val_12
-12	val_12
-12	val_12
-12	val_12
-120	val_120
-120	val_120
-120	val_120
-120	val_120
-125	val_125
-125	val_125
-125	val_125
-125	val_125
-126	val_126
-128	val_128
-128	val_128
-128	val_128
-128	val_128
-128	val_128
-128	val_128
-128	val_128
-128	val_128
-128	val_128
-129	val_129
-129	val_129
-129	val_129
-129	val_129
-131	val_131
-133	val_133
-134	val_134
-134	val_134
-134	val_134
-134	val_134
-136	val_136
-137	val_137
-137	val_137
-137	val_137
-137	val_137
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-138	val_138
-143	val_143
-145	val_145
-146	val_146
-146	val_146
-146	val_146
-146	val_146
-149	val_149
-149	val_149
-149	val_149
-149	val_149
-15	val_15
-15	val_15
-15	val_15
-15	val_15
-150	val_150
-152	val_152
-152	val_152
-152	val_152
-152	val_152
-153	val_153
-155	val_155
-156	val_156
-157	val_157
-158	val_158
-160	val_160
-162	val_162
-163	val_163
-164	val_164
-164	val_164
-164	val_164
-164	val_164
-165	val_165
-165	val_165
-165	val_165
-165	val_165
-166	val_166
-167	val_167
-167	val_167
-167	val_167
-167	val_167
-167	val_167
-167	val_167
-167	val_167
-167	val_167
-167	val_167
-168	val_168
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-169	val_169
-17	val_17
-170	val_170
-172	val_172
-172	val_172
-172	val_172
-172	val_172
-174	val_174
-174	val_174
-174	val_174
-174	val_174
-175	val_175
-175	val_175
-175	val_175
-175	val_175
-176	val_176
-176	val_176
-176	val_176
-176	val_176
-177	val_177
-178	val_178
-179	val_179
-179	val_179
-179	val_179
-179	val_179
-18	val_18
-18	val_18
-18	val_18
-18	val_18
-180	val_180
-181	val_181
-183	val_183
-186	val_186
-187	val_187
-187	val_187
-187	val_187
-187	val_187
-187	val_187
-187	val_187
-187	val_187
-187	val_187
-187	val_187
-189	val_189
-19	val_19
-190	val_190
-191	val_191
-191	val_191
-191	val_191
-191	val_191
-192	val_192
-193	val_193
-193	val_193
-193	val_193
-193	val_193
-193	val_193
-193	val_193
-193	val_193
-193	val_193
-193	val_193
-194	val_194
-195	val_195
-195	val_195
-195	val_195
-195	val_195
-196	val_196
-197	val_197
-197	val_197
-197	val_197
-197	val_197
-199	val_199
-199	val_199
-199	val_199
-199	val_199
-199	val_199
-199	val_199
-199	val_199
-199	val_199
-199	val_199
-2	val_2
-20	val_20
-200	val_200
-200	val_200
-200	val_200
-200	val_200
-201	val_201
-202	val_202
-203	val_203
-203	val_203
-203	val_203
-203	val_203
-205	val_205
-205	val_205
-205	val_205
-205	val_205
-207	val_207
-207	val_207
-207	val_207
-207	val_207
-208	val_208
-208	val_208
-208	val_208
-208	val_208
-208	val_208
-208	val_208
-208	val_208
-208	val_208
-208	val_208
-209	val_209
-209	val_209
-209	val_209
-209	val_209
-213	val_213
-213	val_213
-213	val_213
-213	val_213
-214	val_214
-216	val_216
-216	val_216
-216	val_216
-216	val_216
-217	val_217
-217	val_217
-217	val_217
-217	val_217
-218	val_218
-219	val_219
-219	val_219
-219	val_219
-219	val_219
-221	val_221
-221	val_221
-221	val_221
-221	val_221
-222	val_222
-223	val_223
-223	val_223
-223	val_223
-223	val_223
-224	val_224
-224	val_224
-224	val_224
-224	val_224
-226	val_226
-228	val_228
-229	val_229
-229	val_229
-229	val_229
-229	val_229
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-230	val_230
-233	val_233
-233	val_233
-233	val_233
-233	val_233
-235	val_235
-237	val_237
-237	val_237
-237	val_237
-237	val_237
-238	val_238
-238	val_238
-238	val_238
-238	val_238
-239	val_239
-239	val_239
-239	val_239
-239	val_239
-24	val_24
-24	val_24
-24	val_24
-24	val_24
-241	val_241
-242	val_242
-242	val_242
-242	val_242
-242	val_242
-244	val_244
-247	val_247
-248	val_248
-249	val_249
-252	val_252
-255	val_255
-255	val_255
-255	val_255
-255	val_255
-256	val_256
-256	val_256
-256	val_256
-256	val_256
-257	val_257
-258	val_258
-26	val_26
-26	val_26
-26	val_26
-26	val_26
-260	val_260
-262	val_262
-263	val_263
-265	val_265
-265	val_265
-265	val_265
-265	val_265
-266	val_266
-27	val_27
-272	val_272
-272	val_272
-272	val_272
-272	val_272
-273	val_273
-273	val_273
-273	val_273
-273	val_273
-273	val_273
-273	val_273
-273	val_273
-273	val_273
-273	val_273
-274	val_274
-275	val_275
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-277	val_277
-278	val_278
-278	val_278
-278	val_278
-278	val_278
-28	val_28
-280	val_280
-280	val_280
-280	val_280
-280	val_280
-281	val_281
-281	val_281
-281	val_281
-281	val_281
-282	val_282
-282	val_282
-282	val_282
-282	val_282
-283	val_283
-284	val_284
-285	val_285
-286	val_286
-287	val_287
-288	val_288
-288	val_288
-288	val_288
-288	val_288
-289	val_289
-291	val_291
-292	val_292
-296	val_296
-298	val_298
-298	val_298
-298	val_298
-298	val_298
-298	val_298
-298	val_298
-298	val_298
-298	val_298
-298	val_298
-30	val_30
-302	val_302
-305	val_305
-306	val_306
-307	val_307
-307	val_307
-307	val_307
-307	val_307
-308	val_308
-309	val_309
-309	val_309
-309	val_309
-309	val_309
-310	val_310
-311	val_311
-311	val_311
-311	val_311
-311	val_311
-311	val_311
-311	val_311
-311	val_311
-311	val_311
-311	val_311
-315	val_315
-316	val_316
-316	val_316
-316	val_316
-316	val_316
-316	val_316
-316	val_316
-316	val_316
-316	val_316
-316	val_316
-317	val_317
-317	val_317
-317	val_317
-317	val_317
-318	val_318
-318	val_318
-318	val_318
-318	val_318
-318	val_318
-318	val_318
-318	val_318
-318	val_318
-318	val_318
-321	val_321
-321	val_321
-321	val_321
-321	val_321
-322	val_322
-322	val_322
-322	val_322
-322	val_322
-323	val_323
-325	val_325
-325	val_325
-325	val_325
-325	val_325
-327	val_327
-327	val_327
-327	val_327
-327	val_327
-327	val_327
-327	val_327
-327	val_327
-327	val_327
-327	val_327
-33	val_33
-331	val_331
-331	val_331
-331	val_331
-331	val_331
-332	val_332
-333	val_333
-333	val_333
-333	val_333
-333	val_333
-335	val_335
-336	val_336
-338	val_338
-339	val_339
-34	val_34
-341	val_341
-342	val_342
-342	val_342
-342	val_342
-342	val_342
-344	val_344
-344	val_344
-344	val_344
-344	val_344
-345	val_345
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-348	val_348
-35	val_35
-35	val_35
-35	val_35
-35	val_35
-35	val_35
-35	val_35
-35	val_35
-35	val_35
-35	val_35
-351	val_351
-353	val_353
-353	val_353
-353	val_353
-353	val_353
-356	val_356
-360	val_360
-362	val_362
-364	val_364
-365	val_365
-366	val_366
-367	val_367
-367	val_367
-367	val_367
-367	val_367
-368	val_368
-369	val_369
-369	val_369
-369	val_369
-369	val_369
-369	val_369
-369	val_369
-369	val_369
-369	val_369
-369	val_369
-37	val_37
-37	val_37
-37	val_37
-37	val_37
-373	val_373
-374	val_374
-375	val_375
-377	val_377
-378	val_378
-379	val_379
-382	val_382
-382	val_382
-382	val_382
-382	val_382
-384	val_384
-384	val_384
-384	val_384
-384	val_384
-384	val_384
-384	val_384
-384	val_384
-384	val_384
-384	val_384
-386	val_386
-389	val_389
-392	val_392
-393	val_393
-394	val_394
-395	val_395
-395	val_395
-395	val_395
-395	val_395
-396	val_396
-396	val_396
-396	val_396
-396	val_396
-396	val_396
-396	val_396
-396	val_396
-396	val_396
-396	val_396
-397	val_397
-397	val_397
-397	val_397
-397	val_397
-399	val_399
-399	val_399
-399	val_399
-399	val_399
-4	val_4
-400	val_400
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-401	val_401
-402	val_402
-403	val_403
-403	val_403
-403	val_403
-403	val_403
-403	val_403
-403	val_403
-403	val_403
-403	val_403
-403	val_403
-404	val_404
-404	val_404
-404	val_404
-404	val_404
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-406	val_406
-407	val_407
-409	val_409
-409	val_409
-409	val_409
-409	val_409
-409	val_409
-409	val_409
-409	val_409
-409	val_409
-409	val_409
-41	val_41
-411	val_411
-413	val_413
-413	val_413
-413	val_413
-413	val_413
-414	val_414
-414	val_414
-414	val_414
-414	val_414
-417	val_417
-417	val_417
-417	val_417
-417	val_417
-417	val_417
-417	val_417
-417	val_417
-417	val_417
-417	val_417
-418	val_418
-419	val_419
-42	val_42
-42	val_42
-42	val_42
-42	val_42
-421	val_421
-424	val_424
-424	val_424
-424	val_424
-424	val_424
-427	val_427
-429	val_429
-429	val_429
-429	val_429
-429	val_429
-43	val_43
-430	val_430
-430	val_430
-430	val_430
-430	val_430
-430	val_430
-430	val_430
-430	val_430
-430	val_430
-430	val_430
-431	val_431
-431	val_431
-431	val_431
-431	val_431
-431	val_431
-431	val_431
-431	val_431
-431	val_431
-431	val_431
-432	val_432
-435	val_435
-436	val_436
-437	val_437
-438	val_438
-438	val_438
-438	val_438
-438	val_438
-438	val_438
-438	val_438
-438	val_438
-438	val_438
-438	val_438
-439	val_439
-439	val_439
-439	val_439
-439	val_439
-44	val_44
-443	val_443
-444	val_444
-446	val_446
-448	val_448
-449	val_449
-452	val_452
-453	val_453
-454	val_454
-454	val_454
-454	val_454
-454	val_454
-454	val_454
-454	val_454
-454	val_454
-454	val_454
-454	val_454
-455	val_455
-457	val_457
-458	val_458
-458	val_458
-458	val_458
-458	val_458
-459	val_459
-459	val_459
-459	val_459
-459	val_459
-460	val_460
-462	val_462
-462	val_462
-462	val_462
-462	val_462
-463	val_463
-463	val_463
-463	val_463
-463	val_463
-466	val_466
-466	val_466
-466	val_466
-466	val_466
-466	val_466
-466	val_466
-466	val_466
-466	val_466
-466	val_466
-467	val_467
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-468	val_468
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-469	val_469
-47	val_47
-470	val_470
-472	val_472
-475	val_475
-477	val_477
-478	val_478
-478	val_478
-478	val_478
-478	val_478
-479	val_479
-480	val_480
-480	val_480
-480	val_480
-480	val_480
-480	val_480
-480	val_480
-480	val_480
-480	val_480
-480	val_480
-481	val_481
-482	val_482
-483	val_483
-484	val_484
-485	val_485
-487	val_487
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-489	val_489
-490	val_490
-491	val_491
-492	val_492
-492	val_492
-492	val_492
-492	val_492
-493	val_493
-494	val_494
-495	val_495
-496	val_496
-497	val_497
-498	val_498
-498	val_498
-498	val_498
-498	val_498
-498	val_498
-498	val_498
-498	val_498
-498	val_498
-498	val_498
-5	val_5
-5	val_5
-5	val_5
-5	val_5
-5	val_5
-5	val_5
-5	val_5
-5	val_5
-5	val_5
-51	val_51
-51	val_51
-51	val_51
-51	val_51
-53	val_53
-54	val_54
-57	val_57
-58	val_58
-58	val_58
-58	val_58
-58	val_58
-64	val_64
-65	val_65
-66	val_66
-67	val_67
-67	val_67
-67	val_67
-67	val_67
-69	val_69
-70	val_70
-70	val_70
-70	val_70
-70	val_70
-70	val_70
-70	val_70
-70	val_70
-70	val_70
-70	val_70
-72	val_72
-72	val_72
-72	val_72
-72	val_72
-74	val_74
-76	val_76
-76	val_76
-76	val_76
-76	val_76
-77	val_77
-78	val_78
-8	val_8
-80	val_80
-82	val_82
-83	val_83
-83	val_83
-83	val_83
-83	val_83
-84	val_84
-84	val_84
-84	val_84
-84	val_84
-85	val_85
-86	val_86
-87	val_87
-9	val_9
-90	val_90
-90	val_90
-90	val_90
-90	val_90
-90	val_90
-90	val_90
-90	val_90
-90	val_90
-90	val_90
-92	val_92
-95	val_95
-95	val_95
-95	val_95
-95	val_95
-96	val_96
-97	val_97
-97	val_97
-97	val_97
-97	val_97
-98	val_98
-98	val_98
-98	val_98
-98	val_98
-- 
1.7.0.4

