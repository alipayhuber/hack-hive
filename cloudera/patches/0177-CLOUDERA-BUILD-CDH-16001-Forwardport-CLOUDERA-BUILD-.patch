From 6fe4c826ed965385be8559907695c524198fe7ea Mon Sep 17 00:00:00 2001
From: Brock Noland <brock@cloudera.com>
Date: Wed, 4 Dec 2013 11:33:39 -0600
Subject: [PATCH 177/375] CLOUDERA-BUILD: CDH-16001 - Forwardport CLOUDERA-BUILD: CDH-13626 - Investigate MR1 Build for C5 Hive

---
 .../hcatalog/mapreduce/MultiOutputFormat.java      |    5 +-
 .../hive/hcatalog/mapreduce/MultiOutputFormat.java |    5 +-
 .../hcatalog/mapreduce/TestMultiOutputFormat.java  |    5 +-
 .../hcatalog/mapreduce/TestMultiOutputFormat.java  |    5 +-
 .../java/org/apache/hcatalog/pig/HCatLoader.java   |    2 +-
 .../java/org/apache/hcatalog/pig/HCatStorer.java   |    2 +-
 .../org/apache/hive/hcatalog/pig/HCatLoader.java   |    2 +-
 .../org/apache/hive/hcatalog/pig/HCatStorer.java   |    2 +-
 .../hive/ql/hooks/VerifyOverriddenConfigsHook.java |    2 +-
 .../apache/hadoop/hive/ql/exec/mr/JobDebugger.java |    5 +-
 .../results/clientpositive/overridden_confs.q.out  |    1 -
 .../apache/hadoop/hive/shims/Hadoop23Shims.java    |  119 ++++++++++++++++----
 12 files changed, 114 insertions(+), 41 deletions(-)

diff --git a/src/hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/MultiOutputFormat.java b/src/hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/MultiOutputFormat.java
index 879cd06..0892376 100644
--- a/src/hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/MultiOutputFormat.java
+++ b/src/hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/MultiOutputFormat.java
@@ -48,7 +48,6 @@
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskInputOutputContext;
 import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.hcatalog.common.HCatUtil;
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -156,7 +155,7 @@
     configsToMerge.put(ShimLoader.getHadoopShims().getHCatShim().getPropertyName(HadoopShims.HCatHadoopShims.PropertyName.CACHE_ARCHIVES), COMMA_DELIM);
     configsToMerge.put(ShimLoader.getHadoopShims().getHCatShim().getPropertyName(HadoopShims.HCatHadoopShims.PropertyName.CACHE_FILES), COMMA_DELIM);
     String fileSep;
-    if (HCatUtil.isHadoop23()) {
+    if ("yarn".equalsIgnoreCase(new Configuration().get("mapreduce.framework.name"))) {
       fileSep = ",";
     } else {
       fileSep = System.getProperty("path.separator");
@@ -358,7 +357,7 @@ private static JobConfigurer create(Job job) {
     public void addOutputFormat(String alias,
         Class<? extends OutputFormat> outputFormatClass,
         Class<?> keyClass, Class<?> valueClass) throws IOException {
-      Job copy = new Job(this.job.getConfiguration());
+      Job copy = Job.getInstance(this.job.getConfiguration());
       outputConfigs.put(alias, copy);
       copy.setOutputFormatClass(outputFormatClass);
       copy.setOutputKeyClass(keyClass);
diff --git a/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/MultiOutputFormat.java b/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/MultiOutputFormat.java
index ab0e1ab..1d248da 100644
--- a/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/MultiOutputFormat.java
+++ b/src/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/MultiOutputFormat.java
@@ -49,7 +49,6 @@
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskInputOutputContext;
 import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.hive.hcatalog.common.HCatUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -158,7 +157,7 @@
     configsToMerge.put(ShimLoader.getHadoopShims().getHCatShim().getPropertyName(
         HadoopShims.HCatHadoopShims.PropertyName.CACHE_FILES), COMMA_DELIM);
     String fileSep;
-    if (HCatUtil.isHadoop23()) {
+    if ("yarn".equalsIgnoreCase(new Configuration().get("mapreduce.framework.name"))) {
       fileSep = ",";
     } else {
       fileSep = System.getProperty("path.separator");
@@ -361,7 +360,7 @@ private static JobConfigurer create(Job job) {
     public void addOutputFormat(String alias,
         Class<? extends OutputFormat> outputFormatClass,
         Class<?> keyClass, Class<?> valueClass) throws IOException {
-      Job copy = new Job(this.job.getConfiguration());
+      Job copy = Job.getInstance(this.job.getConfiguration());
       outputConfigs.put(alias, copy);
       copy.setOutputFormatClass(outputFormatClass);
       copy.setOutputKeyClass(keyClass);
diff --git a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestMultiOutputFormat.java b/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestMultiOutputFormat.java
index f6ddb88..4b39f68 100644
--- a/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestMultiOutputFormat.java
+++ b/src/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestMultiOutputFormat.java
@@ -67,7 +67,7 @@
 
   private static final Logger LOG = LoggerFactory.getLogger(TestMultiOutputFormat.class);
   private static File workDir;
-  private static Configuration mrConf = null;
+  private Configuration mrConf = null;
   private static FileSystem fs = null;
   private static MiniMRCluster mrCluster = null;
 
@@ -84,7 +84,6 @@ public static void setup() throws IOException {
     // to use MiniMRCluster. MAPREDUCE-2350
     mrCluster = new MiniMRCluster(1, fs.getUri().toString(), 1, null, null,
       new JobConf(conf));
-    mrConf = mrCluster.createJobConf();
   }
 
   private static void createWorkDir() throws IOException {
@@ -110,6 +109,7 @@ public static void tearDown() throws IOException {
    */
   @Test
   public void testMultiOutputFormatWithoutReduce() throws Throwable {
+    mrConf = mrCluster.createJobConf();
     Job job = new Job(mrConf, "MultiOutNoReduce");
     job.setMapperClass(MultiOutWordIndexMapper.class);
     job.setJarByClass(this.getClass());
@@ -177,6 +177,7 @@ public void testMultiOutputFormatWithoutReduce() throws Throwable {
    */
   @Test
   public void testMultiOutputFormatWithReduce() throws Throwable {
+    mrConf = mrCluster.createJobConf();
     Job job = new Job(mrConf, "MultiOutWithReduce");
 
     job.setMapperClass(WordCountMapper.class);
diff --git a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestMultiOutputFormat.java b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestMultiOutputFormat.java
index d70e5bf..4c47d6a 100644
--- a/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestMultiOutputFormat.java
+++ b/src/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestMultiOutputFormat.java
@@ -64,7 +64,7 @@
 
   private static final Logger LOG = LoggerFactory.getLogger(TestMultiOutputFormat.class);
   private static File workDir;
-  private static Configuration mrConf = null;
+  private Configuration mrConf = null;
   private static FileSystem fs = null;
   private static MiniMRCluster mrCluster = null;
 
@@ -81,7 +81,6 @@ public static void setup() throws IOException {
     // to use MiniMRCluster. MAPREDUCE-2350
     mrCluster = new MiniMRCluster(1, fs.getUri().toString(), 1, null, null,
       new JobConf(conf));
-    mrConf = mrCluster.createJobConf();
   }
 
   private static void createWorkDir() throws IOException {
@@ -107,6 +106,7 @@ public static void tearDown() throws IOException {
    */
   @Test
   public void testMultiOutputFormatWithoutReduce() throws Throwable {
+    mrConf = mrCluster.createJobConf();
     Job job = new Job(mrConf, "MultiOutNoReduce");
     job.setMapperClass(MultiOutWordIndexMapper.class);
     job.setJarByClass(this.getClass());
@@ -174,6 +174,7 @@ public void testMultiOutputFormatWithoutReduce() throws Throwable {
    */
   @Test
   public void testMultiOutputFormatWithReduce() throws Throwable {
+    mrConf = mrCluster.createJobConf();
     Job job = new Job(mrConf, "MultiOutWithReduce");
 
     job.setMapperClass(WordCountMapper.class);
diff --git a/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatLoader.java b/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatLoader.java
index a32149c..ee27816 100644
--- a/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatLoader.java
+++ b/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatLoader.java
@@ -116,7 +116,7 @@ public void setLocation(String location, Job job) throws IOException {
         job.getCredentials().addAll(crd);
       }
     } else {
-      Job clone = new Job(job.getConfiguration());
+      Job clone = Job.getInstance(job.getConfiguration());
       HCatInputFormat.setInput(job, dbName, tableName).setFilter(getPartitionFilterString());
 
       // We will store all the new /changed properties in the job in the
diff --git a/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatStorer.java b/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatStorer.java
index 95583ba..84a1374 100644
--- a/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatStorer.java
+++ b/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatStorer.java
@@ -97,7 +97,7 @@ public void setStoreLocation(String location, Job job) throws IOException {
         job.getCredentials().addAll(crd);
       }
     } else {
-      Job clone = new Job(job.getConfiguration());
+      Job clone = Job.getInstance(job.getConfiguration());
       OutputJobInfo outputJobInfo;
       if (userStr.length == 2) {
         outputJobInfo = OutputJobInfo.create(userStr[0], userStr[1], partitions);
diff --git a/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java b/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java
index b03337d..9702415 100644
--- a/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java
+++ b/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java
@@ -115,7 +115,7 @@ public void setLocation(String location, Job job) throws IOException {
         job.getCredentials().addAll(crd);
       }
     } else {
-      Job clone = new Job(job.getConfiguration());
+      Job clone = Job.getInstance(job.getConfiguration());
       HCatInputFormat.setInput(job, dbName, tableName, getPartitionFilterString());
 
       // We will store all the new /changed properties in the job in the
diff --git a/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatStorer.java b/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatStorer.java
index 062f332..185b5dc 100644
--- a/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatStorer.java
+++ b/src/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatStorer.java
@@ -96,7 +96,7 @@ public void setStoreLocation(String location, Job job) throws IOException {
         job.getCredentials().addAll(crd);
       }
     } else {
-      Job clone = new Job(job.getConfiguration());
+      Job clone = Job.getInstance(job.getConfiguration());
       OutputJobInfo outputJobInfo;
       if (userStr.length == 2) {
         outputJobInfo = OutputJobInfo.create(userStr[0], userStr[1], partitions);
diff --git a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java
index 41c178a..2955d28 100644
--- a/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java
+++ b/src/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOverriddenConfigsHook.java
@@ -38,7 +38,7 @@
   // a config variable not in the default List of config variables, and a config variable in the
   // default list of conifg variables, but which has not been overridden
   private static String[] keysArray =
-    {"mapred.job.tracker", "hive.exec.post.hooks", "hive.config.doesnt.exit",
+    {"hive.exec.post.hooks", "hive.config.doesnt.exit",
      "hive.exec.mode.local.auto"};
   private static List<String> keysList = Arrays.asList(keysArray);
 
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/JobDebugger.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/JobDebugger.java
index 7b77944..5afd2f4 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/JobDebugger.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/JobDebugger.java
@@ -116,8 +116,9 @@ public JobDebugger(JobConf conf, RunningJob rj, LogHelper console,
   public void run() {
     try {
       showJobFailDebugInfo();
-    } catch (IOException e) {
-      console.printError(e.getMessage());
+    } catch (Throwable t) {
+      t.printStackTrace(); // best we can do at this point
+      console.printError("Could not obtain debuging information: " + t.getClass().getSimpleName() + ": " + t.getMessage());
     }
   }
 
diff --git a/src/ql/src/test/results/clientpositive/overridden_confs.q.out b/src/ql/src/test/results/clientpositive/overridden_confs.q.out
index b621feb..f97d1ad 100644
--- a/src/ql/src/test/results/clientpositive/overridden_confs.q.out
+++ b/src/ql/src/test/results/clientpositive/overridden_confs.q.out
@@ -2,7 +2,6 @@ PREHOOK: query: select count(*) from src
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
-Key: mapred.job.tracker, Value: local
 Key: hive.exec.post.hooks, Value: org.apache.hadoop.hive.ql.hooks.VerifyOverriddenConfigsHook
 Key: hive.config.doesnt.exit, Value: abc
 500
diff --git a/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java b/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
index dc5facd..1aabd8d 100644
--- a/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
+++ b/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
@@ -20,6 +20,8 @@
 import java.io.IOException;
 import java.lang.Integer;
 import java.net.InetSocketAddress;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
 import java.net.MalformedURLException;
 import java.net.URL;
 import java.util.Map;
@@ -43,17 +45,14 @@
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.JobID;
-import org.apache.hadoop.mapreduce.MRJobConfig;
 import org.apache.hadoop.mapreduce.OutputFormat;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.mapreduce.TaskID;
 import org.apache.hadoop.mapreduce.TaskType;
 import org.apache.hadoop.mapreduce.task.JobContextImpl;
 import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
-import org.apache.hadoop.mapreduce.util.HostUtil;
 import org.apache.hadoop.net.NetUtils;
 import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.security.authentication.util.KerberosName;
 
@@ -66,17 +65,38 @@
   public String getTaskAttemptLogUrl(JobConf conf,
     String taskTrackerHttpAddress, String taskAttemptId)
     throws MalformedURLException {
-    if (conf.get("mapreduce.framework.name") != null
-      && conf.get("mapreduce.framework.name").equals("yarn")) {
+    if (isMR2(conf)) {
       // if the cluster is running in MR2 mode, return null
       LOG.warn("Can't fetch tasklog: TaskLogServlet is not supported in MR2 mode.");
       return null;
     } else {
-      // if the cluster is running in MR1 mode, using HostUtil to construct TaskLogURL
-      URL taskTrackerHttpURL = new URL(taskTrackerHttpAddress);
-      return HostUtil.getTaskLogUrl(taskTrackerHttpURL.getHost(),
-        Integer.toString(taskTrackerHttpURL.getPort()),
-        taskAttemptId);
+      // MR2 doesn't have TaskLogServlet class, so need to
+      String taskLogURL = null;
+      try {
+        Class<?> taskLogClass= Class.forName("TaskLogServlet");
+        Method taskLogMethod  = taskLogClass.getDeclaredMethod("getTaskLogUrl", String.class, String.class, String.class);
+        URL taskTrackerHttpURL = new URL(taskTrackerHttpAddress);
+        taskLogURL = (String)taskLogMethod.invoke(null, taskTrackerHttpURL.getHost(),
+            Integer.toString(taskTrackerHttpURL.getPort()), taskAttemptId);
+      } catch (IllegalArgumentException e) {
+        LOG.error("Error trying to get task log URL", e);
+        throw new MalformedURLException("Could not execute getTaskLogUrl: " + e.getCause());
+      } catch (IllegalAccessException e) {
+        LOG.error("Error trying to get task log URL", e);
+        throw new MalformedURLException("Could not execute getTaskLogUrl: " + e.getCause());
+      } catch (InvocationTargetException e) {
+        LOG.error("Error trying to get task log URL", e);
+        throw new MalformedURLException("Could not execute getTaskLogUrl: " + e.getCause());
+      } catch (SecurityException e) {
+        LOG.error("Error trying to get task log URL", e);
+        throw new MalformedURLException("Could not execute getTaskLogUrl: " + e.getCause());
+      } catch (NoSuchMethodException e) {
+        LOG.error("Error trying to get task log URL", e);
+        throw new MalformedURLException("Method getTaskLogUrl not found: " + e.getCause());
+      } catch (ClassNotFoundException e) {
+        LOG.warn("Can't fetch tasklog: TaskLogServlet is not supported in MR2 mode.");
+      }
+      return taskLogURL;
     }
   }
 
@@ -115,24 +135,39 @@ public TaskAttemptID newTaskAttemptID(JobID jobId, boolean isMap, int taskId, in
 
   @Override
   public boolean isLocalMode(Configuration conf) {
-    return "local".equals(conf.get("mapreduce.framework.name"));
+    if (isMR2(conf)) {
+      return false;
+    }
+    return "local".equals(conf.get("mapreduce.framework.name")) ||
+      "local".equals(conf.get("mapred.job.tracker"));
   }
 
   @Override
   public String getJobLauncherRpcAddress(Configuration conf) {
-    return conf.get("yarn.resourcemanager.address");
+    if (isMR2(conf)) {
+      return conf.get("yarn.resourcemanager.address");
+    } else {
+      return conf.get("mapred.job.tracker");
+    }
   }
 
   @Override
   public void setJobLauncherRpcAddress(Configuration conf, String val) {
     if (val.equals("local")) {
       // LocalClientProtocolProvider expects both parameters to be 'local'.
-      conf.set("mapreduce.framework.name", val);
-      conf.set("mapreduce.jobtracker.address", val);
+      if (isMR2(conf)) {
+        conf.set("mapreduce.framework.name", val);
+        conf.set("mapreduce.jobtracker.address", val);
+      } else {
+        conf.set("mapred.job.tracker", val);
+      }
     }
     else {
-      conf.set("mapreduce.framework.name", "yarn");
-      conf.set("yarn.resourcemanager.address", val);
+      if (isMR2(conf)) {
+        conf.set("yarn.resourcemanager.address", val);
+      } else {
+        conf.set("mapred.job.tracker", val);
+      }
     }
   }
 
@@ -143,7 +178,11 @@ public String getKerberosShortName(String kerberosLongName) throws IOException {
 
   @Override
   public String getJobLauncherHttpAddress(Configuration conf) {
-    return conf.get("yarn.resourcemanager.webapp.address");
+    if (isMR2(conf)) {
+      return conf.get("yarn.resourcemanager.webapp.address");
+    } else {
+      return conf.get("mapred.job.tracker.http.address");
+    }
   }
 
   @Override
@@ -164,7 +203,18 @@ public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration co
 
   @Override
   public void setTotalOrderPartitionFile(JobConf jobConf, Path partitionFile){
-    TotalOrderPartitioner.setPartitionFile(jobConf, partitionFile);
+    try {
+      Class<?> clazz = Class.forName("org.apache.hadoop.mapred.lib.TotalOrderPartitioner");
+      try {
+        java.lang.reflect.Method method = clazz.getMethod("setPartitionFile", Configuration.class, Path.class);
+        method.invoke(null, jobConf, partitionFile);
+      } catch(NoSuchMethodException nsme) {
+        java.lang.reflect.Method method = clazz.getMethod("setPartitionFile", JobConf.class, Path.class);
+        method.invoke(null, jobConf, partitionFile);
+      }
+    } catch(Exception e) {
+      throw new AssertionError("Unable to find TotalOrderPartitioner.setPartitionFile", e);
+    }
   }
 
   /**
@@ -215,6 +265,8 @@ public void shutdown() throws IOException {
     public void setupConfiguration(Configuration conf) {
       JobConf jConf = mr.createJobConf();
       for (Map.Entry<String, String> pair: jConf) {
+        // TODO figure out why this was wrapped in
+        // if(!"mapred.reduce.tasks".equalsIgnoreCase(pair.getKey()))
         conf.set(pair.getKey(), pair.getValue());
       }
     }
@@ -304,8 +356,15 @@ public JobContext createJobContext(Configuration conf,
     @Override
     public org.apache.hadoop.mapred.JobContext createJobContext(org.apache.hadoop.mapred.JobConf conf,
                                                                 org.apache.hadoop.mapreduce.JobID jobId, Progressable progressable) {
-      return new org.apache.hadoop.mapred.JobContextImpl(
-              new JobConf(conf), jobId, (org.apache.hadoop.mapred.Reporter) progressable);
+      try {
+        java.lang.reflect.Constructor construct = org.apache.hadoop.mapred.JobContextImpl.class.getDeclaredConstructor(
+          org.apache.hadoop.mapred.JobConf.class, org.apache.hadoop.mapreduce.JobID.class, Progressable.class);
+        construct.setAccessible(true);
+        return (org.apache.hadoop.mapred.JobContext) construct.newInstance(
+                new JobConf(conf), jobId, progressable);
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
     }
 
     @Override
@@ -327,13 +386,23 @@ public InetSocketAddress getResourceManagerAddress(Configuration conf) {
 
     @Override
     public String getPropertyName(PropertyName name) {
+      boolean mr2 = isMR2(new Configuration());
       switch (name) {
         case CACHE_ARCHIVES:
-          return MRJobConfig.CACHE_ARCHIVES;
+          if(mr2) {
+            return "mapreduce.job.cache.archives";
+          }
+          return "mapred.cache.archives";
         case CACHE_FILES:
-          return MRJobConfig.CACHE_FILES;
+          if(mr2) {
+            return "mapreduce.job.cache.files";
+          }
+          return "mapred.cache.files";
         case CACHE_SYMLINK:
-          return MRJobConfig.CACHE_SYMLINK;
+          if(mr2) {
+            return "mapreduce.job.cache.symlink.create";
+          }
+          return "mapred.create.symlink";
       }
 
       return "";
@@ -351,6 +420,10 @@ public WebHCatJTShim getWebHCatShim(Configuration conf, UserGroupInformation ugi
     return new WebHCatJTShim23(conf, ugi);//this has state, so can't be cached
   }
 
+  private boolean isMR2(Configuration conf) {
+    return "yarn".equalsIgnoreCase(conf.get("mapreduce.framework.name"));
+  }
+
   class ProxyFileSystem23 extends ProxyFileSystem {
     public ProxyFileSystem23(FileSystem fs) {
       super(fs);
-- 
1.7.0.4

