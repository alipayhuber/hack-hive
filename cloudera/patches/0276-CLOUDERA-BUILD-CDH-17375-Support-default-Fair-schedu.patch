From 913559ec6ebf798aead05f32dc823462f650c5e0 Mon Sep 17 00:00:00 2001
From: Prasad Mujumdar <prasadm@cloudera.com>
Date: Wed, 12 Feb 2014 20:21:53 -0800
Subject: [PATCH 276/375] CLOUDERA-BUILD: CDH-17375: Support default Fair scheduler queue mapping for Hive

---
 .../java/org/apache/hadoop/hive/conf/HiveConf.java |    1 +
 itests/hive-unit/pom.xml                           |   50 +++++++++
 .../org/apache/hive/jdbc/TestSchedulerQueue.java   |  105 ++++++++++++++++++++
 .../java/org/apache/hive/jdbc/miniHS2/MiniHS2.java |   49 +++++++++-
 pom.xml                                            |   14 +++
 .../hive/service/cli/session/HiveSessionImpl.java  |    6 +-
 .../cli/session/HiveSessionImplwithUGI.java        |    5 +-
 .../hive/service/cli/session/SessionManager.java   |   15 +++-
 shims/0.23/pom.xml                                 |    5 +
 .../apache/hadoop/hive/shims/Hadoop23Shims.java    |   28 +++++
 .../org/apache/hadoop/hive/shims/HadoopShims.java  |    5 +
 11 files changed, 276 insertions(+), 7 deletions(-)
 create mode 100644 itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestSchedulerQueue.java

diff --git a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index ef20bc4..01f4a59 100644
--- a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -780,6 +780,7 @@
     HIVE_SERVER2_SSL_KEYSTORE_PATH("hive.server2.keystore.path", ""),
     HIVE_SERVER2_SSL_KEYSTORE_PASSWORD("hive.server2.keystore.password", ""),
     HIVE_SERVER2_ALLOW_USER_SUBSTITUTION("hive.server2.allow.user.substitution", true),
+    HIVE_SERVER2_MAP_FAIR_SCHEDULER_QUEUE("hive.server2.map.fair.scheduler.queue", true),
 
     HIVE_SERVER2_IN_MEM_LOGGING("hive.server2.in.mem.logging", true),
     HIVE_SERVER2_IN_MEM_LOG_SIZE("hive.server2.in.mem.log.size", 128 * 1024),
diff --git a/src/itests/hive-unit/pom.xml b/src/itests/hive-unit/pom.xml
index 8db6278..c7d550a 100644
--- a/src/itests/hive-unit/pom.xml
+++ b/src/itests/hive-unit/pom.xml
@@ -133,6 +133,19 @@
       <dependencies>
         <dependency>
           <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-common</artifactId>
+          <version>${hadoop-23.version}</version>
+          <scope>test</scope>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-common</artifactId>
+          <version>${hadoop-23.version}</version>
+          <scope>test</scope>
+          <classifier>tests</classifier>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
           <artifactId>hadoop-core</artifactId>
           <version>${hadoop-20S.version}</version>
           <scope>test</scope>
@@ -143,6 +156,18 @@
           <version>${hadoop-20S.version}</version>
           <scope>test</scope>
         </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-tools</artifactId>
+          <version>${hadoop-20S.version}</version>
+          <scope>test</scope>
+        </dependency>
+        <dependency>
+          <groupId>com.sun.jersey</groupId>
+          <artifactId>jersey-servlet</artifactId>
+          <version>${jersey.version}</version>
+          <scope>test</scope>
+        </dependency>
       </dependencies>
     </profile>
    <profile>
@@ -175,11 +200,36 @@
           <scope>test</scope>
         </dependency>
         <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-yarn-server-tests</artifactId>
+          <version>${hadoop-23.version}</version>
+          <classifier>tests</classifier>
+          <scope>test</scope>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-yarn-server-tests</artifactId>
+          <version>${hadoop-23.version}</version>
+          <classifier>tests</classifier>
+          <scope>test</scope>
+        </dependency>
+        <dependency>
+          <groupId>com.sun.jersey</groupId>
+          <artifactId>jersey-servlet</artifactId>
+          <version>${jersey.version}</version>
+          <scope>test</scope>
+        </dependency>
+        <dependency>
           <groupId>org.apache.hbase</groupId>
           <artifactId>hbase-server</artifactId>
           <version>${hbase.hadoop2.version}</version>
           <scope>test</scope>
         </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-minicluster</artifactId>
+          <scope>test</scope>
+        </dependency>
       </dependencies>
     </profile>
   </profiles>
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestSchedulerQueue.java b/src/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestSchedulerQueue.java
new file mode 100644
index 0000000..ed59aed
--- /dev/null
+++ b/src/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestSchedulerQueue.java
@@ -0,0 +1,105 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.jdbc;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.ResultSet;
+import java.sql.Statement;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hive.jdbc.miniHS2.MiniHS2;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestSchedulerQueue {
+
+  private MiniHS2 miniHS2 = null;
+  private static HiveConf conf = new HiveConf();
+  private Connection hs2Conn = null;
+  private String dataFileDir = conf.get("test.data.files");
+
+  @BeforeClass
+  public static void beforeTest() throws Exception {
+    Class.forName(MiniHS2.getJdbcDriverName());
+  }
+
+  @Before
+  public void setUp() throws Exception {
+    DriverManager.setLoginTimeout(0);
+    if (!System.getProperty("test.data.files", "").isEmpty()) {
+      dataFileDir = System.getProperty("test.data.files");
+    }
+    dataFileDir = dataFileDir.replace('\\', '/').replace("c:", "");
+    conf.set("mapred.jobtracker.taskScheduler", "org.apache.hadoop.mapred.FairScheduler");
+    conf.setBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS, false);
+    miniHS2 = new MiniHS2(conf, true);
+    miniHS2.start();
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    if (hs2Conn != null) {
+      hs2Conn.close();
+    }
+    if (miniHS2 != null && miniHS2.isStarted()) {
+      miniHS2.stop();
+    }
+  }
+
+  /***
+   * Test SSL default queue mapping
+   * @throws Exception
+   */
+  @Test
+  public void testFairSchedulerQueueMapping() throws Exception {
+    hs2Conn = DriverManager.getConnection(miniHS2.getJdbcURL(), "user1", "bar");
+    verifyProperty("mapreduce.framework.name", "yarn");
+    verifyProperty("mapred.jobtracker.taskScheduler", "org.apache.hadoop.mapred.FairScheduler");
+    verifyProperty("mapred.job.queue.name", "root.user1");
+  }
+
+  @Test
+  public void testFairSchedulerQueueMappingDisabled() throws Exception {
+    miniHS2.setConfProperty(HiveConf.ConfVars.HIVE_SERVER2_MAP_FAIR_SCHEDULER_QUEUE.varname,
+        "false");
+    hs2Conn = DriverManager.getConnection(miniHS2.getJdbcURL(), "user1", "bar");
+    verifyProperty("mapred.job.queue.name", "default");
+    miniHS2.setConfProperty(HiveConf.ConfVars.HIVE_SERVER2_MAP_FAIR_SCHEDULER_QUEUE.varname,
+        "true");
+  }
+
+  /**
+   * Verify if the given property contains the expected value
+   * @param propertyName
+   * @param expectedValue
+   * @throws Exception
+   */
+  private void verifyProperty(String propertyName, String expectedValue) throws Exception {
+    Statement stmt = hs2Conn .createStatement();
+    ResultSet res = stmt.executeQuery("set " + propertyName);
+    assertTrue(res.next());
+    String results[] = res.getString(1).split("=");
+    assertEquals(expectedValue, results[1]);
+  }
+
+}
diff --git a/src/itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java b/src/itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java
index a65e678..0a4a5d1 100644
--- a/src/itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java
+++ b/src/itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java
@@ -20,14 +20,20 @@
 
 import java.io.File;
 import java.io.IOException;
+import java.util.Map;
 import java.util.concurrent.TimeoutException;
 import java.util.concurrent.atomic.AtomicLong;
 
 import org.apache.commons.io.FileUtils;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.HiveMetaStore;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.shims.HadoopShims.MiniDFSShim;
+import org.apache.hadoop.hive.shims.HadoopShims.MiniMrShim;
+import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hive.service.Service;
 import org.apache.hive.service.cli.CLIServiceClient;
 import org.apache.hive.service.cli.SessionHandle;
@@ -40,21 +46,52 @@
 public class MiniHS2 extends AbstarctHiveService {
   private static final String driverName = "org.apache.hive.jdbc.HiveDriver";
   private HiveServer2 hiveServer2 = null;
+  private MiniMrShim mr;
+  private MiniDFSShim dfs;
   private final File baseDir;
+  private final Path baseDfsDir;
   private static final AtomicLong hs2Counter = new AtomicLong();
 
   public MiniHS2(HiveConf hiveConf) throws IOException {
+    this(hiveConf, false);
+  }
+
+  public MiniHS2(HiveConf hiveConf, boolean useMiniMR) throws IOException {
     super(hiveConf, "localhost", MetaStoreUtils.findFreePort());
     baseDir =  Files.createTempDir();
-    setWareHouseDir("file://" + baseDir.getPath() + File.separator + "warehouse");
+    FileSystem fs;
+    if (useMiniMR) {
+      dfs = ShimLoader.getHadoopShims().getMiniDfs(hiveConf, 4, true, null);
+      fs = dfs.getFileSystem();
+      mr = ShimLoader.getHadoopShims().getMiniMrCluster(hiveConf, 4,
+          fs.getUri().toString(), 1);
+      // store the config in system properties
+      mr.setupConfiguration(getHiveConf());
+      baseDfsDir =  new Path(new Path(fs.getUri()), "/base");
+    } else {
+      fs = FileSystem.getLocal(hiveConf);
+      baseDfsDir = new Path("file://"+ baseDir.getPath());
+    }
+
+    fs.mkdirs(baseDfsDir);
+    Path wareHouseDir = new Path(baseDfsDir, "warehouse");
+    fs.mkdirs(wareHouseDir);
+    setWareHouseDir(wareHouseDir.toString());
     String metaStoreURL =  "jdbc:derby:" + baseDir.getAbsolutePath() + File.separator + "test_metastore-" +
         hs2Counter.incrementAndGet() + ";create=true";
 
     System.setProperty(HiveConf.ConfVars.METASTORECONNECTURLKEY.varname, metaStoreURL);
     hiveConf.setVar(HiveConf.ConfVars.METASTORECONNECTURLKEY, metaStoreURL);
+    setPort(MetaStoreUtils.findFreePort());
     hiveConf.setVar(ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST, getHost());
     hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_PORT, getPort());
     HiveMetaStore.HMSHandler.resetDefaultDBFlag();
+
+    Path scratchDir = new Path(baseDfsDir, "scratch");
+    fs.mkdirs(scratchDir);
+    System.setProperty(HiveConf.ConfVars.SCRATCHDIR.varname, scratchDir.toString());
+    System.setProperty(HiveConf.ConfVars.LOCALSCRATCHDIR.varname,
+        baseDir.getPath() + File.separator + "scratch");
   }
 
   public void start() throws Exception {
@@ -69,6 +106,16 @@ public void stop() {
     verifyStarted();
     hiveServer2.stop();
     setStarted(false);
+    try {
+      if (mr != null) {
+        mr.shutdown();
+      }
+      if (dfs != null) {
+        dfs.shutdown();
+      }
+    } catch (IOException e) {
+      // Ignore errors cleaning up miniMR
+    }
     FileUtils.deleteQuietly(baseDir);
   }
 
diff --git a/src/pom.xml b/src/pom.xml
index 1ba194b..76f886f 100644
--- a/src/pom.xml
+++ b/src/pom.xml
@@ -727,6 +727,7 @@
             <exclude>**/ql/exec/vector/udf/generic/*.java</exclude>
             <exclude>**/TestHiveServer2Concurrency.java</exclude>
             <exclude>**/TestHiveMetaStore.java</exclude>
+            <exclude>${excluded.tests}</exclude>
           </excludes>
           <redirectTestOutputToFile>true</redirectTestOutputToFile>
           <reuseForks>false</reuseForks>
@@ -886,6 +887,9 @@
       <activation>
         <activeByDefault>false</activeByDefault>
       </activation>
+      <properties>
+        <excluded.tests>**/TestSchedulerQueue.java</excluded.tests>
+      </properties>
       <dependencyManagement>
         <dependencies>
           <dependency>
@@ -956,6 +960,16 @@
             <version>${hadoop-23.version}</version>
           </dependency>
           <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-yarn-server-resourcemanager</artifactId>
+            <version>${hadoop-23.version}</version>
+          </dependency>
+          <dependency>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-minicluster</artifactId>
+            <version>${hadoop-23.version}</version>
+          </dependency>
+          <dependency>
             <groupId>org.apache.hbase</groupId>
             <artifactId>hbase-common</artifactId>
             <version>${hbase.hadoop2.version}</version>
diff --git a/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
index 6d58f68..34c20ad 100644
--- a/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
+++ b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
@@ -67,7 +67,7 @@
   private String username;
   private final String password;
   private final Map<String, String> sessionConf = new HashMap<String, String>();
-  private final HiveConf hiveConf = new HiveConf();
+  private final HiveConf hiveConf;
   private final SessionState sessionState;
   private String ipAddress;
 
@@ -82,10 +82,12 @@
   private IMetaStoreClient metastoreClient = null;
   private final Set<OperationHandle> opHandleSet = new HashSet<OperationHandle>();
 
-  public HiveSessionImpl(String username, String password, Map<String, String> sessionConf, String ipAddress) {
+  public HiveSessionImpl(HiveConf serverConf, String username, String password,
+      Map<String, String> sessionConf, String ipAddress) {
     this.username = username;
     this.password = password;
     this.ipAddress = ipAddress;
+    this.hiveConf = new HiveConf(serverConf, HiveConf.class);
 
     if (sessionConf != null) {
       for (Map.Entry<String, String> entry : sessionConf.entrySet()) {
diff --git a/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java
index e1b8ec9..1f1e519 100644
--- a/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java
+++ b/src/service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java
@@ -21,6 +21,7 @@
 import java.io.IOException;
 import java.util.Map;
 
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.shims.ShimLoader;
@@ -41,9 +42,9 @@
   private Hive sessionHive = null;
   private HiveSession proxySession = null;
 
-  public HiveSessionImplwithUGI(String username, String password, Map<String, String> sessionConf,
+  public HiveSessionImplwithUGI(HiveConf serverConf, String username, String password, Map<String, String> sessionConf,
       String ipAddress, String delegationToken) throws HiveSQLException {
-    super(username, password, sessionConf, ipAddress);
+    super(serverConf, username, password, sessionConf, ipAddress);
     setSessionUGI(username);
     setDelegationToken(delegationToken);
   }
diff --git a/src/service/src/java/org/apache/hive/service/cli/session/SessionManager.java b/src/service/src/java/org/apache/hive/service/cli/session/SessionManager.java
index b3fc07d..04c26f1 100644
--- a/src/service/src/java/org/apache/hive/service/cli/session/SessionManager.java
+++ b/src/service/src/java/org/apache/hive/service/cli/session/SessionManager.java
@@ -18,6 +18,7 @@
 
 package org.apache.hive.service.cli.session;
 
+import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
@@ -115,12 +116,12 @@ public SessionHandle openSession(String username, String password, Map<String, S
       username = threadLocalUserName.get();
     }
     if (withImpersonation) {
-      HiveSessionImplwithUGI hiveSessionUgi = new HiveSessionImplwithUGI(username, password, sessionConf,
+      HiveSessionImplwithUGI hiveSessionUgi = new HiveSessionImplwithUGI(hiveConf, username, password, sessionConf,
           threadLocalIpAddress.get(), delegationToken);
       session = HiveSessionProxy.getProxy(hiveSessionUgi, hiveSessionUgi.getSessionUgi());
       hiveSessionUgi.setProxySession(session);
     } else {
-      session = new HiveSessionImpl(username, password, sessionConf, threadLocalIpAddress.get());
+      session = new HiveSessionImpl(hiveConf, username, password, sessionConf, threadLocalIpAddress.get());
     }
     session.setSessionManager(this);
     session.setOperationManager(operationManager);
@@ -129,6 +130,16 @@ public SessionHandle openSession(String username, String password, Map<String, S
       handleToSession.put(session.getSessionHandle(), session);
     }
     try {
+      // reload the scheduler queue if possible
+      if (!withImpersonation &&
+          session.getHiveConf().getBoolVar(ConfVars.HIVE_SERVER2_MAP_FAIR_SCHEDULER_QUEUE)) {
+        ShimLoader.getHadoopShims().
+          refreshDefaultQueue(session.getHiveConf(), session.getUserName());
+      }
+    } catch (IOException e1) {
+      LOG.warn("Error setting scheduler queue ", e1);
+    }
+    try {
       executeSessionHooks(session);
     } catch (Exception e) {
       throw new HiveSQLException("Failed to execute session hooks", e);
diff --git a/src/shims/0.23/pom.xml b/src/shims/0.23/pom.xml
index 9dd3fe4..1276558 100644
--- a/src/shims/0.23/pom.xml
+++ b/src/shims/0.23/pom.xml
@@ -65,6 +65,11 @@
     </dependency>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-yarn-server-resourcemanager</artifactId>
+      <version>${hadoop-23.version}</version>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-hdfs</artifactId>
       <version>${hadoop-23.version}</version>
       <type>test-jar</type>
diff --git a/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java b/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
index cc6e9c6..fe0f7b1 100644
--- a/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
+++ b/src/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
@@ -55,6 +55,9 @@
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.security.authentication.util.KerberosName;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration;
+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy;
 
 /**
  * Implemention of shims against Hadoop 0.23.0.
@@ -228,6 +231,31 @@ public void setTotalOrderPartitionFile(JobConf jobConf, Path partitionFile){
   }
 
   /**
+   * Load the fair scheduler queue for given user if available
+   */
+  @Override
+  public void refreshDefaultQueue(Configuration conf, String userName) throws IOException {
+    String requestedQueue = YarnConfiguration.DEFAULT_QUEUE_NAME;
+    if (isMR2(conf) && StringUtils.isNotBlank(userName) && isFairScheduler(conf)) {
+      AllocationConfiguration allocConf = new AllocationConfiguration(conf);
+      QueuePlacementPolicy queuePolicy = allocConf.getPlacementPolicy();
+      if (queuePolicy != null) {
+        requestedQueue = queuePolicy.assignAppToQueue(requestedQueue, userName);
+        LOG.debug("Setting queue name to " + requestedQueue + " for user " + userName);
+        if (StringUtils.isNotBlank(requestedQueue)) {
+          conf.set("mapred.job.queue.name", requestedQueue);
+        }
+      }
+    }
+  }
+
+  // verify if the configured scheduler is fair scheduler
+  private boolean isFairScheduler (Configuration conf) {
+    return "org.apache.hadoop.mapred.FairScheduler".
+          equalsIgnoreCase(conf.get("mapred.jobtracker.taskScheduler", ""));
+  }
+
+  /**
    * Returns a shim to wrap MiniMrCluster
    */
   public MiniMrShim getMiniMrCluster(Configuration conf, int numberOfTaskTrackers,
diff --git a/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
index b5852ca..a08ad70 100644
--- a/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
+++ b/src/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
@@ -456,6 +456,11 @@ public void authorizeProxyAccess(String proxyUser, UserGroupInformation realUser
       String ipAddress, Configuration conf) throws IOException;
 
   /**
+   * Reset the default scheduler queue if applicable
+   */
+  public void refreshDefaultQueue(Configuration conf, String userName) throws IOException;
+
+  /**
    * The method sets to set the partition file has a different signature between
    * hadoop versions.
    * @param jobConf
-- 
1.7.0.4

